@misc{10EssentialPublic,
  title = {10 {{Essential Public Health Services}}},
  urldate = {2022-10-12},
  howpublished = {https://www.apha.org/What-is-Public-Health/10-Essential-Public-Health-Services},
  file = {/Users/nseewald/Zotero/storage/TTV4XE3Q/10-Essential-Public-Health-Services.html}
}

@article{abadieComparativePoliticsSynthetic2015,
  title = {Comparative {{Politics}} and the {{Synthetic Control Method}}},
  author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
  year = {2015},
  journal = {American Journal of Political Science},
  volume = {59},
  number = {2},
  pages = {495--510},
  issn = {1540-5907},
  doi = {10.1111/ajps.12116},
  urldate = {2023-10-26},
  abstract = {In recent years, a widespread consensus has emerged about the necessity of establishing bridges between quantitative and qualitative approaches to empirical research in political science. In this article, we discuss the use of the synthetic control method as a way to bridge the quantitative/qualitative divide in comparative politics. The synthetic control method provides a systematic way to choose comparison units in comparative case studies. This systematization opens the door to precise quantitative inference in small-sample comparative studies, without precluding the application of qualitative approaches. Borrowing the expression from Sidney Tarrow, the synthetic control method allows researchers to put ``qualitative flesh on quantitative bones.'' We illustrate the main ideas behind the synthetic control method by estimating the economic impact of the 1990 German reunification on West Germany.},
  copyright = {{\copyright}2014, Midwest Political Science Association},
  langid = {english},
  keywords = {comparative case studies,difference-in-differences,German reunification,matching,synthetic control method},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Abadie et al_2015_Comparative Politics and the Synthetic Control Method.pdf;/Users/nseewald/Zotero/storage/E2QCESBK/ajps.html}
}

@article{abadieSemiparametricDifferenceinDifferencesEstimators2005,
  title = {Semiparametric {{Difference-in-Differences Estimators}}},
  author = {Abadie, Alberto},
  year = {2005},
  month = jan,
  journal = {The Review of Economic Studies},
  volume = {72},
  number = {1},
  pages = {1--19},
  issn = {0034-6527},
  doi = {10/bfksdh},
  urldate = {2021-06-27},
  abstract = {The difference-in-differences (DID) estimator is one of the most popular tools for applied research in economics to evaluate the effects of public interventions and other treatments of interest on some relevant outcome variables. However, it is well known that the DID estimator is based on strong identifying assumptions. In particular, the conventional DID estimator requires that, in the absence of the treatment, the average outcomes for the treated and control groups would have followed parallel paths over time. This assumption may be implausible if pre-treatment characteristics that are thought to be associated with the dynamics of the outcome variable are unbalanced between the treated and the untreated. That would be the case, for example, if selection for treatment is influenced by individual-transitory shocks on past outcomes (Ashenfelter's dip). This article considers the case in which differences in observed characteristics create non-parallel outcome dynamics between treated and controls. It is shown that, in such a case, a simple two-step strategy can be used to estimate the average effect of the treatment for the treated. In addition, the estimation framework proposed in this article allows the use of covariates to describe how the average effect of the treatment varies with changes in observed characteristics.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Abadie_2005_Semiparametric Difference-in-Differences Estimators.pdf;/Users/nseewald/Zotero/storage/WCVYR6HU/1581053.html}
}

@article{abadieSyntheticControlMethods2010,
  title = {Synthetic {{Control Methods}} for {{Comparative Case Studies}}: {{Estimating}} the {{Effect}} of {{California}}'s {{Tobacco Control Program}}},
  shorttitle = {Synthetic {{Control Methods}} for {{Comparative Case Studies}}},
  author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
  year = {2010},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {105},
  number = {490},
  pages = {493--505},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2009.ap08746},
  urldate = {2023-03-29},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Abadie et al_2010_Synthetic Control Methods for Comparative Case Studies.pdf}
}

@article{abadieUsingSyntheticControls2021,
  title = {Using {{Synthetic Controls}}: {{Feasibility}}, {{Data Requirements}}, and {{Methodological Aspects}}},
  shorttitle = {Using {{Synthetic Controls}}},
  author = {Abadie, Alberto},
  year = {2021},
  month = jun,
  journal = {Journal of Economic Literature},
  volume = {59},
  number = {2},
  pages = {391--425},
  issn = {0022-0515},
  doi = {10.1257/jel.20191450},
  urldate = {2023-10-26},
  abstract = {Probably because of their interpretability and transparent nature, synthetic controls have become widely applied in empirical research in economics and the social sciences. This article aims to provide practical guidance to researchers employing synthetic control methods. The article starts with an overview and an introduction to synthetic control estimation. The main sections discuss the advantages of the synthetic control framework as a research design, and describe the settings where synthetic controls provide reliable estimates and those where they may fail. The article closes with a discussion of recent extensions, related methods, and avenues for future research.},
  langid = {english},
  keywords = {Aggregate Productivity,Cross-Country Output Convergence,Diffusion Processes,Dynamic Quantile Regressions,Dynamic Treatment Effect Models,Economic Methodology Multiple or Simultaneous Equation Models: Time-Series Models,State Space Models Quantitative Policy Modeling Macroeconomics: Production Economic Integration Empirical Studies of Economic Growth},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Abadie_2021_Using Synthetic Controls.pdf}
}

@techreport{abadieWhenShouldYou2017,
  title = {When {{Should You Adjust Standard Errors}} for {{Clustering}}?},
  author = {Abadie, Alberto and Athey, Susan and Imbens, Guido and Wooldridge, Jeffrey},
  year = {2017},
  month = nov,
  number = {w24003},
  pages = {w24003},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w24003},
  urldate = {2022-02-07},
  abstract = {In empirical work in economics it is common to report standard errors that account for clustering of units. Typically, the motivation given for the clustering adjustments is that unobserved components in outcomes for units within clusters are correlated. However, because correlation may occur across more than one dimension, this motivation makes it difficult to justify why researchers use clustering in some dimensions, such as geographic, but not others, such as age cohorts or gender. This motivation also makes it difficult to explain why one should not cluster with data from a randomized experiment. In this paper, we argue that clustering is in essence a design problem, either a sampling design or an experimental design issue. It is a sampling design issue if sampling follows a two stage process where in the first stage, a subset of clusters were sampled randomly from a population of clusters, and in the second stage, units were sampled randomly from the sampled clusters. In this case the clustering adjustment is justified by the fact that there are clusters in the population that we do not see in the sample. Clustering is an experimental design issue if the assignment is correlated within the clusters. We take the view that this second perspective best fits the typical setting in economics where clustering adjustments are used. This perspective allows us to shed new light on three questions: (i) when should one adjust the standard errors for clustering, (ii) when is the conventional adjustment for clustering appropriate, and (iii) when does the conventional adjustment of the standard errors matter.},
  langid = {english},
  keywords = {notion},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Abadie et al_2017_When Should You Adjust Standard Errors for Clustering2.pdf}
}

@misc{abadieWhenShouldYou2017a,
  title = {When {{Should You Adjust Standard Errors}} for {{Clustering}}?},
  author = {Abadie, Alberto and Athey, Susan and Imbens, Guido and Wooldridge, Jeffrey},
  year = {2017},
  month = nov,
  address = {Cambridge, MA},
  doi = {10.3386/w24003},
  urldate = {2021-10-14},
  langid = {english},
  keywords = {notion},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Abadie et al_2017_When Should You Adjust Standard Errors for Clustering.pdf}
}

@article{abadieWhenShouldYou2022,
  title = {When {{Should You Adjust Standard Errors}} for {{Clustering}}?*},
  shorttitle = {When {{Should You Adjust Standard Errors}} for {{Clustering}}?},
  author = {Abadie, Alberto and Athey, Susan and Imbens, Guido W and Wooldridge, Jeffrey M},
  year = {2022},
  month = oct,
  journal = {The Quarterly Journal of Economics},
  pages = {qjac038},
  issn = {0033-5533},
  doi = {10.1093/qje/qjac038},
  urldate = {2022-10-24},
  abstract = {Clustered standard errors, with clusters defined by factors such as geography, are widespread in empirical research in economics and many other disciplines. Formally, clustered standard errors adjust for the correlations induced by sampling the outcome variable from a data-generating process with unobserved cluster-level components. However, the standard econometric framework for clustering leaves important questions unanswered: (i) Why do we adjust standard errors for clustering in some ways but not others, e.g., by state but not by gender, and in observational studies, but not in completely randomized experiments? (ii) Why is conventional clustering an ``all-or-nothing'' adjustment, while within-cluster correlations can be strong or extremely weak? (iii) In what settings does the choice of whether and how to cluster make a difference? We address these and other questions using a novel framework for clustered inference on average treatment effects. In addition to the common sampling component, the new framework incorporates a design component that accounts for the variability induced on the estimator by the treatment assignment mechanism. We show that, when the number of clusters in the sample is a nonnegligible fraction of the number of clusters in the population, conventional cluster standard errors can be severely inflated, and propose new variance estimators that correct for this bias.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Abadie et al_2022_When Should You Adjust Standard Errors for Clustering.pdf;/Users/nseewald/Zotero/storage/XLJDEYC8/6750017.html}
}

@article{abdallahAddressingEnvironmentNonStationarity,
  title = {Addressing {{Environment Non-Stationarity}} by {{Repeating Q-learning Updates}}},
  author = {Abdallah, Sherief},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  pages = {31},
  abstract = {Q-learning (QL) is a popular reinforcement learning algorithm that is guaranteed to converge to optimal policies in Markov decision processes. However, QL exhibits an artifact: in expectation, the effective rate of updating the value of an action depends on the probability of choosing that action. In other words, there is a tight coupling between the learning dynamics and underlying execution policy. This coupling can cause performance degradation in noisy non-stationary environments.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Abdallah_2016_Addressing Environment Non-Stationarity by Repeating Q-learning Updates.pdf}
}

@misc{AdaptiveRandomizationTwostage,
  title = {Adaptive Randomization in a Two-Stage Sequential Multiple Assignment Randomized Trial {\textbar} {{Biostatistics}} {\textbar} {{Oxford Academic}}},
  urldate = {2024-08-13},
  howpublished = {https://academic-oup-com.proxy.library.upenn.edu/biostatistics/article/23/4/1182/6289820?login=true\&token=},
  file = {/Users/nseewald/Zotero/storage/8X9FNTSI/6289820.html}
}

@article{adler-milsteinPaperlessHealthcareProgress2010,
  title = {Paperless Healthcare: {{Progress}} and Challenges of an {{IT-enabled}} Healthcare System},
  shorttitle = {Paperless Healthcare},
  author = {{Adler-Milstein}, Julia and Bates, David W.},
  year = {2010},
  month = mar,
  journal = {Business Horizons},
  volume = {53},
  number = {2},
  pages = {119--130},
  issn = {00076813},
  doi = {10.1016/j.bushor.2009.10.004},
  urldate = {2018-10-12},
  abstract = {For most Americans, a trip to the doctor's office or a hospital stay necessitates that medical personnel search through paper charts and records as care is administered. This remains the status quo, despite the increasingly large role that electronic communication plays in other aspects of our business and personal lives. The elevated use of information technology (IT) in healthcare settings-----primarily via utilization of electronic health records (EHRs), which allow information to be readily communicated and shared among healthcare providers-----has been advocated as a means of improving quality of care and helping to control healthcare costs over the long term. Yet, hastened implementation of healthcare IT will require considerable cost incursion in the near term, and will present various other challenges that must be addressed. Herein, we examine the merits and benefits of healthcare IT, as well as the costs and other challenges that may serve as obstacles to its wider implementation and use. We conclude with a set of recommendations designed to increase the likelihood that extensive expansion in the use of healthcare IT will yield the desired benefits.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Adler-Milstein_Bates_2010_Paperless healthcare.pdf}
}

@article{adomapelsCallCautionUsing2018,
  title = {A {{Call}} for {{Caution}} in {{Using Information Criteria}} to {{Select}} the {{Working Correlation Structure}} in {{Generalized Estimating Equations}}},
  author = {Adoma Pels, Wilhemina and Alam, Shomoita and Carpp, Lindsay N. and Moodie, Erica E. M.},
  year = {2018},
  month = nov,
  journal = {Epidemiology},
  volume = {29},
  number = {6},
  pages = {e51},
  issn = {1044-3983},
  doi = {10/ggfrws},
  urldate = {2019-12-21},
  abstract = {An abstract is unavailable.},
  langid = {american},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Adoma Pels et al_2018_A Call for Caution in Using Information Criteria to Select the Working.pdf;/Users/nseewald/Zotero/storage/DQ5Y9L7C/A_Call_for_Caution_in_Using_Information_Criteria.23.html}
}

@book{agrestiCategoricalDataAnalysis2002,
  title = {Categorical Data Analysis},
  author = {Agresti, Alan},
  year = {2002},
  series = {Wiley Series in Probability and Statistics},
  edition = {2nd ed},
  publisher = {Wiley-Interscience},
  address = {New York},
  isbn = {978-0-471-36093-3},
  lccn = {QA278 .A353 2002},
  keywords = {Multivariate analysis},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Agresti_2002_Categorical data analysis.PDF}
}

@misc{ahmadProvisionalDrugOverdose2024,
  title = {Provisional Drug Overdose Death Counts},
  author = {Ahmad, {\relax FB} and Cisewski, {\relax JA} and Rossen, {\relax JM} and Sutton, P},
  year = {2024},
  month = apr,
  journal = {National Center for Health Statistics},
  urldate = {2024-05-03},
  howpublished = {https://www.cdc.gov/nchs/nvss/vsrr/drug-overdose-data.htm}
}

@article{ahnEfficiencyGeneralEstimating2003,
  title = {Efficiency of {{General Estimating Equations Estimators}} of {{Slopes}} in {{Repeated Measurements}}: {{Adding Subjects}} or {{Adding Measurements}}?},
  shorttitle = {Efficiency of {{General Estimating Equations Estimators}} of {{Slopes}} in {{Repeated Measurements}}},
  author = {Ahn, Chul and Jung, Sin-Ho},
  year = {2003},
  month = jul,
  journal = {Drug Information Journal},
  volume = {37},
  number = {3},
  pages = {309--316},
  issn = {0092-8615},
  doi = {10/fxmk3b},
  urldate = {2019-04-01},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ahn_Jung_2003_Efficiency of General Estimating Equations Estimators of Slopes in Repeated.pdf}
}

@article{ahnEfficiencySlopeEstimator2004,
  title = {Efficiency of the {{Slope Estimator}} in {{Repeated Measurements}}},
  author = {Ahn, Chul and Jung, Sin-Ho},
  year = {2004},
  month = apr,
  journal = {Drug Information Journal},
  volume = {38},
  number = {2},
  pages = {143--148},
  issn = {0092-8615},
  doi = {10/fx8gtm},
  urldate = {2019-04-01},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ahn_Jung_2004_Efficiency of the Slope Estimator in Repeated Measurements.pdf}
}

@book{ahnSampleSizeCalculations2015,
  title = {Sample {{Size Calculations}} for {{Clustered}} and {{Longitudinal Outcomes}} in {{Clinical Research}}},
  author = {Ahn, Chul and Heo, Moonseong and Zhang, Song},
  year = {2015},
  series = {Chapman \& {{Hall}}/{{CRC Biostatistics Series}}},
  publisher = {CRC Press},
  address = {Boca Raton},
  isbn = {978-1-4667-5626-3},
  keywords = {nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ahn et al. - 2015 - Sample Size Calculations for Clustered and Longitudinal Outcomes in Clinical Research.pdf}
}

@article{ahnSampleSizeRequirements2008,
  title = {Sample {{Size Requirements}} for {{Clinical Trials}} with {{Repeated Binary Outcomes}}},
  author = {Ahn, Chul},
  year = {2008},
  journal = {Drug Information Journal},
  volume = {42},
  pages = {107--113},
  doi = {10.1177/009286150804200202},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ahn_2008_Sample Size Requirements for Clinical Trials with Repeated Binary Outcomes.pdf}
}

@article{almirallDesigningPilotSequential2012,
  title = {Designing a Pilot Sequential Multiple Assignment Randomized Trial for Developing an Adaptive Treatment Strategy},
  author = {Almirall, Daniel and Compton, Scott N. and Gunlicks-Stoessel, Meredith and Duan, Naihua and Murphy, Susan A.},
  year = {2012},
  journal = {Statistics in Medicine},
  volume = {31},
  number = {17},
  pages = {1887--1902},
  issn = {1097-0258},
  doi = {10/ggqhnw},
  urldate = {2020-04-28},
  abstract = {There is growing interest in how best to adapt and readapt treatments to individuals to maximize clinical benefit. In response, adaptive treatment strategies (ATS), which operationalize adaptive, sequential clinical decision making, have been developed. From a patient's perspective an ATS is a sequence of treatments, each individualized to the patient's evolving health status. From a clinician's perspective, an ATS is a sequence of decision rules that input the patient's current health status and output the next recommended treatment. Sequential multiple assignment randomized trials (SMART) have been developed to address the sequencing questions that arise in the development of ATSs, but SMARTs are relatively new in clinical research. This article provides an introduction to ATSs and SMART designs. This article also discusses the design of SMART pilot studies to address feasibility concerns, and to prepare investigators for a full-scale SMART. We consider an example SMART for the development of an ATS in the treatment of pediatric generalized anxiety disorders. Using the example SMART, we identify and discuss design issues unique to SMARTs that are best addressed in an external pilot study prior to the full-scale SMART. We also address the question of how many participants are needed in a SMART pilot study. A properly executed pilot study can be used to effectively address concerns about acceptability and feasibility in preparation for (that is, prior to) executing a full-scale SMART. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Almirall et al_2012_Designing a pilot sequential multiple assignment randomized trial for.pdf;/Users/nseewald/Zotero/storage/V7Z6NGMP/sim.html}
}

@article{almirallDevelopingOptimizedAdaptive2018,
  title = {Developing {{Optimized Adaptive Interventions}} in {{Education}}},
  author = {Almirall, Daniel and Kasari, Connie and McCaffrey, Daniel F. and {Nahum-Shani}, Inbal},
  year = {2018},
  month = jan,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {11},
  number = {1},
  pages = {27--34},
  publisher = {Routledge},
  issn = {1934-5747},
  doi = {10/gjh5tw},
  urldate = {2021-03-22},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Almirall et al_2018_Developing Optimized Adaptive Interventions in Education.pdf;/Users/nseewald/Zotero/storage/EFYVCHM2/19345747.2017.html}
}

@incollection{almirallExperimentalDesignsResearch2018,
  title = {Experimental {{Designs}} for {{Research}} on {{Adaptive Interventions}}: {{Singly}} and {{Sequentially Randomized Trials}}},
  shorttitle = {Experimental {{Designs}} for {{Research}} on {{Adaptive Interventions}}},
  booktitle = {Optimization of {{Behavioral}}, {{Biobehavioral}}, and {{Biomedical Interventions}}: {{Advanced Topics}}},
  author = {Almirall, Daniel and {Nahum-Shani}, Inbal and Wang, Lu and Kasari, Connie},
  editor = {Collins, Linda M. and Kugler, Kari C.},
  year = {2018},
  series = {Statistics for {{Social}} and {{Behavioral Sciences}}},
  pages = {89--120},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-91776-4_4},
  urldate = {2018-12-08},
  abstract = {In clinical or educational practice, it is often necessary to use an individually tailored, sequential approach to intervention in order to improve outcomes. Adaptive interventions (also known as dynamic treatment regimens) can be used to guide such sequential intervention decision-making. Adaptive interventions are multicomponent, multistage intervention packages. The multiphase optimization strategy (MOST) is a comprehensive research framework for development, optimization, and evaluation of multicomponent intervention packages, such as adaptive interventions. When working within the optimization phase of MOST, behavioral, biobehavioral, and educational intervention scientists often have important scientific questions about how best to optimize an adaptive intervention. This chapter discusses various types of experimental designs that can be used to optimize an adaptive intervention. In some of these, participants are randomized once over the course of the trial (i.e., singly randomized trials, or SRTs), and in others, participants are randomized at multiple stages (i.e., sequential, multiple assignment, randomized trials, or SMARTs). The choice between SRT and SMART ultimately is driven by the scientific questions that the intervention scientist seeks to answer. Motivated by the development of an adaptive intervention to improve social skills and academic engagement among children with autism, we illustrate these ideas by presenting four example of experimental designs: two examples of a SRT and two examples of a SMART. We present the rationale for each experimental design and the questions each is designed to answer. In doing so, this chapter provides an expanded set of tools that investigators aiming to develop an adaptive intervention can draw from within the MOST optimization phase toolbox.},
  isbn = {978-3-319-91776-4},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Almirall et al_2018_Experimental Designs for Research on Adaptive Interventions.pdf}
}

@article{almirallIntroductionSMARTDesigns2014,
  title = {Introduction to {{SMART}} Designs for the Development of Adaptive Interventions: With Application to Weight Loss Research},
  author = {Almirall, Daniel and {Nahum-Shani}, Inbal and Sherwood, Nancy E. and Murphy, Susan A.},
  year = {2014},
  journal = {Translational Behavioral Medicine},
  volume = {4},
  number = {3},
  pages = {260--274},
  issn = {1869-6716; EN :1613-9860},
  doi = {10.1007/s13142-014-0265-0},
  abstract = {The management of many health disorders often entails a sequential, individualized approach whereby treatment is adapted and readapted over time in response to the specific needs and evolving status of the individual. Adaptive interventions provide one way to operationalize the strategies (e.g., continue, augment, switch, step-down) leading to individualized sequences of treatment. Often, a wide variety of critical questions must be answered when developing a high-quality adaptive intervention. Yet, there is often insufficient empirical evidence or theoretical basis to address these questions. The Sequential Multiple Assignment Randomized Trial (SMART)-a type of research design-was developed explicitly for the purpose of building optimal adaptive interventions by providing answers to such questions. Despite increasing popularity, SMARTs remain relatively new to intervention scientists. This manuscript provides an introduction to adaptive interventions and SMARTs. We discuss SMART design considerations, including common primary and secondary aims. For illustration, we discuss the development of an adaptive intervention for optimizing weight loss among adult individuals who are overweight.},
  keywords = {Adaptive treatment strategies,Dynamic treatment regimens or regimes,Experimental design,Individualized or personalized behavioral interven,Timing and sequencing of intervention components},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Almirall et al_2014_Introduction to SMART designs for the development of adaptive interventions.pdf}
}

@article{almirallLongitudinalEffectsAdaptive2016,
  title = {Longitudinal {{Effects}} of {{Adaptive Interventions With}} a {{Speech-Generating Device}} in {{Minimally Verbal Children With ASD}}},
  author = {Almirall, Daniel and DiStefano, Charlotte and Chang, Ya-Chih and Shire, Stephanie and Kaiser, Ann and Lu, Xi and {Nahum-Shani}, Inbal and Landa, Rebecca and Mathy, Pamela and Kasari, Connie},
  year = {2016},
  month = jul,
  journal = {Journal of Clinical Child \& Adolescent Psychology},
  volume = {45},
  number = {4},
  pages = {442--456},
  issn = {1537-4416, 1537-4424},
  doi = {10.1080/15374416.2016.1138407},
  urldate = {2018-10-12},
  langid = {english},
  keywords = {secondary analysis,trial},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Almirall et al_2016_Longitudinal Effects of Adaptive Interventions With a Speech-Generating Device.pdf}
}

@article{althousePostHocPower2021,
  title = {Post {{Hoc Power}}: {{Not Empowering}}, {{Just Misleading}}},
  shorttitle = {Post {{Hoc Power}}},
  author = {Althouse, Andrew D.},
  year = {2021},
  month = mar,
  journal = {Journal of Surgical Research},
  volume = {259},
  pages = {A3-A6},
  issn = {0022-4804},
  doi = {10.1016/j.jss.2019.10.049},
  urldate = {2025-02-26},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Althouse - 2021 - Post Hoc Power Not Empowering, Just Misleading.pdf;/Users/nseewald/Zotero/storage/GI3KFAEU/S0022480420305023.html}
}

@article{altmanBaselineComparisonsRandomized1991,
  title = {Baseline Comparisons in Randomized Clinical Trails},
  author = {Altman, Douglas G. and Dor{\'e}, Caroline J},
  year = {1991},
  journal = {Statistics in Medicine},
  volume = {10},
  number = {5},
  pages = {797--799},
  issn = {1097-0258},
  doi = {10/b2753p},
  urldate = {2020-01-14},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Altman_Doré_1991_Baseline comparisons in randomized clinical trails.pdf;/Users/nseewald/Zotero/storage/5L2EW4N4/sim.html}
}

@book{ambroseHowLearningWorks2010,
  title = {How {{Learning Works}}: {{Seven Research-Based Principles}} for {{Smart Teaching}}},
  shorttitle = {How {{Learning Works}}},
  author = {Ambrose, Susan A. and Bridges, Michael W. and DiPietro, Michele and Lovett, Marsha C. and Norman, Marie K.},
  year = {2010},
  month = may,
  publisher = {John Wiley \& Sons},
  isbn = {978-0-470-48410-4},
  langid = {english}
}

@inproceedings{anavaAstNearestNeighbors2016,
  title = {K*-{{Nearest Neighbors}}: {{From Global}} to {{Local}}},
  shorttitle = {Kstar-{{Nearest Neighbors}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Anava, Oren and Levy, Kfir},
  year = {2016},
  pages = {4916--4924},
  publisher = {Curran Associates, Inc.},
  address = {Barcelona, Spain},
  urldate = {2019-01-16},
  abstract = {The weighted k-nearest neighbors algorithm is one of the most fundamental nonparametric methods in pattern recognition and machine learning. The question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to have remained unsettled. In this paper we offer a simple approach to locally weighted regression/classification, where we make the bias-variance tradeoff explicit. Our formulation enables us to phrase a notion of optimal weights, and to efficiently find these weights as well as the optimal number of neighbors efficiently and adaptively, for each data point whose value we wish to estimate. The applicability of our approach is demonstrated on several datasets, showing superior performance over standard locally weighted methods.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Anava_Levy_2016_k-Nearest Neighbors.pdf;/Users/nseewald/Zotero/storage/SP6HBEDJ/6373-equality-of-opportunity-in-supervised-learning.html}
}

@incollection{angristChapter23Empirical1999,
  title = {Chapter 23 - {{Empirical Strategies}} in {{Labor Economics}}},
  booktitle = {Handbook of {{Labor Economics}}},
  author = {Angrist, Joshua D. and Krueger, Alan B.},
  editor = {Ashenfelter, Orley C. and Card, David},
  year = {1999},
  month = jan,
  volume = {3},
  pages = {1277--1366},
  publisher = {Elsevier},
  doi = {10.1016/S1573-4463(99)03004-7},
  urldate = {2022-05-20},
  abstract = {This chapter provides an overview of the methodological and practical issues that arise when estimating causal relationships that are of interest to labor economists. The subject matter includes identification, data collection, and measurement problems. Four identification strategies are discussed, and five empirical examples -- the effects of schooling, unions, immigration, military service, and class size -- illustrate the methodological points. In discussing each example, we adopt an experimentalist perspective that emphasizes the distinction between variables that have causal effects, control variables, and outcome variables. The chapter also discusses secondary datasets, primary data collection strategies, and administrative data. The section on measurement issues focuses on recent empirical examples, presents a summary of empirical findings on the reliability of key labor market data, and briefly reviews the role of survey sampling weights and the allocation of missing values in empirical research. {\copyright} 1999 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Angrist_Krueger_1999_Chapter 23 - Empirical Strategies in Labor Economics.pdf;/Users/nseewald/Zotero/storage/NJERQ6HQ/S1573446399030047.html}
}

@article{antonakisIgnoringRandomEffects2021,
  title = {On {{Ignoring}} the {{Random Effects Assumption}} in {{Multilevel Models}}: {{Review}}, {{Critique}}, and {{Recommendations}}},
  shorttitle = {On {{Ignoring}} the {{Random Effects Assumption}} in {{Multilevel Models}}},
  author = {Antonakis, John and Bastardoz, Nicolas and R{\"o}nkk{\"o}, Mikko},
  year = {2021},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {24},
  number = {2},
  pages = {443--483},
  publisher = {SAGE Publications Inc},
  issn = {1094-4281},
  doi = {10.1177/1094428119877457},
  urldate = {2024-10-03},
  abstract = {Entities such as individuals, teams, or organizations can vary systematically from one another. Researchers typically model such data using multilevel models, assuming that the random effects are uncorrelated with the regressors. Violating this testable assumption, which is often ignored, creates an endogeneity problem thus preventing causal interpretations. Focusing on two-level models, we explain how researchers can avoid this problem by including cluster means of the Level 1 explanatory variables as controls; we explain this point conceptually and with a large-scale simulation. We further show why the common practice of centering the predictor variables is mostly unnecessary. Moreover, to examine the state of the science, we reviewed 204 randomly drawn articles from macro and micro organizational science and applied psychology journals, finding that only 106 articles---with a slightly higher proportion from macro-oriented fields---properly deal with the random effects assumption. Alarmingly, most models also failed on the usual exogeneity requirement of the regressors, leaving only 25 mostly macro-level articles that potentially reported trustworthy multilevel estimates. We offer a set of practical recommendations for researchers to model multilevel data appropriately.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Antonakis_et_al_2021_On_Ignoring_the_Random_Effects_Assumption_in_Multilevel_Models.pdf}
}

@misc{APAPsycNetFullTextHTML,
  title = {{{APA PsycNet FullTextHTML}} Page},
  urldate = {2024-08-21},
  howpublished = {https://psycnet-apa-org.proxy.library.upenn.edu/fulltext/2019-39467-001.html},
  file = {/Users/nseewald/Zotero/storage/WN5CTW8R/2019-39467-001.html}
}

@incollection{araoSafeSpacesBrave2013,
  title = {From {{Safe Spaces}} to {{Brave Spaces}}},
  booktitle = {The {{Art}} of {{Effective Facilitation}}: {{Reflections}} from {{Social Justice Educators}}},
  author = {Arao, Brian and Clemens, Kristi},
  year = {2013},
  pages = {16},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Arao_Clemens_2013_From Safe Spaces to Brave Spaces.pdf}
}

@article{arellanoComputingRobustStandard1987,
  title = {Computing {{Robust Standard Errors}} for {{Within-groups Estimators}}},
  author = {Arellano, M.},
  year = {1987},
  journal = {Oxford Bulletin of Economics and Statistics},
  volume = {49},
  number = {4},
  pages = {431--434},
  issn = {1468-0084},
  doi = {10.1111/j.1468-0084.1987.mp49004006.x},
  urldate = {2025-01-28},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Arellano - 1987 - Computing Robust Standard Errors for Within-groups Estimators.pdf;/Users/nseewald/Zotero/storage/QIE8SBV5/j.1468-0084.1987.mp49004006.html}
}

@article{arisidoJointModelRobustness2019,
  title = {Joint Model Robustness Compared with the Time-Varying Covariate {{Cox}} Model to Evaluate the Association between a Longitudinal Marker and a Time-to-Event Endpoint},
  author = {Arisido, Maeregu W. and Antolini, Laura and Bernasconi, Davide P. and Valsecchi, Maria G. and Rebora, Paola},
  year = {2019},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {19},
  number = {1},
  pages = {222},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0873-y},
  urldate = {2024-01-11},
  abstract = {The recent progress in medical research generates an increasing interest in the use of longitudinal biomarkers for characterizing the occurrence of an outcome. The present work is motivated by a study, where the objective was to explore the potential of the long pentraxin 3 (PTX3) as a prognostic marker of Acute Graft-versus-Host Disease (GvHD) after haematopoietic stem cell transplantation. Time-varying covariate Cox model was commonly used, despite its limiting assumptions that marker values are constant in time and measured without error. A joint model has been developed as a viable alternative; however, the approach is computationally intensive and requires additional strong assumptions, in which the impacts of their misspecification were not sufficiently studied.},
  keywords = {Cox model,Joint model simulation,Longitudinal biomarker,Random effects model,Time-varying covariate},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Arisido_et_al_2019_Joint_model_robustness_compared_with_the_time-varying_covariate_Cox_model_to.pdf;/Users/nseewald/Zotero/storage/774WX6KA/s12874-019-0873-y.html}
}

@misc{arkhangelskyCausalModelsLongitudinal2023,
  title = {Causal {{Models}} for {{Longitudinal}} and {{Panel Data}}: {{A Survey}}},
  shorttitle = {Causal {{Models}} for {{Longitudinal}} and {{Panel Data}}},
  author = {Arkhangelsky, Dmitry and Imbens, Guido},
  year = {2023},
  month = nov,
  number = {arXiv:2311.15458},
  eprint = {2311.15458},
  primaryclass = {econ},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.15458},
  urldate = {2023-11-30},
  abstract = {This survey discusses the recent causal panel data literature. This recent literature has focused on credibly estimating causal effects of binary interventions in settings with longitudinal data, with an emphasis on practical advice for empirical researchers. It pays particular attention to heterogeneity in the causal effects, often in situations where few units are treated. The literature has extended earlier work on difference-in-differences or two-way-fixed-effect estimators and more generally incorporated factor models or interactive fixed effects. It has also developed novel methods using synthetic control approaches.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Economics - Econometrics},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Arkhangelsky_Imbens_2023_Causal Models for Longitudinal and Panel Data.pdf;/Users/nseewald/Zotero/storage/ZMFD2IX9/2311.html}
}

@article{arkhangelskyFixedEffectsGeneralized2023a,
  title = {Fixed {{Effects}} and the {{Generalized Mundlak Estimator}}},
  author = {Arkhangelsky, Dmitry and Imbens, Guido W},
  year = {2023},
  month = sep,
  journal = {The Review of Economic Studies},
  pages = {rdad089},
  issn = {0034-6527},
  doi = {10.1093/restud/rdad089},
  urldate = {2024-03-01},
  abstract = {We develop a new approach for estimating average treatment effects in observational studies with unobserved group-level heterogeneity. We consider a general model with group-level unconfoundedness and provide conditions under which aggregate balancing statistics---group-level averages of functions of treatments and covariates---are sufficient to eliminate differences between groups. Building on these results, we re-interpret commonly used linear fixed-effect regression estimators by writing them in the Mundlak form as linear regression estimators without fixed effects but including group averages. We use this representation to develop Generalized Mundlak Estimators that capture group differences through group averages of (functions of) the unit-level variables and adjust for these group differences in flexible and robust ways in the spirit of the modern causal literature.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Arkhangelsky_Imbens_2023_Fixed_Effects_and_the_Generalized_Mundlak_Estimator.pdf;/Users/nseewald/Zotero/storage/PWL8R86J/7262519.html}
}

@article{arndtAddingSubjectsAdding2000,
  title = {Adding Subjects or Adding Measurements: Which Increases the Precision of Longitudinal Research?},
  shorttitle = {Adding Subjects or Adding Measurements},
  author = {Arndt, Stephan and Jorge, Ricardo and Turvey, Carolyn and Robinson, Robert G},
  year = {2000},
  month = dec,
  journal = {Journal of Psychiatric Research},
  volume = {34},
  number = {6},
  pages = {449--455},
  issn = {0022-3956},
  doi = {10/bjgwtn},
  urldate = {2018-12-05},
  abstract = {When designing repeated measurement studies, researchers must strike a balance between increasing the number of subjects and increasing the number of measurement times for each subject. The question often becomes ``Do I gain more statistical precision by adding subjects or by adding additional follow-up measurements?'' This study presents a method for evaluating the relative benefit of adding subjects versus adding measurement times. We used the standard error of estimate (SE) for mean change as the criterion of precision. An existing dataset on post-stroke patients containing six follow-up assessments of six standard rating scales was used. SE values for two common change indices were found and compared for all possible two, three, four, five, and six repeated measurements. Sample sizes required to achieve the same benefit as adding an additional measurement are presented. These data suggest that collecting five or six repeated measurements may be sufficient for accurately assessing change and that attempts to further precision should be accomplished by increasing the sample size.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Arndt et al_2000_Adding subjects or adding measurements.pdf;/Users/nseewald/Zotero/storage/HW9RI8KX/S002239560000042X.html}
}

@article{arnoldSimulationMethodsEstimate2011,
  title = {Simulation Methods to Estimate Design Power: An Overview for Applied Research},
  shorttitle = {Simulation Methods to Estimate Design Power},
  author = {Arnold, Benjamin F. and Hogan, Daniel R. and Colford, John M. and Hubbard, Alan E.},
  year = {2011},
  month = jun,
  journal = {BMC Medical Research Methodology},
  volume = {11},
  number = {1},
  pages = {94},
  issn = {1471-2288},
  doi = {10/cfmhpp},
  urldate = {2020-02-27},
  abstract = {Estimating the required sample size and statistical power for a study is an integral part of study design. For standard designs, power equations provide an efficient solution to the problem, but they are unavailable for many complex study designs that arise in practice. For such complex study designs, computer simulation is a useful alternative for estimating study power. Although this approach is well known among statisticians, in our experience many epidemiologists and social scientists are unfamiliar with the technique. This article aims to address this knowledge gap.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Arnold et al_2011_Simulation methods to estimate design power.pdf;/Users/nseewald/Zotero/storage/2SG6I4JZ/1471-2288-11-94.html}
}

@article{artmanPowerAnalysisSMART2018,
  title = {Power Analysis in a {{SMART}} Design: Sample Size Estimation for Determining the Best Embedded Dynamic Treatment Regime},
  shorttitle = {Power Analysis in a {{SMART}} Design},
  author = {Artman, William J. and {Nahum-Shani}, Inbal and Wu, Tianshuang and Mckay, James R. and Ertefaie, Ashkan},
  year = {2018},
  journal = {Biostatistics},
  doi = {10/ggth75},
  urldate = {2020-04-30},
  abstract = {Summary.  Sequential, multiple assignment, randomized trial (SMART) designs have become increasingly popular in the field of precision medicine by providing a m},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Artman et al_2018_Power analysis in a SMART design.pdf;/Users/nseewald/Zotero/storage/WD3A8NME/5149691.html}
}

@article{ashbeckSingleTimePoint2016,
  title = {Single Time Point Comparisons in Longitudinal Randomized Controlled Trials: Power and Bias in the Presence of Missing Data},
  shorttitle = {Single Time Point Comparisons in Longitudinal Randomized Controlled Trials},
  author = {Ashbeck, Erin L. and Bell, Melanie L.},
  year = {2016},
  month = apr,
  journal = {BMC Medical Research Methodology},
  volume = {16},
  number = {1},
  pages = {43},
  issn = {1471-2288},
  doi = {10/f8gxd2},
  urldate = {2019-07-26},
  abstract = {The primary analysis in a longitudinal randomized controlled trial is sometimes a comparison of arms at a single time point. While a two-sample t-test is often used, missing data are common in longitudinal studies and decreases power by reducing sample size. Mixed models for repeated measures (MMRM) can test treatment effects at specific time points, have been shown to give unbiased estimates in certain missing data contexts, and may be more powerful than a two sample t-test.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ashbeck_Bell_2016_Single time point comparisons in longitudinal randomized controlled trials.pdf;/Users/nseewald/Zotero/storage/SH76XSQZ/s12874-016-0144-0.html}
}

@misc{AssessingTotalEffect,
  title = {Assessing the {{Total Effect}} of {{Time-Varying Predictors}} in {{Prevention Research}} {\textbar} {{SpringerLink}}},
  urldate = {2021-08-13},
  howpublished = {https://link-springer-com.proxy1.library.jhu.edu/article/10.1007\%2Fs11121-005-0023-0\#Sec4},
  file = {/Users/nseewald/Zotero/storage/4GB6YARG/10.html}
}

@misc{AssociationAfricanAncestry,
  title = {The {{Association}} of {{African Ancestry}} and {{Elevated Creatinine}} in the {{Coronary Artery Risk Development}} in {{Young Adults}} ({{CARDIA}}) {{Study}} - {{Abstract}} - {{American Journal}} of {{Nephrology}} 2010, {{Vol}}. 31, {{No}}. 3 - {{Karger Publishers}}},
  urldate = {2022-08-19},
  howpublished = {https://www.karger.com/Article/Abstract/268955}
}

@inproceedings{AssociationsTreatmentemergentSymptoms,
  title = {Associations between Treatment-Emergent Symptoms and Early Discontinuation of Aromatase Inhibitor ({{AI}}) Therapy},
  urldate = {2024-10-22},
  file = {/Users/nseewald/Zotero/storage/3ES8N2VG/jco.2015.33.15_suppl.html}
}

@article{atheyDesignbasedAnalysisDifferenceInDifferences2021,
  title = {Design-Based Analysis in {{Difference-In-Differences}} Settings with Staggered Adoption},
  author = {Athey, Susan and Imbens, Guido W.},
  year = {2021},
  month = apr,
  journal = {Journal of Econometrics},
  issn = {0304-4076},
  doi = {10/gjsmfr},
  urldate = {2021-06-27},
  abstract = {In this paper we study estimation of and inference for average treatment effects in a setting with panel data. We focus on the staggered adoption setting where units, e.g, individuals, firms, or states, adopt the policy or treatment of interest at a particular point in time, and then remain exposed to this treatment at all times afterwards. We take a design perspective where we investigate the properties of estimators and procedures given assumptions on the assignment process. We show that under random assignment of the adoption date the standard Difference-In-Differences (DID) estimator is an unbiased estimator of a particular weighted average causal effect. We characterize the exact finite sample properties of this estimand, and show that the standard variance estimator is conservative.},
  langid = {english},
  keywords = {Difference-In-Differences,Fixed effects,Randomization distribution,Staggered adoption design},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Athey_Imbens_2021_Design-based analysis in Difference-In-Differences settings with staggered.pdf;/Users/nseewald/Zotero/storage/SE68924T/S0304407621000488.html;/Users/nseewald/Zotero/storage/U85IQ6GD/S0304407621000488.html}
}

@article{augustBeingSMARTAdolescent2016,
  title = {Being ``{{SMART}}'' {{About Adolescent Conduct Problems Prevention}}: {{Executing}} a {{SMART Pilot Study}} in a {{Juvenile Diversion Agency}}},
  shorttitle = {Being ``{{SMART}}'' {{About Adolescent Conduct Problems Prevention}}},
  author = {August, Gerald J. and Piehler, Timothy F. and Bloomquist, Michael L.},
  year = {2016},
  month = jul,
  journal = {Journal of Clinical Child \& Adolescent Psychology},
  volume = {45},
  number = {4},
  pages = {495--509},
  publisher = {Routledge},
  issn = {1537-4416},
  doi = {10.1080/15374416.2014.945212},
  urldate = {2024-07-20},
  abstract = {The development of adaptive treatment strategies (ATS) represents the next step in innovating conduct problems prevention programs within a juvenile diversion context. Toward this goal, we present the theoretical rationale, associated methods, and anticipated challenges for a feasibility pilot study in preparation for implementing a full-scale SMART (i.e., sequential, multiple assignment, randomized trial) for conduct problems prevention. The role of a SMART design in constructing ATS is presented. The SMART feasibility pilot study includes a sample of 100 youth (13--17 years of age) identified by law enforcement as early stage offenders and referred for precourt juvenile diversion programming. Prior data on the sample population detail a high level of ethnic diversity and approximately equal representations of both genders. Within the SMART, youth and their families are first randomly assigned to one of two different brief-type evidence-based prevention programs, featuring parent-focused behavioral management or youth-focused strengths-building components. Youth who do not respond sufficiently to brief first-stage programming will be randomly assigned a second time to either an extended parent- or youth-focused second-stage programming. Measures of proximal intervention response and measures of potential candidate tailoring variables for developing ATS within this sample are detailed. Results of the described pilot study will include information regarding feasibility and acceptability of the SMART design. This information will be used to refine a subsequent full-scale SMART. The use of a SMART to develop ATS for prevention will increase the efficiency and effectiveness of prevention programing for youth with developing conduct problems.},
  pmid = {25256135},
  file = {C:\Users\Nick\Box\Zotero\August_et_al_2016_Being_“SMART”_About_Adolescent_Conduct_Problems_Prevention.pdf}
}

@article{austinImputethenexcludeExcludethenimputeLessons2023,
  title = {Impute-Then-Exclude versus Exclude-Then-Impute: {{Lessons}} When Imputing a Variable Used Both in Cohort Creation and as an Independent Variable in the Analysis Model},
  shorttitle = {Impute-Then-Exclude versus Exclude-Then-Impute},
  author = {Austin, Peter C. and Giardiello, Daniele and {van Buuren}, Stef},
  year = {2023},
  journal = {Statistics in Medicine},
  volume = {42},
  number = {10},
  pages = {1525--1541},
  issn = {1097-0258},
  doi = {10.1002/sim.9685},
  urldate = {2024-02-14},
  abstract = {We examined the setting in which a variable that is subject to missingness is used both as an inclusion/exclusion criterion for creating the analytic sample and subsequently as the primary exposure in the analysis model that is of scientific interest. An example is cancer stage, where patients with stage IV cancer are often excluded from the analytic sample, and cancer stage (I to III) is an exposure variable in the analysis model. We considered two analytic strategies. The first strategy, referred to as ``exclude-then-impute,'' excludes subjects for whom the observed value of the target variable is equal to the specified value and then uses multiple imputation to complete the data in the resultant sample. The second strategy, referred to as ``impute-then-exclude,'' first uses multiple imputation to complete the data and then excludes subjects based on the observed or filled-in values in the completed samples. Monte Carlo simulations were used to compare five methods (one based on ``exclude-then-impute'' and four based on ``impute-then-exclude'') along with the use of a complete case analysis. We considered both missing completely at random and missing at random missing data mechanisms. We found that an impute-then-exclude strategy using substantive model compatible fully conditional specification tended to have superior performance across 72 different scenarios. We illustrated the application of these methods using empirical data on patients hospitalized with heart failure when heart failure subtype was used for cohort creation (excluding subjects with heart failure with preserved ejection fraction) and was also an exposure in the analysis model.},
  copyright = {{\copyright} 2023 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {_tablet,missing data,Monte Carlo simulations,multiple imputation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Austin_et_al_2023_Impute-then-exclude_versus_exclude-then-impute.pdf}
}

@article{austinInformingPowerSample2021,
  title = {Informing Power and Sample Size Calculations When Using Inverse Probability of Treatment Weighting Using the Propensity Score},
  author = {Austin, Peter C.},
  year = {2021},
  month = nov,
  journal = {Statistics in Medicine},
  volume = {40},
  number = {27},
  pages = {6150--6163},
  issn = {0277-6715},
  doi = {10.1002/sim.9176},
  urldate = {2024-09-16},
  abstract = {Propensity score weighting is increasingly being used in observational studies to estimate the effects of treatments. The use of such weights induces a within-person homogeneity in outcomes that must be accounted for when estimating the variance of the estimated treatment effect. Knowledge of the variance inflation factor (VIF), which describes the extent to which the effective sample size has been reduced by weighting, allows for conducting sample size and power calculations for observational studies that use propensity score weighting. However, estimation of the VIF requires knowledge of the weights, which are only known once the study has been conducted. We describe methods to estimate the VIF based on two characteristics of the observational study: the anticipated prevalence of treatment and the anticipated c-statistic of the propensity score model. We considered five different sets of weights: those for estimating the average treatment effect (ATE), the average treated effect in the treated (ATT), and three recently described sets of weights: overlap weights, matching weights, and entropy weights. The VIF was substantially smaller for the latter three sets of weights than for the first two sets of weights. Once the VIF has been estimated during the design phase of the study, sample size and power calculations can be done using calculations appropriate for a randomized controlled trial with similar prevalence of treatment and similar outcome variable, and then multiplying the requisite sample size by the estimated VIF. Implementation of these methods allows for improving the design and reporting of observational studies that use propensity score weighting.},
  pmcid = {PMC9293235},
  pmid = {34510501},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Austin_2021_Informing_power_and_sample_size_calculations_when_using_inverse_probability_of.pdf}
}

@article{austinMovingBestPractice2015,
  title = {Moving towards Best Practice When Using Inverse Probability of Treatment Weighting ({{IPTW}}) Using the Propensity Score to Estimate Causal Treatment Effects in Observational Studies},
  author = {Austin, Peter C. and Stuart, Elizabeth A.},
  year = {2015},
  month = dec,
  journal = {Statistics in Medicine},
  volume = {34},
  number = {28},
  pages = {3661--3679},
  issn = {0277-6715},
  doi = {10.1002/sim.6607},
  urldate = {2024-07-10},
  abstract = {The propensity score is defined as a subject's probability of treatment selection, conditional on observed baseline covariates. Weighting subjects by the inverse probability of treatment received creates a synthetic sample in which treatment assignment is independent of measured baseline covariates. Inverse probability of treatment weighting (IPTW) using the propensity score allows one to obtain unbiased estimates of average treatment effects. However, these estimates are only valid if there are no residual systematic differences in observed baseline characteristics between treated and control subjects in the sample weighted by the estimated inverse probability of treatment. We report on a systematic literature review, in which we found that the use of IPTW has increased rapidly in recent years, but that in the most recent year, a majority of studies did not formally examine whether weighting balanced measured covariates between treatment groups. We then proceed to describe a suite of quantitative and qualitative methods that allow one to assess whether measured baseline covariates are balanced between treatment groups in the weighted sample. The quantitative methods use the weighted standardized difference to compare means, prevalences, higher-order moments, and interactions. The qualitative methods employ graphical methods to compare the distribution of continuous baseline covariates between treated and control subjects in the weighted sample. Finally, we illustrate the application of these methods in an empirical case study. We propose a formal set of balance diagnostics that contribute towards an evolving concept of `best practice' when using IPTW to estimate causal treatment effects using observational data. {\copyright} 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  pmcid = {PMC4626409},
  pmid = {26238958},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Austin_Stuart_2015_Moving_towards_best_practice_when_using_inverse_probability_of_treatment.pdf}
}

@article{auyeungSequentialMultipleassignmentRandomized2009,
  title = {Sequential Multiple-Assignment Randomized Trial Design of Neurobehavioral Treatment for Patients with Metastatic Malignant Melanoma Undergoing High-Dose Interferon-Alpha Therapy},
  author = {Auyeung, S. Freda and Long, Qi and Royster, Erica Bruce and Murthy, Smitha and McNutt, Marcia D and Lawson, David and Miller, Andrew and Manatunga, Amita and Musselman, Dominique L},
  year = {2009},
  month = oct,
  journal = {Clinical Trials},
  volume = {6},
  number = {5},
  pages = {480--490},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774509344633},
  urldate = {2018-10-12},
  abstract = {Background Interferon-alpha therapy, which is used to treat metastatic malignant melanoma, can cause patients to develop two distinct neurobehavioral symptom complexes: a mood syndrome and a neurovegetative syndrome. Interferon-alpha effects on serotonin metabolism appear to contribute to the mood and anxiety syndrome, while the neurovegetative syndrome appears to be related to interferonalpha effects on dopamine. Purpose Our goal is to propose a design for utilizing a sequential, multiple assignment, randomized trial design for patients with malignant melanoma to test the relative efficacy of drugs that target serotonin versus dopamine metabolism during 4 weeks of intravenous, then 8 weeks of subcutaneous, interferon-alpha therapy. Methods Patients will be offered participation in a double-blinded, randomized, controlled, 14-week trial involving two treatment phases. During the first month of intravenous interferon-alpha therapy, we will test the hypotheses that escitalopram will be more effective in reducing depressed mood, anxiety, and irritability, whereas methylphenidate will be more effective in diminishing interferon-alpha-induced neurovegetative symptoms, such as fatigue and psychomotor slowing. During the next 8 weeks of subcutaneous interferon therapy, participants whose symptoms do not improve significantly will be randomized to the alternate agent alone versus escitalopram and methylphenidate together. Results We present a prototype for a single-center, sequential, multiple assignment, randomized trial, which seeks to determine the efficacy of sequenced and targeted treatment for the two distinct symptom complexes suffered by patients treated with interferon-alpha. Limitations Because we cannot completely control for external factors, a relevant question is whether or not `short-term' neuropsychiatric interventions can increase the number of interferon-alpha doses tolerated and improve long-term survival. Conclusions This sequential, multiple assignment, randomized trial proposes a framework for developing optimal treatment strategies; however, additional studies are needed to determine the best strategy for treating or preventing neurobehavioral symptoms induced by the immunotherapy interferon-alpha. Clinical Trials 2009; 6: 480--490. http://ctj.sagepub.com},
  langid = {english},
  keywords = {trial},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Auyeung et al_2009_Sequential multiple-assignment randomized trial design of neurobehavioral.pdf}
}

@article{azurMultipleImputationChained2011,
  title = {Multiple Imputation by Chained Equations: What Is It and How Does It Work?},
  shorttitle = {Multiple Imputation by Chained Equations},
  author = {Azur, Melissa J. and Stuart, Elizabeth A. and Frangakis, Constantine and Leaf, Philip J.},
  year = {2011},
  journal = {International Journal of Methods in Psychiatric Research},
  volume = {20},
  number = {1},
  pages = {40--49},
  issn = {1557-0657},
  doi = {10.1002/mpr.329},
  urldate = {2024-01-30},
  abstract = {Multivariate imputation by chained equations (MICE) has emerged as a principled method of dealing with missing data. Despite properties that make MICE particularly useful for large imputation procedures and advances in software development that now make it accessible to many researchers, many psychiatric researchers have not been trained in these methods and few practical resources exist to guide researchers in the implementation of this technique. This paper provides an introduction to the MICE method with a focus on practical aspects and challenges in using this method. A brief review of software programs available to implement MICE and then analyze multiply imputed data is also provided. Copyright {\copyright} 2011 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2011 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {_tablet,analyze,missing data,multiple imputation},
  file = {/Users/nseewald/Zotero/storage/WTC2ZC6L/Azur et al_2011_Multiple imputation by chained equations.pdf;/Users/nseewald/Zotero/storage/3PEBGAWQ/mpr.html}
}

@article{azzaliniClassDistributionsWhich1985,
  title = {A {{Class}} of {{Distributions Which Includes}} the {{Normal Ones}}},
  author = {Azzalini, A.},
  year = {1985},
  journal = {Scandinavian Journal of Statistics},
  volume = {12},
  number = {2},
  eprint = {4615982},
  eprinttype = {jstor},
  pages = {171--178},
  issn = {0303-6898},
  urldate = {2019-01-09},
  abstract = {[A new class of density functions depending on a shape parameter {$\lambda$} is introduced, such that {$\lambda$}=0 corresponds to the standard normal density. The properties of this class of density functions are studied.]},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Azzalini_1985_A Class of Distributions Which Includes the Normal Ones.pdf}
}

@article{baathStateNamingConventions2012,
  title = {The {{State}} of {{Naming Conventions}} in {{R}}},
  author = {B{\aa}{\aa}th, Rasmus},
  year = {2012},
  journal = {The R Journal},
  volume = {4},
  number = {2},
  pages = {74},
  issn = {2073-4859},
  doi = {10.32614/RJ-2012-018},
  urldate = {2021-10-17},
  abstract = {Most programming language communities have naming conventions that are generally agreed upon, that is, a set of rules that governs how functions and variables are named. This is not the case with R, and a review of unofficial style guides and naming convention usage on CRAN shows that a number of different naming conventions are currently in use. Some naming conventions are, however, more popular than others and as a newcomer to the R community or as a developer of a new package this could be useful to consider when choosing what naming convention to adopt.},
  langid = {english},
  keywords = {nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bååth_2012_The State of Naming Conventions in R.pdf}
}

@inproceedings{bachBolassoModelConsistent2008,
  title = {Bolasso: Model Consistent {{Lasso}} Estimation through the Bootstrap},
  shorttitle = {Bolasso},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Bach, Francis R.},
  year = {2008},
  month = jul,
  series = {{{ICML}} '08},
  pages = {33--40},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1390156.1390161},
  urldate = {2024-03-03},
  abstract = {We consider the least-square linear regression problem with regularization by the l1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.},
  isbn = {978-1-60558-205-4},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bach_2008_Bolasso.pdf}
}

@article{bachhuberMedicalCannabisLaws2014,
  title = {Medical {{Cannabis Laws}} and {{Opioid Analgesic Overdose Mortality}} in the {{United States}}, 1999-2010},
  author = {Bachhuber, Marcus A. and Saloner, Brendan and Cunningham, Chinazo O. and Barry, Colleen L.},
  year = {2014},
  month = oct,
  journal = {JAMA Internal Medicine},
  volume = {174},
  number = {10},
  pages = {1668--1673},
  issn = {2168-6106},
  doi = {10.1001/jamainternmed.2014.4005},
  urldate = {2022-06-17},
  abstract = {Opioid analgesic overdose mortality continues to rise in the United States, driven by increases in prescribing for chronic pain. Because chronic pain is a major indication for medical cannabis, laws that establish access to medical cannabis may change overdose mortality related to opioid analgesics in states that have enacted them.To determine the association between the presence of state medical cannabis laws and opioid analgesic overdose mortality.A time-series analysis was conducted of medical cannabis laws and state-level death certificate data in the United States from 1999 to 2010; all 50 states were included.Presence of a law establishing a medical cannabis program in the state.Age-adjusted opioid analgesic overdose death rate per 100 000 population in each state. Regression models were developed including state and year fixed effects, the presence of 3 different policies regarding opioid analgesics, and the state-specific unemployment rate.Three states (California, Oregon, and Washington) had medical cannabis laws effective prior to 1999. Ten states (Alaska, Colorado, Hawaii, Maine, Michigan, Montana, Nevada, New Mexico, Rhode Island, and Vermont) enacted medical cannabis laws between 1999 and 2010. States with medical cannabis laws had a 24.8\% lower mean annual opioid overdose mortality rate (95\% CI, -37.5\% to -9.5\%; P\,=\,.003) compared with states without medical cannabis laws. Examination of the association between medical cannabis laws and opioid analgesic overdose mortality in each year after implementation of the law showed that such laws were associated with a lower rate of overdose mortality that generally strengthened over time: year 1 (-19.9\%; 95\% CI, -30.6\% to -7.7\%; P\,=\,.002), year 2 (-25.2\%; 95\% CI, -40.6\% to -5.9\%; P\,=\,.01), year 3 (-23.6\%; 95\% CI, -41.1\% to -1.0\%; P\,=\,.04), year 4 (-20.2\%; 95\% CI, -33.6\% to -4.0\%; P\,=\,.02), year 5 (-33.7\%; 95\% CI, -50.9\% to -10.4\%; P\,=\,.008), and year 6 (-33.3\%; 95\% CI, -44.7\% to -19.6\%; P\,\&lt;\,.001). In secondary analyses, the findings remained similar.Medical cannabis laws are associated with significantly lower state-level opioid overdose mortality rates. Further investigation is required to determine how medical cannabis laws may interact with policies aimed at preventing opioid analgesic overdose.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bachhuber et al_2014_Medical Cannabis Laws and Opioid Analgesic Overdose Mortality in the United.pdf;/Users/nseewald/Zotero/storage/PU7XFUFK/1898878.html}
}

@article{baiocchiReasoningUsingData2021,
  title = {Reasoning {{Using Data}}: {{Two Old Ways}} and {{One New}}},
  shorttitle = {Reasoning {{Using Data}}},
  author = {Baiocchi, Michael and Rodu, Jordan},
  year = {2021},
  journal = {Observational Studies},
  volume = {7},
  number = {1},
  pages = {3--12},
  publisher = {University of Pennsylvania Press},
  issn = {2767-3324},
  doi = {10.1353/obs.2021.0016},
  urldate = {2022-08-19},
  abstract = {Instead of two cultures, the story of the last couple decades of data science is about the interplay between three different types of reasoning using data. Two of these types of reasoning were well known when Breiman wrote his Two Cultures paper -- warranted reasoning (e.g., randomized trials and sampling) and model reasoning (e.g., linear models). Breiman, though he does not appear to have realized it fully, was in fact describing the dynamics arising in a data community that was making progress using the newest, third type of reasoning -- outcome reasoning. In this commentary we clarify this dynamic a bit, and suggest some useful language for identifying and differentiating types of problems better suited for outcome reasoning.},
  keywords = {common task framework,MARA,model reasoning,outcome reasoning,warranted reasoning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Baiocchi_Rodu_2021_Reasoning Using Data.pdf}
}

@article{baiOptimalTreatmentRegimes2017,
  title = {Optimal Treatment Regimes for Survival Endpoints Using a Locally-Efficient Doubly-Robust Estimator from a Classification Perspective},
  author = {Bai, Xiaofei and Tsiatis, Anastasios A. and Lu, Wenbin and Song, Rui},
  year = {2017},
  month = oct,
  journal = {Lifetime Data Analysis},
  volume = {23},
  number = {4},
  pages = {585--604},
  issn = {1572-9249},
  doi = {10.1007/s10985-016-9376-x},
  urldate = {2023-11-03},
  abstract = {A treatment regime at a single decision point is a rule that assigns a treatment, among the available options, to a patient based on the patient's baseline characteristics. The value of a treatment regime is the average outcome of a population of patients if they were all treated in accordance to the treatment regime, where large values are desirable. The optimal treatment regime is a regime which results in the greatest value. Typically, the optimal treatment regime is estimated by positing a regression relationship for the outcome of interest as a function of treatment and baseline characteristics. However, this can lead to suboptimal treatment regimes when the regression model is misspecified. We instead consider value search estimators for the optimal treatment regime where we directly estimate the value for any treatment regime and then maximize this estimator over a class of regimes. For many studies the primary outcome of interest is survival time which is often censored. We derive a locally efficient, doubly robust, augmented inverse probability weighted complete case estimator for the value function with censored survival data and study the large sample properties of this estimator. The optimization is realized from a weighted classification perspective that allows us to use available off the shelf software. In some studies one treatment may have greater toxicity or side effects, thus we also consider estimating a quality adjusted optimal treatment regime that allows a patient to trade some additional risk of death in order to avoid the more invasive treatment.},
  langid = {english},
  keywords = {Classification,Doubly-robust,Observational survival study,Optimal treatment regime,Value search},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bai et al_2017_Optimal treatment regimes for survival endpoints using a locally-efficient.pdf}
}

@article{bakdashRepeatedMeasuresCorrelation2017,
  title = {Repeated {{Measures Correlation}}},
  author = {Bakdash, Jonathan Z. and Marusich, Laura R.},
  year = {2017},
  journal = {Frontiers in Psychology},
  volume = {8},
  pages = {456},
  issn = {1664-1078},
  doi = {10/gfvzmj},
  urldate = {2021-09-10},
  abstract = {Repeated measures correlation (rmcorr) is a statistical technique for determining the common within-individual association for paired measures assessed on two or more occasions for multiple individuals. Simple regression/correlation is often applied to non-independent observations or aggregated data; this may produce biased, specious results due to violation of independence and/or differing patterns between-participants versus within-participants. Unlike simple regression/correlation, rmcorr does not violate the assumption of independence of observations. Also, rmcorr tends to have much greater statistical power because neither averaging nor aggregation is necessary for an intra-individual research question. Rmcorr estimates the common regression slope, the association shared among individuals. To make rmcorr accessible, we provide background information for its assumptions and equations, visualization, power, and tradeoffs with rmcorr compared to multilevel modeling. We introduce the R package (rmcorr) and demonstrate its use for inferential statistics and visualization with two example datasets. The examples are used to illustrate research questions at different levels of analysis, intra-individual, and inter-individual. Rmcorr is well-suited for research questions regarding the common linear association in paired repeated measures data. All results are fully reproducible.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bakdash_Marusich_2017_Repeated Measures Correlation.pdf}
}

@article{bakerHowMuchShould2022,
  title = {How Much Should We Trust Staggered Difference-in-Differences Estimates?},
  author = {Baker, Andrew C. and Larcker, David F. and Wang, Charles C. Y.},
  year = {2022},
  month = may,
  journal = {Journal of Financial Economics},
  volume = {144},
  number = {2},
  pages = {370--395},
  issn = {0304-405X},
  doi = {10.1016/j.jfineco.2022.01.004},
  urldate = {2023-12-06},
  abstract = {We explain when and how staggered difference-in-differences regression estimators, commonly applied to assess the impact of policy changes, are biased. These biases are likely to be relevant for a large portion of research settings in finance, accounting, and law that rely on staggered treatment timing, and can result in Type-I and Type-II errors. We summarize three alternative estimators developed in the econometrics and applied literature for addressing these biases, including their differences and tradeoffs. We apply these estimators to re-examine prior published results and show, in many cases, the alternative causal estimates or inferences differ substantially from prior papers.},
  keywords = {Difference in differences,dynamic treatment effects,Dynamic treatment effects,generalized difference-in-differences,Generalized difference-in-differences,Staggered difference-in-differences,staggered difference-in-differences designs,Treatment effect heterogeneity},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Baker et al_2022_How much should we trust staggered difference-in-differences estimates.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Baker et al_2022_How much should we trust staggered difference-in-differences estimates2.pdf;/Users/nseewald/Zotero/storage/C7M48FX4/S0304405X22000204.html;/Users/nseewald/Zotero/storage/LFBT9SHV/papers.html}
}

@article{bakerRandomizedTrialsReal2008,
  title = {Randomized Trials for the Real World: Making as Few and as Reasonable Assumptions as Possible},
  shorttitle = {Randomized Trials for the Real World},
  author = {Baker, Stuart G and Kramer, Barnett S},
  year = {2008},
  month = jun,
  journal = {Statistical Methods in Medical Research},
  volume = {17},
  number = {3},
  pages = {243--252},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280207080640},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Baker_Kramer_2008_Randomized trials for the real world.pdf}
}

@article{baksalaryPropertiesMatrixPartial1989,
  title = {Some Properties of Matrix Partial Orderings},
  author = {Baksalary, Jerzy K. and Pukelsheim, Friedrich and Styan, George P.H.},
  year = {1989},
  month = jul,
  journal = {Linear Algebra and its Applications},
  volume = {119},
  pages = {57--85},
  issn = {00243795},
  doi = {10/bdcgnf},
  urldate = {2019-04-09},
  abstract = {The matrix partial orderings considered are: (1) the star ordering and (2) the minus ordering or rank subtractivity, both in the set of m X n complex matrices, and (3) the L\&vner ordering, in the set of m X m matrices. The problems discussed are: (1) inheriting certain properties under a given ordering, (2) preserving an ordering under some matrix multiplications, (3) relationships between an ordering among direct (or Kronecker) and Hadamard products and the corresponding orderings between the factors involved, (4) orderings between generalized inverses of a given matrix, and (5) preserving or reversing a given ordering under generalized inversions. Several generalizations of results known in the literature and a number of new results are derived.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Baksalary et al_1989_Some properties of matrix partial orderings.pdf}
}

@article{balanTutorialFrailtyModels,
  title = {A Tutorial on Frailty Models},
  author = {Balan, Theodor A and Putter, Hein},
  year = {2020},
  journal = {Statistical Methods in Medical Research},
  doi = {10/ggxvq7},
  abstract = {The hazard function plays a central role in survival analysis. In a homogeneous population, the distribution of the time to event, described by the hazard, is the same for each individual. Heterogeneity in the distributions can be accounted for by including covariates in a model for the hazard, for instance a proportional hazards model. In this model, individuals with the same value of the covariates will have the same distribution. It is natural to think that not all covariates that are thought to influence the distribution of the survival outcome are included in the model. This implies that there is unobserved heterogeneity; individuals with the same value of the covariates may have different distributions. One way of accounting for this unobserved heterogeneity is to include random effects in the model. In the context of hazard models for time to event outcomes, such random effects are called frailties, and the resulting models are called frailty models. In this tutorial, we study frailty models for survival outcomes. We illustrate how frailties induce selection of healthier individuals among survivors, and show how shared frailties can be used to model positively dependent survival outcomes in clustered data. The Laplace transform of the frailty distribution plays a central role in relating the hazards, conditional on the frailty, to hazards and survival functions observed in a population. Available software, mainly in R, will be discussed, and the use of frailty models is illustrated in two different applications, one on center effects and the other on recurrent events.},
  keywords = {nosource,Researcher App},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Balan_Putter_2020_A tutorial on frailty models.pdf}
}

@article{ballartImpactLegislationRequiring1995,
  title = {Impact of Legislation Requiring Moped and Motorbike Riders to Wear Helmets},
  author = {Ballart, Xavier and Riba, Clara},
  year = {1995},
  month = oct,
  journal = {Evaluation and Program Planning},
  volume = {18},
  number = {4},
  pages = {311--320},
  issn = {0149-7189},
  doi = {10.1016/0149-7189(95)00032-1},
  urldate = {2024-06-05},
  abstract = {In 1992, a package of measures was adopted by the city of Barcelona to obligate riders of motorcycles with a power capacity inferior to 125 cc to use helmets, until then only compulsory for larger motorcycles. Interrupted time series analyses with the use of a control group is used to investigate the effect of these measures on the number of serious injuries and number of deaths. The results indicate a significant decrease in the number of serious cases in the treated group---smaller motorcycles and mopeds---a decrease which does not occur in the control group ---larger motorcycles. The impact of the adopted measures is abrupt and permanent but the change in riders helmet wearing habits has not been influenced so much by the media campaign as by the coming into force of the legislation with the potential application of sanctions.},
  file = {/Users/nseewald/Zotero/storage/LPNDXN9S/0149718995000321.html}
}

@article{bandaraEffectsMarylandMedicaid2020,
  title = {The Effects of the {{Maryland Medicaid Health Home Waiver}} on {{Emergency Department}} and Inpatient Utilization among Individuals with Serious Mental Illness},
  author = {Bandara, Sachini N. and {Kennedy-Hendricks}, Alene and Stuart, Elizabeth A. and Barry, Colleen L. and Abrams, Michael T. and Daumit, Gail L. and McGinty, Emma E.},
  year = {2020},
  month = may,
  journal = {General Hospital Psychiatry},
  volume = {64},
  pages = {99--104},
  issn = {0163-8343},
  doi = {10.1016/j.genhosppsych.2019.12.004},
  urldate = {2022-04-29},
  abstract = {Objective The Maryland Medicaid health home program, established through the Affordable Care Act's Medicaid health home waiver, integrates primary care services into specialty mental health programs for adults with serious mental illness (SMI). We evaluated the effect of this program on all-cause, physical, and behavioral health emergency department (ED) and inpatient utilization. Method Using marginal structural modeling to control for time-invariant and time-varying confounding, we analyzed Medicaid administrative claims data for 12,232 enrollees with SMI from October 1, 2012 to December 31, 2016; 3319 individuals were enrolled in a BHH and 8913 were never enrolled. Results Health home enrollment was associated with reduced probability of all-cause (PP: 0.23 BHH enrollment vs. 0.26 non-enrollment, p~{$<~$}0.01) and physical health ED visits (PP: 0.21 BHH enrollment vs. 0.24 non-enrollment, p~{$<~$}0.01) and no effect on inpatient admissions per person-three-month period. Conclusion These results suggest the Maryland Medicaid health home waiver's focus on supporting physical health care coordination by specialty mental health programs may be preventing ED visits among adults with SMI, although effect sizes are small.},
  langid = {english},
  keywords = {Behavioral health home,Care coordination,Medicaid,Serious mental illness},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bandara et al_2020_The effects of the Maryland Medicaid Health Home Waiver on Emergency Department.pdf;/Users/nseewald/Zotero/storage/8TCKDKDG/S0163834319303950.html}
}

@article{bandaraTrendsOpioidNonopioid2022,
  title = {Trends in Opioid and Non-Opioid Treatment for Chronic Non-Cancer Pain and Cancer Pain among Privately Insured Adults in the {{United States}}, 2012--2019},
  author = {Bandara, Sachini and Bicket, Mark C. and McGinty, Emma E.},
  year = {2022},
  month = aug,
  journal = {PLOS ONE},
  volume = {17},
  number = {8},
  pages = {e0272142},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0272142},
  urldate = {2024-05-01},
  abstract = {Recent clinical guidelines have emphasized non-opioid treatments in lieu of prescription opioids for chronic non-cancer pain, exempting cancer patients from these recommendations. In this study, we determine trends in opioid and non-opioid treatment among privately insured adults with chronic non-cancer pain (CNCP) or cancer. Using administrative claims data from IBM MarketScan Research Databases, we identified privately-insured adults who were continuously enrolled in insurance for at least one calendar year from 2012 to 2019. We identified individuals with CNCP diagnosis, defined as a diagnosis of arthritis, headache, low back pain, and/or neuropathic pain, and a individuals with cancer diagnosis in a calendar year. Outcomes included receipt of any opioid, non-opioid medication, or non-pharmacologic CNCP therapy and opioid prescribing volume, MME-per-day, and days' supply. Estimates were regression-adjusted for age, sex, and region. Between 2012 and 2019, the proportion of patients who received any opioid decreased across both groups (CNCP: 49.7 to 30.5\%, p{$<$}0.01; cancer: 86.0 to 78.7\%, p{$<$}0.01). Non-opioid pain medication receipt remained steady for individuals with CNCP (66.7 to 66.4\%, p{$<$}0.01) and increased for individuals with cancer (74.4 to 78.8\%, p{$<$}0.01), while non-pharmacologic therapy use rose among individuals with CNCP (62.4 to 66.1\%, p{$<$}0.01). Among those prescribed opioids, there was a decrease in the receipt of at least one prescription with {$>$}90 MME/day (CNCP: 13.9\% in 2012 to 4.9\% in 2019, p{$<$}0.01; Cancer: 26.2\% to 7.6\%, p{$<$}0.01); {$>$}7 days of supply (CNCP: 56.3\% to 30.7\%, p {$<$}0.01; Cancer: 47.5\% to 22.7\%, p{$<$}0.01), the mean number of opioid prescriptions (CNCP: 5.2 to 3.9, p{$<$}0.01; Cancer: 4.0 to 2.7, p{$<$}0.01) and mean MME/day (CNCP: 49.9 to 38.0, p{$<$}0.01; Cancer: 62.4 to 44.7, p{$<$}0.01). Overall, from 2012--2019, opioid prescribing declined for CNCP and cancer, with larger reductions for patients with CNCP. For both groups, reductions in prescribed opioids outpaced increases in non-opioid alternatives.},
  langid = {english},
  keywords = {Cancer detection and diagnosis,Cancer treatment,Cancers and neoplasms,Neuropathic pain,Opioids,Pain,Pain management,Treatment guidelines},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bandara_et_al_2022_Trends_in_opioid_and_non-opioid_treatment_for_chronic_non-cancer_pain_and.pdf}
}

@article{bartoRecentAdvancesHierarchical2003,
  title = {Recent {{Advances}} in {{Hierarchical Reinforcement Learning}}},
  author = {Barto, Andrew G and Mahadevan, Sridhar},
  year = {2003},
  journal = {Discrete Event Dynamic Systems: Theory and Applications},
  volume = {13},
  pages = {41--77},
  keywords = {Multiple DOI},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Barto_Mahadevan_2003_Recent Advances in Hierarchical Reinforcement Learning.pdf}
}

@article{bassoReinforcementLearningNonStationary,
  title = {Reinforcement {{Learning}} in {{Non-Stationary Continuous Time}} and {{Space Scenarios}}},
  author = {Basso, Eduardo W and Engel, Paulo M},
  pages = {10},
  abstract = {In this paper we propose a neural architecture for solving continuous time and space reinforcement learning problems in non-stationary environments. The method is based on a mechanism for creating, updating and selecting partial models of the environment. The partial models are incrementally estimated using linear approximation functions and are built according to the system's capability of making predictions regarding a given sequence of observations. We propose, formalize and show the efficiency of this method in the non-stationary pendulum task. We show that the neural architecture with context detection performs better than a model-based RL algorithm and that it performs almost as well as the optimum, that is, a hypothetical system with extended sensor capabilities in a way that the environment effectively appears to be stationary. Finally, we present known limitations of the method and future works.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Basso_Engel_Reinforcement Learning in Non-Stationary Continuous Time and Space Scenarios.pdf}
}

@article{bauerIntroductionImplementationScience2015,
  title = {An Introduction to Implementation Science for the Non-Specialist},
  author = {Bauer, Mark S. and Damschroder, Laura and Hagedorn, Hildi and Smith, Jeffrey and Kilbourne, Amy M.},
  year = {2015},
  month = dec,
  journal = {BMC Psychology},
  volume = {3},
  number = {1},
  pages = {32},
  issn = {2050-7283},
  doi = {10/f72mpn},
  urldate = {2019-11-30},
  abstract = {Background: The movement of evidence-based practices (EBPs) into routine clinical usage is not spontaneous, but requires focused efforts. The field of implementation science has developed to facilitate the spread of EBPs, including both psychosocial and medical interventions for mental and physical health concerns. Discussion: The authors aim to introduce implementation science principles to non-specialist investigators, administrators, and policymakers seeking to become familiar with this emerging field. This introduction is based on published literature and the authors' experience as researchers in the field, as well as extensive service as implementation science grant reviewers. Implementation science is ``the scientific study of methods to promote the systematic uptake of research findings and other EBPs into routine practice, and, hence, to improve the quality and effectiveness of health services.'' Implementation science is distinct from, but shares characteristics with, both quality improvement and dissemination methods. Implementation studies can be either assess naturalistic variability or measure change in response to planned intervention. Implementation studies typically employ mixed quantitative-qualitative designs, identifying factors that impact uptake across multiple levels, including patient, provider, clinic, facility, organization, and often the broader community and policy environment. Accordingly, implementation science requires a solid grounding in theory and the involvement of trans-disciplinary research teams. Summary: The business case for implementation science is clear: As healthcare systems work under increasingly dynamic and resource-constrained conditions, evidence-based strategies are essential in order to ensure that research investments maximize healthcare value and improve public health. Implementation science plays a critical role in supporting these efforts.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bauer et al_2015_An introduction to implementation science for the non-specialist.pdf}
}

@article{bekierDoesCannabisUse2024,
  title = {Does Cannabis Use Substitute for Opioids? {{A}} Preliminary Exploratory Survey in Opioid Maintenance Patients},
  shorttitle = {Does Cannabis Use Substitute for Opioids?},
  author = {Bekier, Nina Kim and Frischknecht, Ulrich and Eidenmueller, Katharina and Grimm, Franz and Bach, Patrick and Stenger, Manuel and Kiefer, Falk and Hermann, Derik},
  year = {2024},
  month = mar,
  journal = {European Archives of Psychiatry and Clinical Neuroscience},
  issn = {1433-8491},
  doi = {10.1007/s00406-023-01718-3},
  urldate = {2024-03-26},
  abstract = {Various studies showed that people with substance use disorder use cannabis to reduce withdrawal or dose of their main drug. Using a questionnaire about their cannabis use, 118 participants in an opioid maintenance treatment (OMT) in Germany were examined regarding this strategy. 60\% reported to use cannabis. Of those, 72\% were using cannabis in the suggested way. Cannabis was used to substitute for, e.g., heroin (44.8\%) and benzodiazepines (16.4\%). We also asked for an estimation of how good cannabis was able to substitute for several substances (in German school grades (1 till 6)); heroin average grade: 2.6\,{\textpm}\,1.49. Besides that we asked about the idea of cannabis as ``self-medication'', e.g., to reduce pain (47\%) and about negative consequences from cannabis use. Our results suggest to consider the use of cannabis by patients in OMT rather as a harm reduction strategy to reduce the intake of more dangerous drugs.},
  langid = {english},
  keywords = {Cannabis,Methadone,OMT,Opioid addiction,Opioid maintenance treatment},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bekier et al_2024_Does cannabis use substitute for opioids.pdf}
}

@article{ben-michaelAugmentedSyntheticControl2021,
  title = {The {{Augmented Synthetic Control Method}}},
  author = {{Ben-Michael}, Eli and Feller, Avi and Rothstein, Jesse},
  year = {2021},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {116},
  number = {536},
  pages = {1789--1803},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.1929245},
  urldate = {2022-01-03},
  abstract = {The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The ``synthetic control'' is a weighted average of control units that balances the treated unit's pretreatment outcomes and other covariates as closely as possible. A critical feature of the original proposal is to use SCM only when the fit on pretreatment outcomes is excellent. We propose Augmented SCM as an extension of SCM to settings where such pretreatment fit is infeasible. Analogous to bias correction for inexact matching, augmented SCM uses an outcome model to estimate the bias due to imperfect pretreatment fit and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pretreatment fit while minimizing extrapolation from the convex hull. This estimator can also be expressed as a solution to a modified synthetic controls problem that allows negative weights on some donor units. We bound the estimation error of this approach under different data-generating processes, including a linear factor model, and show how regularization helps to avoid over-fitting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package.},
  keywords = {bias correction,Bias correction,causal inference,Causal inference,panel data,Panel data,synthetic control,Synthetic control},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ben-Michael et al_2021_The Augmented Synthetic Control Method.pdf;/Users/nseewald/Zotero/storage/454BZWIQ/01621459.2021.html;/Users/nseewald/Zotero/storage/NSXTAQG2/01621459.2021.html}
}

@misc{ben-michaelStatisticalMethodsEstimate2024,
  title = {Statistical Methods to Estimate the Impact of Gun Policy on Gun Violence},
  author = {{Ben-Michael}, Eli and Doucette, Mitchell L. and Feller, Avi and McCourt, Alexander D. and Stuart, Elizabeth A.},
  year = {2024},
  month = apr,
  number = {arXiv:2404.11506},
  eprint = {2404.11506},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.11506},
  urldate = {2024-10-22},
  abstract = {Gun violence is a critical public health and safety concern in the United States. There is considerable variability in policy proposals meant to curb gun violence, ranging from increasing gun availability to deter potential assailants (e.g., concealed carry laws or arming school teachers) to restricting access to firearms (e.g., universal background checks or banning assault weapons). Many studies use state-level variation in the enactment of these policies in order to quantify their effect on gun violence. In this paper, we discuss the policy trial emulation framework for evaluating the impact of these policies, and show how to apply this framework to estimating impacts via difference-in-differences and synthetic controls when there is staggered adoption of policies across jurisdictions, estimating the impacts of right-to-carry laws on violent crime as a case study.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ben-Michael et al. - 2024 - Statistical methods to estimate the impact of gun policy on gun violence.pdf;/Users/nseewald/Zotero/storage/5TSU45B9/2404.html}
}

@techreport{ben-michaelSyntheticControlsStaggered2021,
  title = {Synthetic {{Controls}} with {{Staggered Adoption}}},
  author = {{Ben-Michael}, Eli and Feller, Avi and Rothstein, Jesse},
  year = {2021},
  month = jun,
  number = {w28886},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w28886},
  urldate = {2021-06-27},
  abstract = {Founded in 1920, the NBER is a private, non-profit, non-partisan organization dedicated to conducting economic research and to disseminating research findings among academics, public policy makers, and business professionals.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ben-Michael et al_2021_Synthetic Controls with Staggered Adoption.pdf;/Users/nseewald/Zotero/storage/MXD56BDD/w28886.html}
}

@article{ben-michaelTrialEmulationApproach2021,
  title = {A {{Trial Emulation Approach}} for {{Policy Evaluations}} with {{Group-level Longitudinal Data}}},
  author = {{Ben-Michael}, Eli and Feller, Avi and Stuart, Elizabeth A.},
  year = {2021},
  month = jul,
  journal = {Epidemiology},
  volume = {32},
  number = {4},
  pages = {533--540},
  issn = {1044-3983},
  doi = {10/gk55kk},
  urldate = {2021-07-06},
  abstract = {To limit the spread of the novel coronavirus, governments across the world implemented extraordinary physical distancing policies, such as stay-at-home orders. Numerous studies aim to estimate the effects of these policies. Many statistical and econometric methods, such as difference-in-differences, leverage repeated measurements, and variation in timing to estimate policy effects, including in the COVID19 context. Although these methods are less common in epidemiology, epidemiologic researchers are well accustomed to handling similar complexities in studies of individual-level interventions. Target trial emulation emphasizes the need to carefully design a nonexperimental study in terms of inclusion and exclusion criteria, covariates, exposure definition, and outcome measurement---and the timing of those variables. We argue that policy evaluations using group-level longitudinal (``panel'') data need to take a similar careful approach to study design that we refer to as policy trial emulation. This approach is especially important when intervention timing varies across jurisdictions; the main idea is to construct target trials separately for each treatment cohort (states that implement the policy at the same time) and then aggregate. We present a stylized analysis of the impact of state-level stay-at-home orders on total coronavirus cases. We argue that estimates from panel methods---with the right data and careful modeling and diagnostics---can help add to our understanding of many policies, though doing so is often challenging.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ben-Michael et al_2021_A Trial Emulation Approach for Policy Evaluations with Group-level Longitudinal.pdf}
}

@article{ben-zeevStrategiesMHealthResearch2015,
  title = {Strategies for {{mHealth Research}}: {{Lessons}} from 3 {{Mobile Intervention Studies}}},
  shorttitle = {Strategies for {{mHealth Research}}},
  author = {{Ben-Zeev}, Dror and Schueller, Stephen M. and Begale, Mark and Duffecy, Jennifer and Kane, John M. and Mohr, David C.},
  year = {2015},
  month = mar,
  journal = {Administration and Policy in Mental Health and Mental Health Services Research},
  volume = {42},
  number = {2},
  pages = {157--167},
  issn = {0894-587X, 1573-3289},
  doi = {10.1007/s10488-014-0556-2},
  urldate = {2018-10-12},
  abstract = {The capacity of Mobile Health (mHealth) technologies to propel healthcare forward is directly linked to the quality of mobile interventions developed through careful mHealth research. mHealth research entails several unique characteristics, including collaboration with technologists at all phases of a project, reliance on regional telecommunication infrastructure and commercial mobile service providers, and deployment and evaluation of interventions ``in the wild'', with participants using mobile tools in uncontrolled environments. In the current paper, we summarize the lessons our multi-institutional/multi-disciplinary team has learned conducting a range of mHealth projects using mobile phones with diverse clinical populations. First, we describe three ongoing projects that we draw from to illustrate throughout the paper. We then provide an example for multidisciplinary teamwork and conceptual mHealth intervention development that we found to be particularly useful. Finally, we discuss mHealth research challenges (i.e. evolving technology, mobile phone selection, user characteristics, the deployment environment, and mHealth system ``bugs and glitches''), and provide recommendations for identifying and resolving barriers, or preventing their occurrence altogether.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ben-Zeev et al_2015_Strategies for mHealth Research.pdf}
}

@article{benjaminiControllingFalseDiscovery1995,
  title = {Controlling the {{False Discovery Rate}}: {{A Practical}} and {{Powerful Approach}} to {{Multiple Testing}}},
  author = {Benjamini, Yoav and Hochberg, Yosef},
  year = {1995},
  journal = {Journal of the Royal Statistical Society Series B (Methodological)},
  volume = {57},
  number = {1},
  pages = {289--300},
  abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses -the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Benjamini_Hochberg_1995_Controlling the False Discovery Rate.pdf}
}

@article{benjetPrecisionTreatmentModel2023,
  title = {A {{Precision Treatment Model}} for {{Internet-Delivered Cognitive Behavioral Therapy}} for {{Anxiety}} and {{Depression Among University Students}}: {{A Secondary Analysis}} of a {{Randomized Clinical Trial}}},
  shorttitle = {A {{Precision Treatment Model}} for {{Internet-Delivered Cognitive Behavioral Therapy}} for {{Anxiety}} and {{Depression Among University Students}}},
  author = {Benjet, Corina and Zainal, Nur Hani and Albor, Yesica and {Alvis-Barranco}, Libia and {Carrasco-Tapias}, Nayib and {Contreras-Ib{\'a}{\~n}ez}, Carlos C. and {Cudris-Torres}, Lorena and {de la Pe{\~n}a}, Francisco R. and Gonz{\'a}lez, No{\'e} and {Guerrero-L{\'o}pez}, Jos{\'e} Benjam{\'i}n and {Gutierrez-Garcia}, Ra{\'u}l A. and {Jim{\'e}nez-Per{\'e}z}, Ana Luc{\'i}a and {Medina-Mora}, Maria Elena and Pati{\~n}o, Pamela and Cuijpers, Pim and Gildea, Sarah M. and Kazdin, Alan E. and Kennedy, Chris J. and Luedtke, Alex and Sampson, Nancy A. and Petukhova, Maria V. and Kessler, Ronald C.},
  year = {2023},
  month = jun,
  journal = {JAMA Psychiatry},
  issn = {2168-622X},
  doi = {10.1001/jamapsychiatry.2023.1675},
  urldate = {2023-06-12},
  abstract = {Guided internet-delivered cognitive behavioral therapy (i-CBT) is a low-cost way to address high unmet need for anxiety and depression treatment. Scalability could be increased if some patients were helped as much by self-guided i-CBT as guided i-CBT.To develop an individualized treatment rule using machine learning methods for guided i-CBT vs self-guided i-CBT based on a rich set of baseline predictors.This prespecified secondary analysis of an assessor-blinded, multisite randomized clinical trial of guided i-CBT, self-guided i-CBT, and treatment as usual included students in Colombia and Mexico who were seeking treatment for anxiety (defined as a 7-item Generalized Anxiety Disorder [GAD-7] score of {$\geq$}10) and/or depression (defined as a 9-item Patient Health Questionnaire [PHQ-9] score of {$\geq$}10). Study recruitment was from March 1 to October 26, 2021. Initial data analysis was conducted from May 23 to October 26, 2022.Participants were randomized to a culturally adapted transdiagnostic i-CBT that was guided (n\,=\,445), self-guided (n\,=\,439), or treatment as usual (n\,=\,435).Remission of anxiety (GAD-7 scores of {$\leq$}4) and depression (PHQ-9 scores of {$\leq$}4) 3 months after baseline.The study included 1319 participants (mean [SD] age, 21.4 [3.2] years; 1038 women [78.7\%]; 725 participants [55.0\%] came from Mexico). A total of 1210 participants (91.7\%) had significantly higher mean (SE) probabilities of joint remission of anxiety and depression with guided i-CBT (51.8\% [3.0\%]) than with self-guided i-CBT (37.8\% [3.0\%]; P\,=\,.003) or treatment as usual (40.0\% [2.7\%]; P\,=\,.001). The remaining 109 participants (8.3\%) had low mean (SE) probabilities of joint remission of anxiety and depression across all groups (guided i-CBT: 24.5\% [9.1\%]; P\,=\,.007; self-guided i-CBT: 25.4\% [8.8\%]; P\,=\,.004; treatment as usual: 31.0\% [9.4\%]; P\,=\,.001). All participants with baseline anxiety had nonsignificantly higher mean (SE) probabilities of anxiety remission with guided i-CBT (62.7\% [5.9\%]) than the other 2 groups (self-guided i-CBT: 50.2\% [6.2\%]; P\,=\,.14; treatment as usual: 53.0\% [6.0\%]; P\,=\,.25). A total of 841 of 1177 participants (71.5\%) with baseline depression had significantly higher mean (SE) probabilities of depression remission with guided i-CBT (61.5\% [3.6\%]) than the other 2 groups (self-guided i-CBT: 44.3\% [3.7\%]; P\,=\,.001; treatment as usual: 41.8\% [3.2\%]; P\,\&lt;\,.001). The other 336 participants (28.5\%) with baseline depression had nonsignificantly higher mean (SE) probabilities of depression remission with self-guided i-CBT (54.4\% [6.0\%]) than guided i-CBT (39.8\% [5.4\%]; P\,=\,.07).Guided i-CBT yielded the highest probabilities of remission of anxiety and depression for most participants; however, these differences were nonsignificant for anxiety. Some participants had the highest probabilities of remission of depression with self-guided i-CBT. Information about this variation could be used to optimize allocation of guided and self-guided i-CBT in resource-constrained settings.ClinicalTrials.gov Identifier: NCT04780542},
  file = {/Users/nseewald/Zotero/storage/HEL5N3ZZ/2805591.html}
}

@article{bergerLikelihoodPrinciple1988,
  title = {The {{Likelihood Principle}}},
  author = {Berger, James O. and Wolpert, Robert L. and Bayarri, M. J. and DeGroot, M. H. and Hill, Bruce M. and Lane, David A. and LeCam, Lucien},
  year = {1988},
  journal = {Lecture Notes-Monograph Series},
  volume = {6},
  eprint = {4355509},
  eprinttype = {jstor},
  pages = {iii-v+vii- xii+1-199},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Berger et al_1988_The Likelihood Principle.pdf}
}

@article{berkCausalInferenceChallenges2022,
  title = {Causal {{Inference Challenges}} with {{Interrupted Time Series Designs}}: {{An Evaluation}} of an {{Assault Weapons Ban}} in {{California}}},
  shorttitle = {Causal {{Inference Challenges}} with {{Interrupted Time Series Designs}}},
  author = {Berk, Richard A.},
  year = {2022},
  journal = {Observational Studies},
  volume = {8},
  number = {1},
  pages = {1--27},
  publisher = {University of Pennsylvania Press},
  issn = {2767-3324},
  urldate = {2024-05-30},
  abstract = {The interrupted time series design was introduced to social scientists in 1963 by Campbell and Stanley, analysis methods were proposed by Box and Tiao in 1975, and more recent treatments are easily found (Box et al., 2016). Despite its popularity, current results in statistics reveal fundamental oversights in the standard statistical methods employed. Adaptive model selection built into recommended practice causes challenging problems for post-model-selection-inference. What one might call model cherry picking can invalidate conventional statistical inference, statistical tests and confidence intervals with damaging consequences for causal inference. There are technical developments that can correct for these problems, but these remedies raise conceptual difficulties for causal inference when proper estimands are defined. The issues are illustrated with an analysis of the impact of an assault weapons ban on daily handgun sales in California from 1996 through 2018. Statistically valid regression functionals are obtained, but their causal meaning is unclear. Researchers might be best served by interpreting only the sign of such functionals.},
  keywords = {Causal Inference,Firearm Policy,Gun Sales,Interrupted Time Series Analysis,Post Selection Statistical Inference,Transfer Functions},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Berk_2022_Causal Inference Challenges with Interrupted Time Series Designs.pdf}
}

@article{bernalInterruptedTimeSeries2017,
  title = {Interrupted Time Series Regression for the Evaluation of Public Health Interventions: A Tutorial},
  shorttitle = {Interrupted Time Series Regression for the Evaluation of Public Health Interventions},
  author = {Bernal, James Lopez and Cummins, Steven and Gasparrini, Antonio},
  year = {2017},
  month = feb,
  journal = {International Journal of Epidemiology},
  volume = {46},
  number = {1},
  pages = {348--355},
  issn = {0300-5771},
  doi = {10/gdtnj9},
  urldate = {2020-12-12},
  abstract = {Interrupted time series (ITS) analysis is a valuable study design for evaluating the effectiveness of population-level health interventions that have been implemented at a clearly defined point in time. It is increasingly being used to evaluate the effectiveness of interventions ranging from clinical therapy to national public health legislation. Whereas the design shares many properties of regression-based approaches in other epidemiological studies, there are a range of unique features of time series data that require additional methodological considerations. In this tutorial we use a worked example to demonstrate a robust approach to ITS analysis using segmented regression. We begin by describing the design and considering when ITS is an appropriate design choice. We then discuss the essential, yet often omitted, step of proposing the impact model a priori. Subsequently, we demonstrate the approach to statistical analysis including the main segmented regression model. Finally we describe the main methodological issues associated with ITS analysis: over-dispersion of time series data, autocorrelation, adjusting for seasonal trends and controlling for time-varying confounders, and we also outline some of the more complex design adaptations that can be used to strengthen the basic ITS design.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bernal et al_2017_Interrupted time series regression for the evaluation of public health.pdf;/Users/nseewald/Zotero/storage/TZANNQIH/2622842.html}
}

@article{bernalInterruptedTimeSeries2017a,
  title = {Interrupted Time Series Regression for the Evaluation of Public Health Interventions: A Tutorial},
  shorttitle = {Interrupted Time Series Regression for the Evaluation of Public Health Interventions},
  author = {Bernal, James Lopez and Cummins, Steven and Gasparrini, Antonio},
  year = {2017},
  month = feb,
  journal = {International Journal of Epidemiology},
  volume = {46},
  number = {1},
  pages = {348--355},
  issn = {0300-5771},
  doi = {10.1093/ije/dyw098},
  urldate = {2024-05-28},
  abstract = {Interrupted time series (ITS) analysis is a valuable study design for evaluating the effectiveness of population-level health interventions that have been implemented at a clearly defined point in time. It is increasingly being used to evaluate the effectiveness of interventions ranging from clinical therapy to national public health legislation. Whereas the design shares many properties of regression-based approaches in other epidemiological studies, there are a range of unique features of time series data that require additional methodological considerations. In this tutorial we use a worked example to demonstrate a robust approach to ITS analysis using segmented regression. We begin by describing the design and considering when ITS is an appropriate design choice. We then discuss the essential, yet often omitted, step of proposing the impact model a priori. Subsequently, we demonstrate the approach to statistical analysis including the main segmented regression model. Finally we describe the main methodological issues associated with ITS analysis: over-dispersion of time series data, autocorrelation, adjusting for seasonal trends and controlling for time-varying confounders, and we also outline some of the more complex design adaptations that can be used to strengthen the basic ITS design.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bernal et al_2017_Interrupted time series regression for the evaluation of public health2.pdf;/Users/nseewald/Zotero/storage/DJ56H6DA/2622842.html}
}

@article{berryMultiplicitiesCancerResearch2012,
  title = {Multiplicities in {{Cancer Research}}: {{Ubiquitous}} and {{Necessary Evils}}},
  shorttitle = {Multiplicities in {{Cancer Research}}},
  author = {Berry, D.},
  year = {2012},
  month = aug,
  journal = {JNCI Journal of the National Cancer Institute},
  volume = {104},
  number = {15},
  pages = {1125--1133},
  issn = {0027-8874, 1460-2105},
  doi = {10.1093/jnci/djs301},
  urldate = {2018-10-18},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Berry_2012_Multiplicities in Cancer Research.pdf}
}

@article{bertrandHowMuchShould2004,
  title = {How {{Much Should We Trust Differences-In-Differences Estimates}}?},
  shorttitle = {How {{Much Should We Trust Differences-In-Differences Estimates}}?},
  author = {Bertrand, Marianne and Duflo, Esther and Mullainathan, Sendhil},
  year = {2004},
  month = feb,
  journal = {The Quarterly Journal of Economics},
  volume = {119},
  number = {1},
  pages = {249--275},
  issn = {0033-5533},
  doi = {10/ck2cfn},
  urldate = {2021-07-02},
  abstract = {Most papers that employ Differences-in-Differences estimation (DD) use many years of data and focus on serially correlated outcomes but ignore that the resulting standard errors are inconsistent. To illustrate the severity of this issue, we randomly generate placebo laws in state-level data on female wages from the Current Population Survey. For each law, we use OLS to compute the DD estimate of its ``effect'' as well as the standard error of this estimate. These conventional DD standard errors severely understate the standard deviation of the estimators: we find an ``effect'' significant at the 5 percent level for up to 45 percent of the placebo interventions. We use Monte Carlo simulations to investigate how well existing methods help solve this problem. Econometric corrections that place a specific parametric form on the time-series process do not perform well. Bootstrap (taking into account the autocorrelation of the data) works well when the number of states is large enough. Two corrections based on asymptotic approximation of the variance-covariance matrix work well for moderate numbers of states and one correction that collapses the time series information into a ``pre''- and ``post''-period and explicitly takes into account the effective sample size works well even for small numbers of states.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bertrand et al_2004_How Much Should We Trust Differences-In-Differences Estimates.pdf;/Users/nseewald/Zotero/storage/S7NFNQF7/1876068.html}
}

@article{bhatlaChangesPatientCare2025,
  title = {Changes in {{Patient Care Experience After Private Equity Acquisition}} of {{US Hospitals}}},
  author = {Bhatla, Anjali and Bartlett, Victoria L. and Liu, Michael and Zheng, ZhaoNian and Wadhera, Rishi K.},
  year = {2025},
  month = jan,
  journal = {JAMA},
  issn = {0098-7484},
  doi = {10.1001/jama.2024.23450},
  urldate = {2025-01-10},
  abstract = {OBJECTIVE To evaluate whether the acquisition of hospitals by private equity firms was associated with changes in measures of patient-reported experience compared with matched control hospitals from 2008 through 2019. DESIGN, SETTINGS, AND PARTICIPANTS This cohort study identified 73 US hospitals newly acquired by private equity firms and 293 matched control (nonacquired) US hospitals from 2008 through 2019. An event study, difference-in-differences design was used to evaluate changes in patient experiences measures from 3 years before to 3 years after private equity acquisition. MAIN OUTCOMES AND MEASURES The primary outcomes were 2 global measures of patient-reported care experience from the Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS) survey, which included patients' overall hospital rating and willingness to recommend the hospital. Secondary outcomes included the 7 other HCAHPS measures encompassing clinical process, communication, and environmental measures. RESULTS There were 73 private equity--acquired hospitals and 293 matched control hospitals. The percentage of patients rating hospitals as a 9 or 10, on a scale of 0 to 10, decreased at private equity--acquired hospitals (65.0\% before acquisition and 65.2\% after acquisition) when compared with control hospitals (66.2\% to 69.2\%) during the postacquisition period relative to the preacquisition period with a difference-in-differences estimate of -2.4 percentage points (95\% CI, -3.9 to -0.9). In addition, the percentage of patients who would definitely recommend the hospital also decreased at private equity--acquired hospitals (66.9\% before acquisition and 65.5\% after acquisition) compared with control hospitals (68.2\% to 69.3\%) with a difference-in-difference estimate of -2.1 percentage points (95\% CI, -3.6 to -0.7). For both of these global measures of patient experience, the difference between private equity--acquired and control hospitals increased over time and was largest in year 3 after acquisition (-5.2 percentage points [95\% CI, -8.8 to -1.5] and -4.4 percentage points [95\% CI, -8.0 to -0.70] for each measure, respectively). For secondary measures of patient care experience, there was a decrease in patient-reported responsiveness of hospital staff at private equity--acquired hospitals compared with control hospitals (-1.3 percentage points [95\% CI, -2.4 to -0.2]), but no differential change across other measures of clinical process, communication, and environment. CONCLUSIONS AND RELEVANCE Patient care experience worsened after private equity acquisition of hospitals. These findings raise concern about the implications of private equity acquisitions on patient care experience at US hospitals.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bhatla et al. - 2025 - Changes in Patient Care Experience After Private Equity Acquisition of US Hospitals.pdf;/Users/nseewald/Zotero/storage/7NCQVMW5/2829041.html}
}

@article{bicketUseCannabisOther2023,
  title = {Use of {{Cannabis}} and {{Other Pain Treatments Among Adults With Chronic Pain}} in {{US States With Medical Cannabis Programs}}},
  author = {Bicket, Mark C. and Stone, Elizabeth M. and McGinty, Emma E.},
  year = {2023},
  month = jan,
  journal = {JAMA Network Open},
  volume = {6},
  number = {1},
  pages = {e2249797},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2022.49797},
  urldate = {2023-06-27},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bicket et al_2023_Use of Cannabis and Other Pain Treatments Among Adults With Chronic Pain in US.pdf;/Users/nseewald/Zotero/storage/WKGRPY7W/2800119.html}
}

@misc{bilinskiNothingSeeHere2020,
  title = {Nothing to See Here? {{Non-inferiority}} Approaches to Parallel Trends and Other Model Assumptions},
  shorttitle = {Nothing to See Here?},
  author = {Bilinski, Alyssa and Hatfield, Laura A.},
  year = {2020},
  month = jan,
  number = {arXiv:1805.03273},
  eprint = {1805.03273},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.03273},
  urldate = {2023-12-11},
  abstract = {Many causal models make assumptions of "no difference" or "no effect." For example, difference-in-differences (DID) assumes that there is no trend difference between treatment and comparison groups' untreated potential outcomes ("parallel trends"). Tests of these assumptions typically assume a null hypothesis that there is no violation. When researchers fail to reject the null, they consider the assumption to hold. We argue this approach is incorrect and frequently misleading. These tests reverse the roles of Type I and Type II error and have a high probability of missing assumption violations. Even when power is high, they may detect statistically significant violations too small to be of practical importance. We present test reformulations in a non-inferiority framework that rule out violations of model assumptions that exceed some threshold. We then focus on the parallel trends assumption, for which we propose a "one step up" method: 1) reporting treatment effect estimates from a model with a more complex trend difference than is believed to be the case and 2) testing that that the estimated treatment effect falls within a specified distance of the treatment effect from the simpler model. We show that this reduces bias while also considering power, controlling mean-squared error. Our base model also aligns power to detect a treatment effect with power to rule out meaningful violations of parallel trends. We apply our approach to 4 data sets used to analyze the Affordable Care Act's dependent coverage mandate and demonstrate that coverage gains may have been smaller than previously estimated.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bilinski_Hatfield_2020_Nothing_to_see_here.pdf;/Users/nseewald/Zotero/storage/ITHFN5A2/1805.html}
}

@article{bitlerEvidenceRaceWelfare2003,
  title = {Some {{Evidence}} on {{Race}}, {{Welfare Reform}}, and {{Household Income}}},
  author = {Bitler, Marianne P. and Gelbach, Jonah B. and Hoynes, Hilary W.},
  year = {2003},
  journal = {The American Economic Review},
  volume = {93},
  number = {2},
  eprint = {3132242},
  eprinttype = {jstor},
  pages = {293--298},
  publisher = {American Economic Association},
  issn = {0002-8282},
  doi = {10/d97p56},
  urldate = {2021-06-27},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bitler et al_2003_Some Evidence on Race, Welfare Reform, and Household Income.pdf}
}

@article{blandLogrankTest2004,
  title = {The Logrank Test},
  author = {Bland, J. Martin and Altman, Douglas G.},
  year = {2004},
  month = apr,
  journal = {BMJ},
  volume = {328},
  number = {7447},
  pages = {1073},
  publisher = {British Medical Journal Publishing Group},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.328.7447.1073},
  urldate = {2025-03-11},
  abstract = {We often wish to compare the survival experience of two (or more) groups of individuals. For example, the table shows survival times of 51 adult patients with recurrent malignant gliomas1 tabulated by type of tumour and indicating whether the patient had died or was still alive at analysis---that is, their survival time was censored.2 As the figure shows, the survival curves differ, but is this sufficient to conclude that in the population patients with anaplastic astrocytoma have worse survival than patients with glioblastoma? View this table: Weeks to death or censoring in 51 adults with recurrent gliomas1 (A=astrocytoma, G=glioblastoma) Fig 1  Survival curves for women with glioma by diagnosis We could compute survival curves3 for each group and compare the proportions surviving at any specific time. The weakness of this approach is that it does not provide a comparison of the total survival experience of the two groups, but rather gives a comparison at some arbitrary time point(s). In the figure the difference in survival is greater at some times than others and eventually becomes {\dots}},
  chapter = {Education and debate},
  copyright = {{\copyright} 2004 BMJ Publishing Group Ltd.},
  langid = {english},
  pmid = {15117797},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bland and Altman - 2004 - The logrank test.pdf}
}

@article{blochSampleSizeRequirements1986,
  title = {Sample Size Requirements and the Cost of a Randomized Clinical Trial with Repeated Measurements},
  author = {Bloch, Daniel A.},
  year = {1986},
  journal = {Statistics in Medicine},
  volume = {5},
  number = {6},
  pages = {663--667},
  issn = {1097-0258},
  doi = {10/bqvr7z},
  urldate = {2020-06-29},
  abstract = {This paper discusses the advantage of using repeated outcome measurements on subjects in a clinical trial. We choose the number of subjects and the number of repeated outcome measurements to minimize a given cost function so that the clinical trial has power (1 -{$\beta$}) for specified significance level {$<$} {$\alpha$}. An example illustrates the results and emphasizes the importance of study design and of critical evaluation of how one measures the study endpoint.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bloch_1986_Sample size requirements and the cost of a randomized clinical trial with.pdf;/Users/nseewald/Zotero/storage/M9KQWNUZ/sim.html}
}

@article{bloomfieldInefficiencyLeastSquares1975,
  title = {The Inefficiency of Least Squares},
  author = {Bloomfield, Peter and Watson, Geoffrey S.},
  year = {1975},
  month = apr,
  journal = {Biometrika},
  volume = {62},
  number = {1},
  pages = {121--128},
  issn = {0006-3444},
  doi = {10.1093/biomet/62.1.121},
  urldate = {2021-10-17},
  abstract = {Two criteria are set up to judge the relative performance of the least squares estimator and the best linear unbiased estimator of {$\beta$} in the linear model y=X{$\beta$}+u, where E(u) = 0, E(uu{$\prime$}) = {\cyrchar\CYRG}. The matrices X and {\cyrchar\CYRG} are found so that the relative performance of least squares is worst. Both criteria give the same least favourable situation:when {$M$}(X) is any one of the 2k manifolds {$M$}({$\gamma$}1{\textpm}{$\gamma$}n{\dots}{$\gamma$}k{\textpm}{$\gamma$}n-k+1), where {\cyrchar\CYRG}{$\gamma$}i = fi{$\gamma$}i and f1{\IJ}{\dots}{\IJ}fn are fixed, {$M$}(˙) denoting the subspace spanned by the columns of the relevant matrix. The case where allfi may be chosen in a preassigned interval is also discussed. The practical implications of the various results are mentioned.},
  keywords = {nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bloomfield_Watson_1975_The inefficiency of least squares.pdf;/Users/nseewald/Zotero/storage/FXFCH2UV/220163.html}
}

@article{bloomModernRegressionDiscontinuity2012,
  title = {Modern {{Regression Discontinuity Analysis}}},
  author = {Bloom, Howard S.},
  year = {2012},
  month = jan,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {5},
  number = {1},
  pages = {43--82},
  publisher = {Routledge},
  issn = {1934-5747},
  doi = {10.1080/19345747.2011.578707},
  urldate = {2024-06-10},
  abstract = {This article provides a detailed discussion of the theory and practice of modern regression discontinuity (RD) analysis for estimating the effects of interventions or treatments. Part 1 briefly chronicles the history of RD analysis and summarizes its past applications. Part 2 explains how in theory an RD analysis can identify an average effect of treatment for a population and how different types of RD analyses---``sharp'' versus ``fuzzy''---can identify average treatment effects for different conceptual subpopulations. Part 3 of the article introduces graphical methods, parametric statistical methods, and nonparametric statistical methods for estimating treatment effects in practice from regression discontinuity data plus validation tests and robustness tests for assessing these estimates. Section 4 considers generalizing RD findings and presents several different views on and approaches to the issue. Part 5 notes some important issues to pursue in future research about or applications of RD analysis.},
  keywords = {program impacts,regression discontinuity designs,sample design},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bloom_2012_Modern_Regression_Discontinuity_Analysis.pdf}
}

@article{boehnkePillsPotObservational2019,
  title = {Pills to {{Pot}}: {{Observational Analyses}} of {{Cannabis Substitution Among Medical Cannabis Users With Chronic Pain}}},
  shorttitle = {Pills to {{Pot}}},
  author = {Boehnke, Kevin F. and Scott, J. Ryan and Litinas, Evangelos and Sisley, Suzanne and Williams, David A. and Clauw, Daniel J.},
  year = {2019},
  month = jul,
  journal = {The Journal of Pain},
  volume = {20},
  number = {7},
  pages = {830--841},
  issn = {1526-5900},
  doi = {10.1016/j.jpain.2019.01.010},
  urldate = {2024-05-01},
  abstract = {Chronic pain is common, costly, and challenging to treat. Many individuals with chronic pain have turned to cannabis as an alternative form of pain management. We report results from an ongoing, online survey of medical cannabis users with chronic pain nationwide about how cannabis affects pain management, health, and pain medication use. We also examined whether and how these parameters were affected by concomitant recreational use, and duration of use (novice: {$<$}1 year vs experienced: {$\geq$}1 year). There were 1,321 participants (59\% female, 54\% {$\geq$}50 years old) who completed the survey. Consistent with other observational studies, approximately 80\% reported substituting cannabis for traditional pain medications (53\% for opioids, 22\% for benzodiazepines), citing fewer side effects and better symptom management as their rationale for doing so. Medical-only users were older (52 vs 47 years old; P {$<$} .0001), less likely to drink alcohol (66\% vs 79\%, P {$<$} .0001), and more likely to be currently taking opioids (21\% vs 11\%, P {$<$} .0001) than users with a combined recreational and medical history. Compared with novice users, experienced users were more likely to be male (64\% vs 58\%; P {$<$} .0001), take no concomitant pain medications (43\% vs 30\%), and report improved health (74\% vs 67\%; P\,=\,.004) with use. Given that chronic pain is the most common reason for obtaining a medical cannabis license, these results highlight clinically important differences among the changing population of medical cannabis users. More research is needed to better understand effective pain management regimens for medical cannabis users. Perspective: This article presents results that confirm previous clinical studies suggesting that cannabis may be an effective analgesic and potential opioid substitute. Participants reported improved pain, health, and fewer side effects as rationale for substituting. This article highlights how use duration and intentions for use affect reported treatment and substitution effects.},
  keywords = {Cannabis,chronic pain,opioid substitute,pain management,side effects},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Boehnke_et_al_2019_Pills_to_Pot.pdf;/Users/nseewald/Zotero/storage/XH79RX3H/S1526590018307351.html}
}

@book{boenHumanSideStatistical1982,
  title = {The Human Side of Statistical Consulting},
  author = {Boen, James R. and Zahn, Douglas J.},
  year = {1982},
  publisher = {Lifetime Learning Publications},
  address = {Belmont, California},
  isbn = {0-534-97949-1},
  file = {/Users/nseewald/Zotero/storage/AQSJ785Z/pt.html}
}

@article{bolkerGeneralizedLinearMixed2009,
  title = {Generalized Linear Mixed Models: A Practical Guide for Ecology and Evolution},
  shorttitle = {Generalized Linear Mixed Models},
  author = {Bolker, Benjamin M. and Brooks, Mollie E. and Clark, Connie J. and Geange, Shane W. and Poulsen, John R. and Stevens, M. Henry H. and White, Jada-Simone S.},
  year = {2009},
  month = mar,
  journal = {Trends in Ecology \& Evolution},
  volume = {24},
  number = {3},
  pages = {127--135},
  issn = {0169-5347},
  doi = {10.1016/j.tree.2008.10.008},
  urldate = {2021-11-10},
  abstract = {How should ecologists and evolutionary biologists analyze nonnormal data that involve random effects? Nonnormal data such as counts or proportions often defy classical statistical procedures. Generalized linear mixed models (GLMMs) provide a more flexible approach for analyzing nonnormal data when random effects are present. The explosion of research on GLMMs in the last decade has generated considerable uncertainty for practitioners in ecology and evolution. Despite the availability of accurate techniques for estimating GLMM parameters in simple cases, complex GLMMs are challenging to fit and statistical inference such as hypothesis testing remains difficult. We review the use (and misuse) of GLMMs in ecology and evolution, discuss estimation and inference and summarize `best-practice' data analysis procedures for scientists facing this challenge.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bolker et al_2009_Generalized linear mixed models.pdf}
}

@article{bolukbasiManComputerProgrammer2016,
  title = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}? {{Debiasing Word Embeddings}}},
  shorttitle = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}?},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.06520 [cs, stat]},
  eprint = {1607.06520},
  primaryclass = {cs, stat},
  urldate = {2018-10-12},
  abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to ``debias'' the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bolukbasi et al_2016_Man is to Computer Programmer as Woman is to Homemaker.pdf}
}

@article{borganNoteConfidenceIntervals1990,
  title = {A {{Note}} on {{Confidence Intervals}} and {{Bands}} for the {{Survival Function Based}} on {{Transformations}}},
  author = {Borgan, {\O}rnulf and Liest{\o}l, Knut},
  year = {1990},
  journal = {Scandinavian Journal of Statistics},
  volume = {17},
  number = {1},
  eprint = {4616153},
  eprinttype = {jstor},
  pages = {35--41},
  publisher = {[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
  issn = {0303-6898},
  urldate = {2024-02-14},
  abstract = {Pointwise confidence intervals and simultaneous confidence bands for the survival function based on the log-minus-log and arcsine-square root transformations are considered, and their small sample performance is compared with that of their non-transformed counter-parts. For the confidence intervals and "the equal precision" bands a substantial improvement is obtained by using one of the transformed versions. For bands of the Hall-Wellner type there seems to be less reason for using transformations.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Borgan_Liestøl_1990_A Note on Confidence Intervals and Bands for the Survival Function Based on.pdf}
}

@article{bormSimpleSampleSize2007,
  title = {A Simple Sample Size Formula for Analysis of Covariance in Randomized Clinical Trials},
  author = {Borm, George F. and Fransen, Jaap and Lemmens, Wim A. J. G.},
  year = {2007},
  month = dec,
  journal = {Journal of Clinical Epidemiology},
  volume = {60},
  number = {12},
  pages = {1234--1238},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2007.02.006},
  abstract = {OBJECTIVE: Randomized clinical trials that compare two treatments on a continuous outcome can be analyzed using analysis of covariance (ANCOVA) or a t-test approach. We present a method for the sample size calculation when ANCOVA is used. STUDY DESIGN AND SETTING: We derived an approximate sample size formula. Simulations were used to verify the accuracy of the formula and to improve the approximation for small trials. The sample size calculations are illustrated in a clinical trial in rheumatoid arthritis. RESULTS: If the correlation between the outcome measured at baseline and at follow-up is rho, ANCOVA comparing groups of (1-rho(2))n subjects has the same power as t-test comparing groups of n subjects. When on the same data, ANCOVA is used instead of t-test, the precision of the treatment estimate is increased, and the length of the confidence interval is reduced by a factor 1-rho(2). CONCLUSION: ANCOVA may considerably reduce the number of patients required for a trial.},
  langid = {english},
  pmid = {17998077},
  keywords = {Analysis of Variance,Antirheumatic Agents,Arthritis Rheumatoid,Drug Therapy Combination,Humans,Isoxazoles,Leflunomide,Randomized Controlled Trials as Topic,Research Design,Sample Size,Sulfasalazine,Treatment Outcome},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Borm_et_al_2007_A_simple_sample_size_formula_for_analysis_of_covariance_in_randomized_clinical.pdf}
}

@article{bormSimpleSampleSize2007a,
  title = {A Simple Sample Size Formula for Analysis of Covariance in Randomized Clinical Trials},
  author = {Borm, George F. and Fransen, Jaap and Lemmens, Wim A. J. G.},
  year = {2007},
  month = dec,
  journal = {Journal of Clinical Epidemiology},
  volume = {60},
  number = {12},
  pages = {1234--1238},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2007.02.006},
  urldate = {2024-08-19},
  abstract = {Objective Randomized clinical trials that compare two treatments on a continuous outcome can be analyzed using analysis of covariance (ANCOVA) or a t-test approach. We present a method for the sample size calculation when ANCOVA is used. Study Design and Setting We derived an approximate sample size formula. Simulations were used to verify the accuracy of the formula and to improve the approximation for small trials. The sample size calculations are illustrated in a clinical trial in rheumatoid arthritis. Results If the correlation between the outcome measured at baseline and at follow-up is {$\rho$}, ANCOVA comparing groups of (1-{$\rho$}2)n subjects has the same power as t-test comparing groups of n subjects. When on the same data, ANCOVA is used instead of t-test, the precision of the treatment estimate is increased, and the length of the confidence interval is reduced by a factor 1-{$\rho$}2. Conclusion ANCOVA may considerably reduce the number of patients required for a trial.},
  keywords = {Analysis of covariance,Clinical trial,Power,Precision,Sample size,Statistical test},
  file = {/Users/nseewald/Zotero/storage/BAR6HGUF/S0895435607000613.html}
}

@techreport{borusyakRevisitingEventStudy,
  title = {Revisiting {{Event Study Designs}}: {{Robust}} and {{Efficient Estimation}}},
  author = {Borusyak, Kirill},
  pages = {48},
  abstract = {A broad empirical literature uses ``event study,'' or ``difference-in-differences with staggered rollout,'' research designs for treatment effect estimation: settings in which units in the panel receive treatment at different times. We show a series of problems with conventional regression-based two-way fixed effects estimators, both static and dynamic. These problems arise when researchers conflate the identifying assumptions of parallel trends and no anticipatory effects, implicit assumptions that restrict treatment effect heterogeneity, and the specification of the estimand as a weighted average of treatment effects. We then derive the efficient estimator robust to treatment effect heterogeneity for this setting, show that it has a particularly intuitive ``imputation'' form when treatment-effect heterogeneity is unrestricted, characterize its asymptotic behavior, provide tools for inference, and illustrate its attractive properties in simulations. We further discuss appropriate tests for parallel trends, and show how our estimation approach extends to many settings beyond standard event studies.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Borusyak_Revisiting Event Study Designs.pdf}
}

@article{boruvkaAssessingTimeVaryingCausal2018,
  title = {Assessing {{Time-Varying Causal Effect Moderation}} in {{Mobile Health}}},
  author = {Boruvka, Audrey and Almirall, Daniel and Witkiewitz, Katie and Murphy, Susan A.},
  year = {2018},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {523},
  pages = {1112--1121},
  issn = {0162-1459, 1537-274X},
  doi = {10/gfgg6c},
  urldate = {2019-08-28},
  abstract = {In mobile health interventions aimed at behavior change and maintenance, treatments are provided in real time to manage current or impending high-risk situations or promote healthy behaviors in near real time. Currently there is great scientific interest in developing data analysis approaches to guide the development of mobile interventions. In particular data from mobile health studies might be used to examine effect moderators---individual characteristics, time-varying context, or past treatment response that moderate the effect of current treatment on a subsequent response. This article introduces a formal definition for moderated effects in terms of potential outcomes, a definition that is particularly suited to mobile interventions, where treatment occasions are numerous, individuals are not always available for treatment, and potential moderators might be influenced by past treatment. Methods for estimating moderated effects are developed and compared. The proposed approach is illustrated using BASICS-Mobile, a smartphone-based intervention designed to curb heavy drinking and smoking among college students. Supplementary materials for this article are available online.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Boruvka et al_2018_Assessing Time-Varying Causal Effect Moderation in Mobile Health.pdf}
}

@article{botevNormalLawLinear2017,
  title = {The Normal Law under Linear Restrictions: Simulation and Estimation via Minimax Tilting},
  shorttitle = {The Normal Law under Linear Restrictions},
  author = {Botev, Z. I.},
  year = {2017},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {79},
  number = {1},
  pages = {125--148},
  issn = {1467-9868},
  doi = {10/f9nwkw},
  urldate = {2020-09-01},
  abstract = {Simulation from the truncated multivariate normal distribution in high dimensions is a recurrent problem in statistical computing and is typically only feasible by using approximate Markov chain Monte Carlo sampling. We propose a minimax tilting method for exact independently and identically distributed data simulation from the truncated multivariate normal distribution. The new methodology provides both a method for simulation and an efficient estimator to hitherto intractable Gaussian integrals. We prove that the estimator has a rare vanishing relative error asymptotic property. Numerical experiments suggest that the scheme proposed is accurate in a wide range of set-ups for which competing estimation schemes fail. We give an application to exact independently and identically distributed data simulation from the Bayesian posterior of the probit regression model.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Botev_2017_The normal law under linear restrictions.pdf;/Users/nseewald/Zotero/storage/HH6JEUWL/rssb.html}
}

@article{botvinickModelbasedHierarchicalReinforcement2014,
  title = {Model-Based Hierarchical Reinforcement Learning and Human Action Control},
  author = {Botvinick, Matthew and Weinstein, Ari},
  year = {2014},
  journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
  volume = {369},
  number = {1655},
  pages = {20130480--},
  issn = {1471-2970},
  doi = {10.1098/rstb.2013.0480},
  abstract = {Recent work has reawakened interest in goal-directed or 'model-based' choice, where decisions are based on prospective evaluation of potential action outcomes. Concurrently, there has been growing attention to the role of hierarchy in decision-making and action control. We focus here on the intersection between these two areas of interest, considering the topic of hierarchical model-based control. To characterize this form of action control, we draw on the computational framework of hierarchical reinforcement learning, using this to interpret recent empirical findings. The resulting picture reveals how hierarchical model-based mechanisms might play a special and pivotal role in human decision-making, dramatically extending the scope and complexity of human behaviour.},
  pmid = {25267822},
  keywords = {behaviour,cognition,computational biology,Decision Making,Decision Making: physiology,Goals,Humans,Learning,Learning: physiology,Models,Neurological,Reinforcement (Psychology)},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Botvinick_Weinstein_2014_Model-based hierarchical reinforcement learning and human action control.pdf}
}

@misc{bowdenSequentialMultipleAssignment2017,
  title = {Sequential {{Multiple Assignment Treatment}} for {{Bipolar Disorder}}},
  author = {Bowden, Charles L.},
  year = {2017},
  month = mar,
  journal = {ClinicalTrials.gov},
  urldate = {2019-07-04},
  abstract = {Sequential Multiple Assignment Treatment for Bipolar Disorder - Full Text View.},
  howpublished = {https://clinicaltrials.gov/ct2/show/NCT01588457},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/LLU6DRYC/NCT01588457.html}
}

@article{boxSamplingBayesInference1980,
  title = {Sampling and {{Bayes}}' {{Inference}} in {{Scientific Modelling}} and {{Robustness}}},
  author = {Box, George E. P.},
  year = {1980},
  journal = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {143},
  number = {4},
  eprint = {10.2307/2982063},
  eprinttype = {jstor},
  pages = {383},
  issn = {00359238},
  doi = {10.2307/2982063},
  urldate = {2018-11-06},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Box_1980_Sampling and Bayes' Inference in Scientific Modelling and Robustness.pdf}
}

@book{boydConvexOptimization2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK ; New York},
  isbn = {978-0-521-83378-3},
  lccn = {QA402.5 .B69 2004},
  keywords = {Convex functions,Mathematical optimization},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Boyd_Vandenberghe_2004_Convex optimization.pdf}
}

@article{boyneHeadtoheadComparisonFOLFIRINOX2023,
  title = {Head-to-Head Comparison of {{FOLFIRINOX}} versus Gemcitabine plus Nab-Paclitaxel in Advanced Pancreatic Cancer: A Target Trial Emulation Using Real-World Data},
  shorttitle = {Head-to-Head Comparison of {{FOLFIRINOX}} versus Gemcitabine plus Nab-Paclitaxel in Advanced Pancreatic Cancer},
  author = {Boyne, Devon J. and Brenner, Darren R. and Gupta, Alind and Mackay, Eric and Arora, Paul and Wasiak, Radek and Cheung, Winson Y and Hern{\'a}n, Miguel A.},
  year = {2023},
  month = feb,
  journal = {Annals of Epidemiology},
  volume = {78},
  pages = {28--34},
  issn = {1047-2797},
  doi = {10.1016/j.annepidem.2022.12.005},
  urldate = {2024-01-30},
  abstract = {Purpose To emulate a hypothetical target trial assessing the effect of initiating 5-fluorouracil, folinic acid, irinotecan, and oxaliplatin (FOLFIRINOX) versus gemcitabine plus nab-paclitaxel (GN) within 8 weeks of diagnosis on overall survival. Methods An observational cohort study was conducted using population-level data from Alberta, Canada. Individuals diagnosed with advanced pancreatic cancer between April 2015 and December 2019 were identified through the provincial cancer registry and followed until March 2021. Records were linked to other administrative databases containing information on relevant variables. Individuals were excluded if they did not have adequate hemoglobin, platelet, white blood cell, and serum creatinine measures or if they received prior therapy. The observational analog of the per-protocol effect was estimated using inverse probability weighted Kaplan-Meier curves with bootstrapped 95\% confidence intervals. Results Four hundred seven individuals were eligible. The weighted median overall survival was 8.3 months (95\% CI, 5.7--11.9) for FOLFIRINOX and 5.1 months (95\% CI: 4.3 to 5.8) for GN. The adjusted difference in median overall survival was 3.2 months (95\% CI, 1.1--7.4) and the mortality hazard ratio was 0.78 (95\% CI, 0.61--0.97). Conclusions Our estimates favored the initiation of FOLFIRINOX over GN with respect to overall survival.},
  keywords = {_tablet,Comparative efficacy,Folfirinox,Gemcitabine,Nab-paclitaxel,Pancreatic cancer,Real-world data,Target trial emulation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Boyne_et_al_2023_Head-to-head_comparison_of_FOLFIRINOX_versus_gemcitabine_plus_nab-paclitaxel_in.pdf;/Users/nseewald/Zotero/storage/KLBSPNPB/S1047279722003088.html}
}

@article{bradfordAssociationUSState2018,
  title = {Association {{Between US State Medical Cannabis Laws}} and {{Opioid Prescribing}} in the {{Medicare Part D Population}}},
  author = {Bradford, Ashley C. and Bradford, W. David and Abraham, Amanda and Bagwell Adams, Grace},
  year = {2018},
  month = may,
  journal = {JAMA Internal Medicine},
  volume = {178},
  number = {5},
  pages = {667--672},
  issn = {2168-6106},
  doi = {10.1001/jamainternmed.2018.0266},
  urldate = {2022-06-16},
  abstract = {Opioid-related mortality increased by 15.6\% from 2014 to 2015 and increased almost 320\% between 2000 and 2015. Recent research finds that the use of all pain medications (opioid and nonopioid collectively) decreases in Medicare Part D and Medicaid populations when states approve medical cannabis laws (MCLs). The association between MCLs and opioid prescriptions is not well understood.To examine the association between prescribing patterns for opioids in Medicare Part D and the implementation of state MCLs.Longitudinal analysis of the daily doses of opioids filled in Medicare Part D for all opioids as a group and for categories of opioids by state and state-level MCLs from 2010 through 2015. Separate models were estimated first for whether the state had implemented any MCL and second for whether a state had implemented either a dispensary-based or a home cultivation only--based MCL.The primary outcome measure was the total number of daily opioid doses prescribed (in millions) in each US state for all opioids. The secondary analysis examined the association between MCLs separately by opioid class.From 2010 to 2015 there were 23.08 million daily doses of any opioid dispensed per year in the average state under Medicare Part D. Multiple regression analysis results found that patients filled fewer daily doses of any opioid in states with an MCL. The associations between MCLs and any opioid prescribing were statistically significant when we took the type of MCL into account: states with active dispensaries saw 3.742 million fewer daily doses filled (95\% CI, -6.289 to -1.194); states with home cultivation only MCLs saw 1.792 million fewer filled daily doses (95\% CI, -3.532 to -0.052). Results varied by type of opioid, with statistically significant estimated negative associations observed for hydrocodone and morphine. Hydrocodone use decreased by 2.320 million daily doses (or 17.4\%) filled with dispensary-based MCLs (95\% CI, -3.782 to -0.859; P\,=\,.002) and decreased by 1.256 million daily doses (or 9.4\%) filled with home-cultivation--only-based MCLs (95\% CI, -2.319 to -0.193; P\,=\,.02). Morphine use decreased by 0.361 million daily doses (or 20.7\%) filled with dispensary-based MCLs (95\% CI, -0.718 to -0.005; P\,=\,.047).Medical cannabis laws are associated with significant reductions in opioid prescribing in the Medicare Part D population. This finding was particularly strong in states that permit dispensaries, and for reductions in hydrocodone and morphine prescriptions.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bradford et al_2018_Association Between US State Medical Cannabis Laws and Opioid Prescribing in.pdf;/Users/nseewald/Zotero/storage/DZ7XUZ6J/2676999.html}
}

@article{bradfordMedicalMarijuanaLaws2016,
  title = {Medical {{Marijuana Laws Reduce Prescription Medication Use In Medicare Part D}}},
  author = {Bradford, Ashley C. and Bradford, W. David},
  year = {2016},
  month = jul,
  journal = {Health Affairs},
  volume = {35},
  number = {7},
  pages = {1230--1236},
  publisher = {Health Affairs},
  issn = {0278-2715},
  doi = {10.1377/hlthaff.2015.1661},
  urldate = {2022-06-17},
  abstract = {Legalization of medical marijuana has been one of the most controversial areas of state policy change over the past twenty years. However, little is known about whether medical marijuana is being used clinically to any significant degree. Using data on all prescriptions filled by Medicare Part D enrollees from 2010 to 2013, we found that the use of prescription drugs for which marijuana could serve as a clinical alternative fell significantly, once a medical marijuana law was implemented. National overall reductions in Medicare program and enrollee spending when states implemented medical marijuana laws were estimated to be \$165.2~million per year in 2013. The availability of medical marijuana has a significant effect on prescribing patterns and spending in Medicare Part D.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bradford_Bradford_2016_Medical Marijuana Laws Reduce Prescription Medication Use In Medicare Part D.pdf}
}

@article{braitmaierEffectivenessMammographyScreening2022,
  title = {Effectiveness of {{Mammography Screening}} on {{Breast Cancer Mortality}} -- {{A Study Protocol}} for {{Emulation}} of {{Target Trials Using German Health Claims Data}}},
  author = {Braitmaier, Malte and Kollhorst, Bianca and Heinig, Miriam and Langner, Ingo and Czwikla, Jonas and Heinze, Franziska and Buschmann, Laura and Minnerup, Heike and {Garc{\'i}a-Alb{\'e}niz}, Xabi{\'e}r and Hense, Hans-Werner and Karch, Andr{\'e} and Zeeb, Hajo and Haug, Ulrike and Didelez, Vanessa},
  year = {2022},
  month = jan,
  journal = {Clinical Epidemiology},
  volume = {14},
  pages = {1293--1303},
  publisher = {Dove Medical Press},
  issn = {null},
  doi = {10.2147/CLEP.S376107},
  urldate = {2024-07-26},
  abstract = {The efficacy of mammography screening in reducing breast cancer mortality has been demonstrated in randomized trials. However, treatment options - and hence prognosis -- for advanced tumor stages as well as mammography techniques have considerably improved since completion of these trials. Consequently, the effectiveness of mammography screening under current conditions is unclear and controversial. The German mammography screening program (MSP), an organized population-based screening program, was gradually introduced between 2005 and 2008 and achieved nation-wide coverage in 2009. We describe in detail a study protocol for investigating the effectiveness of the German MSP in reducing breast cancer mortality in women aged 50 to 69 years based on health claims data. Specifically, the proposed study aims at estimating per-protocol effects of several screening strategies on cumulative breast cancer mortality. The first analysis will be conducted once 10-year follow-up data are available. We will use claims data from five statutory health insurance providers in Germany, covering approximately 37.6 million individuals. To estimate the effectiveness of the MSP, hypothetical target trials will be emulated across time, an approach that has been demonstrated to minimize design-related biases. Specifically, the primary contrast will be in terms of the cumulative breast cancer mortality comparing the screening strategies of ``never screen'' versus ``regular screening as intended by the MSP''. In Germany, the utilization of data from health insurances for scientific research is regulated by the Code of Social Law. All involved health insurance providers as well as the responsible authorities approved the use of the health claims data for this study. The Ethics Committee of the University of Bremen determined that studies based on claims data are exempt from institutional review. The findings of the proposed study will be published in peer-reviewed journals.},
  keywords = {cancer screening,claims data,effectiveness,emulated target trial,mammography},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Braitmaier_et_al_2022_Effectiveness_of_Mammography_Screening_on_Breast_Cancer_Mortality_–_A_Study.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Braitmaier_et_al_2022_Effectiveness_of_Mammography_Screening_on_Breast_Cancer_Mortality_–_A_Study2.pdf}
}

@article{brauerUseHealthyEating2021,
  title = {Use of the {{Healthy Eating Index}} in {{Intervention Studies}} for {{Cardiometabolic Risk Conditions}}: {{A Systematic Review}}},
  shorttitle = {Use of the {{Healthy Eating Index}} in {{Intervention Studies}} for {{Cardiometabolic Risk Conditions}}},
  author = {Brauer, Paula and Royall, Dawna and Rodrigues, Ariellia},
  year = {2021},
  month = jul,
  journal = {Advances in Nutrition},
  volume = {12},
  number = {4},
  pages = {1317--1331},
  issn = {2161-8313},
  doi = {10.1093/advances/nmaa167},
  urldate = {2025-02-13},
  abstract = {Researchers and counselors need diet-assessment tools that characterize diet at baseline and over time in diet counseling and coaching interventions. Among possible tools, the Healthy Eating Index (HEI) is of interest in cardiometabolic treatment as it has undergone significant validation and development. The objective of this study was to systematically review relevant intervention studies using the HEI and its adaptations to examine whether diet interventions improve diet quality as measured by the HEI and the magnitude of change in included diet-quality scores following dietary intervention. Two databases [Cumulative Index to Nursing and Allied Health Literature (CINAHL) and PubMed] were searched for articles published from January 1995 to December 2019. The review included intervention studies in adults presenting with overweight/obesity and obesity-related chronic disease (metabolic syndrome, diabetes, prediabetes, hypertension, dyslipidemia) who received education or counseling, and the HEI was evaluated from baseline to follow-up (US or Canadian version) or Alternate HEI. Study quality was assessed using Cochrane risk of bias for randomized controlled trials (RCTs) or Cochrane Risk of Bias for Nonrandomized interventions (ROBINS-I). A total of 25 studies were included: 15 RCTs, 3 quasi-experimental studies, and 7 pre-post studies. Eight different versions of the HEI were used. Results demonstrated that diet quality assessed by HEI and its adaptations improved to a clinically relevant degree, especially in studies where multiple food behaviors/food-behavior goals were the focus and where an intensive, long-term intervention was compared with a no-treatment control group. There was wide variation in magnitude of change in included diet-quality indicators. Use of the HEI and its adaptations and other diet-quality tools is promising for better characterization of diet-counseling interventions and results when multiple food behaviors are a focus. Additional development is encouraged.},
  keywords = {cardiometabolic,diet quality,HEI,intervention,obesity},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Brauer et al. - 2021 - Use of the Healthy Eating Index in Intervention Studies for Cardiometabolic Risk Conditions A Syste.pdf;/Users/nseewald/Zotero/storage/E97BPGVQ/S2161831322001417.html}
}

@article{bravemanAbandonRaceFocus2021,
  title = {Abandon ``{{Race}}.'' {{Focus}} on {{Racism}}},
  author = {Braveman, Paula and Parker Dominguez, Tyan},
  year = {2021},
  journal = {Frontiers in Public Health},
  volume = {9},
  issn = {2296-2565},
  urldate = {2022-10-06},
  abstract = {The concept of ``race'' emerged in the 1600s with the trans-Atlantic slave trade, justifying slavery; it has been used to justify exploitation, denigration and decimation. Since then, despite contrary scientific evidence, a deeply-rooted belief has taken hold that ``race,'' indicated by, e.g., skin color or facial features, reflects fundamental biological differences. We propose that the term ``race'' be abandoned, substituting ``ethnic group'' while retaining ``racism,'' with the goal of dismantling it. Despite scientific consensus that ``race'' is a social construct, in official U.S. classifications, ``Hispanic''/''Latino'' is an ``ethnicity'' while African American/Black, American Indian/Alaska Native, Asian/Pacific Islander, and European American/White are ``races.'' There is no scientific basis for this. Each grouping reflects ancestry in a particular continent/region and shared history, e.g., the genocide and expropriation of Indigenous peoples, African Americans' enslavement, oppression and ongoing disenfranchisement, Latin America's Indigenous roots and colonization. Given migrations over millennia, each group reflects extensive genetic admixture across and within continents/regions. ``Ethnicity'' evokes social characteristics such as history, language, beliefs, customs. ``Race'' reinforces notions of inherent biological differences based on physical appearance. While not useful as a biological category, geographic ancestry is a key social category for monitoring and addressing health inequities because of racism's profound influence on health and well-being. We must continue to collect and analyze data on the population groups that have been racialized into socially constructed categories called ``races.'' We must not, however, continue to use that term; it is not the only obstacle to dismantling racism, but it is a significant one.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Braveman_Parker Dominguez_2021_Abandon “Race.pdf}
}

@article{brayAssessingTotalEffect2006,
  title = {Assessing the {{Total Effect}} of {{Time-Varying Predictors}} in {{Prevention Research}}},
  author = {Bray, Bethany Cara and Almirall, Daniel and Zimmerman, Rick S. and Lynam, Donald and Murphy, Susan A.},
  year = {2006},
  month = mar,
  journal = {Prevention Science},
  volume = {7},
  number = {1},
  pages = {1--17},
  issn = {1389-4986, 1573-6695},
  doi = {10/d8tczg},
  urldate = {2021-08-13},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bray et al_2006_Assessing the Total Effect of Time-Varying Predictors in Prevention Research.pdf}
}

@article{breimanStatisticalModelingTwo2001,
  title = {Statistical {{Modeling}}: {{The Two Cultures}} (with Comments and a Rejoinder by the Author)},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  year = {2001},
  month = aug,
  journal = {Statistical Science},
  volume = {16},
  number = {3},
  pages = {199--231},
  issn = {0883-4237, 2168-8745},
  doi = {10/bd86gq},
  urldate = {2018-12-05},
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  langid = {english},
  mrnumber = {MR1874152},
  zmnumber = {1059.62505},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Breiman_2001_Statistical Modeling.pdf;/Users/nseewald/Zotero/storage/UV3BB92W/1009213726.html}
}

@article{browneVariancePartitioningMultilevel2005,
  title = {Variance Partitioning in Multilevel Logistic Models That Exhibit Overdispersion},
  author = {Browne, W. J. and Subramanian, S. V. and Jones, K. and Goldstein, H.},
  year = {2005},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {168},
  number = {3},
  pages = {599--613},
  issn = {1467-985X},
  doi = {10.1111/j.1467-985X.2004.00365.x},
  urldate = {2021-11-10},
  abstract = {Summary. A common application of multilevel models is to apportion the variance in the response according to the different levels of the data. Whereas partitioning variances is straightforward in models with a continuous response variable with a normal error distribution at each level, the extension of this partitioning to models with binary responses or to proportions or counts is less obvious. We describe methodology due to Goldstein and co-workers for apportioning variance that is attributable to higher levels in multilevel binomial logistic models. This partitioning they referred to as the variance partition coefficient. We consider extending the variance partition coefficient concept to data sets when the response is a proportion and where the binomial assumption may not be appropriate owing to overdispersion in the response variable. Using the literacy data from the 1991 Indian census we estimate simple and complex variance partition coefficients at multiple levels of geography in models with significant overdispersion and thereby establish the relative importance of different geographic levels that influence educational disparities in India.},
  langid = {english},
  keywords = {Contextual variation,Illiteracy,India,Multilevel modelling,Multiple spatial levels,Overdispersion,Variance partition coefficient},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Browne et al_2005_Variance partitioning in multilevel logistic models that exhibit overdispersion.pdf;/Users/nseewald/Zotero/storage/H8D3XZP5/j.1467-985X.2004.00365.html}
}

@book{brumbackFundamentalsCausalInference2022,
  title = {Fundamentals of {{Causal Inference}}: {{With R}}},
  shorttitle = {Fundamentals of {{Causal Inference}}},
  author = {Brumback, Babette A.},
  year = {2022},
  series = {Texts in Statistical Science},
  publisher = {CRC Press},
  address = {Boca Raton},
  urldate = {2025-02-18},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/BHKPK7X2/9780367705053.html}
}

@article{brumbackNoteUsingEstimated2009,
  title = {A Note on Using the Estimated versus the Known Propensity Score to Estimate the Average Treatment Effect},
  author = {Brumback, Babette A.},
  year = {2009},
  journal = {Statistics and Probability Letters},
  volume = {79},
  number = {4},
  pages = {537--542},
  doi = {10.1016/j.spl.2008.09.032},
  abstract = {We provide simple intuition why using the estimated versus known propensity score tends to increase, and never decreases, asymptotic efficiency. When a covariate is independent of response conditional on treatment, using the known score can have greater finite-sample efficiency. ?? 2008 Elsevier B.V. All rights reserved.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Brumback_2009_A note on using the estimated versus the known propensity score to estimate the.pdf}
}

@article{bryanAnalysisLongitudinalMarginal2004,
  title = {Analysis of Longitudinal Marginal Structural Models},
  author = {Bryan, Jenny and Yu, Zhuo and {van der Laan}, Mark J.},
  year = {2004},
  month = jul,
  journal = {Biostatistics},
  volume = {5},
  number = {3},
  pages = {361--380},
  issn = {1465-4644},
  doi = {10/cp4tdh},
  urldate = {2021-08-17},
  abstract = {In this article we construct and study estimators of the causal effect of a time-dependent treatment on survival in longitudinal studies. We employ a particular marginal structural model (MSM), proposed by Robins (2000; Statistical models in Epidemiology, the  Environment, and Clinical Trials, 95--133), and follow a general methodology for constructing estimating functions in censored data models. The inverse probability of treatment weighted (IPTW) estimator of Robins et al. (2000; Epidemiology, 11, 550--560) is used as an initial estimator and forms the basis for an improved, one-step estimator that is consistent and asymptotically linear when the treatment mechanism is consistently estimated. We extend these methods to handle informative censoring. The proposed methodology is employed to estimate the causal effect of exercise on mortality in a longitudinal study of seniors in Sonoma County. A simulation study demonstrates the bias of naive estimators in the presence of time-dependent confounders and also shows the efficiency gain of the IPTW estimator, even in the absence such confounding. The efficiency gain of the improved, one-step estimator is demonstrated through simulation.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bryan et al_2004_Analysis of longitudinal marginal structural models.pdf;/Users/nseewald/Zotero/storage/HPLYB6IS/310169.html}
}

@article{buchholzStudyProtocolSequential2020,
  title = {Study Protocol for a Sequential Multiple Assignment Randomized Trial ({{SMART}}) to Improve Physical Activity in Employed Women},
  author = {Buchholz, Susan W. and Wilbur, JoEllen and Halloway, Shannon and Schoeny, Michael and Johnson, Tricia and Vispute, Sachin and Kitsiou, Spyros},
  year = {2020},
  month = feb,
  journal = {Contemporary Clinical Trials},
  volume = {89},
  pages = {105921},
  issn = {15517144},
  doi = {10.1016/j.cct.2019.105921},
  urldate = {2024-10-22},
  abstract = {Introduction: Physical activity monitors, motivational text messages, personal calls, and group meetings, have proven to be efficacious physical activity interventions. However, individual participant response to these interventions varies drastically. A SMART design (sequential multiple assignment randomized trial) provides an effective way to test interventions that start with an initial treatment and then transition to an augmented treatment for non-responders. We describe a SMART to determine the most effective adaptive intervention to increase physical activity (steps, moderate-to-vigorous physical activity) and improve cardiovascular health among employed women who are not regularly physically active. The SMART uses combinations of four treatments: 1) enhanced physical activity monitor (Fitbit wearable activity monitor and mobile app with goal setting and physical activity prescription), 2) text messages, 3) personal calls, and 4) group meetings.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Buchholz et al. - 2020 - Study protocol for a sequential multiple assignment randomized trial (SMART) to improve physical act.pdf}
}

@misc{budneyBehavioralTreatmentAdolescent2014,
  title = {Behavioral {{Treatment}} of {{Adolescent Substance Use}}},
  author = {Budney, Alan J},
  year = {2014},
  urldate = {2018-12-08},
  abstract = {Behavioral Treatment of Adolescent Substance Use - Full Text View.},
  howpublished = {https://clinicaltrials.gov/ct2/show/NCT02063984},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/DHVAM4W8/NCT02063984.html}
}

@article{bungerAdviceseekingImplementationNetwork2018,
  title = {Advice-Seeking during Implementation: A Network Study of Clinicians Participating in a Learning Collaborative},
  shorttitle = {Advice-Seeking during Implementation},
  author = {Bunger, Alicia C. and Doogan, Nathan and Hanson, Rochelle F. and Birken, Sarah A.},
  year = {2018},
  month = jul,
  journal = {Implementation Science},
  volume = {13},
  number = {1},
  pages = {101},
  issn = {1748-5908},
  doi = {10.1186/s13012-018-0797-7},
  urldate = {2023-05-12},
  abstract = {Successful implementation depends on the transfer of knowledge and expertise among clinicians, which can occur when professionals seek advice from one another. This study examines advice-seeking patterns among mental health clinicians participating in learning collaboratives (a multi-component implementation and quality improvement strategy) to implement trauma-focused cognitive behavioral therapy (TF-CBT). We apply transactive memory system theory, which explains how professionals access and retrieve knowledge, to examine factors associated with the evolution of advice-seeking relationships during implementation. Our aim is to unpack learning collaboratives' mechanisms by investigating how and why advice-seeking networks change, which may help us understand how implementation strategies can best target networks.},
  keywords = {Advice-seeking,Learning collaborative,Mental health services,Social network analysis},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Bunger et al_2018_Advice-seeking during implementation.pdf;/Users/nseewald/Zotero/storage/ZQ9F3TJ4/s13012-018-0797-7.html}
}

@article{burnsEffectsMedicarePart2014,
  title = {Effects of {{Medicare Part D}} on Guideline-Concordant Pharmacotherapy for Bipolar {{I}} Disorder among Dual Beneficiaries},
  author = {Burns, Marguerite E. and Busch, Alisa B. and Madden, Jeanne M. and Le Cates, Robert F. and Zhang, Fang and Adams, Alyce S. and {Ross-Degnan}, Dennis and Soumerai, Stephen B. and Huskamp, Haiden A.},
  year = {2014},
  month = mar,
  journal = {Psychiatric Services},
  volume = {65},
  number = {3},
  pages = {323--329},
  issn = {1557-9700},
  doi = {10.1176/appi.ps.201300123},
  abstract = {OBJECTIVE: In January 2006 insurance coverage for medications shifted from Medicaid to Medicare Part D private drug plans for the six million individuals enrolled in both programs. Dual beneficiaries faced new formularies and utilization management policies. It is unclear whether Part D, compared with Medicaid, relaxed or tightened psychiatric medication management, which could affect receipt of recommended pharmacotherapy, and emergency department use related to treatment discontinuities. This study examined the impact of the transition from Medicaid to Part D on guideline-concordant pharmacotherapy for bipolar I disorder and emergency department use. METHODS: Using interrupted-time-series analysis and Medicaid and Medicare administrative data from 2004 to 2007, the authors analyzed the effect of the coverage transition on receipt of guideline-concordant antimanic medication, guideline-discordant antidepressant monotherapy, and emergency department visits for a nationally representative continuous cohort of 1,431 adults with diagnosed bipolar I disorder. RESULTS: Sixteen months after the transition to Part D, the proportion of the population with any recommended use of antimanic drugs was an estimated 3.1 percentage points higher than expected once analyses controlled for baseline trends. The monthly proportion of beneficiaries with seven or more days of antidepressant monotherapy was 2.1 percentage points lower than expected. The number of emergency department visits per month temporarily increased by 19\% immediately posttransition. CONCLUSIONS: Increased receipt of guideline-concordant pharmacotherapy for bipolar I disorder may reflect relatively less restrictive management of antimanic medications under Part D. The clinical significance of the change is unclear, given the small effect sizes. However, increased emergency department visits merit attention for the Medicaid beneficiaries who continue to transition to Part D.},
  langid = {english},
  pmcid = {PMC4038978},
  pmid = {24337444},
  keywords = {Adult,Antidepressive Agents,Antimanic Agents,Bipolar Disorder,Cohort Studies,Emergency Service Hospital,Female,Guideline Adherence,Health Services Research,Humans,Insurance Coverage,Male,Medicaid,Medicare Part D,Middle Aged,Outcome Assessment Health Care,United States,Young Adult},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Burns et al_2014_Effects of Medicare Part D on guideline-concordant pharmacotherapy for bipolar.pdf}
}

@article{burtonExtendingSimpleLinear1998,
  title = {Extending the Simple Linear Regression Model to Account for Correlated Responses: {{An}} Introduction to Generalized Estimating Equations and Multi-Level Mixed Modelling},
  shorttitle = {Extending the Simple Linear Regression Model to Account for Correlated Responses},
  author = {Burton, Paul and Gurrin, Lyle and Sly, Peter},
  year = {1998},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {17},
  number = {11},
  pages = {1261--1291},
  issn = {02776715},
  doi = {10.1002/(SICI)1097-0258(19980615)17:11<1261::AID-SIM846>3.0.CO;2-Z},
  urldate = {2018-10-12},
  abstract = {Much of the research in epidemiology and clinical science is based upon longitudinal designs which involve repeated measurements of a variable of interest in each of a series of individuals. Such designs can be very powerful, both statistically and scientifically, because they enable one to study changes within individual subjects over time or under varied conditions. However, this power arises because the repeated measurements tend to be correlated with one another, and this must be taken into proper account at the time of analysis or misleading conclusions may result. Recent advances in statistical theory and in software development mean that studies based upon such designs can now be analysed more easily, in a valid yet flexible manner, using a variety of approaches which include the use of generalized estimating equations, and mixed models which incorporate random effects. This paper provides a particularly simple illustration of the use of these two approaches, taking as a practical example the analysis of a study which examined the response of portable peak expiratory flow meters to changes in true peak expiratory flow in 12 children with asthma. The paper takes the reader through the relevant practicalities of model fitting, interpretation and criticism and demonstrates that, in a simple case such as this, analyses based upon these model-based approaches produce reassuringly similar inferences to standard analyses based upon more conventional methods. 1998 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Burton et al_1998_Extending the simple linear regression model to account for correlated responses.pdf}
}

@book{buurenFlexibleImputationMissing2018,
  title = {Flexible Imputation of Missing Data},
  author = {van Buuren, Stef},
  year = {2018},
  series = {Chapman \& {{Hall}}/{{CRC Interdisciplinary}} Statistics Series},
  edition = {Second edition},
  publisher = {CRC Press, Taylor \& Francis Group},
  address = {Boca Raton London New York},
  isbn = {978-0-429-49225-9 978-1-138-58831-8},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Buuren_2018_Flexible imputation of missing data.pdf}
}

@article{buurenMiceMultivariateImputation2011,
  title = {Mice: {{Multivariate Imputation}} by {{Chained Equations}} in {{R}}},
  shorttitle = {Mice},
  author = {van Buuren, Stef and {Groothuis-Oudshoorn}, Karin},
  year = {2011},
  month = dec,
  journal = {Journal of Statistical Software},
  volume = {45},
  pages = {1--67},
  issn = {1548-7660},
  doi = {10.18637/jss.v045.i03},
  urldate = {2024-01-31},
  abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
  copyright = {Copyright (c) 2009 Stef van Buuren, Karin Groothuis-Oudshoorn},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Buuren_Groothuis-Oudshoorn_2011_mice.pdf}
}

@article{cableTypologiesAlcoholConsumption2007,
  title = {Typologies of Alcohol Consumption in Adolescence: {{Predictors}} and Adult Outcomes},
  shorttitle = {Typologies of Alcohol Consumption in Adolescence},
  author = {Cable, N. and Sacker, A.},
  year = {2007},
  month = oct,
  journal = {Alcohol and Alcoholism},
  volume = {43},
  number = {1},
  pages = {81--90},
  issn = {0735-0414, 1464-3502},
  doi = {10/fpmm33},
  urldate = {2020-12-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cable_Sacker_2007_Typologies of alcohol consumption in adolescence.pdf}
}

@article{cairneyMythEvidencebasedPolicymaking2022,
  title = {The Myth of `Evidence-Based Policymaking' in a Decentred State},
  author = {Cairney, Paul},
  year = {2022},
  month = jan,
  journal = {Public Policy and Administration},
  volume = {37},
  number = {1},
  pages = {46--66},
  publisher = {SAGE Publications Ltd},
  issn = {0952-0767},
  doi = {10.1177/0952076720905016},
  urldate = {2025-03-23},
  abstract = {I describe a policy theory story in which a decentred state results from choice and necessity. Governments often choose not to centralise policymaking but they would not succeed if they tried. Many policy scholars take this story for granted, but it is often ignored in other academic disciplines and wider political debate. Instead, commentators call for more centralisation to deliver more accountable, `rational,' and `evidence-based' policymaking. Such contradictory arguments, about the feasibility and value of government centralisation, raise an ever-present dilemma for governments to accept or challenge decentring. They also accentuate a modern dilemma about how to seek `evidence-based policymaking' in a decentred state. I identify three ideal-type ways in which governments can address both dilemmas consistently. I then identify their ad hoc use by UK and Scottish governments. Although each government has a reputation for more or less centralist approaches, both face similar dilemmas and address them in similar ways. Their choices reflect their need to appear to be in control while dealing with the fact that they are not.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cairney - 2022 - The myth of ‘evidence-based policymaking’ in a decentred state.pdf}
}

@article{callawayDifferenceinDifferencesContinuousTreatment2021,
  title = {Difference-in-{{Differences}} with a {{Continuous Treatment}}},
  author = {Callaway, Brantly and {Goodman-Bacon}, Andrew and Sant'Anna, Pedro H. C.},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.02637 [econ]},
  eprint = {2107.02637},
  primaryclass = {econ},
  urldate = {2021-07-16},
  abstract = {This paper analyzes difference-in-differences setups with a continuous treatment. We show that treatment effect on the treated-type parameters can be identified under a generalized parallel trends assumption that is similar to the binary treatment setup. However, interpreting differences in these parameters across different values of the treatment can be particularly challenging due to treatment effect heterogeneity. We discuss alternative, typically stronger, assumptions that alleviate these challenges. We also provide a variety of treatment effect decomposition results, highlighting that parameters associated with popular two-way fixed-effect specifications can be hard to interpret, even when there are only two time periods. We introduce alternative estimation strategies that do not suffer from these drawbacks. Our results also cover cases where (i) there is no available untreated comparison group and (ii) there are multiple periods and variation in treatment timing, which are both common in empirical work.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Callaway et al_2021_Difference-in-Differences with a Continuous Treatment.pdf;/Users/nseewald/Zotero/storage/5GIA72J3/2107.html}
}

@article{callawayDifferenceinDifferencesMultipleTime2021,
  ids = {callawayDifferenceinDifferencesMultipleTime,callawayDifferenceinDifferencesMultipleTime2020},
  title = {Difference-in-{{Differences}} with Multiple Time Periods},
  author = {Callaway, Brantly and Sant'Anna, Pedro H. C.},
  year = {2021},
  journal = {Journal of Econometrics},
  volume = {225},
  number = {2},
  pages = {200--230},
  issn = {0304-4076},
  doi = {10/ghqtjn},
  urldate = {2021-06-27},
  abstract = {In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the ``parallel trends assumption'' holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001--2007. Open-source software is available for implementing the proposed methods.},
  langid = {english},
  keywords = {Difference-in-Differences,Doubly robust,Dynamic treatment effects,Event study,No DOI found,Semi-parametric,Treatment effect heterogeneity,Variation in treatment timing},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Callaway_Sant’Anna_2021_Difference-in-Differences with multiple time periods.pdf;/Users/nseewald/Zotero/storage/J2VHYVPU/S0304407620303948.html}
}

@article{cameronBootstrapBasedImprovementsInference2008,
  title = {Bootstrap-{{Based Improvements}} for {{Inference}} with {{Clustered Errors}}},
  author = {Cameron, A. Colin and Gelbach, Jonah B. and Miller, Douglas L.},
  year = {2008},
  month = aug,
  journal = {The Review of Economics and Statistics},
  volume = {90},
  number = {3},
  pages = {414--427},
  issn = {0034-6535},
  doi = {10/fs6dpc},
  urldate = {2021-10-13},
  abstract = {Researchers have increasingly realized the need to account for within-group dependence in estimating standard errors of regression parameter estimates. The usual solution is to calculate cluster-robust standard errors that permit heteroskedasticity and within-cluster error correlation, but presume that the number of clusters is large. Standard asymptotic tests can over-reject, however, with few (five to thirty) clusters. We investigate inference using cluster bootstrap-t procedures that provide asymptotic refinement. These procedures are evaluated using Monte Carlos, including the example of Bertrand, Duflo, and Mullainathan (2004). Rejection rates of 10\% using standard methods can be reduced to the nominal size of 5\% using our methods.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cameron et al_2008_Bootstrap-Based Improvements for Inference with Clustered Errors.pdf;/Users/nseewald/Zotero/storage/SID45H96/Bootstrap-Based-Improvements-for-Inference-with.html}
}

@article{cameronPractitionerGuideClusterRobust2015,
  title = {A {{Practitioner}}'s {{Guide}} to {{Cluster-Robust Inference}}},
  author = {Cameron, A. Colin and Miller, Douglas L.},
  year = {2015},
  month = mar,
  journal = {Journal of Human Resources},
  volume = {50},
  number = {2},
  pages = {317--372},
  publisher = {University of Wisconsin Press},
  issn = {0022-166X, 1548-8004},
  doi = {10/gcx77h},
  urldate = {2021-10-13},
  abstract = {We consider statistical inference for regression when data are grouped into clusters, with regression model errors independent across clusters but correlated within clusters. Examples include data on individuals with clustering on village or region or other category such as industry, and state-year differences-in-differences studies with clustering on state. In such settings, default standard errors can greatly overstate estimator precision. Instead, if the number of clusters is large, statistical inference after OLS should be based on cluster-robust standard errors. We outline the basic method as well as many complications that can arise in practice. These include cluster-specific fixed effects, few clusters, multiway clustering, and estimators other than OLS.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cameron_Miller_2015_A Practitioner’s Guide to Cluster-Robust Inference.pdf;/Users/nseewald/Zotero/storage/XRWHSXCV/317.html}
}

@article{cameronRobustInferenceMultiway2011,
  title = {Robust {{Inference With Multiway Clustering}}},
  author = {Cameron, A. Colin and Gelbach, Jonah B. and Miller, Douglas L.},
  year = {2011},
  month = apr,
  journal = {Journal of Business \& Economic Statistics},
  volume = {29},
  number = {2},
  pages = {238--249},
  publisher = {ASA Website},
  issn = {0735-0015},
  doi = {10.1198/jbes.2010.07136},
  urldate = {2025-01-15},
  abstract = {In this article we propose a variance estimator for the OLS estimator as well as for nonlinear estimators such as logit, probit, and GMM. This variance estimator enables cluster-robust inference when there is two-way or multiway clustering that is nonnested. The variance estimator extends the standard cluster-robust variance estimator or sandwich estimator for one-way clustering (e.g., Liang and Zeger 1986; Arellano 1987) and relies on similar relatively weak distributional assumptions. Our method is easily implemented in statistical packages, such as Stata and SAS, that already offer cluster-robust standard errors when there is one-way clustering. The method is demonstrated by a Monte Carlo analysis for a two-way random effects model; a Monte Carlo analysis of a placebo law that extends the state--year effects example of Bertrand, Duflo, and Mullainathan (2004) to two dimensions; and by application to studies in the empirical literature where two-way clustering is present.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cameron et al. - 2011 - Robust Inference With Multiway Clustering.pdf}
}

@book{campbellHowDesignAnalyse2014,
  title = {How to Design, Analyse and Report Cluster Randomised Trials in Medicine and Health Related Research},
  author = {Campbell, Michael J. and Walters, Stephen John},
  year = {2014},
  series = {Statistics in Practice},
  publisher = {John Wiley \& Sons},
  address = {Chichester, West Sussex},
  abstract = {"A much-needed guide to the design and analysis of cluster randomized trials, How to Design, Analyse and Report Cluster Randomised Trials in Medicine and Health Related Research delivers practical guidance on the design and analysis of cluster randomised trials (cRCTs) in healthcare research. Detailing how to use Stata and SPSS and R for statistical analysis, each analysis technique is carefully explained with mathematics kept to a minimum. Written in a clear, accessible style by experienced statisticians, the text provides a practical approach for applied statisticians and biomedical researchers"--Provided by publisher},
  isbn = {978-1-118-76345-2},
  langid = {english},
  lccn = {610.724},
  keywords = {Data Interpretation Statistical,Health Services Research,methods,Randomized Controlled Trials as Topic,Research Design},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Campbell_Walters_2014_How to design, analyse and report cluster randomised trials in medicine and.pdf}
}

@article{campbellIntraclusterCorrelationCoefficients2004,
  title = {Intracluster Correlation Coefficients in Cluster Randomized Trials: Empirical Insights into How Should They Be Reported},
  shorttitle = {Intracluster Correlation Coefficients in Cluster Randomized Trials},
  author = {Campbell, Marion K. and Grimshaw, Jeremy M. and Elbourne, Diana R.},
  year = {2004},
  month = apr,
  journal = {BMC Medical Research Methodology},
  volume = {4},
  number = {1},
  pages = {9},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-4-9},
  urldate = {2021-12-22},
  abstract = {Increasingly, researchers are recognizing that there are many situations where the use of a cluster randomized trial may be more appropriate than an individually randomized trial. Similarly, the need for appropriate standards of reporting of cluster trials is more widely acknowledged.},
  keywords = {Average Cluster Size,Cluster Trial,Consort Statement,Intracluster Correlation Coefficient,Sample Size Calculation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Campbell et al_2004_Intracluster correlation coefficients in cluster randomized trials.pdf;/Users/nseewald/Zotero/storage/WK6TL7TH/1471-2288-4-9.html}
}

@article{campbellWorldResearchHas,
  title = {The {{World}} of {{Research Has Gone Berserk}}: {{Modeling}} the {{Consequences}} of {{Requiring}} "{{Greater Statistical Stringency}}" for {{Scientific Publication}}},
  author = {Campbell, Harlan and Gustafson, Paul},
  journal = {American Statistician},
  doi = {10/gfxcw7},
  abstract = {\textbf{ABSTRACT} In response to growing concern about the reliability and reproducibility of published science, researchers have proposed adopting measures of "greater statistical stringency," including suggestions to require larger sample sizes and to lower the highly criticized "\emph{p}\,},
  keywords = {nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Campbell_Gustafson_The World of Research Has Gone Berserk.pdf}
}

@article{candesDecodingLinearProgramming2005,
  title = {Decoding by {{Linear Programming}}},
  author = {Candes, E.J. and Tao, T.},
  year = {2005},
  month = dec,
  journal = {IEEE Transactions on Information Theory},
  volume = {51},
  number = {12},
  pages = {4203--4215},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.858979},
  urldate = {2018-10-12},
  abstract = {This paper considers a natural error correcting = + problem with real valued input/output. We wish to recover an input vector from corrupted measurements .},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Candes_Tao_2005_Decoding by Linear Programming.pdf}
}

@article{canigliaWhenMonitorCD42016,
  title = {When to {{Monitor CD4 Cell Count}} and {{HIV RNA}} to {{Reduce Mortality}} and {{AIDS-Defining Illness}} in {{Virologically Suppressed HIV-Positive Persons}} on {{Antiretroviral Therapy}} in {{High-Income Countries}}: {{A Prospective Observational Study}}},
  shorttitle = {When to {{Monitor CD4 Cell Count}} and {{HIV RNA}} to {{Reduce Mortality}} and {{AIDS-Defining Illness}} in {{Virologically Suppressed HIV-Positive Persons}} on {{Antiretroviral Therapy}} in {{High-Income Countries}}},
  author = {Caniglia, Ellen C. and Sabin, Caroline and Robins, James M. and Logan, Roger and Cain, Lauren E. and Abgrall, Sophie and Mugavero, Michael J. and {Hernandez-Diaz}, Sonia and Meyer, Laurence and Seng, Remonie and Drozd, Daniel R. and Seage, George R. and Bonnet, Fabrice and Dabis, Francois and Moore, Richard R. and Reiss, Peter and Van Sighem, Ard and Mathews, William C. and Del Amo, Julia and Moreno, Santiago and Deeks, Steven G. and Muga, Roberto and Boswell, Stephen L. and Ferrer, Elena and Eron, Joseph J. and Napravnik, Sonia and Jose, Sophie and Phillips, Andrew and Olson, Ashley and Justice, Amy C. and Tate, Janet P. and Bucher, Heiner C. and Egger, Matthias and Touloumi, Giota and Sterne, Jonathan A. and Costagliola, Dominique and Saag, Michael and Hern{\'a}n, Miguel A.},
  year = {2016},
  month = jun,
  journal = {JAIDS Journal of Acquired Immune Deficiency Syndromes},
  volume = {72},
  number = {2},
  pages = {214--221},
  issn = {1525-4135},
  doi = {10.1097/QAI.0000000000000956},
  urldate = {2024-06-25},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Caniglia_et_al_2016_When_to_Monitor_CD4_Cell_Count_and_HIV_RNA_to_Reduce_Mortality_and.pdf}
}

@misc{cannabisindustryCombatingOpioidEpidemic,
  title = {Combating the {{Opioid Epidemic}}},
  author = {{National Cannabis Industry Association}},
  year = {2023},
  journal = {The National Cannabis Industry Association},
  urldate = {2023-05-21},
  abstract = {Combating the Opioid Epidemic The opioid overdose crisis has become one of the most pressing public health challenges facing the country today, with more than 33,000 Americans losing their lives to opioid overdoses in 2015 alone. There is mounting evidence, however, that cannabis can help reduce opioid abuse and addiction when available as an alternative {\dots}},
  howpublished = {https://thecannabisindustry.org/combating-the-opioid-epidemic/},
  langid = {american},
  file = {/Users/nseewald/Zotero/storage/CGZ8F4RJ/combating-the-opioid-epidemic.html}
}

@article{caoBayesianHierarchicalReinforcement,
  title = {Bayesian {{Hierarchical Reinforcement Learning}}},
  author = {Cao, Feng and Ray, Soumya},
  pages = {9},
  abstract = {We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cao_Ray_Bayesian Hierarchical Reinforcement Learning.pdf}
}

@article{cardosoStrategicAnalysisTuberculosis2017,
  title = {Strategic Analysis of Tuberculosis Prevention and Control Actions in {{Brazil}} and {{Ethiopia}}: One Size Fits All?},
  shorttitle = {Strategic Analysis of Tuberculosis Prevention and Control Actions in {{Brazil}} and {{Ethiopia}}},
  author = {Cardoso, Gisela and {dos Santos}, Elizabeth Moreira and Kiflie, Yibeltal and Woldemichael, Kifle and Wilson, Suzanne and Lemma, Wuleta},
  year = {2017},
  month = mar,
  journal = {International Journal of Public Health},
  volume = {62},
  number = {2},
  pages = {305--315},
  publisher = {Springer International Publishing},
  issn = {1661-8564},
  doi = {10.1007/s00038-016-0934-5},
  urldate = {2022-10-12},
  abstract = {Objectives This study aimed at conducting a strategic analysis of Tuberculosis prevention and control actions in Brazil and Ethiopia, looking at the potential of directly observed treatment short-course strategy (DOTS) and community DOTS in both countries. Methods Literature review was conducted using PubMed, Medline-Ovid, EMBASE, and SCIELO databases. The reviewed terms were Tuberculosis, prevention and control and Brazil (or Brasil) or Ethiopia (or Etiopia). Study's eligibility included article's title or abstract in English or Portuguese and comprised the following Tuberculosis policy components: management; care; communication, and social mobilization; training and professional development; epidemiological surveillance, and monitoring and evaluation. The study identified, compared, and analyzed the challenges and recommendations reported in the literature. Results Although DOTS was not able to address all the difficulties regarding Tuberculosis control and prevention, it contributes to overcome challenges identified in the literature review. Decentralizing DOTS in Ethiopia and implementing DOTS in Brazil were key recommendations to overcome problems of access and treatment default. Conclusions DOTS and Community DOTS cannot solve every identified Tuberculosis challenge, but together they complement each other. Both strategies need to be tailored to site's challenges.},
  copyright = {2016 Swiss School of Public Health (SSPH+)},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cardoso et al_2017_Strategic analysis of tuberculosis prevention and control actions in Brazil and.pdf;/Users/nseewald/Zotero/storage/85ZCVJIF/s00038-016-0934-5.html}
}

@article{careyModellingMultivariateBinary1993,
  title = {Modelling Multivariate Binary Data with Alternating Logistic Regressions},
  author = {Carey, Vincent and Zeger, Scott L. and Diggle, Peter},
  year = {1993},
  journal = {Biometrika},
  volume = {80},
  number = {3},
  pages = {517--526},
  issn = {00063444},
  doi = {10.1093/biomet/80.3.517},
  abstract = {Marginal models for multivariate binary data permit separate modelling of the relationship of the response with explanatory variables, and the association between pairs of responses. When the former is the scientific focus, a first-order generalized estimating equation method (Liang \& Zeger, 1986) is easy to implement and gives efficient estimates of regression coefficients, although estimates of the association among the binary outcomes can be inefficient. When the association model is a focus, simultaneous modelling of the responses and all pairwise products (Prentice, 1988) using second-order estimating equations gives more efficient estimates of association parameters as well. However, this procedure can become computationally infeasible as the cluster size gets large. This paper proposes an alternative approach, alternating logistic regressions, for simultaneously regressing the response on explanatory variables as well as modelling the association among responses in terms of pairwise odds ratios. This algorithm iterates between a logistic regression using first-order generalized estimating equations to estimate regression coefficients and a logistic regression of each response on others from the same cluster using an appropriate offset to update the odds ratio parameters. For clusters of size n, alternating logistic regression involves evaluation and inversion of matrices of order n2 rather than n4 as required for second-order generalized estimating equations. The alternating logistic regression estimates are shown to be reasonably efficient relative to solutions of second-order equations in a few problems. The new method is illustrated with an analysis of neuropsychological tests on patients with epileptic seizures.},
  pmid = {426},
  keywords = {Clustered data,generalized estimating equation,LOGISTIC REGRESSION},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Carey et al_1993_Modelling multivariate binary data with alternating logistic regressions.pdf}
}

@book{casellaStatisticalInference2002,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  year = {2002},
  edition = {2nd ed},
  publisher = {Thomson Learning},
  address = {Australia ; Pacific Grove, CA},
  isbn = {978-0-534-24312-8},
  lccn = {QA276 .C37 2002},
  keywords = {Mathematical statistics,Probabilities},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Casella_Berger_2002_Statistical inference.pdf}
}

@article{cassaReidentificationHomeAddresses2008,
  title = {Re-Identification of Home Addresses from Spatial Locations Anonymized by {{Gaussian}} Skew},
  author = {Cassa, Christopher A and Wieland, Shannon C and Mandl, Kenneth D},
  year = {2008},
  journal = {International Journal of Health Geographics},
  volume = {7},
  number = {1},
  pages = {45},
  issn = {1476-072X},
  doi = {10.1186/1476-072X-7-45},
  urldate = {2018-10-12},
  abstract = {Background: Knowledge of the geographical locations of individuals is fundamental to the practice of spatial epidemiology. One approach to preserving the privacy of individual-level addresses in a data set is to de-identify the data using a non-deterministic blurring algorithm that shifts the geocoded values. We investigate a vulnerability in this approach which enables an adversary to reidentify individuals using multiple anonymized versions of the original data set. If several such versions are available, each can be used to incrementally refine estimates of the original geocoded location. Results: We produce multiple anonymized data sets using a single set of addresses and then progressively average the anonymized results related to each address, characterizing the steep decline in distance from the re-identified point to the original location, (and the reduction in privacy). With ten anonymized copies of an original data set, we find a substantial decrease in average distance from 0.7 km to 0.2 km between the estimated, re-identified address and the original address. With fifty anonymized copies of an original data set, we find a decrease in average distance from 0.7 km to 0.1 km. Conclusion: We demonstrate that multiple versions of the same data, each anonymized by nondeterministic Gaussian skew, can be used to ascertain original geographic locations. We explore solutions to this problem that include infrastructure to support the safe disclosure of anonymized medical data to prevent inference or re-identification of original address data, and the use of a Markov-process based algorithm to mitigate this risk.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cassa et al_2008_Re-identification of home addresses from spatial locations anonymized by.pdf}
}

@misc{cdcOverdoseDeaths2021,
  title = {U.{{S}}. {{Overdose Deaths In}} 2021 {{Increased Half}} as {{Much}} as in 2020 - {{But Are Still Up}} 15\%},
  author = {{Centers for Disease Control {and} Prevention}},
  year = {2022},
  month = may,
  urldate = {2023-05-21},
  abstract = {National Center for Health Statistics},
  howpublished = {https://www.cdc.gov/nchs/pressroom/nchs\_press\_releases/2022/202205.htm},
  langid = {american},
  file = {/Users/nseewald/Zotero/storage/NX6RZYQN/202205.html}
}

@article{cengizEffectMinimumWages2019,
  title = {The {{Effect}} of {{Minimum Wages}} on {{Low-Wage Jobs}}},
  author = {Cengiz, Doruk and Dube, Arindrajit and Lindner, Attila and Zipperer, Ben},
  year = {2019},
  month = aug,
  journal = {The Quarterly Journal of Economics},
  volume = {134},
  number = {3},
  pages = {1405--1454},
  issn = {0033-5533},
  doi = {10/gf4q2d},
  urldate = {2021-08-03},
  abstract = {We estimate the effect of minimum wages on low-wage jobs using 138 prominent state-level minimum wage changes between 1979 and 2016 in the United States using a difference-in-differences approach. We first estimate the effect of the minimum wage increase on employment changes by wage bins throughout the hourly wage distribution. We then focus on the bottom part of the wage distribution and compare the number of excess jobs paying at or slightly above the new minimum wage to the missing jobs paying below it to infer the employment effect. We find that the overall number of low-wage jobs remained essentially unchanged over the five years following the increase. At the same time, the direct effect of the minimum wage on average earnings was amplified by modest wage spillovers at the bottom of the wage distribution. Our estimates by detailed demographic groups show that the lack of job loss is not explained by labor-labor substitution at the bottom of the wage distribution. We also find no evidence of disemployment when we consider higher levels of minimum wages. However, we do find some evidence of reduced employment in tradeable sectors. We also show how decomposing the overall employment effect by wage bins allows a transparent way of assessing the plausibility of estimates.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cengiz et al_2019_The Effect of Minimum Wages on Low-Wage Jobs.pdf;/Users/nseewald/Zotero/storage/AU7DVXS6/5484905.html}
}

@misc{centerforhistoryandnewmediaZoteroQuickStart,
  title = {Zotero {{Quick Start Guide}}},
  author = {{Center for History and New Media}},
  howpublished = {http://zotero.org/support/quick\_start\_guide},
  keywords = {nosource}
}

@article{chagantyEfficiencyGeneralizedEstimating2004,
  title = {Efficiency of Generalized Estimating Equations for Binary Responses},
  author = {Chaganty, N. Rao and Joe, Harry},
  year = {2004},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {66},
  number = {4},
  pages = {851--860},
  issn = {1467-9868},
  doi = {10/bz85d2},
  urldate = {2019-12-21},
  abstract = {Summary. Using standard correlation bounds, we show that in generalized estimation equations (GEEs) the so-called `working correlation matrix'R({$\alpha$}) for analysing binary data cannot in general be the true correlation matrix of the data. Methods for estimating the correlation param-eter in current GEE software for binary responses disregard these bounds. To show that the GEE applied on binary data has high efficiency, we use a multivariate binary model so that the covariance matrix from estimating equation theory can be compared with the inverse Fisher information matrix. But R({$\alpha$}) should be viewed as the weight matrix, and it should not be confused with the correlation matrix of the binary responses. We also do a comparison with more general weighted estimating equations by using a matrix Cauchy--Schwarz inequality. Our analysis leads to simple rules for the choice of {$\alpha$} in an exchangeable or autoregressive AR(1) weight matrix R({$\alpha$}), based on the strength of dependence between the binary variables. An example is given to illustrate the assessment of dependence and choice of {$\alpha$}.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chaganty_Joe_2004_Efficiency of generalized estimating equations for binary responses.pdf;/Users/nseewald/Zotero/storage/6NSK9EJU/j.1467-9868.2004.05741.html}
}

@article{chagantyRangeCorrelationMatrices2006,
  title = {Range of Correlation Matrices for Random {{Bernoulli}} Dependent Variables},
  author = {Chaganty, N. Rao and Joe, Harry},
  year = {2006},
  journal = {Biometrika},
  volume = {93},
  number = {1},
  pages = {197--206},
  doi = {10.1093/biomet/93.1.197},
  abstract = {We say that a pair ( p, R) is compatible if there exists a multivariate binary distribution with mean vector p and correlation matrix R. In this paper we study necessary and sufficient conditions for compatibility for structured and unstructured correlation matrices. We give examples of correlation matrices that are incompatible with any p. Using our results we show that the parametric binary models of Emrich \& Piedmonte (1991) and Qaqish (2003) allow a good range of correlations between the binary variables. We also obtain necessary and sufficient conditions for a matrix of odds ratios to be compatible with a given p. Our findings support the popular belief that the odds ratios are less constrained and more flexible than the correlations.},
  keywords = {Frechet bound,Generalised estimating equation,Multivariate binary,Odds ratio},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chaganty_Joe_2006_Range of correlation matrices for random Bernoulli dependent variables.pdf}
}

@article{chakrabortyDevelopingMulticomponentInterventions2009,
  title = {Developing Multicomponent Interventions Using Fractional Factorial Designs},
  author = {Chakraborty, Bibhas and Collins, Linda M. and Strecher, Victor J. and Murphy, Susan A.},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {21},
  pages = {2687--2708},
  issn = {1097-0258},
  doi = {10/bk9r3z},
  urldate = {2020-12-13},
  abstract = {Multicomponent interventions composed of behavioral, delivery, or implementation factors in addition to medications are becoming increasingly common in health sciences. A natural experimental approach to developing and refining such multicomponent interventions is to start with a large number of potential components and screen out the least active ones. Factorial designs can be used efficiently in this endeavor. We address common criticisms and misconceptions regarding the use of factorial designs in these screening studies. We also provide an operationalization of screening studies. As an example, we consider the use of a screening study in the development of a multicomponent smoking cessation intervention. Simulation results are provided to support the discussions. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chakraborty et al_2009_Developing multicomponent interventions using fractional factorial designs.pdf;/Users/nseewald/Zotero/storage/BP6365F3/sim.html}
}

@article{chakrabortyDynamicTreatmentRegimes2014,
  title = {Dynamic {{Treatment Regimes}}},
  author = {Chakraborty, Bibhas and Murphy, Susan A.},
  year = {2014},
  month = jan,
  journal = {Annual Review of Statistics and Its Application},
  volume = {1},
  number = {1},
  pages = {447--464},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-022513-115553},
  urldate = {2018-10-12},
  abstract = {A dynamic treatment regime consists of a sequence of decision rules, one per stage of intervention, that dictate how to individualize treatments to patients, based on evolving treatment and covariate history. These regimes are particularly useful for managing chronic disorders and fit well into the larger paradigm of personalized medicine. They provide one way to operationalize a clinical decision support system. Statistics plays a key role in the construction of evidence-based dynamic treatment regimes---informing the best study design as well as efficient estimation and valid inference. Owing to the many novel methodological challenges this area offers, it has been growing in popularity among statisticians in recent years. In this article, we review the key developments in this exciting field of research. In particular, we discuss the sequential multiple assignment randomized trial designs, estimation techniques like Q-learning and marginal structural models, and several inference techniques designed to address the associated nonstandard asymptotics. We reference software whenever available. We also outline some important future directions.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chakraborty_Murphy_2014_Dynamic Treatment Regimes.pdf}
}

@article{chakrabortyInferenceExpectedPerformance2014,
  title = {Inference about the Expected Performance of a Data-Driven Dynamic Treatment Regime},
  author = {Chakraborty, Bibhas and Laber, Eric B and Zhao, Ying-Qi},
  year = {2014},
  month = aug,
  journal = {Clinical Trials},
  volume = {11},
  number = {4},
  pages = {408--417},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774514537727},
  urldate = {2023-12-17},
  abstract = {Background A dynamic treatment regime (DTR) comprises a sequence of decision rules, one per stage of intervention, that recommends how to individualize treatment to patients based on evolving treatment and covariate history. These regimes are useful for managing chronic disorders, and fit into the larger paradigm of personalized medicine. The Value of a DTR is the expected outcome when the DTR is used to assign treatments to a population of interest. Purpose The Value of a data-driven DTR, estimated using data from a Sequential Multiple Assignment Randomized Trial, is both a data-dependent parameter and a non-smooth function of the underlying generative distribution. These features introduce additional variability that is not accounted for by standard methods for conducting statistical inference, for example, the bootstrap or normal approximations, if applied without adjustment. Our purpose is to provide a feasible method for constructing valid confidence intervals (CIs) for this quantity of practical interest. Methods We propose a conceptually simple and computationally feasible method for constructing valid CIs for the Value of an estimated DTR based on subsampling. The method is self-tuning by virtue of an approach called the double bootstrap. We demonstrate the proposed method using a series of simulated experiments. Results The proposed method offers considerable improvement in terms of coverage rates of the CIs over the standard bootstrap approach. Limitations In this article, we have restricted our attention to Q-learning for estimating the optimal DTR. However, other methods can be employed for this purpose; to keep the discussion focused, we have not explored these alternatives. Conclusion Subsampling-based CIs provide much better performance compared to standard bootstrap for the Value of an estimated DTR.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chakraborty_et_al_2014_Inference_about_the_expected_performance_of_a_data-driven_dynamic_treatment.pdf}
}

@article{chakrabortyMethodEstimateIntracluster2021,
  title = {A Method to Estimate Intra-Cluster Correlation for Clustered Categorical Data},
  author = {Chakraborty, Hrishikesh and Solomon, Nicole and Anstrom, Kevin J},
  year = {2021},
  month = apr,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {0},
  number = {0},
  pages = {1--18},
  publisher = {Taylor \& Francis},
  issn = {0361-0926},
  doi = {10.1080/03610926.2021.1914660},
  urldate = {2021-12-22},
  abstract = {Correlated categorical data often arise from studies involving cluster randomized trials, a cluster sampling scheme, or repeated measurements. The intra-cluster correlation coefficient (ICC) is used to estimate the average correlation within clusters. There have been numerous methods proposed to estimate ICC for correlated binary data, the ANOVA method for continuous data, and several methods for time-to-event outcomes. However, no method currently exists to estimate ICC for nominal or ordinal categorical responses with more than two categories. We developed a method based on resampling principles to estimate the ICC and its 95\% confidence interval for categorical variables. We conducted a simulation study to show how our method estimates the population ICC under varying event rates, numbers of clusters, and cluster sizes. We also used real study datasets to estimate the ICC for ordinal and nominal categorical variables. We observed that the resampling method estimates the population ICC well for moderate to large numbers of clusters and moderate to large cluster sizes.},
  keywords = {clustered categorical data,Intra-cluster correlation coefficient,resampling},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chakraborty et al_2021_A method to estimate intra-cluster correlation for clustered categorical data.pdf;/Users/nseewald/Zotero/storage/AXV6T9IM/03610926.2021.html}
}

@book{chakrabortyStatisticalMethodsDynamic2013,
  title = {Statistical {{Methods}} for {{Dynamic Treatment Regimes}}},
  author = {Chakraborty, Bibhas and Moodie, Erica E. M.},
  year = {2013},
  series = {Statistics for {{Biology}} and {{Health}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4614-7428-9},
  urldate = {2018-10-12},
  isbn = {978-1-4614-7427-2},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chakraborty_Moodie_2013_Statistical Methods for Dynamic Treatment Regimes.pdf}
}

@article{chanAdjustmentBaselineMeasurement2004,
  title = {Adjustment for Baseline Measurement Error in Randomized Controlled Trials Induces Bias},
  author = {Chan, Siew F and Macaskill, Petra and Irwig, Les and Walter, Stephen D},
  year = {2004},
  month = aug,
  journal = {Controlled Clinical Trials},
  volume = {25},
  number = {4},
  pages = {408--416},
  issn = {01972456},
  doi = {10/cwx2sh},
  urldate = {2019-12-09},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chan et al_2004_Adjustment for baseline measurement error in randomized controlled trials.pdf}
}

@article{chanEffectsRecreationalMarijuana2020,
  title = {The {{Effects}} of {{Recreational Marijuana Legalization}} and {{Dispensing}} on {{Opioid Mortality}}},
  author = {Chan, Nathan W. and Burkhardt, Jesse and Flyr, Matthew},
  year = {2020},
  journal = {Economic Inquiry},
  volume = {58},
  number = {2},
  pages = {589--606},
  issn = {1465-7295},
  doi = {10.1111/ecin.12819},
  urldate = {2024-05-01},
  abstract = {This study documents how the changing legal status of marijuana has impacted mortality in the United States over the past two decades. We use a difference-in-difference approach to estimate the effect of medical marijuana laws (MML) and recreational marijuana laws (RML) on fatalities from opioid overdoses, and we find that marijuana access induces sharp reductions in opioid mortality rates. Our research corroborates prior findings on MMLs and offers the first causal estimates of RML impacts on opioid mortality to date, the latter of which is particularly important given that RMLs are far more expansive in scope and reach than MMLs. In our preferred econometric specification, we estimate that RMLs reduce annual opioid mortality in the range of 20\%--35\%, with particularly pronounced effects for synthetic opioids. In further analysis, we demonstrate how RML impacts vary among demographic groups, shedding light on the distributional consequences of these laws. Our findings are especially important and timely given the scale of the opioid crisis in the United States and simultaneously evolving attitudes and regulations on marijuana use. (JEL I18, K32, H75)},
  copyright = {{\copyright} 2019 Western Economic Association International},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chan_et_al_2020_The_Effects_of_Recreational_Marijuana_Legalization_and_Dispensing_on_Opioid.pdf;/Users/nseewald/Zotero/storage/ICM3X3I3/ecin.html}
}

@article{changPropensityScoreMethods2022,
  title = {Propensity Score Methods for Observational Studies with Clustered Data: {{A}} Review},
  shorttitle = {Propensity Score Methods for Observational Studies with Clustered Data},
  author = {Chang, Ting-Hsuan and Stuart, Elizabeth A.},
  year = {2022},
  journal = {Statistics in Medicine},
  volume = {41},
  number = {18},
  pages = {3612--3626},
  issn = {1097-0258},
  doi = {10.1002/sim.9437},
  urldate = {2023-07-26},
  abstract = {Propensity score methods are a popular approach to mitigating confounding bias when estimating causal effects in observational studies. When study units are clustered (eg, patients nested within health systems), additional challenges arise such as accounting for unmeasured confounding at multiple levels and dependence between units within the same cluster. While clustered observational data are widely used to draw causal inferences in many fields, including medicine and healthcare, extensions of propensity score methods to clustered settings are still a relatively new area of research. This article presents a framework for estimating causal effects using propensity scores when study units are nested within clusters and are nonrandomly assigned to treatment conditions. We emphasize the need for investigators to examine the nature of the clustering, among other properties, of the observational data at hand in order to guide their choice of causal estimands and the corresponding propensity score approach.},
  copyright = {{\copyright} 2022 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {causal inference,clustered data,multilevel,observational studies,propensity score},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chang_Stuart_2022_Propensity score methods for observational studies with clustered data.pdf;/Users/nseewald/Zotero/storage/63EHKC4Z/sim.html}
}

@misc{changShinyWebApplication2018,
  title = {Shiny: {{Web Application Framework}} for {{R}}},
  shorttitle = {Shiny},
  author = {Chang, Winston and Cheng, Joe and Allaire, J. J. and Xie, Yihui and McPherson, Jonathan},
  year = {2018},
  month = nov,
  urldate = {2018-12-05},
  abstract = {Makes it incredibly easy to build interactive web applications with R. Automatic "reactive" binding between inputs and outputs and extensive prebuilt widgets make it possible to build beautiful, responsive, and powerful applications with minimal effort.},
  copyright = {GPL-3 {\textbar} file LICENSE},
  keywords = {nosource}
}

@article{charnesEquivalenceGeneralizedLeast,
  ids = {charnesEquivalenceGeneralizedLeast1976},
  title = {The {{Equivalence}} of {{Generalized Least Squares}} and {{Maximum Likelihood Estimates}} in the {{Exponential Family}}},
  author = {Charnes, A and Frome, E L and Yu, P L},
  pages = {4},
  doi = {10/gghp9x},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Charnes et al_The Equivalence of Generalized Least Squares and Maximum Likelihood Estimates.pdf}
}

@article{chatterjeeNewCoefficientCorrelation2020,
  title = {A {{New Coefficient}} of {{Correlation}}},
  author = {Chatterjee, Sourav},
  year = {2020},
  month = apr,
  journal = {Journal of the American Statistical Association},
  pages = {1--21},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1758115},
  urldate = {2021-10-17},
  abstract = {--Is it possible to define a coefficient of correlation which is (a) as simple as the classical coefficients like Pearson's correlation or Spearman's correlation, and yet (b) consistently estimates some simple and interpretable measure of the degree of dependence between the variables, which is 0 if and only if the variables are independent and 1 if and only if one is a measurable function of the other, and (c) has a simple asymptotic theory under the hypothesis of independence, like the classical coefficients? This article answers this question in the affirmative, by producing such a coefficient. No assumptions are needed on the distributions of the variables. There are several coefficients in the literature that converge to 0 if and only if the variables are independent, but none that satisfy any of the other properties mentioned above. Supplementary materials for this article are available online.},
  keywords = {Correlation,Independence,Measure of association,nosource,Researcher App},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chatterjee_2020_A New Coefficient of Correlation.pdf;/Users/nseewald/Zotero/storage/XZDRHYJ9/01621459.2020.html}
}

@article{chenEstimandsRealWorldEvidence2024,
  title = {Estimands in {{Real-World Evidence Studies}}},
  author = {Chen, Jie and Scharfstein, Daniel and Wang, Hongwei and Yu, Binbing and Song, Yang and He, Weili and Scott, John and Lin, Xiwu and Lee, Hana},
  year = {2024},
  month = apr,
  journal = {Statistics in Biopharmaceutical Research},
  volume = {16},
  number = {2},
  pages = {257--269},
  issn = {1946-6315},
  doi = {10.1080/19466315.2023.2259829},
  urldate = {2025-01-15},
  abstract = {A Real-World Evidence (RWE) Scientific Working Group (SWG) of the American Statistical Association Biopharmaceutical Section (ASA BIOP) has been reviewing statistical considerations for the generation of RWE to support regulatory decision-making. As part of the effort, the working group is addressing estimands in RWE studies. Constructing the right estimand---the target of estimation---which reflects the research question and the study objective, is one of the key components in formulating a clinical study. ICH E9(R1) describes statistical principles for constructing estimands in clinical trials with a focus on five attributes---population, treatment, endpoints, intercurrent events, and population-level summary. However, defining estimands for clinical studies using real-world data (RWD), that is, RWE studies, requires additional considerations due to, for example, heterogeneity of study population, complexity of treatment regimes, different types and patterns of intercurrent events, and complexities in choosing study endpoints. This article reviews the essential components of estimands and causal inference framework, discusses considerations in constructing estimands for RWE studies, highlights similarities and differences in traditional clinical trial and RWE study estimands, and provides a roadmap for choosing appropriate estimands for RWE studies.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chen et al. - 2024 - Estimands in Real-World Evidence Studies.pdf}
}

@article{chengEffectFlavoredNicotine2024,
  title = {Effect of {{Flavored}} on! {{Nicotine Pouch Products}} on {{Smoking Behaviors}}: {{Protocol}} for a {{Sequential}}, {{Multiple Assignment}}, {{Randomized Controlled Trial}}},
  shorttitle = {Effect of {{Flavored}} on! {{Nicotine Pouch Products}} on {{Smoking Behaviors}}},
  author = {Cheng, Hui G. and Rose, Jed E. and Karelitz, Joshua L. and Botts, David R. and Botts, Tanaia L. and Willette, Perry N. and Cohen, Gal},
  year = {2024},
  month = jun,
  journal = {JMIR Research Protocols},
  volume = {13},
  number = {1},
  pages = {e56565},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/56565},
  urldate = {2024-10-11},
  abstract = {Background: Cigarette smoking is a leading cause of morbidity and mortality. For adults who smoke cigarettes and cannot or will not quit smoking, smoke-free products, such as nicotine pouches, have been recognized as a potential alternative to smoking combusted cigarettes to reduce harm due to cigarette smoking. The role of flavors in these smoke-free products in tobacco harm reduction has not been fully understood. Objective: This study evaluates the effect of flavors in on! nicotine pouch products (research products) in the reduction of cigarette smoking among adults who smoke cigarettes in their natural environment. Methods: This study uses a sequential, multiple assignment, randomized trial design. Approximately 400 eligible adults who smoke cigarettes will be enrolled and randomized to have access to either the Original (unflavored) on! nicotine pouch product only or a complete flavor profile (ie, Berry, Cinnamon, Citrus, Coffee, Mint, Original, and Wintergreen) of on! nicotine pouch products. After 3 weeks, participants in the Original-only arm will be randomized again, with half remaining in the Original-only arm and half having access to the complete flavor profile for another 3 weeks. Primary outcomes are expired-air carbon monoxide (CO) levels. Secondary outcomes are self-reported cigarette consumption and CO-verified cigarette abstinence. Results: Recruitment and data collection started in September 2023 and is projected to last until March 2025. We anticipate completing the data analysis in 2025. As of May 2024, we have enrolled 314 participants. Conclusions: This study will provide empirical evidence about the effect that flavor availability in smoke-free products may have in reducing cigarette smoking. Trial Registration: ClinicalTrials.gov NCT06072547; https://clinicaltrials.gov/study/NCT06072547},
  copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in JMIR Research Protocols...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.researchprotocols.org/, as well as this copyright and license information must be included.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cheng_et_al_2024_Effect_of_Flavored_on.pdf;/Users/nseewald/Zotero/storage/KL6BW3GD/Cheng_et_al_2024_Effect_of_Flavored_on_annotated.pdf;/Users/nseewald/Zotero/storage/GM9NLWC9/e56565.html}
}

@article{chengHealthyEatingIndex2023,
  title = {Healthy {{Eating Index Diet Quality}} in {{Randomized Weight Loss Trials}}: {{A Systematic Review}}},
  shorttitle = {Healthy {{Eating Index Diet Quality}} in {{Randomized Weight Loss Trials}}},
  author = {Cheng, Jessica and Liang, Hai-Wei and Klem, Mary Lou and Costacou, Tina and Burke, Lora E.},
  year = {2023},
  month = jan,
  journal = {Journal of the Academy of Nutrition and Dietetics},
  volume = {123},
  number = {1},
  pages = {117--143},
  issn = {2212-2672},
  doi = {10.1016/j.jand.2022.08.114},
  urldate = {2025-02-17},
  abstract = {Background Weight loss interventions focus on dietary and physical activity changes to induce weight loss. Both through weight loss and independent of it, diet quality is important for reducing chronic disease risk. However, whether and how diet quality changes over the course of a behavioral intervention is unclear. Objective To systematically review the evidence from randomized controlled trials on the effect of behavioral interventions on diet quality as defined by the Healthy Eating Index (HEI) among adults with overweight and obesity. Methods PubMed, Ebscohost CINAHL, Embase, OVID APA PsycInfo, Scopus, and Web of Science were searched through May 2021. Inclusion criteria comprised randomized controlled trial design, a primary or secondary aim of weight loss, a sample of US adults with overweight or obesity, measurement using the HEI-2005, 2010, or 2015, and assessment of the time by treatment effect. Interventions must have included behavioral components and lasted at least 3 months. Risk of bias was assessed using the Cochrane Risk of Bias 2 tool. The systematic review protocol was published on Open Science Framework. Results Of 3,707 citations retrieved, 18 studies met inclusion criteria. A wide array of behavioral interventions were assessed, including in-person and mobile health interventions as well as those prescribing intake of specific foods. Risk of bias in the included studies primarily arose from the measurement of the outcome variable. Sample sizes ranged from 34 to 413 participants. Nine studies used multiple dietary recalls, with few using the recommended method of Healthy Eating Index calculation. Changes in diet quality ranged from no improvement to a 20-point improvement. More often, improvement was in the 4- to 7-point range. Conclusions The evidence for the efficacy of behavioral weight loss interventions for improving diet quality among adults with overweight and obesity is limited. Modest improvements in HEI scores were observed in the reviewed studies.},
  keywords = {behavioral intervention,diet quality,healthy eating index,systematic review,weight loss},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cheng et al. - 2023 - Healthy Eating Index Diet Quality in Randomized Weight Loss Trials A Systematic Review.pdf;/Users/nseewald/Zotero/storage/A6I2CRXX/S2212267222008449.html}
}

@techreport{chenInfluenceCorrelationMissing2013,
  type = {Dissertation},
  title = {Influence of {{Correlation}} and {{Missing Data}} on {{Sample Size Determination}} in {{Mixed Models}}},
  author = {Chen, Yanran},
  year = {2013},
  pages = {125},
  institution = {Bowling Green State University},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chen_2013_Influence of Correlation and Missing Data on Sample Size Determination in Mixed.pdf}
}

@article{chenJointEstimationModel1993,
  title = {Joint {{Estimation}} of {{Model Parameters}} and {{Outlier Effects}} in {{Time Series}}},
  author = {Chen, Chung and Liu, Lon-Mu},
  year = {1993},
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {421},
  pages = {284--297},
  doi = {10.1080/01621459.1993.10594321},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chen_Liu_1993_Joint Estimation of Model Parameters and Outlier Effects in Time Series.pdf}
}

@article{chernozhukovAverageQuantileEffects2013,
  title = {Average and {{Quantile Effects}} in {{Nonseparable Panel Models}}},
  author = {Chernozhukov, Victor and {Fern{\'a}ndez-Val}, Iv{\'a}n and Hahn, Jinyong and Newey, Whitney},
  year = {2013},
  journal = {Econometrica},
  volume = {81},
  number = {2},
  pages = {535--580},
  issn = {1468-0262},
  doi = {10/gg3375},
  urldate = {2021-06-27},
  abstract = {Nonseparable panel models are important in a variety of economic settings, including discrete choice. This paper gives identification and estimation results for nonseparable models under time-homogeneity conditions that are like ``time is randomly assigned'' or ``time is an instrument.'' Partial-identification results for average and quantile effects are given for discrete regressors, under static or dynamic conditions, in fully nonparametric and in semiparametric models, with time effects. It is shown that the usual, linear, fixed-effects estimator is not a consistent estimator of the identified average effect, and a consistent estimator is given. A simple estimator of identified quantile treatment effects is given, providing a solution to the important problem of estimating quantile treatment effects from panel data. Bounds for overall effects in static and dynamic models are given. The dynamic bounds provide a partial-identification solution to the important problem of estimating the effect of state dependence in the presence of unobserved heterogeneity. The impact of T, the number of time periods, is shown by deriving shrinkage rates for the identified set as T grows. We also consider semiparametric, discrete-choice models and find that semiparametric panel bounds can be much tighter than nonparametric bounds. Computationally convenient methods for semiparametric models are presented. We propose a novel inference method that applies in panel data and other settings and show that it produces uniformly valid confidence regions in large samples. We give empirical illustrations.},
  copyright = {{\copyright} 2013 The Econometric Society},
  langid = {english},
  keywords = {labor force participation,nonseparable model,Panel data,perturbed bootstrap,treatment effects,union premium},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chernozhukov et al_2013_Average and Quantile Effects in Nonseparable Panel Models.pdf;/Users/nseewald/Zotero/storage/3FX7MGL7/ECTA8405.html;/Users/nseewald/Zotero/storage/4ZU5XLVA/ECTA8405.html}
}

@article{chernozhukovExactRobustConformal2021,
  title = {An {{Exact}} and {{Robust Conformal Inference Method}} for {{Counterfactual}} and {{Synthetic Controls}}},
  author = {Chernozhukov, Victor and W{\"u}thrich, Kaspar and Zhu, Yinchu},
  year = {2021},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {116},
  number = {536},
  pages = {1849--1864},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2021.1920957},
  urldate = {2022-07-07},
  abstract = {We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. We recast the causal inference problem as a counterfactual prediction and a structural breaks testing problem. This allows us to exploit insights from conformal prediction and structural breaks testing to develop permutation inference procedures that accommodate modern high-dimensional estimators, are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. Our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of the policy intervention. Examples include synthetic controls, difference-indifferences, factor and matrix completion models, and (fused) time series panel data models. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution. Open-source software for implementing our conformal inference methods is available.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chernozhukov et al_2021_An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic.pdf}
}

@article{cheungSequentialMultipleAssignment2015,
  title = {Sequential Multiple Assignment Randomized Trial ({{SMART}}) with Adaptive Randomization for Quality Improvement in Depression Treatment Program: {{SMART}} with {{Adaptive Randomization}}},
  shorttitle = {Sequential Multiple Assignment Randomized Trial ({{SMART}}) with Adaptive Randomization for Quality Improvement in Depression Treatment Program},
  author = {Cheung, Ying Kuen and Chakraborty, Bibhas and Davidson, Karina W.},
  year = {2015},
  month = jun,
  journal = {Biometrics},
  volume = {71},
  number = {2},
  pages = {450--459},
  issn = {0006341X},
  doi = {10.1111/biom.12258},
  urldate = {2018-10-12},
  abstract = {Implementation study is an important tool for deploying state-of-the-art treatments from clinical efficacy studies into a treatment program, with the dual goals of learning about effectiveness of the treatments and improving the quality of care for patients enrolled into the program. In this article, we deal with the design of a treatment program of dynamic treatment regimens (DTRs) for patients with depression post-acute coronary syndrome. We introduce a novel adaptive randomization scheme for a sequential multiple assignment randomized trial of DTRs. Our approach adapts the randomization probabilities to favor treatment sequences having comparatively superior Q-functions used in Q-learning. The proposed approach addresses three main concerns of an implementation study: it allows incorporation of historical data or opinions, it includes randomization for learning purposes, and it aims to improve care via adaptation throughout the program. We demonstrate how to apply our method to design a depression treatment program using data from a previous study. By simulation, we illustrate that the inputs from historical data are important for the program performance measured by the expected outcomes of the enrollees, but also show that the adaptive randomization scheme is able to compensate poorly specified historical inputs by improving patient outcomes within a reasonable horizon. The simulation results also confirm that the proposed design allows efficient learning of the treatments by alleviating the curse of dimensionality.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cheung et al_2015_Sequential multiple assignment randomized trial (SMART) with adaptive.pdf}
}

@article{chiPowerSampleSize2019,
  title = {Power and {{Sample Size}} for {{Fixed-Effects Inference}} in {{Reversible Linear Mixed Models}}},
  author = {Chi, Yueh-Yun and Glueck, Deborah H. and Muller, Keith E.},
  year = {2019},
  month = oct,
  journal = {The American Statistician},
  volume = {73},
  number = {4},
  pages = {350--359},
  issn = {Undefined},
  doi = {10/ggdf24},
  urldate = {2019-11-23},
  annotation = {Saved from BrowZine: http://thirdiron.com/download},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chi et al_2019_Power and Sample Size for Fixed-Effects Inference in Reversible Linear Mixed.pdf}
}

@misc{ChronicleHigherEducation,
  title = {The {{Chronicle}} of {{Higher Education}} {\textbar} {{Higher Ed News}}, {{Opinion}} \& {{Advice}}},
  journal = {The Chronicle of Higher Education},
  urldate = {2022-02-18},
  abstract = {In-depth and breaking news, opinion, advice and jobs for professors, deans and others in higher education from The Chronicle of Higher Education.},
  howpublished = {https://www.chronicle.com/},
  langid = {english}
}

@article{chronis-tuscanoPersonalizedTreatmentMothers2016,
  title = {Personalized {{Treatment}} of {{Mothers With ADHD}} and {{Their Young At-Risk Children}}: {{A SMART Pilot}}},
  shorttitle = {Personalized {{Treatment}} of {{Mothers With ADHD}} and {{Their Young At-Risk Children}}},
  author = {{Chronis-Tuscano}, Andrea and Wang, Christine H. and Strickland, Jennifer and Almirall, Daniel and Stein, Mark A.},
  year = {2016},
  month = jul,
  journal = {Journal of Clinical Child \& Adolescent Psychology},
  volume = {45},
  number = {4},
  pages = {510--521},
  issn = {1537-4416, 1537-4424},
  doi = {10/gg2h36},
  urldate = {2020-12-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Chronis-Tuscano et al_2016_Personalized Treatment of Mothers With ADHD and Their Young At-Risk Children.pdf}
}

@article{ciolinoIdealVsReal2019,
  title = {Ideal vs. Real: A Systematic Review on Handling Covariates in Randomized Controlled Trials},
  shorttitle = {Ideal vs. Real},
  author = {Ciolino, Jody D. and Palac, Hannah L. and Yang, Amy and Vaca, Mireya and Belli, Hayley M.},
  year = {2019},
  month = jul,
  journal = {BMC Medical Research Methodology},
  volume = {19},
  number = {1},
  pages = {136},
  issn = {1471-2288},
  doi = {10/ggf7mt},
  urldate = {2020-01-05},
  abstract = {In theory, efficient design of randomized controlled trials (RCTs) involves randomization algorithms that control baseline variable imbalance efficiently, and corresponding analysis involves pre-specified adjustment for baseline covariates. This review sought to explore techniques for handling potentially influential baseline variables in both the design and analysis phase of RCTs.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ciolino et al_2019_Ideal vs.pdf;/Users/nseewald/Zotero/storage/94JSPGYV/s12874-019-0787-8.html}
}

@article{clancyOpinionUseScience2020,
  title = {Opinion: {{Use}} Science to Stop Sexual Harassment in Higher Education},
  shorttitle = {Opinion},
  author = {Clancy, Kathryn B. H. and Cortina, Lilia M. and Kirkland, Anna R.},
  year = {2020},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {37},
  pages = {22614--22618},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10/ghtv9x},
  urldate = {2021-01-19},
  abstract = {Sexual harassment abounds in academia. We know this from a 2018 report published by the National Academies of Sciences, Engineering, and Medicine (1). As members of the committee who authored that report,* we have presented its findings to colleges and universities around the country. It has been deeply gratifying to see so many leaders want to address sexual harassment in their institutions. But according to a large body of social science evidence, the strategies that many of these same leaders are pursuing simply don't work. Academia should lead and inspire change in other organizations. Instead, we have the highest rate of sexual harassment after the military (2). Several problems stand in the way of effective institutional response to sexual harassment: oversexualization of the problem, overreliance on fast fixes that fail to grapple with long histories of exclusion in the academy, and overemphasis on formal legal compliance. We need a radical redesign of anti-harassment efforts in higher education. This is a tall order, but decades of research can guide this work and brave leaders can implement it. The term ``sexual harassment'' is largely a misnomer. Most sexual harassment entails disrespect, not desire, and certainly not romance (3). There are the occasional come-ons: unwanted sexual advances, touches, kisses, or bribes and threats used to coerce sexual activity. But by far the most prevalent form of sexual harassment is the put-down, or what social scientists call gender harassment : comments, cartoons, jokes, gestures, and other insults to members of one sex/gender group (4, 5). Sometimes the put-downs are sexually degrading and crude, and other times they are contemptuous without sexual content. Women of color are likely to experience harassment that is based in both race and gender stereotypes (6). For lesbian, gay, bisexual, and transgender (LGBT) people, harassment often includes comments {\dots}  [↵][1]1To whom correspondence may be addressed. Email: kclancy\{at\}illinois.edu.  [1]: \#xref-corresp-1-1},
  chapter = {Opinion},
  copyright = {{\copyright} 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {32817430},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Clancy et al_2020_Opinion.pdf;/Users/nseewald/Zotero/storage/E8PVA2TS/22614.html}
}

@article{cnaanUsingGeneralLinear1997,
  title = {Using the General Linear Mixed Model to Analyse Unbalanced Repeated Measures and Longitudinal Data},
  author = {Cnaan, Avital and Laird, Nan M. and Slasor, Peter},
  year = {1997},
  journal = {Statistics in Medicine},
  volume = {16},
  number = {20},
  pages = {2349--2380},
  issn = {1097-0258},
  doi = {10/dw53gs},
  urldate = {2021-05-18},
  abstract = {The general linear mixed model provides a useful approach for analysing a wide variety of data structures which practising statisticians often encounter. Two such data structures which can be problematic to analyse are unbalanced repeated measures data and longitudinal data. Owing to recent advances in methods and software, the mixed model analysis is now readily available to data analysts. The model is similar in many respects to ordinary multiple regression, but because it allows correlation between the observations, it requires additional work to specify models and to assess goodness-of-fit. The extra complexity involved is compensated for by the additional flexibility it provides in model fitting. The purpose of this tutorial is to provide readers with a sufficient introduction to the theory to understand the method and a more extensive discussion of model fitting and checking in order to provide guidelines for its use. We provide two detailed case studies, one a clinical trial with repeated measures and dropouts, and one an epidemiological survey with longitudinal follow-up. {\copyright} 1997 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cnaan et al_1997_Using the general linear mixed model to analyse unbalanced repeated measures.pdf;/Users/nseewald/Zotero/storage/P6DH3XG8/(SICI)1097-0258(19971030)16202349AID-SIM6673.0.html}
}

@article{coffmanConditionNotCondition2016,
  title = {To Condition or Not Condition? {{Analysing}} `Change' in Longitudinal Randomised Controlled Trials},
  shorttitle = {To Condition or Not Condition?},
  author = {Coffman, Cynthia J and Edelman, David and Woolson, Robert F},
  year = {2016},
  month = dec,
  journal = {BMJ Open},
  volume = {6},
  number = {12},
  pages = {e013096},
  issn = {2044-6055, 2044-6055},
  doi = {10/f9j32j},
  urldate = {2019-08-29},
  abstract = {Objective: The statistical analysis for a 2-arm randomised controlled trial (RCT) with a baseline outcome followed by a few assessments at fixed follow-up times typically invokes traditional analytic methods (eg, analysis of covariance (ANCOVA), longitudinal data analysis (LDA)). `Constrained' longitudinal data analysis (cLDA) is a well-established unconditional technique that constrains means of baseline to be equal between arms. We use an analysis of fasting lipid profiles from the Group Medical Clinics (GMC) longitudinal RCT on patients with diabetes to illustrate applications of ANCOVA, LDA and cLDA to demonstrate theoretical concepts of these methods including the impact of missing data. Methods: For the analysis of the illustrated example, all models were fit using linear mixed models to participants with only complete data and to participants using all available data. Results: With complete data (n=195), 95\% CI coverage are equivalent for ANCOVA and cLDA with an estimated 11.2 mg/dL (95\% CI -19.2 to -3.3; p=0.006) lower mean low-density lipoprotein (LDL) cholesterol in GMC compared with usual care. With all available data (n=233), applying the cLDA model yielded an LDL improvement of 8.9 mg/dL (95\% CI -16.7 to -1.0; p=0.03) for GMC compared with usual care. The less efficient, LDA analysis yielded an LDL improvement of 7.2 mg/dL (95\% CI -17.2 to 2.8; p=0.15) for GMC compared with usual care. Conclusions: Under reasonable missing data assumptions, cLDA will yield efficient treatment effect estimates and robust inferential statistics. It may be regarded as the method of choice over ANCOVA and LDA.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Coffman et al_2016_To condition or not condition.pdf}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {2nd ed},
  publisher = {L. Erlbaum Associates},
  address = {Hillsdale, N.J},
  isbn = {978-0-8058-0283-2},
  lccn = {HA29 .C66 1988},
  keywords = {nosource}
}

@article{coleConsistencyStatementCausal2009,
  title = {The {{Consistency Statement}} in {{Causal Inference}}: {{A Definition}} or an {{Assumption}}?},
  shorttitle = {The {{Consistency Statement}} in {{Causal Inference}}},
  author = {Cole, Stephen R. and Frangakis, Constantine E.},
  year = {2009},
  month = jan,
  journal = {Epidemiology},
  volume = {20},
  number = {1},
  pages = {3--5},
  issn = {1044-3983},
  doi = {10/b79cvb},
  urldate = {2021-07-13},
  abstract = {An abstract is unavailable.},
  langid = {american},
  file = {/Users/nseewald/Zotero/storage/VLCBJY7B/The_Consistency_Statement_in_Causal_Inference__A.3.html}
}

@article{coleConstructingInverseProbability2008,
  title = {Constructing Inverse Probability Weights for Marginal Structural Models},
  author = {Cole, Stephen R. and Hern{\'a}n, Miguel A.},
  year = {2008},
  journal = {American Journal of Epidemiology},
  volume = {168},
  number = {6},
  pages = {656--664},
  issn = {00029262},
  doi = {10.1093/aje/kwn164},
  abstract = {The method of inverse probability weighting (henceforth, weighting) can be used to adjust for measured confounding and selection bias under the four assumptions of consistency, exchangeability, positivity, and no misspecification of the model used to estimate weights. In recent years, several published estimates of the effect of time-varying exposures have been based on weighted estimation of the parameters of marginal structural models because, unlike standard statistical methods, weighting can appropriately adjust for measured time-varying confounders affected by prior exposure. As an example, the authors describe the last three assumptions using the change in viral load due to initiation of antiretroviral therapy among 918 human immunodeficiency virus-infected US men and women followed for a median of 5.8 years between 1996 and 2005. The authors describe possible tradeoffs that an epidemiologist may encounter when attempting to make inferences. For instance, a tradeoff between bias and precision is illustrated as a function of the extent to which confounding is controlled. Weight truncation is presented as an informal and easily implemented method to deal with these tradeoffs. Inverse probability weighting provides a powerful methodological tool that may uncover causal effects of exposures that are otherwise obscured. However, as with all methods, diagnostics and sensitivity analyses are essential for proper use.},
  pmid = {18682488},
  keywords = {Bias (epidemiology),Causality,Confounding factors (epidemiology),Probability weighting,Regression model},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cole_Hernán_2008_Constructing inverse probability weights for marginal structural models.pdf}
}

@article{coleMarginalStructuralModels2005,
  title = {Marginal {{Structural Models}} for {{Estimating}} the {{Effect}} of {{Highly Active Antiretroviral Therapy Initiation}} on {{CD4 Cell Count}}},
  author = {Cole, Stephen R. and Hern{\'a}n, Miguel A. and Margolick, Joseph B. and Cohen, Mardge H. and Robins, James M.},
  year = {2005},
  month = sep,
  journal = {American Journal of Epidemiology},
  volume = {162},
  number = {5},
  pages = {471--478},
  issn = {0002-9262},
  doi = {10.1093/aje/kwi216},
  urldate = {2023-09-06},
  abstract = {The effect of highly active antiretroviral therapy (HAART) on the evolution of CD4-positive T-lymphocyte (CD4 cell) count among human immunodeficiency virus (HIV)-positive participants was estimated using inverse probability-of-treatment-and-censoring (IPTC)-weighted estimation of a marginal structural model. Of 1,763 eligible participants from two US cohort studies followed between 1996 and 2002, 60 percent initiated HAART. The IPTC-weighted estimate of the difference in mean CD4 cell count at 1 year among participants continuously treated versus those never treated was 71 cells/mm3 (95\% confidence interval: 47.5, 94.6), which agrees with the reported results of randomized experiments. The corresponding estimate from a standard generalized estimating equations regression model that included baseline and most recent CD4 cell count and HIV type 1 RNA viral load as regressors was 26 cells/mm3 (95\% confidence interval: 17.7, 34.3). These results indicate that nonrandomized studies of HIV treatment need to be analyzed with methods (e.g., IPTC-weighted estimation) that, in contrast to standard methods, appropriately adjust for time-varying covariates that are simultaneously confounders and intermediate variables. The 1-year estimate of 71 cells/mm3 was followed by an estimated continued increase of 29 cells/mm3 per year (estimated effect at 6 years: 216 cells/mm3), providing evidence that the large short-term effect found in randomized experiments persists and continues to improve over 6 years.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cole et al_2005_Marginal Structural Models for Estimating the Effect of Highly Active.pdf;/Users/nseewald/Zotero/storage/29Z2QQ8E/82419.html}
}

@incollection{collinsBalancedUnbalancedReduced2018,
  title = {Balanced and {{Unbalanced Reduced Factorial Designs}}},
  booktitle = {Optimization of {{Behavioral}}, {{Biobehavioral}}, and {{Biomedical Interventions}}: {{The Multiphase Optimization Strategy}} ({{MOST}})},
  author = {Collins, Linda M.},
  editor = {Collins, Linda M.},
  year = {2018},
  series = {Statistics for {{Social}} and {{Behavioral Sciences}}},
  pages = {145--191},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-72206-1_5},
  urldate = {2020-12-13},
  abstract = {In this chapter, balanced and unbalanced reduced factorial designs for use in optimization of multicomponent behavioral, biobehavioral, and biomedical interventions are discussed. These designs, which include a carefully selected subset of the experimental conditions in a corresponding complete factorial, can be more efficient and economical than complete factorials when implementation of experimental conditions is expensive or logistically challenging. However, there are important trade-offs that the investigator must make in exchange for this efficiency and economy. Reduced factorial designs are not for every situation, but when used appropriately and strategically, they can make excellent use of limited research resources. Readers should be familiar with the material in all previous chapters, particularly Chaps. 3 and 4.},
  isbn = {978-3-319-72206-1},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Collins_2018_Balanced and Unbalanced Reduced Factorial Designs.pdf}
}

@article{collinsConceptualFrameworkAdaptive2004,
  title = {A {{Conceptual Framework}} for {{Adaptive Preventive Interventions}}},
  author = {Collins, Linda M. and Murphy, Susan A. and Bierman, Karen L.},
  year = {2004},
  month = sep,
  journal = {Prevention science : the official journal of the Society for Prevention Research},
  volume = {5},
  number = {3},
  pages = {185--196},
  issn = {1389-4986},
  urldate = {2022-10-12},
  abstract = {Recently, adaptive interventions have emerged as a new perspective on prevention and treatment. Adaptive interventions resemble clinical practice in that different dosages of certain prevention or treatment components are assigned to different individuals, and/or within individuals across time, with dosage varying in response to the intervention needs of individuals. To determine intervention need and thus assign dosage, adaptive interventions use prespecified decision rules based on each participant's values on key characteristics, called tailoring variables. In this paper, we offer a conceptual framework for adaptive interventions, discuss principles underlying the design and evaluation of such interventions, and review some areas where additional research is needed.},
  pmcid = {PMC3544191},
  pmid = {15470938},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Collins et al_2004_A Conceptual Framework for Adaptive Preventive Interventions.pdf}
}

@incollection{collinsIntroductionFactorialOptimization2018,
  title = {Introduction to the {{Factorial Optimization Trial}}},
  booktitle = {Optimization of {{Behavioral}}, {{Biobehavioral}}, and {{Biomedical Interventions}}: {{The Multiphase Optimization Strategy}} ({{MOST}})},
  author = {Collins, Linda M.},
  editor = {Collins, Linda M.},
  year = {2018},
  series = {Statistics for {{Social}} and {{Behavioral Sciences}}},
  pages = {67--113},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-72206-1_3},
  urldate = {2020-12-13},
  abstract = {In the multiphase optimization strategy (MOST), an intervention is optimized before it is evaluated in an RCT. The optimization is based on empirical evidence gathered in an optimization trial. This chapter reviews the factorial experiment, which is often a highly efficient way of conducting an optimization trial. The factorial experiment can be used to estimate the effectiveness of each candidate component and also to estimate the extent to which the effect of a component depends on the levels of one or more other components. This chapter also reviews the conclusion-priority and decision-priority perspectives on evaluating empirical data and when it is appropriate to take each perspective. Readers should be familiar with the material in Chaps. 1 and 2.},
  isbn = {978-3-319-72206-1},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Collins_2018_Introduction to the Factorial Optimization Trial.pdf}
}

@article{collinsNewInitiativePrecision2015,
  title = {A {{New Initiative}} on {{Precision Medicine}}},
  author = {Collins, Francis S. and Varmus, Harold},
  year = {2015},
  month = feb,
  journal = {New England Journal of Medicine},
  volume = {372},
  number = {9},
  pages = {793--795},
  issn = {0028-4793, 1533-4406},
  doi = {10/gfvd4g},
  urldate = {2020-12-11},
  langid = {english},
  keywords = {nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Collins_Varmus_2015_A New Initiative on Precision Medicine.pdf}
}

@article{collinsOptimizationBehavioralDynamic2014,
  title = {Optimization of Behavioral Dynamic Treatment Regimens Based on the Sequential, Multiple Assignment, Randomized Trial ({{SMART}})},
  author = {Collins, Linda M and {Nahum-Shani}, Inbal and Almirall, Daniel},
  year = {2014},
  month = aug,
  journal = {Clinical Trials},
  volume = {11},
  number = {4},
  pages = {426--434},
  issn = {1740-7745, 1740-7753},
  doi = {10/f6cjxm},
  urldate = {2020-12-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Collins et al_2014_Optimization of behavioral dynamic treatment regimens based on the sequential,.pdf}
}

@article{collinsTransparentReportingMultivariable2015,
  title = {Transparent {{Reporting}} of a Multivariable Prediction Model for {{Individual Prognosis Or Diagnosis}} ({{TRIPOD}}): The {{TRIPOD Statement}}},
  shorttitle = {Transparent {{Reporting}} of a Multivariable Prediction Model for {{Individual Prognosis Or Diagnosis}} ({{TRIPOD}})},
  author = {Collins, G S and Reitsma, J B and Altman, D G and Moons, K G M},
  year = {2015},
  month = feb,
  journal = {British Journal of Surgery},
  volume = {102},
  number = {3},
  pages = {148--158},
  issn = {0007-1323},
  doi = {10.1002/bjs.9736},
  urldate = {2022-09-08},
  abstract = {Prediction models are developed to aid healthcare providers in estimating the probability or risk that a specific disease or condition is present (diagnostic models) or that a specific event will occur in the future (prognostic models), to inform their decision-making. However, the overwhelming evidence shows that the quality of reporting of prediction model studies is poor. Only with full and clear reporting of information on all aspects of a prediction model can risk of bias and potential usefulness of prediction models be adequately assessed. The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) Initiative developed a set of recommendations for the reporting of studies developing, validating or updating a prediction model, whether for diagnostic or prognostic purposes. This article describes how the TRIPOD Statement was developed.An extensive list of items based on a review of the literature was created, which was reduced after a web-based survey and revised during a 3-day meeting in June 2011 with methodologists, healthcare professionals and journal editors. The list was refined during several meetings of the steering group and in e-mail discussions with the wider group of TRIPOD contributors.The resulting TRIPOD Statement is a checklist of 22 items, deemed essential for transparent reporting of a prediction model study.The TRIPOD Statement aims to improve the transparency of the reporting of a prediction model study regardless of the study methods used. The TRIPOD Statement is best used in conjunction with the TRIPOD explanation and elaboration document. A complete checklist is available at http://www.tripod-statement.org.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Collins et al_2015_Transparent Reporting of a multivariable prediction model for Individual.pdf;/Users/nseewald/Zotero/storage/YZP8965Q/6136428.html}
}

@article{collyerEyeBeholderHow2024,
  title = {The Eye of the Beholder: How Do Public Health Researchers Interpret Regression Coefficients? {{A}} Qualitative Study},
  shorttitle = {The Eye of the Beholder},
  author = {Collyer, Taya A.},
  year = {2024},
  month = jan,
  journal = {BMC Public Health},
  volume = {24},
  number = {1},
  pages = {10},
  issn = {1471-2458},
  doi = {10.1186/s12889-023-17541-3},
  urldate = {2024-01-03},
  abstract = {Calls for improved statistical literacy and transparency in population health research are widespread, but empirical accounts describing how researchers understand statistical methods are lacking. To address this gap, this study aimed to explore variation in researchers' interpretations and understanding of regression coefficients, and the extent to which these statistics are viewed as straightforward statements about health.},
  keywords = {Biostatistics,Health equity,Regression,Sociology of science,Statistical knowledge,Statistical practice},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Collyer_2024_The_eye_of_the_beholder_annotated.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Collyer_2024_The_eye_of_the_beholder.pdf;/Users/nseewald/Zotero/storage/2DGEFI3W/s12889-023-17541-3.html}
}

@article{ComparisonThreeMethods2002,
  title = {Comparison of {{Three Methods}} for {{Estimating}} the {{Standard Error}} of the {{Area}} under the {{Curve}} in {{ROC Analysis}} of {{Quantitative Data}}},
  year = {2002},
  month = nov,
  journal = {Academic Radiology},
  volume = {9},
  number = {11},
  pages = {1278--1285},
  publisher = {Elsevier},
  issn = {1076-6332},
  doi = {10.1016/S1076-6332(03)80561-5},
  urldate = {2024-08-30},
  abstract = {Several methods have been proposed for estimating the standard error (SE) of the area under the curve (AUC) in receiver operating characteristic analy{\dots}},
  langid = {american}
}

@article{cookDesignAnalysisMethods1983,
  ids = {cookDesignAnalysisMethods1983a,cookDesignAnalysisMethods1983b},
  title = {Design and {{Analysis Methods}} for {{Longitudinal Research}}},
  author = {Cook, Nancy R. and Ware, James H.},
  year = {1983},
  month = may,
  journal = {Annual Review of Public Health},
  volume = {4},
  number = {1},
  pages = {1--23},
  issn = {0163-7525, 1545-2093},
  doi = {10/br5tbh},
  urldate = {2018-12-07},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cook_Ware_1983_Design and Analysis Methods for Longitudinal Research.pdf}
}

@misc{CorrectingDependentCensoring,
  title = {Correcting for Dependent Censoring in Routine Outcome Monitoring Data by Applying the Inverse Probability Censoring Weighted Estimator},
  doi = {10.1177/0962280216628900},
  urldate = {2024-07-18},
  howpublished = {https://journals.sagepub.com/doi/epub/10.1177/0962280216628900},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Correcting for dependent censoring in routine outcome monitoring data by.pdf;/Users/nseewald/Zotero/storage/5739HEMI/0962280216628900.html}
}

@article{coxRegressionModelsLifeTables1972,
  title = {Regression {{Models}} and {{Life-Tables}}},
  author = {Cox, D. R.},
  year = {1972},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {34},
  number = {2},
  eprint = {2985181},
  eprinttype = {jstor},
  pages = {187--220},
  abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cox_1972_Regression Models and Life-Tables.pdf}
}

@article{coxStatisticalThinking21st2017,
  title = {Statistical Thinking for 21st Century Scientists},
  author = {Cox, D. R. and Efron, Bradley},
  year = {2017},
  month = jun,
  journal = {Science Advances},
  volume = {3},
  number = {6},
  pages = {e1700768},
  issn = {2375-2548},
  doi = {10/gbtwnr},
  urldate = {2020-02-18},
  abstract = {Statistical science provides a wide range of concepts and methods for studying situations subject to unexplained variability. Such considerations enter fields ranging from particle physics and astrophysics to genetics, sociology and economics, and beyond; to associated areas of application such as engineering, agriculture, and medicine, in particular in clinical trials. Successful application hinges on absorption of statistical thinking into the subject matter and, hence, depends strongly on the field in question and on the individual investigators. It is the job of theoretical statisticians both to be alive to the challenges of specific applications and, at the same time, to develop methods and concepts that, with good fortune, will be broadly applicable. We offer a brief review of important questions concerning modern inference and how these questions relate to classical statistics. We offer a brief review of important questions concerning modern inference and how these questions relate to classical statistics.},
  copyright = {Copyright {\copyright} 2017, The Authors. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cox_Efron_2017_Statistical thinking for 21st century scientists.pdf;/Users/nseewald/Zotero/storage/MDP33B9B/tab-pdf.html}
}

@article{cropanzanoMoralVirtuesFairness2001,
  title = {Moral {{Virtues}}, {{Fairness Heuristics}}, {{Social Entities}}, and {{Other Denizens}} of {{Organizational Justice}}},
  author = {Cropanzano, Russell and Byrne, Zinta S. and Bobocel, D.Ramona and Rupp, Deborah E.},
  year = {2001},
  month = apr,
  journal = {Journal of Vocational Behavior},
  volume = {58},
  number = {2},
  pages = {164--209},
  issn = {00018791},
  doi = {10.1006/jvbe.2001.1791},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Cropanzano et al_2001_Moral Virtues, Fairness Heuristics, Social Entities, and Other Denizens of.pdf}
}

@article{crowderUseWorkingCorrelation1995,
  title = {On the Use of a Working Correlation Matrix in Using Generalised Linear Models for Repeated Measures},
  author = {Crowder, Martin},
  year = {1995},
  journal = {Biometrika},
  volume = {82},
  number = {2},
  pages = {407--10},
  doi = {10/cq2kkh},
  abstract = {In a seminal paper Liang \& Zeger (1986) extended the use of generalised linear models to repeated measures data. They based the analysis on specifications for the means and variances of the observations, as usual for generalised linear models, but showed how specifications for the correlations between measurements made on the same unit could be avoided by using a 'working' correlation matrix. In some cases the parameters involved in this matrix are subject to an uncertainty of definition which can lead to a breakdown of the asymptotic properties of the estimators.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Crowder_1995_On the use of a working correlation matrix in using generalised linear models.pdf}
}

@book{d.allisonFixedEffectsRegression2009,
  title = {Fixed {{Effects Regression Models}}},
  author = {D.Allison, Paul},
  year = {2009},
  publisher = {SAGE Publications, Inc.},
  doi = {10.4135/9781412993869},
  urldate = {2024-07-12},
  abstract = {{$<$}p{$>$}This book will show how to estimate and interpret fixed-effects models in a variety of different modeling contexts: linear models, logistic models, Poisson m},
  isbn = {978-1-4129-9386-9},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/7MWXQUY6/fixed-effects-regression-models.html}
}

@article{daaneTeachingRacialEquity2017,
  title = {Teaching {{About Racial Equity}} in {{Introductory Physics Courses}}},
  author = {Daane, Abigail R. and Decker, Sierra R. and Sawtelle, Vashti},
  year = {2017},
  month = aug,
  journal = {The Physics Teacher},
  volume = {55},
  number = {6},
  pages = {328--333},
  publisher = {American Association of Physics Teachers},
  issn = {0031-921X},
  doi = {10/gg6d3c},
  urldate = {2020-07-30},
  abstract = {Even after you have decided to tackle a problem like racial equity, it may seem daunting           to broach the subject in a physics classroom. After all, the idea of a (typically White)           instructor in power tackling a sensitive topic such as social justice can be scary in             any (mostly White) classroom. Not only that, but physics is typically           viewed as a ``culture with no culture.'' The physicist's quest for objectivity, along with a           general focus on a fixed set of laws and formulae, support the treatment of this subject           as untouched by people. Sometimes it is easier to ignore the problem and just focus on the           Conservation of Energy Principle. However, ignoring the striking underrepresentation of           ethnic/racial minorities and women in both the physics classroom and the field at large is           a great disservice to all our students. We take the position that the persistence of           representation disparities in physics is evidence that culture plays a role in who and           what is involved in physics. Instructors have an opportunity to explicitly address the           absence of equitable circumstances in classrooms and highlight the obstacles that           contribute to the disparity (e.g., varied access to learning opportunities and support           structures, dominant cultural norms, stereotype threat, implicit bias, hidden curricula,           etc.). We acknowledge that incorporating these discussions in a physics classroom is           fraught with difficulty, but we also believe that trying to lead these           discussions is better than ignoring the problem. Furthermore, a set of resources for           teachers interested in leading these discussions has been developing in the physics           teacher community. Rifkin offers resources for leading a two-week unit on equity designed           for secondary science classrooms. Here we describe another possible pathway for           integrating a shorter equity unit into the traditional content of a (predominantly White)           university physics classroom, addressing racial inequity and sharing common student           responses that may arise.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Daane et al_2017_Teaching About Racial Equity in Introductory Physics Courses.pdf;/Users/nseewald/Zotero/storage/T2E65ZEZ/1.html}
}

@article{dafniLandmarkAnalysis25Year2011,
  title = {Landmark {{Analysis}} at the 25-{{Year Landmark Point}}},
  author = {Dafni, Urania},
  year = {2011},
  month = may,
  journal = {Circulation: Cardiovascular Quality and Outcomes},
  volume = {4},
  number = {3},
  pages = {363--371},
  publisher = {American Heart Association},
  doi = {10.1161/CIRCOUTCOMES.110.957951},
  urldate = {2024-02-04},
  abstract = {This statistical primer presents the landmark analysis method, exploring its appropriate use and interpretation while recognizing its limitations. This observational method is used for comparing time-to-event outcome between groups determined during study follow-up. The goal of the landmark method is to estimate in an unbiased way the time-to-event probabilities in each group conditional on the group membership of patients at a specific time point, the landmark time. The need that led to its development, the impact of the method, and its pros and cons, along with available alternative approaches, are presented. Simulations explore its performance, using realistic parameters from a recent cardiovascular study. As long as the limitations of the method are recognized and the interpretation of its results clearly reflect their ``conditional'' nature, landmark analysis, 25 years from its introduction, can still be of value.},
  keywords = {_tablet,clinical trials,observational studies,prognostic factors,time-to-event outcome,time-varying covariate},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dafni_2011_Landmark_Analysis_at_the_25-Year_Landmark_Point.pdf}
}

@article{dahabrehCausalInferenceEffects2024,
  title = {Causal {{Inference About}} the {{Effects}} of {{Interventions From Observational Studies}} in {{Medical Journals}}},
  author = {Dahabreh, Issa J. and {Bibbins-Domingo}, Kirsten},
  year = {2024},
  month = jun,
  journal = {JAMA},
  volume = {331},
  number = {21},
  pages = {1845--1853},
  issn = {0098-7484},
  doi = {10.1001/jama.2024.7741},
  urldate = {2024-07-12},
  abstract = {Many medical journals, including JAMA, restrict the use of causal language to the reporting of randomized clinical trials. Although well-conducted randomized clinical trials remain the preferred approach for answering causal questions, methods for observational studies have advanced such that causal interpretations of the results of well-conducted observational studies may be possible when strong assumptions hold. Furthermore, observational studies may be the only practical source of information for answering some questions about the causal effects of medical or policy interventions, can support the study of interventions in populations and settings that reflect practice, and can help identify interventions for further experimental investigation. Identifying opportunities for the appropriate use of causal language when describing observational studies is important for communication in medical journals.A structured approach to whether and how causal language may be used when describing observational studies would enhance the communication of research goals, support the assessment of assumptions and design and analytic choices, and allow for more clear and accurate interpretation of results. Building on the extensive literature on causal inference across diverse disciplines, we suggest a framework for observational studies that aim to provide evidence about the causal effects of interventions based on 6 core questions: what is the causal question; what quantity would, if known, answer the causal question; what is the study design; what causal assumptions are being made; how can the observed data be used to answer the causal question in principle and in practice; and is a causal interpretation of the analyses tenable?Adoption of the proposed framework to identify when causal interpretation is appropriate in observational studies promises to facilitate better communication between authors, reviewers, editors, and readers. Practical implementation will require cooperation between editors, authors, and reviewers to operationalize the framework and evaluate its effect on the reporting of empirical research.},
  file = {/Users/nseewald/Zotero/storage/I7RHICLX/2818746.html}
}

@article{dahabrehUsingTrialObservational2023,
  title = {Using {{Trial}} and {{Observational Data}} to {{Assess Effectiveness}}: {{Trial Emulation}}, {{Transportability}}, {{Benchmarking}}, and {{Joint Analysis}}},
  shorttitle = {Using {{Trial}} and {{Observational Data}} to {{Assess Effectiveness}}},
  author = {Dahabreh, Issa J and Matthews, Anthony and Steingrimsson, Jon A and Scharfstein, Daniel O and Stuart, Elizabeth A},
  year = {2023},
  month = feb,
  journal = {Epidemiologic Reviews},
  pages = {mxac011},
  issn = {1478-6729},
  doi = {10.1093/epirev/mxac011},
  urldate = {2023-08-03},
  abstract = {Comparisons between randomized trial analyses and observational analyses that attempt to address similar research questions have generated many controversies in epidemiology and the social sciences. There has been little consensus on when such comparisons are reasonable, what their implications are for the validity of observational analyses, or whether trial and observational analyses can be integrated to address effectiveness questions. Here, we consider methods for using observational analyses to complement trial analyses when assessing treatment effectiveness. First, we review the framework for designing observational analyses that emulate target trials and present an evidence map of its recent applications. We then review approaches for estimating the average treatment effect in the target population underlying the emulation: using observational analyses of the emulation data alone; and using transportability analyses to extend inferences from a trial to the target population. We explain how comparing treatment effect estimates from the emulation against those from the trial can provide evidence on whether observational analyses can be trusted to deliver valid estimates of effectiveness -- a process we refer to as benchmarking -- and, in some cases, allow the joint analysis of the trial and observational data. We illustrate different approaches using a simplified example of a pragmatic trial and its emulation in registry data. We conclude that synthesizing trial and observational data -- in transportability, benchmarking, or joint analyses -- can leverage their complementary strengths to enhance learning about comparative effectiveness, through a process combining quantitative methods and epidemiological judgements.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dahabreh et al_2023_Using Trial and Observational Data to Assess Effectiveness.pdf;/Users/nseewald/Zotero/storage/TJDRI8J7/7031231.html}
}

@article{dahmenGeneralizedEstimatingEquations2004,
  title = {Generalized Estimating Equations in Controlled Clinical Trials: {{Hypotheses}} Testing},
  author = {Dahmen, G. and Ziegler, A.},
  year = {2004},
  journal = {Biometrical Journal},
  volume = {46},
  number = {2},
  pages = {214--232},
  issn = {03233847},
  doi = {10.1002/bimj.200310018},
  abstract = {Generalized estimating equations (GEE) for the analysis of clustered data have gained increasing popu- larity. Recently, the first monograph on this method has been published. GEE have been repeatedly applied in controlled clinical trials. They have, however, been generally used as secondary or supple- mentary analysis. Instead, the primary analysis was mostly based on a classical method that usually ignored the clustered mostly longitudinal nature of the data. In this paper, we discuss the applic- ability of GEE as primary analysis in controlled clinical trials. From theoretical results in the literature, we derive recommendations how GEE should be used in therapeutic studies for testing statistical hy- potheses. We hope that our paper is the starting point for a thorough discussion on the most appropriate analysis of controlled clinical trials with clustered dependent variables.},
  keywords = {Controlled clinical trials,Generalized estimating equations,Missing data,Sample size calculations,Weighted estimating equations},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dahmen_Ziegler_2004_Generalized estimating equations in controlled clinical trials.pdf}
}

@article{daiBehaviouralNudgesIncrease2021,
  title = {Behavioural Nudges Increase {{COVID-19}} Vaccinations},
  author = {Dai, Hengchen and Saccardo, Silvia and Han, Maria A. and Roh, Lily and Raja, Naveen and Vangala, Sitaram and Modi, Hardikkumar and Pandya, Shital and Sloyan, Michael and Croymans, Daniel M.},
  year = {2021},
  month = sep,
  journal = {Nature},
  volume = {597},
  number = {7876},
  pages = {404--409},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03843-2},
  urldate = {2022-10-12},
  abstract = {Enhancing vaccine uptake is a critical public health challenge1. Overcoming vaccine hesitancy2,3 and failure to follow through on vaccination intentions3 requires effective communication strategies3,4. Here we present two sequential randomized controlled trials to test the effect of behavioural interventions on the uptake of COVID-19 vaccines. We designed text-based reminders that make vaccination salient and easy, and delivered them to participants drawn from a healthcare system one day (first randomized controlled trial) (n~=~93,354~participants; clinicaltrials number NCT04800965) and eight days (second randomized controlled trial) (n~=~67,092 individuals; clinicaltrials number NCT04801524) after they received a notification of vaccine eligibility. The first reminder boosted appointment and vaccination rates within the healthcare system by 6.07 (84\%) and 3.57 (26\%) percentage points, respectively; the second reminder increased those outcomes by 1.65 and 1.06 percentage points, respectively. The first reminder had a greater effect when it was designed to make participants feel ownership of the vaccine dose. However, we found no evidence that combining the first reminder with a video-based information intervention designed to address vaccine hesitancy heightened its effect. We performed online studies (n~=~3,181~participants) to examine vaccination intentions, which revealed patterns that diverged from those of the first randomized controlled trial; this underscores the importance of pilot-testing interventions in the field. Our findings inform the design of behavioural nudges for promoting health decisions5, and highlight the value of making vaccination easy and inducing feelings of ownership over vaccines.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Decision making,Human behaviour},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dai et al_2021_Behavioural nudges increase COVID-19 vaccinations.pdf;/Users/nseewald/Zotero/storage/NMWDR5JF/s41586-021-03843-2.html}
}

@incollection{daleniusIntroductionNeyman19341992,
  title = {Introduction to {{Neyman}} (1934) {{On}} the {{Two Different Aspects}} of the {{Representative Method}}: {{The Method}} of {{Stratified Sampling}} and the {{Method}} of {{Purposive Selection}}},
  shorttitle = {Introduction to {{Neyman}} (1934) {{On}} the {{Two Different Aspects}} of the {{Representative Method}}},
  booktitle = {Breakthroughs in {{Statistics}}},
  author = {Dalenius, T.},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  year = {1992},
  pages = {115--122},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4380-9_11},
  urldate = {2018-11-28},
  isbn = {978-0-387-94039-7 978-1-4612-4380-9},
  keywords = {nosource}
}

@article{danaeiElectronicMedicalRecords2018,
  title = {Electronic Medical Records Can Be Used to Emulate Target Trials of Sustained Treatment Strategies},
  author = {Danaei, Goodarz and Garc{\'i}a Rodr{\'i}guez, Luis Alberto and Cantero, Oscar Fern{\'a}ndez and Logan, Roger W. and Hern{\'a}n, Miguel A.},
  year = {2018},
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {96},
  pages = {12--22},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2017.11.021},
  urldate = {2024-11-06},
  abstract = {Objective To emulate three target trials: single treatment vs. no treatment, joint treatment vs. no treatment, and head-to-head comparison of two treatments, we explain how to estimate the observational analogs of intention-to-treat and per-protocol effects, using hazard ratios and survival curves. For per-protocol effects, we describe two methods for adherence adjustment via inverse-probability weighting. Study Design and Setting Prospective observational study using electronic medical records of individuals aged 55--84 with coronary heart disease from {$>$}500 practices in the United Kingdom between 2000 and~2010. Results The intention-to-treat mortality hazard ratio (95\% confidence interval) was 0.90 (0.84, 0.97) for statins vs. no treatment, 0.88 (0.73, 1.06) for statins plus antihypertensives vs. no treatment, and 0.91 (0.77, 1.06) for atorvastatin vs. simvastatin. When censoring nonadherent person-times, the per-protocol mortality hazard ratio was 0.74 (0.64, 0.85) for statins vs. no treatment, 0.55 (0.35, 0.87) for statins plus antihypertensives vs. no treatment, and 1.13 (0.88, 1.45) for atorvastatin vs. simvastatin. We estimated per-protocol hazard ratios for a 5-year treatment using different dose-response marginal structural models and standardized survival curves for each target trial using intention-to-treat and per-protocol analyses. Conclusion When randomized trials are not available or feasible, observational analyses can emulate a variety of target trials.},
  keywords = {Comparative effectiveness,Confounding,Electronic health records,Medication adherence,Secondary prevention,Survival analysis},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Danaei et al. - 2018 - Electronic medical records can be used to emulate target trials of sustained treatment strategies.pdf;/Users/nseewald/Zotero/storage/8RKIZUPM/S0895435617304997.html}
}

@article{danaeiObservationalDataComparative2013,
  title = {Observational Data for Comparative Effectiveness Research: An Emulation of Randomised Trials of Statins and Primary Prevention of Coronary Heart Disease},
  shorttitle = {Observational Data for Comparative Effectiveness Research},
  author = {Danaei, Goodarz and Rodr{\'i}guez, Luis A. Garc{\'i}a and Cantero, Oscar Fern{\'a}ndez and Logan, Roger and Hern{\'a}n, Miguel A.},
  year = {2013},
  month = feb,
  journal = {Statistical Methods in Medical Research},
  volume = {22},
  number = {1},
  pages = {70--96},
  issn = {1477-0334},
  doi = {10/bqmmf9},
  abstract = {This article reviews methods for comparative effectiveness research using observational data. The basic idea is using an observational study to emulate a hypothetical randomised trial by comparing initiators versus non-initiators of treatment. After adjustment for measured baseline confounders, one can then conduct the observational analogue of an intention-to-treat analysis. We also explain two approaches to conduct the analogues of per-protocol and as-treated analyses after further adjusting for measured time-varying confounding and selection bias using inverse-probability weighting. As an example, we implemented these methods to estimate the effect of statins for primary prevention of coronary heart disease (CHD) using data from electronic medical records in the UK. Despite strong confounding by indication, our approach detected a potential benefit of statin therapy. The analogue of the intention-to-treat hazard ratio (HR) of CHD was 0.89 (0.73, 1.09) for statin initiators versus non-initiators. The HR of CHD was 0.84 (0.54, 1.30) in the per-protocol analysis and 0.79 (0.41, 1.41) in the as-treated analysis for 2 years of use versus no use. In contrast, a conventional comparison of current users versus never users of statin therapy resulted in a HR of 1.31 (1.04, 1.66). We provide a flexible and annotated SAS program to implement the proposed analyses.},
  langid = {english},
  pmcid = {PMC3613145},
  pmid = {22016461},
  keywords = {Coronary Disease,Humans,Medical Records Systems Computerized,Randomized Controlled Trials as Topic,United Kingdom},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Danaei et al_2013_Observational data for comparative effectiveness research.pdf}
}

@article{dartTrendsOpioidAnalgesic2015,
  title = {Trends in {{Opioid Analgesic Abuse}} and {{Mortality}} in the {{United States}}},
  author = {Dart, Richard C. and Surratt, Hilary L. and Cicero, Theodore J. and Parrino, Mark W. and Severtson, S. Geoff and {Bucher-Bartelson}, Becki and Green, Jody L.},
  year = {2015},
  month = jan,
  journal = {New England Journal of Medicine},
  volume = {372},
  number = {3},
  pages = {241--248},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMsa1406143},
  urldate = {2022-08-07},
  abstract = {Whatever the measure, the past two decades have been characterized by increasing abuse and diversion of prescription drugs, including opioid medications, in the United States. An estimated 25 million people initiated nonmedical use of pain relievers between 2002 and 2011.1 The number of deaths per year attributed to prescription opioid medications reached 16,651 in 2010.2 In response to the epidemic, hundreds of local, regional, state, and federal interventions have been implemented. For example, 49 states have enacted legislation to create prescription-drug monitoring programs.3 The U.S. Office of National Drug Control Policy has responded to the epidemic with numerous recommendations, including . . .},
  pmid = {25587948},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dart et al_2015_Trends in Opioid Analgesic Abuse and Mortality in the United States.pdf}
}

@inproceedings{dasilvaDealingNonstationaryEnvironments2006,
  title = {Dealing with Non-Stationary Environments Using Context Detection},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Machine Learning}}},
  author = {{da Silva}, Bruno C. and Basso, Eduardo W. and Bazzan, Ana L. C. and Engel, Paulo M.},
  year = {2006},
  pages = {217--224},
  publisher = {ACM Press},
  address = {Pittsburgh, Pennsylvania},
  doi = {10.1145/1143844.1143872},
  urldate = {2018-10-12},
  abstract = {In this paper we introduce RL-CD, a method for solving reinforcement learning problems in non-stationary environments. The method is based on a mechanism for creating, updating and selecting one among several partial models of the environment. The partial models are incrementally built according to the system's capability of making predictions regarding a given sequence of observations. We propose, formalize and show the efficiency of this method both in a simple non-stationary environment and in a noisy scenario. We show that RL-CD performs better than two standard reinforcement learning algorithms and that it has advantages over methods specifically designed to cope with non-stationarity. Finally, we present known limitations of the method and future works.},
  isbn = {978-1-59593-383-6},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/da Silva et al_2006_Dealing with non-stationary environments using context detection.pdf}
}

@article{daumitBehavioralWeightLossIntervention2013,
  title = {A {{Behavioral Weight-Loss Intervention}} in {{Persons}} with {{Serious Mental Illness}}},
  author = {Daumit, Gail L. and Dickerson, Faith B. and Wang, Nae-Yuh and Dalcin, Arlene and Jerome, Gerald J. and Anderson, Cheryl A.M. and Young, Deborah R. and Frick, Kevin D. and Yu, Airong and Gennusa, Joseph V. and Oefinger, Meghan and Crum, Rosa M. and Charleston, Jeanne and Casagrande, Sarah S. and Guallar, Eliseo and Goldberg, Richard W. and Campbell, Leslie M. and Appel, Lawrence J.},
  year = {2013},
  month = apr,
  journal = {New England Journal of Medicine},
  volume = {368},
  number = {17},
  pages = {1594--1602},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMoa1214530},
  urldate = {2021-11-15},
  abstract = {Persons with serious mental illness, such as schizophrenia, bipolar disorder, and major depression, have mortality rates that are two to more than three times as high as the rate in the overall population, and the primary cause of death in such persons is cardiovascular disease.1--4 Concomitantly, this vulnerable population has an extremely high prevalence of obesity, nearly twice that of the overall population.5--9 Therefore, it is not surprising that persons with serious mental illness have an increased burden of weight-related conditions, including heightened risk of diabetes mellitus, hypertension, dyslipidemia, and certain cancers.10--15 Obesity is multifactorial in persons . . .},
  pmid = {23517118},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Daumit et al_2013_A Behavioral Weight-Loss Intervention in Persons with Serious Mental Illness.pdf;/Users/nseewald/Zotero/storage/C97V5XUM/NEJMoa1214530.html}
}

@article{davidsonTraumaInformedPracticesPostsecondary,
  title = {Trauma-{{Informed Practices}} for {{Postsecondary Education}}: {{A Guide}}},
  author = {Davidson, Shannon},
  pages = {28},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Davidson_Trauma-Informed Practices for Postsecondary Education.pdf}
}

@incollection{davisRegressionMeanClinical2014,
  title = {Regression to the {{Mean}} in {{Clinical Trials}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Davis, Clarence E.},
  year = {2014},
  publisher = {American Cancer Society},
  doi = {10.1002/9781118445112.stat06951},
  urldate = {2020-01-14},
  abstract = {When a person is selected for having an extreme measurement (e.g., high blood pressure), a second measure of the variable will, on average, be closer to the mean of the population distribution. This function is called regression to the mean. It is used as a justification for a randomized control group in clinical trials. It may also affect recruitment in clinical trials in which potential participants are selected because they hold an extreme value on some variable.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Davis_2014_Regression to the Mean in Clinical Trials.pdf;/Users/nseewald/Zotero/storage/HVS9INVC/9781118445112.html}
}

@article{dawidFormalTreatmentSequential2014,
  title = {A {{Formal Treatment}} of {{Sequential Ignorability}}},
  author = {Dawid, A. Philip and Constantinou, Panayiota},
  year = {2014},
  month = nov,
  journal = {Statistics in Biosciences},
  volume = {6},
  number = {2},
  pages = {166--188},
  issn = {1867-1764, 1867-1772},
  doi = {10/ggvb26},
  urldate = {2020-05-08},
  abstract = {Taking a rigorous formal approach, we consider sequential decision problems involving observable variables, unobservable variables, and action variables. We can typically assume the property of extended stability, which allows identification (by means of ``G-computation'') of the consequence of a specified treatment strategy if the ``unobserved'' variables are, in fact, observed---but not generally otherwise. However, under certain additional special conditions we can infer simple stability (or sequential ignorability), which supports G-computation based on the observed variables alone. One such additional condition is sequential randomization, where the unobserved variables essentially behave as random noise in their effects on the actions. Another is sequential irrelevance, where the unobserved variables do not influence future observed variables. In the latter case, to deduce sequential ignorability in full generality requires additional positivity conditions. We show here that these positivity conditions are not required when all variables are discrete.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dawid_Constantinou_2014_A Formal Treatment of Sequential Ignorability.pdf}
}

@article{dawMatchingDifferenceinDifferencesRock2018,
  title = {Matching in {{Difference-in-Differences}}: Between a {{Rock}} and a {{Hard Place}}},
  shorttitle = {Matching in {{Difference-in-Differences}}},
  author = {Daw, Jamie R. and Hatfield, Laura A.},
  year = {2018},
  journal = {Health Services Research},
  volume = {53},
  number = {6},
  pages = {4111--4117},
  issn = {1475-6773},
  doi = {10.1111/1475-6773.13017},
  urldate = {2024-02-29},
  copyright = {{\copyright} Health Research and Educational Trust},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Daw_Hatfield_2018_Matching in Difference-in-Differences.pdf;/Users/nseewald/Zotero/storage/ZLRBGA3C/1475-6773.html}
}

@article{dawMatchingRegressionMean2018,
  title = {Matching and {{Regression}} to the {{Mean}} in {{Difference}}-in-{{Differences Analysis}}},
  author = {Daw, Jamie R. and Hatfield, Laura A.},
  year = {2018},
  month = dec,
  journal = {Health Services Research},
  volume = {53},
  number = {6},
  pages = {4138--4156},
  issn = {0017-9124},
  doi = {10.1111/1475-6773.12993},
  urldate = {2023-12-06},
  abstract = {Objective To demonstrate regression to the mean bias introduced by matching on preperiod variables in difference-in-differences studies. Data Sources Simulated data. Study Design We performed a Monte Carlo simulation to estimate the effect of a placebo intervention on simulated longitudinal data for units in treatment and control groups using unmatched and matched difference-in-differences analyses. We varied the preperiod level and trend differences between the treatment and control groups, and the serial correlation of the matching variables. We assessed estimator bias as the mean absolute deviation of estimated program effects from the true value of zero. Principal Findings When preperiod outcome level is correlated with treatment assignment, an unmatched analysis is unbiased, but matching units on preperiod outcome levels produces biased estimates. The bias increases with greater preperiod level differences and weaker serial correlation in the outcome. This problem extends to matching on preperiod level of a time-varying covariate. When treatment assignment is correlated with preperiod trend only, the unmatched analysis is biased, and matching units on preperiod level or trend does not introduce additional bias. Conclusions Researchers should be aware of the threat of regression to the mean when constructing matched samples for difference-in-differences. We provide guidance on when to incorporate matching in this study design.},
  pmcid = {PMC6232412},
  pmid = {29957834},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Daw_Hatfield_2018_Matching and Regression to the Mean in Difference‐in‐Differences Analysis.pdf}
}

@article{dawsonEfficientDesignInference2012,
  title = {Efficient Design and Inference for Multistage Randomized Trials of Individualized Treatment Policies},
  author = {Dawson, R. and Lavori, P. W.},
  year = {2012},
  month = jan,
  journal = {Biostatistics},
  volume = {13},
  number = {1},
  pages = {142--152},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxr016},
  urldate = {2018-10-12},
  abstract = {Clinical demand for individualized ``adaptive'' treatment policies in diverse fields has spawned development of clinical trial methodology for their experimental evaluation via multistage designs, building upon methods intended for the analysis of naturalistically observed strategies. Because often there is no need to parametrically smooth multistage trial data (in contrast to observational data for adaptive strategies), it is possible to establish direct connections among different methodological approaches. We show by algebraic proof that the maximum likelihood (ML) and optimal semiparametric (SP) estimators of the population mean of the outcome of a treatment policy and its standard error are equal under certain experimental conditions. This result is used to develop a unified and efficient approach to design and inference for multistage trials of policies that adapt treatment according to discrete responses. We derive a sample size formula expressed in terms of a parametric version of the optimal SP population variance. Nonparametric (sample-based) ML estimation performed well in simulation studies, in terms of achieved power, for scenarios most likely to occur in real studies, even though sample sizes were based on the parametric formula. ML outperformed the SP estimator; differences in achieved power predominately reflected differences in their estimates of the population mean (rather than estimated standard errors). Neither methodology could mitigate the potential for overestimated sample sizes when strong nonlinearity was purposely simulated for certain discrete outcomes; however, such departures from linearity may not be an issue for many clinical contexts that make evaluation of competitive treatment policies meaningful.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dawson_Lavori_2012_Efficient design and inference for multistage randomized trials of.pdf}
}

@article{dawsonSampleSizeCalculations2010,
  title = {Sample Size Calculations for Evaluating Treatment Policies in Multi-Stage Designs},
  author = {Dawson, Ree and Lavori, Philip W},
  year = {2010},
  month = dec,
  journal = {Clinical Trials},
  volume = {7},
  number = {6},
  pages = {643--652},
  issn = {1740-7745, 1740-7753},
  doi = {10/fcszjd},
  urldate = {2019-06-14},
  abstract = {Background Sequential multiple assignment randomized (SMAR) designs are used to evaluate treatment policies, also known as adaptive treatment strategies (ATS). The determination of SMAR sample sizes is challenging because of the sequential and adaptive nature of ATS, and the multi-stage randomized assignment used to evaluate them. Purpose We derive sample size formulae appropriate for the nested structure of successive SMAR randomizations. This nesting gives rise to ATS that have overlapping data, and hence between-strategy covariance. We focus on the case when covariance is substantial enough to reduce sample size through improved inferential efficiency. Methods Our design calculations draw upon two distinct methodologies for SMAR trials, using the equality of the optimal semi-parametric and Bayesian predictive estimators of standard error. This `hybrid' approach produces a generalization of the t-test power calculation that is carried out in terms of effect size and regression quantities familiar to the trialist. Results Simulation studies support the reasonableness of underlying assumptions as well as the adequacy of the approximation to between-strategy covariance when it is substantial. Investigation of the sensitivity of formulae to misspecification shows that the greatest influence is due to changes in effect size, which is an a priori clinical judgment on the part of the trialist. Limitations We have restricted simulation investigation to SMAR studies of two and three stages, although the methods are fully general in that they apply to `K-stage' trials. Conclusions Practical guidance is needed to allow the trialist to size a SMAR design using the derived methods. To this end, we define ATS to be `distinct' when they differ by at least the (minimal) size of effect deemed to be clinically relevant. Simulation results suggest that the number of subjects needed to distinguish distinct strategies will be significantly reduced by adjustment for covariance only when small effects are of interest. Clinical Trials 2010; 7: 643--652. http:// ctj.sagepub.com},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dawson_Lavori_2010_Sample size calculations for evaluating treatment policies in multi-stage.pdf}
}

@article{dawsonSequentialCausalInference2008,
  title = {Sequential Causal Inference: {{Application}} to Randomized Trials of Adaptive Treatment Strategies},
  shorttitle = {Sequential Causal Inference},
  author = {Dawson, Ree and Lavori, Philip W.},
  year = {2008},
  month = may,
  journal = {Statistics in Medicine},
  volume = {27},
  number = {10},
  pages = {1626--1645},
  issn = {02776715, 10970258},
  doi = {10/b6qjp2},
  urldate = {2020-05-08},
  abstract = {Clinical trials that randomize subjects to decision algorithms, which adapt treatments over time according to individual response, have gained considerable interest as investigators seek designs that directly inform clinical decision making. We consider designs in which subjects are randomized sequentially at decision points, among adaptive treatment options under evaluation. We present a sequential method to estimate the comparative effects of the randomized adaptive treatments, which are formalized as adaptive treatment strategies. Our causal estimators are derived using Bayesian predictive inference. We use analytical and empirical calculations to compare the predictive estimators to (i) the `standard' approach that allocates the sequentially obtained data to separate strategy-specific groups as would arise from randomizing subjects at baseline; (ii) the semi-parametric approach of marginal mean models that, under appropriate experimental conditions, provides the same sequential estimator of causal differences as the proposed approach. Simulation studies demonstrate that sequential causal inference offers substantial efficiency gains over the standard approach to comparing treatments, because the predictive estimators can take advantage of the monotone structure of shared data among adaptive strategies. We further demonstrate that the semi-parametric asymptotic variances, which are marginal `one-step' estimators, may exhibit significant bias, in contrast to the predictive variances. We show that the conditions under which the sequential method is attractive relative to the other two approaches are those most likely to occur in real studies. Copyright q 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dawson_Lavori_2008_Sequential causal inference.pdf}
}

@article{dayanReinforcementLearningGood2008,
  title = {Reinforcement Learning: {{The Good}}, {{The Bad}} and {{The Ugly}}},
  shorttitle = {Reinforcement Learning},
  author = {Dayan, Peter and Niv, Yael},
  year = {2008},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  volume = {18},
  number = {2},
  pages = {185--196},
  issn = {09594388},
  doi = {10.1016/j.conb.2008.08.003},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dayan_Niv_2008_Reinforcement learning.pdf}
}

@article{dechaisemartinFuzzyDifferencesinDifferences2018,
  title = {Fuzzy {{Differences-in-Differences}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultf{\oe}uille, Xavier},
  year = {2018},
  month = apr,
  journal = {The Review of Economic Studies},
  volume = {85},
  number = {2},
  pages = {999--1028},
  issn = {0034-6527},
  doi = {10/gdbd8p},
  urldate = {2021-06-27},
  abstract = {Difference-in-differences (DID) is a method to evaluate the effect of a treatment. In its basic version, a ``control group'' is untreated at two dates, whereas a ``treatment group'' becomes fully treated at the second date. However, in many applications of the DID method, the treatment rate only increases more in the treatment group. In such fuzzy designs, a popular estimator of the treatment effect is the DID of the outcome divided by the DID of the treatment. We show that this ratio identifies a local average treatment effect only if the effect of the treatment is stable over time, and if the effect of the treatment is the same in the treatment and in the control group. We then propose two alternative estimands that do not rely on any assumption on treatment effects, and that can be used when the treatment rate does not change over time in the control group. We prove that the corresponding estimators are asymptotically normal. Finally, we use our results to reassess the returns to schooling in Indonesia.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/de Chaisemartin_D'Haultfœuille_2018_Fuzzy Differences-in-Differences.pdf}
}

@article{dechaisemartinTwoWayFixedEffects2020,
  title = {Two-{{Way Fixed Effects Estimators}} with {{Heterogeneous Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultf{\oe}uille, Xavier},
  year = {2020},
  month = sep,
  journal = {American Economic Review},
  volume = {110},
  number = {9},
  pages = {2964--2996},
  issn = {0002-8282},
  doi = {10/gg94xm},
  urldate = {2021-07-02},
  abstract = {Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE ) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator. (JEL C21, C23, D72, J31, J51, L82)},
  langid = {english},
  keywords = {Entertainment,Media,Media L82,Political Processes: Rent-seeking Lobbying Elections Legislatures and Voting Behavior D72,Quantile Regressions C21,Quantile Regressions Single Equation Models,Single Equation Models,Single Variables: Cross-Sectional Models,Single Variables: Panel Data Models,Spatial Models,Spatio-temporal Models C23,Spatio-temporal Models Political Processes: Rent-seeking Lobbying Elections Legislatures and Voting Behavior Wage Level and Structure,Trade Unions: Objectives Structure and Effects J51,Treatment Effect Models,Wage Differentials J31,Wage Differentials Trade Unions: Objectives Structure and Effects Entertainment,Wage Level and Structure},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/de Chaisemartin_D’Haultfœuille_2020_Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.pdf;/Users/nseewald/Zotero/storage/4GKLZVTN/articles.html}
}

@article{degliespostiCanSyntheticControls2020,
  title = {Can Synthetic Controls Improve Causal Inference in Interrupted Time Series Evaluations of Public Health Interventions?},
  author = {Degli Esposti, Michelle and Spreckelsen, Thees and Gasparrini, Antonio and Wiebe, Douglas J and Bonander, Carl and Yakubovich, Alexa R and Humphreys, David K},
  year = {2020},
  month = dec,
  journal = {International Journal of Epidemiology},
  volume = {49},
  number = {6},
  pages = {2010--2020},
  issn = {0300-5771},
  doi = {10.1093/ije/dyaa152},
  urldate = {2024-05-30},
  abstract = {Interrupted time series designs are a valuable quasi-experimental approach for evaluating public health interventions. Interrupted time series extends a single group pre-post comparison by using multiple time points to control for underlying trends. But history bias---confounding by unexpected events occurring at the same time of the intervention---threatens the validity of this design and limits causal inference. Synthetic control methodology, a popular data-driven technique for deriving a control series from a pool of unexposed populations, is increasingly recommended. In this paper, we evaluate if and when synthetic controls can strengthen an interrupted time series design. First, we summarize the main observational study designs used in evaluative research, highlighting their respective uses, strengths, biases and design extensions for addressing these biases. Second, we outline when the use of synthetic controls can strengthen interrupted time series studies and when their combined use may be problematic. Third, we provide recommendations for using synthetic controls in interrupted time series and, using a real-world example, we illustrate the potential pitfalls of using a data-driven approach to identify a suitable control series. Finally, we emphasize the importance of theoretical approaches for informing study design and argue that synthetic control methods are not always well suited for generating a counterfactual that minimizes critical threats to interrupted time series studies. Advances in synthetic control methods bring new opportunities to conduct rigorous research in evaluating public health interventions. However, incorporating synthetic controls in interrupted time series studies may not always nullify important threats to validity nor improve causal inference.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Degli Esposti et al_2020_Can synthetic controls improve causal inference in interrupted time series.pdf;/Users/nseewald/Zotero/storage/DA7CTJFH/5917161.html}
}

@misc{dembyCanWeTalk,
  title = {Can We Talk about Whiteness?},
  author = {Demby, Gene and Meraji, Shereen Marisol},
  number = {1},
  keywords = {nosource}
}

@article{demidenkoSampleSizeDetermination2007,
  title = {Sample Size Determination for Logistic Regression Revisited},
  author = {Demidenko, Eugene},
  year = {2007},
  journal = {Statistics in Medicine},
  volume = {26},
  number = {18},
  pages = {3385--3397},
  issn = {02776715},
  doi = {10.1002/sim.2771},
  urldate = {2018-10-19},
  abstract = {There is no consensus on the approach to compute the power and sample size with logistic regression. Some authors use the likelihood ratio test; some use the test on proportions; some suggest various approximations to handle the multivariate case. We advocate the use of the Wald test since the Z -score is routinely used for statistical significance testing of regression coefficients. The null-variance formula became popular from early studies, which contradicts modern software, which utilizes the method of maximum likelihood estimation (MLE), when the variance of the MLE is estimated at the MLE, not at the null. We derive general Wald-based power and sample size formulas for logistic regression and then apply them to binary exposure and confounder to obtain a closed-form expression. These formulas are applied to minimize the total sample size in a case--control study to achieve a given power by optimizing the ratio of controls to cases. Approximately, the optimal number of controls to cases is equal to the square root of the alternative odds ratio. Our sample size and power calculations can be carried out online at www.dartmouth.edu/ {$\sim$}eugened. Copyright q 2006 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Demidenko_2007_Sample size determination for logistic regression revisited.pdf}
}

@article{demidenkoSampleSizeOptimal2008,
  title = {Sample Size and Optimal Design for Logistic Regression with Binary Interaction},
  author = {Demidenko, Eugene},
  year = {2008},
  month = jan,
  journal = {Statistics in Medicine},
  volume = {27},
  number = {1},
  pages = {36--46},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.2980},
  urldate = {2018-10-12},
  abstract = {There is no consensus on what test to use as the basis for sample size determination and power analysis. Some authors advocate the Wald test and some the likelihood-ratio test. We argue that the Wald test should be used because the Z -score is commonly applied for regression coefficient significance testing and therefore the same statistic should be used in the power function. We correct a widespread mistake on sample size determination when the variance of the maximum likelihood estimate (MLE) is estimated at null value. In our previous paper, we developed a correct sample size formula for logistic regression with single exposure (Statist. Med. 2007; 26(18):3385--3397). In the present paper, closed-form formulas are derived for interaction studies with binary exposure and covariate in logistic regression. The formula for the optimal control--case ratio is derived such that it maximizes the power function given other parameters. Our sample size and power calculations with interaction can be carried out online at www.dartmouth.edu/{$\sim$}eugened. Copyright q 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Demidenko_2008_Sample size and optimal design for logistic regression with binary interaction.pdf}
}

@article{demirtasBinNorPackageConcurrent2014,
  title = {{{BinNor}}: {{An R Package}} for {{Concurrent Generation}} of {{Binary}} and {{Normal Data}}},
  shorttitle = {{{BinNor}}},
  author = {Demirtas, Hakan and Amatya, Anup and Doganay, Beyza},
  year = {2014},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {43},
  number = {3},
  pages = {569--579},
  issn = {0361-0918, 1532-4141},
  doi = {10.1080/03610918.2012.707725},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Demirtas et al_2014_BinNor.pdf}
}

@article{dempseyRandomisedTrialsFitbit2015,
  title = {Randomised Trials for the {{Fitbit}} Generation},
  author = {Dempsey, Walter and Liao, Peng and Klasnja, Pedja and {Nahum-Shani}, Inbal and Murphy, Susan A.},
  year = {2015},
  month = dec,
  journal = {Significance},
  volume = {12},
  number = {6},
  pages = {20--23},
  issn = {17409705},
  doi = {10.1111/j.1740-9713.2015.00863.x},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dempsey et al_2015_Randomised trials for the Fitbit generation.pdf}
}

@article{dempseyStratifiedMicrorandomizedTrial2020,
  ids = {dempseyStratifiedMicrorandomizedTrial2017},
  title = {The Stratified Micro-Randomized Trial Design: {{Sample}} Size Considerations for Testing Nested Causal Effects of Time-Varying Treatments},
  shorttitle = {The Stratified Micro-Randomized Trial Design},
  author = {Dempsey, Walter and Liao, Peng and Kumar, Santosh and Murphy, Susan A.},
  year = {2020},
  month = jun,
  journal = {Annals of Applied Statistics},
  volume = {14},
  number = {2},
  eprint = {1711.03587},
  pages = {661--684},
  publisher = {Institute of Mathematical Statistics},
  issn = {1932-6157, 1941-7330},
  doi = {10/ghvbfd},
  urldate = {2021-01-23},
  abstract = {Technological advancements in the field of mobile devices and wearable sensors have helped overcome obstacles in the delivery of care, making it possible to deliver behavioral treatments anytime and anywhere. Here, we discuss our work on the design of a mobile health smoking cessation intervention study with the goal of assessing whether reminders, delivered at times of stress, result in a reduction/prevention of stress in the near-term and whether this effect changes with time in study. Multiple statistical challenges arose in this effort, leading to the development of the stratified micro-randomized trial design. In these designs each individual is randomized to treatment repeatedly at times determined by predictions of risk. These risk times may be impacted by prior treatment. We describe the statistical challenges and detail how they can be met.},
  archiveprefix = {arXiv},
  langid = {english},
  mrnumber = {MR4117824},
  zmnumber = {07239878},
  keywords = {mobile health,nested causal effects,No DOI found,Sequential randomization,stratified micro-randomized trials,weighted-centered least-squares method},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dempsey et al_2020_The stratified micro-randomized trial design.pdf;G\:\\My Drive\\Papers\\Dempsey_et_al_2020_The_stratified_micro-randomized_trial_design2.pdf;/Users/nseewald/Zotero/storage/4DDRYWKM/1593449320.html}
}

@article{dempsterSimulationStudyAlternatives1977,
  title = {A {{Simulation Study}} of {{Alternatives}} to {{Ordinary Least Squares}}},
  author = {Dempster, A. P. and Schatzoff, Martin and Wermuth, Nanny},
  year = {1977},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {72},
  number = {357},
  pages = {77--91},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1977.10479910},
  urldate = {2018-10-18},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dempster et al_1977_A Simulation Study of Alternatives to Ordinary Least Squares.pdf}
}

@article{deshpandeWhoScreenedOut2019,
  title = {Who {{Is Screened Out}}? {{Application Costs}} and the {{Targeting}} of {{Disability Programs}}},
  shorttitle = {Who {{Is Screened Out}}?},
  author = {Deshpande, Manasi and Li, Yue},
  year = {2019},
  journal = {American Economic Journal: Economic Policy},
  volume = {11},
  number = {4},
  eprint = {26817918},
  eprinttype = {jstor},
  pages = {213--248},
  publisher = {American Economic Association},
  issn = {1945-7731},
  urldate = {2024-07-10},
  abstract = {We study the effect of application costs on the targeting of disability programs. We identify these effects using the closings of Social Security Administration field offices, which provide assistance with filing disability applications. Closings lead to a persistent 16 percent decline in the number of disability recipients in surrounding areas, with the largest effects for applicants with moderately severe conditions and low education levels. Disability applications fall by only 10 percent, implying that the closings reduce targeting efficiency based on current eligibility standards. Increased congestion at neighboring offices appears more important as a channel than higher travel or information costs.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Deshpande_Li_2019_Who Is Screened Out.pdf}
}

@article{devochtConceptualisingNaturalQuasi2021,
  title = {Conceptualising Natural and Quasi Experiments in Public Health},
  author = {{de Vocht}, Frank and Katikireddi, Srinivasa Vittal and McQuire, Cheryl and Tilling, Kate and Hickman, Matthew and Craig, Peter},
  year = {2021},
  month = feb,
  journal = {BMC Medical Research Methodology},
  volume = {21},
  number = {1},
  pages = {32},
  issn = {1471-2288},
  doi = {10.1186/s12874-021-01224-x},
  urldate = {2024-02-26},
  abstract = {Natural or quasi experiments are appealing for public health research because they enable the evaluation of events or interventions that are difficult or impossible to manipulate experimentally, such as many policy and health system reforms. However, there remains ambiguity in the literature about their definition and how they differ from randomized controlled experiments and from other observational designs. We conceptualise natural experiments in the context of public health evaluations and align the study design to the Target Trial Framework.},
  keywords = {Evaluations,Natural experiments,Public health,Public health policy,Quasi experiments},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/de Vocht et al_2021_Conceptualising natural and quasi experiments in public health.pdf;/Users/nseewald/Zotero/storage/NIAAFWXQ/s12874-021-01224-x.html}
}

@article{dhaultfoeuilleNonparametricDifferenceindifferencesRepeated2022,
  title = {Nonparametric Difference-in-Differences in Repeated Cross-Sections with Continuous Treatments},
  author = {D'Haultf{\oe}uille, Xavier and Hoderlein, Stefan and Sasaki, Yuya},
  year = {2022},
  month = sep,
  journal = {Journal of Econometrics},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2022.07.003},
  urldate = {2022-10-04},
  abstract = {This paper studies the identification of causal effects of a continuous treatment using a new difference-in-difference strategy. Our approach allows for endogeneity of the treatment, and employs repeated cross-sections. It requires an exogenous change over time which affects the treatment in a heterogeneous way, stationarity of the distribution of unobservables and a rank invariance condition on the time trend. On the other hand, we do not impose any functional form restrictions or an additive time trend, and we are invariant to the scaling of the dependent variable. Under our conditions, the time trend can be identified using a control group, as in the binary difference-in-differences literature. In our scenario, however, this control group is defined by the data. We then identify average and quantile treatment effect parameters. We develop corresponding nonparametric estimators and study their asymptotic properties. Finally, we apply our results to the effect of disposable income on consumption.},
  langid = {english},
  keywords = {Continuous treatment,Difference-in-differences,Endogeneity,Identification,Repeated cross-sections},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/D’Haultfœuille et al_2022_Nonparametric difference-in-differences in repeated cross-sections with.pdf;/Users/nseewald/Zotero/storage/WA88UEPN/S0304407622001452.html}
}

@article{diaoSearchBetterEquation2021,
  title = {In {{Search}} of a {{Better Equation}} --- {{Performance}} and {{Equity}} in {{Estimates}} of {{Kidney Function}}},
  author = {Diao, James A. and Inker, Lesley A. and Levey, Andrew S. and Tighiouart, Hocine and Powe, Neil R. and Manrai, Arjun K.},
  year = {2021},
  month = feb,
  journal = {New England Journal of Medicine},
  volume = {384},
  number = {5},
  pages = {396--399},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMp2028243},
  urldate = {2022-10-05},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Diao et al_2021_In Search of a Better Equation — Performance and Equity in Estimates of Kidney.pdf}
}

@article{dickermanCounterfactualPredictionNot2020,
  title = {Counterfactual Prediction Is Not Only for Causal Inference},
  author = {Dickerman, Barbra A. and Hern{\'a}n, Miguel A.},
  year = {2020},
  month = jul,
  journal = {European Journal of Epidemiology},
  volume = {35},
  number = {7},
  pages = {615--617},
  issn = {1573-7284},
  doi = {10.1007/s10654-020-00659-8},
  urldate = {2022-08-18},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dickerman_Hernán_2020_Counterfactual prediction is not only for causal inference.pdf}
}

@article{dickermanPredictingCounterfactualRisks2022,
  title = {Predicting Counterfactual Risks under Hypothetical Treatment Strategies: An Application to {{HIV}}},
  shorttitle = {Predicting Counterfactual Risks under Hypothetical Treatment Strategies},
  author = {Dickerman, Barbra A. and Dahabreh, Issa J. and Cantos, Krystal V. and Logan, Roger W. and Lodi, Sara and Rentsch, Christopher T. and Justice, Amy C. and Hern{\'a}n, Miguel A.},
  year = {2022},
  month = apr,
  journal = {European Journal of Epidemiology},
  volume = {37},
  number = {4},
  pages = {367--376},
  issn = {1573-7284},
  doi = {10.1007/s10654-022-00855-8},
  urldate = {2022-08-19},
  abstract = {The accuracy of a prediction algorithm depends on contextual factors that may vary across deployment settings. To address this inherent limitation of prediction, we propose an approach to counterfactual prediction based on the g-formula to predict risk across populations that differ in their distribution of treatment strategies. We apply this to predict 5-year risk of mortality among persons receiving care for HIV in the U.S. Veterans Health Administration under different hypothetical treatment strategies. First, we implement a conventional approach to develop a prediction algorithm in the observed data and show how the algorithm may fail when transported to new populations with different treatment strategies. Second, we generate counterfactual data under different treatment strategies and use it to assess the robustness of the original algorithm's performance to these differences and to develop counterfactual prediction algorithms. We discuss how estimating counterfactual risks under a particular treatment strategy is more challenging than conventional prediction as it requires the same data, methods, and unverifiable assumptions as causal inference. However, this may be required when the alternative assumption of constant treatment patterns across deployment settings is unlikely to hold and new data is not yet available to retrain the algorithm.},
  langid = {english},
  keywords = {Causal inference,Counterfactual prediction,Dataset shift,Machine learning,Parametric g-formula,Transportability},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dickerman et al_2022_Predicting counterfactual risks under hypothetical treatment strategies.pdf}
}

@article{diegidioHypertrophicBurnScar2017,
  title = {Hypertrophic {{Burn Scar Research}}: {{From Quantitative Assessment}} to {{Designing Clinical Sequential Multiple Assignment Randomized Trials}}},
  author = {Diegidio, Paul and Hermiz, Steven and Hibbard, Jonathan and Kosorok, Michael and Hultman, Charles Scott},
  year = {2017},
  month = oct,
  journal = {Clinics in Plastic Surgery},
  volume = {44},
  number = {4},
  pages = {917--924},
  issn = {00941298},
  doi = {10.1016/j.cps.2017.05.024},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Diegidio et al_2017_Hypertrophic Burn Scar Research.pdf}
}

@book{diggleAnalysisLongitudinalData2013,
  title = {Analysis of Longitudinal Data},
  author = {Diggle, Peter J. and Heagerty, Patrick J. and Liang, Kung-Yee and Zeger, Scott L.},
  year = {2013},
  series = {Oxford Statistical Science Series},
  edition = {Second Paperback Edition},
  number = {25},
  publisher = {Oxford University Press},
  address = {Oxford},
  isbn = {978-0-19-967675-0},
  lccn = {QA278 .A49735 2013},
  keywords = {Life sciences,Longitudinal method,Multivariate analysis,nosource,Statistical methods,Time-series analysis}
}

@incollection{dinardoNaturalExperimentsQuasiNatural2016,
  title = {Natural {{Experiments}} and {{Quasi-Natural Experiments}}},
  booktitle = {The {{New Palgrave Dictionary}} of {{Economics}}},
  author = {DiNardo, J.},
  year = {2016},
  pages = {1--12},
  publisher = {Palgrave Macmillan UK},
  address = {London},
  doi = {10.1057/978-1-349-95121-5_2006-1},
  urldate = {2024-05-24},
  abstract = {Natural experiments or quasi-natural experiments in economics are serendipitous situations in which persons are assigned randomly to a treatment (or multiple treatments) and a control group, and outcomes are analysed for the purposes of putting a hypothesis to a severe test; they are also serendipitous situations where assignment to treatment `approximates' randomized design or a well-controlled experiment.},
  isbn = {978-1-349-95121-5},
  langid = {english},
  keywords = {C42,Experiment,Natural experiments,Quasi-natural experiments,Randomization,Regression discontinuity design,Returns to schooling,Social experiments,Treatment effect},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/DiNardo_2016_Natural_Experiments_and_Quasi-Natural_Experiments.pdf}
}

@article{dixonCrisisTheoryActive1974,
  title = {Crisis Theory, Active Learning and the Training of Telephone Crisis Volunteers},
  author = {Dixon, Michael C. and Burns, J. L.},
  year = {1974},
  month = apr,
  journal = {Journal of Community Psychology},
  volume = {2},
  number = {2},
  pages = {120--125},
  issn = {00904392, 15206629},
  doi = {10.1002/1520-6629(197404)2:2<120::AID-JCOP2290020207>3.0.CO;2-J},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dixon_Burns_1974_Crisis theory, active learning and the training of telephone crisis volunteers.pdf}
}

@article{dongPowerUpToolCalculating2013a,
  title = {{{PowerUp}}!: {{A Tool}} for {{Calculating Minimum Detectable Effect Sizes}} and {{Minimum Required Sample Sizes}} for {{Experimental}} and {{Quasi-Experimental Design Studies}}},
  shorttitle = {{{PowerUp}}!},
  author = {Dong, Nianbo and Maynard, Rebecca},
  year = {2013},
  month = jan,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {6},
  number = {1},
  pages = {24--67},
  publisher = {Routledge},
  issn = {1934-5747},
  doi = {10.1080/19345747.2012.673143},
  urldate = {2025-01-17},
  abstract = {This paper and the accompanying tool are intended to complement existing supports for conducting power analysis tools by offering a tool based on the framework of Minimum Detectable Effect Sizes (MDES) formulae that can be used in determining sample size requirements and in estimating minimum detectable effect sizes for a range of individual- and group-random assignment design studies and for common quasi-experimental design studies. The paper and accompanying tool cover computation of minimum detectable effect sizes under the following study designs: individual random assignment designs, hierarchical random assignment designs (2-4 levels), block random assignment designs (2-4 levels), regression discontinuity designs (6 types), and short interrupted time-series designs. In each case, the discussion and accompanying tool consider the key factors associated with statistical power and minimum detectable effect sizes, including the level at which treatment occurs and the statistical models (e.g., fixed effect and random effect) used in the analysis. The tool also includes a module that estimates for one and two level random assignment design studies the minimum sample sizes required in order for studies to attain user-defined minimum detectable effect sizes.},
  keywords = {minimum detectable effect size,multilevel experimental,Power analysis,quasi-experimental designs},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dong and Maynard - 2013 - PowerUp! A Tool for Calculating Minimum Detectable Effect Sizes and Minimum Required Sample Sizes f.pdf}
}

@article{donoho50YearsData2017,
  title = {50 {{Years}} of {{Data Science}}},
  author = {Donoho, David},
  year = {2017},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {4},
  pages = {745--766},
  issn = {1061-8600, 1537-2715},
  doi = {10/gcpgdf},
  urldate = {2018-12-11},
  abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In ``The Future of Data Analysis,'' he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or ``data analysis.'' Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name ``data science'' for this envisioned field. A recent and growing phenomenon has been the emergence of ``data science''programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a \$100M ``Data Science Initiative'' that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as ``cultural appropriation.'' This article reviews some ingredients of the current ``data science moment,'' including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for ``scaling up'' to ``big data.'' This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere ``scaling up,'' but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts fieldby-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are ``learning from data,'' and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today's data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.},
  langid = {english},
  keywords = {Cross-study analysis,Data analysis,Data science,Meta analysis,Predictive modeling,Quantitative programming environments,Statistics},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Donoho_2017_50 Years of Data Science.pdf;/Users/nseewald/Zotero/storage/A3GI6RXJ/10618600.2017.html}
}

@article{dowellCDCClinicalPractice2022,
  title = {{{CDC Clinical Practice Guideline}} for {{Prescribing Opioids}} for {{Pain}} --- {{United States}}, 2022},
  author = {Dowell, Deborah and Ragan, Kathleen R. and Jones, Christopher M. and Baldwin, Grant T. and Chou, Roger},
  year = {2022},
  journal = {MMWR Recomm Rep},
  volume = {71},
  number = {No. RR-3},
  pages = {1--95},
  doi = {10.15585/mmwr.rr7103a1},
  urldate = {2023-06-26},
  abstract = {This report describes CDC's updated clinical practice guideline for prescribing opioids for pain.},
  langid = {american},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dowell et al_2022_CDC Clinical Practice Guideline for Prescribing Opioids for Pain — United.pdf;/Users/nseewald/Zotero/storage/UL4N2YFM/rr7103a1.html}
}

@article{dowellCDCGuidelinePrescribing2016,
  title = {{{CDC Guideline}} for {{Prescribing Opioids}} for {{Chronic Pain}} --- {{United States}}, 2016},
  author = {Dowell, Deborah and Haegerich, Tamara M. and Chou, Roger},
  year = {2016},
  journal = {MMWR Recommendations and Reports},
  volume = {65},
  number = {No. RR-1},
  pages = {1--49},
  issn = {1057-59871545-8601},
  doi = {10.15585/mmwr.rr6501e1er},
  urldate = {2023-06-26},
  abstract = {This report from CDC's Morbidity and Mortality Weekly Report (MMWR) provides recommendations for primary care clinicians who are prescribing opioids for chronic pain outside of active cancer treatment, palliative care, and end-of-life care.},
  langid = {american},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dowell et al_2016_CDC Guideline for Prescribing Opioids for Chronic Pain — United States, 2016.pdf;/Users/nseewald/Zotero/storage/IS3ESZK7/rr6501e1.html}
}

@article{doyaMultipleModelBasedReinforcement2002,
  title = {Multiple {{Model-Based Reinforcement Learning}}},
  author = {Doya, Kenji and Samejima, Kazuyuki and Katagiri, Ken-ichi and Kawato, Mitsuo},
  year = {2002},
  month = jun,
  journal = {Neural Computation},
  volume = {14},
  number = {6},
  pages = {1347--1369},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602753712972},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Doya et al_2002_Multiple Model-Based Reinforcement Learning.pdf}
}

@article{dragalinAdaptiveDesignsTerminology2006,
  title = {Adaptive {{Designs}}: {{Terminology}} and {{Classification}}},
  shorttitle = {Adaptive {{Designs}}},
  author = {Dragalin, Vladimir},
  year = {2006},
  month = oct,
  journal = {Drug Information Journal},
  volume = {40},
  number = {4},
  pages = {425--435},
  issn = {0092-8615, 2164-9200},
  doi = {10/ghpbrt},
  urldate = {2020-12-12},
  langid = {english},
  keywords = {nosource}
}

@article{duarteAutomatedApproachCausal2023,
  title = {An {{Automated Approach}} to {{Causal Inference}} in {{Discrete Settings}}},
  author = {Duarte, Guilherme and Finkelstein, Noam and Knox, Dean and Mummolo, Jonathan and Shpitser, Ilya},
  year = {2023},
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--16},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2023.2216909},
  urldate = {2023-09-21},
  abstract = {Applied research conditions often make it impossible to point-identify causal estimands without untenable assumptions. Partial identification---bounds on the range of possible solutions---is a principled alternative, but the difficulty of deriving bounds in idiosyncratic settings has restricted its application. We present a general, automated numerical approach to causal inference in discrete settings. We show causal questions with discrete data reduce to polynomial programming problems, then present an algorithm to automatically bound causal effects using efficient dual relaxation and spatial branch-and-bound techniques. The user declares an estimand, states assumptions, and provides data---however incomplete or mismeasured. The algorithm then searches over admissible data-generating processes and outputs the most precise possible range consistent with available information---that is, sharp bounds---including a point-identified solution if one exists. Because this search can be computationally intensive, our procedure reports and continually refines non-sharp ranges guaranteed to contain the truth at all times, even when the algorithm is not run to completion. Moreover, it offers an {$\varepsilon$}-sharpness guarantee, characterizing the worst-case looseness of the incomplete bounds. These techniques are implemented in our Python package, autobounds. Analytically validated simulations show the method accommodates classic obstacles---including confounding, selection, measurement error, noncompliance, and nonresponse. Supplementary materials for this article are available online.},
  keywords = {Causal inference,Constrained optimization,Linear programming,Partial identification,Polynomial programming},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Duarte et al_2023_An Automated Approach to Causal Inference in Discrete Settings.pdf}
}

@article{dusetzinaOutofPocketHealthCare2018,
  title = {Out-of-{{Pocket}} and {{Health Care Spending Changes}} for {{Patients Using Orally Administered Anticancer Therapy After Adoption}} of {{State Parity Laws}}},
  author = {Dusetzina, Stacie B. and Huskamp, Haiden A. and Winn, Aaron N. and Basch, Ethan and Keating, Nancy L.},
  year = {2018},
  month = jun,
  journal = {JAMA oncology},
  volume = {4},
  number = {6},
  pages = {e173598},
  issn = {2374-2445},
  doi = {10.1001/jamaoncol.2017.3598},
  abstract = {Importance: Oral anticancer medications are increasingly important but costly treatment options for patients with cancer. By early 2017, 43 states and Washington, DC, had passed laws to ensure patients with private insurance enrolled in fully insured health plans pay no more for anticancer medications administered by mouth than anticancer medications administered by infusion. Federal legislation regarding this issue is currently pending. Despite their rapid acceptance, the changes associated with state adoption of oral chemotherapy parity laws have not been described. Objective: To estimate changes in oral anticancer medication use, out-of-pocket spending, and health plan spending associated with oral chemotherapy parity law adoption. Design, Setting, and Participants: Analysis of administrative health plan claims data from 2008-2012 for 3 large nationwide insurers aggregated by the Health Care Cost Institute. Data analysis was first completed in 2015 and updated in 2017. The study population included 63\,780 adults living in 1 of 16 states that passed parity laws during the study period and who received anticancer drug treatment for which orally administered treatment options were available. Study analysis used a difference-in-differences approach. Exposures: Time period before and after adoption of state parity laws, controlling for whether the patient was enrolled in a plan subject to parity (fully insured) or not (self-funded, exempt via the Employee Retirement Income Security Act). Main Outcomes and Measures: Oral anticancer medication use, out-of-pocket spending, and total health care spending. Results: Of the 63\,780 adults aged 18 through 64 years, 51.4\% participated in fully insured plans and 48.6\% in self-funded plans (57.2\% were women; 76.8\% were aged 45 to 64 years). The use of oral anticancer medication treatment as a proportion of all anticancer treatment increased from 18\% to 22\% (adjusted difference-in-differences risk ratio [aDDRR], 1.04; 95\% CI, 0.96-1.13; P\,=\,.34) comparing months before vs after parity. In plans subject to parity laws, the proportion of prescription fills for orally administered therapy without copayment increased from 15.0\% to 53.0\%, more than double the increase (12.3\%-18.0\%) in plans not subject to parity (P\,{$<$}\,.001). The proportion of patients with out-of-pocket spending of more than \$100 per month increased from 8.4\% to 11.1\% compared with a slight decline from 12.0\% to 11.7\% in plans not subject to parity (P\,=\,.004). In plans subject to parity laws, estimated monthly out-of-pocket spending decreased by \$19.44 at the 25th percentile, by \$32.13 at the 50th percentile, and by \$10.83 at the 75th percentile but increased at the 90th (\$37.19) and 95th (\$143.25) percentiles after parity (all P\,{$<$}\,.001, controlling for changes in plans not subject to parity). Parity laws did not increase 6-month total spending for users of any anticancer therapy or for users of oral anticancer therapy alone. Conclusions and Relevance: While oral chemotherapy parity laws modestly improved financial protection for many patients without increasing total health care spending, these laws alone may be insufficient to ensure that patients are protected from high out-of-pocket medication costs.},
  langid = {english},
  pmcid = {PMC6054307},
  pmid = {29121177},
  keywords = {Administration Oral,Adolescent,Adult,Antineoplastic Agents,Drug Utilization,Female,For-Profit Insurance Plans,Health Benefit Plans Employee,Health Expenditures,Humans,Infusions Intravenous,Insurance Benefits,Insurance Carriers,Insurance Coverage,Insurance Pharmaceutical Services,Male,Middle Aged,Prescription Fees,Propensity Score,United States,Young Adult},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dusetzina et al_2018_Out-of-Pocket and Health Care Spending Changes for Patients Using Orally.pdf}
}

@article{dworkFairnessAwareness2011,
  title = {Fairness {{Through Awareness}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Rich},
  year = {2011},
  month = apr,
  journal = {arXiv:1104.3913 [cs]},
  eprint = {1104.3913},
  primaryclass = {cs},
  urldate = {2018-10-12},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of ``fair affirmative action,'' which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dwork et al_2011_Fairness Through Awareness.pdf}
}

@article{dziakDataAnalysisMethod2019,
  title = {A {{Data Analysis Method}} for {{Using Longitudinal Binary Outcome Data}} from a {{SMART}} to {{Compare Adaptive Interventions}}},
  author = {Dziak, John J. and Yap, Jamie R. T. and Almirall, Daniel and McKay, James R. and Lynch, Kevin G. and {Nahum-Shani}, Inbal},
  year = {2019},
  month = jan,
  journal = {Multivariate Behavioral Research},
  volume = {0},
  number = {0},
  pages = {1--24},
  issn = {0027-3171},
  doi = {10/gftzjg},
  urldate = {2019-01-29},
  abstract = {Sequential multiple assignment randomized trials (SMARTs) are a useful and increasingly popular approach for gathering information to inform the construction of adaptive interventions to treat psychological and behavioral health conditions. Until recently, analysis methods for data from SMART designs considered only a single measurement of the outcome of interest when comparing the efficacy of adaptive interventions. Lu et al. proposed a method for considering repeated outcome measurements to incorporate information about the longitudinal trajectory of change. While their proposed method can be applied to many kinds of outcome variables, they focused mainly on linear models for normally distributed outcomes. Practical guidelines and extensions are required to implement this methodology with other types of repeated outcome measures common in behavioral research. In this article, we discuss implementation of this method with repeated binary outcomes. We explain how to compare adaptive interventions in terms of various summaries of repeated binary outcome measures, including average outcome (area under the curve) and delayed effects. The method is illustrated using an empirical example from a SMART study to develop an adaptive intervention for engaging alcohol- and cocaine-dependent patients in treatment. Monte Carlo simulations are provided to demonstrate the good performance of the proposed technique.},
  pmid = {30663401},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dziak et al_2019_A Data Analysis Method for Using Longitudinal Binary Outcome Data from a SMART.pdf;/Users/nseewald/Zotero/storage/87GH2F5I/00273171.2018.html}
}

@article{dziakDataAnalysisMethod2019a,
  title = {A {{Data Analysis Method}} for {{Using Longitudinal Binary Outcome Data}} from a {{SMART}} to {{Compare Adaptive Interventions}}},
  author = {Dziak, John J. and Yap, Jamie R. T. and Almirall, Daniel and McKay, James R. and Lynch, Kevin G. and {Nahum-Shani}, Inbal},
  year = {2019},
  month = sep,
  journal = {Multivariate Behavioral Research},
  volume = {54},
  number = {5},
  pages = {613--636},
  publisher = {Routledge},
  issn = {0027-3171},
  doi = {10.1080/00273171.2018.1558042},
  urldate = {2024-08-22},
  abstract = {Sequential multiple assignment randomized trials (SMARTs) are a useful and increasingly popular approach for gathering information to inform the construction of adaptive interventions to treat psychological and behavioral health conditions. Until recently, analysis methods for data from SMART designs considered only a single measurement of the outcome of interest when comparing the efficacy of adaptive interventions. Lu et al. proposed a method for considering repeated outcome measurements to incorporate information about the longitudinal trajectory of change. While their proposed method can be applied to many kinds of outcome variables, they focused mainly on linear models for normally distributed outcomes. Practical guidelines and extensions are required to implement this methodology with other types of repeated outcome measures common in behavioral research. In this article, we discuss implementation of this method with repeated binary outcomes. We explain how to compare adaptive interventions in terms of various summaries of repeated binary outcome measures, including average outcome (area under the curve) and delayed effects. The method is illustrated using an empirical example from a SMART study to develop an adaptive intervention for engaging alcohol- and cocaine-dependent patients in treatment. Monte Carlo simulations are provided to demonstrate the good performance of the proposed technique.},
  pmid = {30663401},
  keywords = {binary outcome,logistic regression,longitudinal data,Sequential multiple randomization trial (SMART)},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dziak_et_al_2019_A_Data_Analysis_Method_for_Using_Longitudinal_Binary_Outcome_Data_from_a_SMART.pdf}
}

@article{dziakMultilevelFactorialExperiments20120206,
  title = {Multilevel Factorial Experiments for Developing Behavioral Interventions: {{Power}}, Sample Size, and Resource Considerations.},
  shorttitle = {Multilevel Factorial Experiments for Developing Behavioral Interventions},
  author = {Dziak, John J. and {Nahum-Shani}, Inbal and Collins, Linda M.},
  year = {20120206},
  journal = {Psychological Methods},
  volume = {17},
  number = {2},
  pages = {153},
  publisher = {US: American Psychological Association},
  issn = {1939-1463},
  doi = {10/f3224p},
  urldate = {2020-12-29},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dziak et al_0000_Multilevel factorial experiments for developing behavioral interventions.pdf;/Users/nseewald/Zotero/storage/J9IAAXNI/2012-02919-001.html}
}

@article{dziakSMARTBinaryNew2024,
  title = {{{SMART Binary}}: {{New Sample Size Planning Resources}} for {{SMART Studies}} with {{Binary Outcome Measurements}}},
  shorttitle = {{{SMART Binary}}},
  author = {Dziak, John J. and Almirall, Daniel and Dempsey, Walter and Stanger, Catherine and {Nahum-Shani}, Inbal},
  year = {2024},
  month = jan,
  journal = {Multivariate Behavioral Research},
  volume = {59},
  number = {1},
  pages = {1--16},
  publisher = {Routledge},
  issn = {0027-3171},
  doi = {10.1080/00273171.2023.2229079},
  urldate = {2024-08-19},
  abstract = {Sequential Multiple-Assignment Randomized Trials (SMARTs) play an increasingly important role in psychological and behavioral health research. This experimental approach enables researchers to answer scientific questions about how to sequence and match interventions to the unique, changing needs of individuals. A variety of sample size planning resources for SMART studies have been developed, enabling researchers to plan SMARTs for addressing different types of scientific questions. However, relatively limited attention has been given to planning SMARTs with binary (dichotomous) outcomes, which often require higher sample sizes relative to continuous outcomes. Existing resources for estimating sample size requirements for SMARTs with binary outcomes do not consider the potential to improve power by including a baseline measurement and/or multiple repeated outcome measurements. The current paper addresses this issue by providing sample size planning simulation procedures and approximate formulas for two-wave repeated measures binary outcomes (i.e., two measurement times for the outcome variable, before and after intervention delivery). The simulation results agree well with the formulas. We also discuss how to use simulations to calculate power for studies with more than two outcome measurement occasions. Results show that having at least one repeated measurement of the outcome can substantially improve power under certain conditions.},
  pmid = {37459401},
  keywords = {adaptive interventions,binary outcome,power,sample size,Sequential multiple assignment randomized trials (SMARTs)},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Dziak_et_al_2024_SMART_Binary.pdf}
}

@misc{eckshtainUsingSMARTExperimental2013,
  title = {Using {{SMART Experimental Design}} to {{Personalize Treatment}} for {{Child Depression}}},
  author = {Eckshtain, Dikla},
  year = {2013},
  urldate = {2018-12-08},
  abstract = {Using SMART Experimental Design to Personalize Treatment for Child Depression - Full Text View.},
  howpublished = {https://clinicaltrials.gov/ct2/show/NCT01880814},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/SE9662B2/NCT01880814.html}
}

@article{edwardsMultipleComparisonsBest1983,
  title = {Multiple {{Comparisons With}} the {{Best Treatment}}},
  author = {Edwards, Donald G. and Hsu, Jason C.},
  year = {1983},
  journal = {Journal of the American Statistical Association},
  volume = {78},
  number = {384},
  eprint = {2288211},
  eprinttype = {jstor},
  pages = {965--971},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2288211},
  urldate = {2022-03-26},
  abstract = {Let {$\pi$}\textsubscript{1}, {$\pi$}\textsubscript{2}, {$\cdots$}, {$\pi$}\textsubscript{k} be k {$\geq$} 2 sources of observations (treatments, populations) and suppose the "goodness" of treatment {$\pi$}\textsubscript{i} is characterized by the size of an unknown real-valued parameter {\texttheta}\textsubscript{i}. Let {\texttheta}\textsubscript{[k]} = max\textsubscript{1 {$\leq$} i {$\leq$} k} {\texttheta}\textsubscript{i}. If {$\pi$}\textsubscript{i} is preferred to {$\pi$}\textsubscript{j} when {$<$}latex{$>\$\backslash$}theta\_i {$>$} {\textbackslash}theta\_j\${$<$}/latex{$>$}, the parameters {$\delta$}\textsubscript{i} = {\texttheta}\textsubscript{[k]} - {\texttheta}\textsubscript{i}, i = 1, 2, {$\cdots$}, k reflect in an inverse sense the "goodness" of each treatment relative to the "best" treatment. A general technique for obtaining simultaneous confidence intervals on the {$\delta$}\textsubscript{i} is demonstrated with several examples. This technique can be applied in any setting where comparison-with-control intervals can be computed regarding any {$\pi$}\textsubscript{j} as the control. These results have special importance in ranking and selection problems in that the process of generating upper bounds on the {$\delta$}\textsubscript{i} generates traditional confidence statements of both the indifference zone and the subset selection schools, simultaneously, as established by Hsu (1981).},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Edwards_Hsu_1983_Multiple Comparisons With the Best Treatment.pdf}
}

@article{efronBayesiansFrequentistsScientists2005,
  title = {Bayesians, {{Frequentists}}, and {{Scientists}}},
  author = {Efron, Bradley},
  year = {2005},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {469},
  pages = {1--5},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214505000000033},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Efron_2005_Bayesians, Frequentists, and Scientists.pdf}
}

@article{efronBootstrapMethodsAnother1979,
  title = {Bootstrap {{Methods}}: {{Another Look}} at the {{Jackknife}}},
  shorttitle = {Bootstrap {{Methods}}},
  author = {Efron, B.},
  year = {1979},
  month = jan,
  journal = {The Annals of Statistics},
  volume = {7},
  number = {1},
  pages = {1--26},
  issn = {0090-5364},
  doi = {10.1214/aos/1176344552},
  urldate = {2018-11-01},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Efron_1979_Bootstrap Methods.pdf}
}

@article{efronBootstrapMethodsStandard1986,
  title = {Bootstrap {{Methods}} for {{Standard Errors}}, {{Confidence Intervals}}, and {{Other Measures}} of {{Statistical Accuracy}}},
  author = {Efron, B. and Tibshirani, R.},
  year = {1986},
  month = feb,
  journal = {Statistical Science},
  volume = {1},
  number = {1},
  pages = {54--75},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177013815},
  urldate = {2024-12-16},
  abstract = {This is a review of bootstrap methods, concentrating on basic ideas and applications rather than theoretical considerations. It begins with an exposition of the bootstrap estimate of standard error for one-sample situations. Several examples, some involving quite complicated statistical procedures, are given. The bootstrap is then extended to other measures of statistical accuracy such as bias and prediction error, and to complicated data structures such as time series, censored data, and regression models. Several more examples are presented illustrating these ideas. The last third of the paper deals mainly with bootstrap confidence intervals.},
  keywords = {approximate confidence intervals,bootstrap method,estimated standard errors,nonparametric methods},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Efron and Tibshirani - 1986 - Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accur.pdf}
}

@book{efronComputerAgeStatistical2016,
  title = {Computer Age Statistical Inference: Algorithms, Evidence, and Data Science},
  shorttitle = {Computer Age Statistical Inference},
  author = {Efron, Bradley and Hastie, Trevor},
  year = {2016},
  series = {Institute of {{Mathematical Statistics}} Monographs},
  publisher = {Cambridge University Press},
  address = {New York, NY},
  isbn = {978-1-107-14989-2},
  lccn = {QA276.4 .E376 2016},
  keywords = {Data processing,Mathematical statistics},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Efron_Hastie_2016_Computer age statistical inference.pdf}
}

@article{efronSteinParadoxStatistics1977,
  title = {Stein's {{Paradox}} in {{Statistics}}},
  author = {Efron, Bradley and Morris, Carl},
  year = {1977},
  journal = {SCIENTIFIC AMERICAN},
  pages = {9},
  doi = {10.1038/scientificamerican0577-119},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Efron_Morris_1977_Stein's Paradox in Statistics.pdf}
}

@article{eisenbergStudyingHowState2021,
  title = {Studying How State Health Services Delivery Policies Can Mitigate the Effects of Disasters on Drug Addiction Treatment and Overdose: {{Protocol}} for a Mixed-Methods Study},
  shorttitle = {Studying How State Health Services Delivery Policies Can Mitigate the Effects of Disasters on Drug Addiction Treatment and Overdose},
  author = {Eisenberg, Matthew D. and McCourt, Alexander and Stuart, Elizabeth A. and Rutkow, Lainie and Tormohlen, Kayla N. and Fingerhood, Michael I. and Quintero, Luis and White, Sarah A. and McGinty, Emma Elizabeth},
  year = {2021},
  month = dec,
  journal = {PLOS ONE},
  volume = {16},
  number = {12},
  pages = {e0261115},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0261115},
  urldate = {2022-06-15},
  abstract = {Background The United States is experiencing a drug addiction and overdose crisis, made worse by the COVID-19 pandemic. Relative to other types of health services, addiction treatment and overdose prevention services are particularly vulnerable to disaster-related disruptions for multiple reasons including fragmentation from the general medical system and stigma, which may lead decisionmakers and providers to de-prioritize these services during disasters. In response to the COVID-19 pandemic, U.S. states implemented multiple policies designed to mitigate disruptions to addiction treatment and overdose prevention services, for example policies expanding access to addiction treatment delivered via telehealth and policies designed to support continuity of naloxone distribution programs. There is limited evidence on the effects of these policies on addiction treatment and overdose. This evidence is needed to inform state policy design in future disasters, as well as to inform decisions regarding whether to sustain these policies post-pandemic. Methods The overall study uses a concurrent-embedded design. Aims 1--2 use difference-in-differences analyses of large-scale observational databases to examine how state policies designed to mitigate the effects of the COVID-19 pandemic on health services delivery influenced addiction treatment delivery and overdose during the pandemic. Aim 3 uses a qualitative embedded multiple case study approach, in which we characterize local implementation of the state policies of interest; most public health disaster policies are enacted at the state level but implemented at the local level by healthcare systems and local public health authorities. Discussion Triangulation of results across methods will yield robust understanding of whether and how state disaster-response policies influenced drug addiction treatment and overdose during the COVID-19 pandemic. Results will inform policy enactment and implementation in future public health disasters. Results will also inform decisions about whether to sustain COVID-19 pandemic-related changes to policies governing delivery addiction and overdose prevention services long-term.},
  langid = {english},
  keywords = {Addiction,COVID 19,Drug addiction,Health care policy,Pandemics,Public and occupational health,Public policy,Qualitative studies},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Eisenberg et al_2021_Studying how state health services delivery policies can mitigate the effects.pdf;/Users/nseewald/Zotero/storage/ZJ4QUE9A/article.html}
}

@article{eisensteinSensibleApproachesReducing2008,
  title = {Sensible Approaches for Reducing Clinical Trial Costs},
  author = {Eisenstein, Eric L and Collins, Rory and Cracknell, Beena S and Podesta, Oscar and Reid, Elizabeth D and Sandercock, Peter and Shakhov, Yuriy and Terrin, Michael L and Sellers, Mary Ann and Califf, Robert M and Granger, Christopher B and Diaz, Rafael},
  year = {2008},
  month = feb,
  journal = {Clinical Trials},
  volume = {5},
  number = {1},
  pages = {75--84},
  issn = {1740-7745, 1740-7753},
  doi = {10/fb3zzk},
  urldate = {2021-03-21},
  abstract = {Background Over the past decade, annual funding for biomedical research has more than doubled while new molecular entity approvals have declined by one third. Objective To assess the value of practices commonly employed in the conduct of large-scale clinical trials, and to identify areas where costs could be reduced without compromising scientific validity. Methods In the qualitative phase of the study, an expert panel recommended potential modifications of mega-trial designs and operations in order to maximize their value (cost versus scientific benefit tradeoff). In the quantitative phase, a mega-trial economic model was used to assess the financial implications of these recommendations. Our initial chronic disease trial design included 20,000 patients randomized at 1000 sites. Each site was assigned 24 monitoring visits and a \$10,000 per patient site payment. The case report form (CRF) was 60 pages long, and trial duration was assumed to be 48 months. Results The total costs of the initial trial design were \$421 million (\$US 2007). Following the expert panel's recommendations, we varied study duration, CRF length, number of sites, electronic data capture (EDC), and site management components to determine their individual and combined effects upon total trial costs. The use of EDC and modified site management practices were associated with significant reductions in total trial costs. When reductions in all five trial components were combined in a streamlined pharmaceutical industry design, a 59\% reduction in total trial costs resulted. When we assumed an even more streamlined trial design than has typically been considered for regulatory submissions in the past, there was a 90\% reduction in total trial costs. Conclusion Our results suggest that it is possible to reduce substantially the cost of large-scale clinical trials without compromising the scientific validity of their results. If implemented, our recommendations could free billions of dollars annually for additional clinical studies. Research in the setting of clinical trials should be conducted to refine these findings. Clinical Trials 2008; 5: 75--84. http:// ctj.sagepub.com},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Eisenstein et al_2008_Sensible approaches for reducing clinical trial costs.pdf}
}

@article{emrichMethodGeneratingHighDimensional1991,
  title = {A {{Method}} for {{Generating High-Dimensional Multivariate Binary Variates}}},
  author = {Emrich, Lawrence J. and Piedmonte, Marion R.},
  year = {1991},
  month = nov,
  journal = {The American Statistician},
  volume = {45},
  number = {4},
  eprint = {2684460},
  eprinttype = {jstor},
  pages = {302},
  issn = {00031305},
  doi = {10.2307/2684460},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Emrich_Piedmonte_1991_A Method for Generating High-Dimensional Multivariate Binary Variates.pdf}
}

@article{eneanyaHealthInequitiesInappropriate2022,
  title = {Health Inequities and the Inappropriate Use of Race in Nephrology},
  author = {Eneanya, Nwamaka D. and Boulware, L. Ebony and Tsai, Jennifer and Bruce, Marino A. and Ford, Chandra L. and Harris, Christina and Morales, Leo S. and Ryan, Michael J. and Reese, Peter P. and Thorpe, Roland J. and Morse, Michelle and Walker, Valencia and Arogundade, Fatiu A. and Lopes, Antonio A. and Norris, Keith C.},
  year = {2022},
  month = feb,
  journal = {Nature Reviews Nephrology},
  volume = {18},
  number = {2},
  pages = {84--94},
  publisher = {Nature Publishing Group},
  issn = {1759-507X},
  doi = {10.1038/s41581-021-00501-8},
  urldate = {2022-10-05},
  abstract = {Chronic kidney disease is an important clinical condition beset with racial and ethnic disparities that are associated with social inequities. Many medical schools and health centres across the USA have raised concerns about the use of race --- a socio-political construct that mediates the effect of structural racism --- as a fixed, measurable biological variable in the assessment of kidney disease. We discuss the role of race and racism in medicine and outline many of the concerns that have been raised by the medical and social justice communities regarding the use of race in estimated glomerular filtration rate equations, including its relationship with structural racism and racial inequities. Although race can be used to identify populations who experience racism and subsequent differential treatment, ignoring the biological and social heterogeneity within any racial group and inferring innate individual-level attributes is methodologically flawed. Therefore, although more accurate measures for estimating kidney function are under investigation, we support the use of biomarkers for determining estimated glomerular filtration rate without adjustments for race. Clinicians have a duty to recognize and elucidate the nuances of racism and its effects on health and disease. Otherwise, we risk perpetuating historical racist concepts in medicine that exacerbate health inequities and impact marginalized patient populations.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,End-stage renal disease,Health policy,Medical ethics},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Eneanya et al_2022_Health inequities and the inappropriate use of race in nephrology.pdf}
}

@article{epsteinHabituationDeterminantHuman2009,
  title = {Habituation as a Determinant of Human Food Intake.},
  author = {Epstein, Leonard H. and Temple, Jennifer L. and Roemmich, James N. and Bouton, Mark E.},
  year = {2009},
  journal = {Psychological Review},
  volume = {116},
  number = {2},
  pages = {384--407},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0015074},
  urldate = {2018-10-12},
  abstract = {Research has shown that animals and humans habituate on a variety of behavioral and physiological responses to repeated presentations of food cues, and habituation is related to amount of food consumed and cessation of eating. The purpose of this article is to provide an overview of experimental paradigms used to study habituation, integrate a theoretical approach to habituation to food based on memory and associative conditioning models, and review research on factors that influence habituation. Individual differences in habituation as they relate to obesity and eating disorders are reviewed, along with research on how individual differences in memory can influence habituation. Other associative conditioning approaches to ingestive behavior are reviewed, as well as how habituation provides novel approaches to preventing or treating obesity. Finally, new directions for habituation research are presented. Habituation provides a novel theoretical framework from which to understand factors that regulate ingestive behavior.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Epstein et al_2009_Habituation as a determinant of human food intake.pdf}
}

@article{ertefaieIdentifyingSetThat2016,
  title = {Identifying a Set That Contains the Best Dynamic Treatment Regimes},
  author = {Ertefaie, Ashkan and Wu, Tianshuang and Lynch, Kevin G. and {Nahum-Shani}, Inbal},
  year = {2016},
  journal = {Biostatistics},
  volume = {17},
  number = {1},
  pages = {135--148},
  issn = {14684357},
  doi = {10.1093/biostatistics/kxv025},
  abstract = {A dynamic treatment regime (DTR) is a treatment design that seeks to accommodate patient heterogeneity in response to treatment. DTRs can be operationalized by a sequence of decision rules that map patient information to treatment options at specific decision points. The sequential, multiple assignment, randomized trial (SMART) is a trial design that was developed specifically for the purpose of obtaining data that informs the construction of good (i.e. efficacious) decision rules. One of the scientific questions motivating a SMART concerns the comparison of multiple DTRs that are embedded in the design. Typical approaches for identifying the best DTRs involve all possible comparisons between DTRs that are embedded in a SMART, at the cost of greatly reduced power to the extent that the number of embedded DTRs (EDTRs) increase. Here, we propose a method that will enable investigators to use SMART study data more efficiently to identify the set that contains the most efficacious EDTRs. Our method ensures that the true best EDTRs are included in this set with at least a given probability. Simulation results are presented to evaluate the proposed method, and the Extending Treatment Effectiveness of Naltrexone SMART study data are analyzed to illustrate its application.},
  pmid = {26243172},
  keywords = {Double robust,Marginal structural model,Multiple comparisons with the best,SMART designs.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ertefaie et al_2016_Identifying a set that contains the best dynamic treatment regimes.pdf}
}

@article{ertefaieTutorialUseInstrumental2017,
  title = {A Tutorial on the Use of Instrumental Variables in Pharmacoepidemiology},
  author = {Ertefaie, Ashkan and Small, Dylan S. and Flory, James H. and Hennessy, Sean},
  year = {2017},
  journal = {Pharmacoepidemiology and Drug Safety},
  volume = {26},
  number = {4},
  pages = {357--367},
  issn = {1099-1557},
  doi = {10.1002/pds.4158},
  urldate = {2025-02-17},
  abstract = {Purpose Instrumental variable (IV) methods are used increasingly in pharmacoepidemiology to address unmeasured confounding. In this tutorial, we review the steps used in IV analyses and the underlying assumptions. We also present methods to assess the validity of those assumptions and describe sensitivity analysis to examine the effects of possible violations of those assumptions. Methods Observational studies based on regression or propensity score analyses rely on the untestable assumption that there are no unmeasured confounders. IV analysis is a tool that removes the bias caused by unmeasured confounding provided that key assumptions (some of which are also untestable) are met. Results When instruments are valid, IV methods provided unbiased treatment effect estimation in the presence of unmeasured confounders. However, the standard error of the IV estimate is higher than the standard error of non-IV estimates, e.g., regression and propensity score methods. Sensitivity analyses provided insight about the robustness of the IV results to the plausible degrees of violation of assumptions. Conclusions IV analysis should be used cautiously because the validity of IV estimates relies on assumptions that are, in general, untestable and difficult to be certain about. Thus, assessing the sensitivity of the estimate to violations of these assumptions is important and can better inform the causal inferences that can be drawn from the study. Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {assumptions,instrumental variables,observational studies,pharmacoepidemiology,sensitivity analysis,unmeasured confounders},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ertefaie et al. - 2017 - A tutorial on the use of instrumental variables in pharmacoepidemiology.pdf;/Users/nseewald/Zotero/storage/GWP96PJE/pds.html}
}

@misc{EugenicsEthicsStatistical2019,
  title = {Eugenics and the {{Ethics}} of {{Statistical Analysis}}},
  year = {2019},
  month = dec,
  journal = {Georgetown Public Policy Review},
  urldate = {2020-01-29},
  abstract = {While statistics has contributed to scientific advancements, the origins of this discipline are surprisingly dark. What is the place of ethics in statistical analysis? ~ In 1930, the British statistician Ronald Fisher published a book entitled The Genetical Theory of Natural Selection, which argued, among other topics, that women are naturally attracted to men whose...},
  langid = {american},
  file = {/Users/nseewald/Zotero/storage/MXJBATVX/eugenics-ethics-statistical-analysis.html}
}

@article{EvidenceRaceWelfare2021,
  title = {Some {{Evidence}} on {{Race}}, {{Welfare Reform}}, and {{Household Income}}},
  year = {2021},
  pages = {7},
  langid = {english},
  keywords = {nosource}
}

@article{fellerAddressingMissingDataForthcoming,
  title = {Addressing {{Missing Data Due}} to {{COVID-19}}: {{Two Early Childhood Case Studies}}},
  author = {Feller, Avi and Connors, Maia C. and Weiland, Christina and Easton, John Q. and Ehrlich Loewe, Stacy and Francis, John and Kabourek, Sarah E. and Leyva, Diana and Shapiro, Anna and {Yeomans-Maldonado}, Gloria},
  year = {Forthcoming},
  journal = {Journal of Research on Educational Effectiveness},
  abstract = {One part of COVID-19's staggering impact on education has been to suspend or fundamentally alter ongoing education research projects. This paper addresses how to analyze the simple but fundamental example of a multi-cohort study in which student assessment data for at least one cohort are missing because schools were closed, learning was virtual, and/or assessments were canceled or inconsistently collected due to COVID-19. We argue that current best-practice recommendations for addressing missing data may fall short in such studies because the assumptions that underpin these recommendations are violated. We then provide a new, simple decision-making framework for empirical researchers facing this situation and provide two empirical examples of how to apply this framework drawn from early childhood studies, one a cluster randomized trial and the other a descriptive longitudinal study. Based on this framework and the assumptions required to address missing data, we advise against the standard recommendation of adjusting for issing outcomes (e.g., via imputation or weighting). Instead, we generally recommend changing the target quantity by restricting to fully-observed cohorts or by pivoting to focusing on an alternative outcome. We also consider implications for missingness patterns in studies not affected by COVID-19.}
}

@article{fengSampleSizeTwostage2009,
  title = {Sample Size for Two-Stage Studies with Maintenance Therapy},
  author = {Feng, Wentao and Wahed, Abdus S.},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {15},
  pages = {2028--2041},
  issn = {0277-6715},
  doi = {10.1002/sim.3593},
  abstract = {An adaptive treatment strategy (ATS) is defined as a sequence of treatments and intermediate responses. ATS' arise when chronic diseases such as cancer and depression are treated over time with various treatment alternatives depending on intermediate responses to earlier treatments. Clinical trials are often designed to compare ATSs based on appropriate designs such as sequential randomization designs. Although recent literature provides statistical methods for analyzing data from such trials, very few articles have focused on statistical power and sample size issues. This paper presents a sample size formula for comparing the survival probabilities under two treatment strategies sharing same initial, but different maintenance treatment. The formula is based on the large sample properties of inverse-probability-weighted estimator. Simulation study shows strong evidence that the proposed sample size formula guarantees desired power, regardless of the true distributions of survival times.},
  keywords = {Adaptive treatment strategies,Censoring distribution,Counting process,Inverse-probability-weighting,Potential outcomes,Survival probabilities,Two-stage randomization designs},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Feng_Wahed_2009_Sample size for two-stage studies with maintenance therapy.pdf}
}

@book{fergusonCourseLargeSample1996,
  title = {A Course in Large Sample Theory},
  author = {Ferguson, Thomas S},
  year = {1996},
  urldate = {2021-10-27},
  isbn = {978-1-4899-4549-5},
  langid = {english},
  annotation = {OCLC: 883391175},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ferguson_1996_A course in large sample theory.pdf}
}

@article{fieldBootstrappingClusteredData2007,
  title = {Bootstrapping Clustered Data},
  author = {Field, C. A. and Welsh, A. H.},
  year = {2007},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {3},
  pages = {369--390},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2007.00593.x},
  urldate = {2021-11-20},
  abstract = {Various bootstraps have been proposed for bootstrapping clustered data from one-way arrays. The simulation results in the literature suggest that some of these methods work quite well in practice; the theoretical results are limited and more mixed in their conclusions. For example, McCullagh reached negative conclusions about the use of non-parametric bootstraps for one-way arrays. The purpose of this paper is to extend our understanding of the issues by discussing the effect of different ways of modelling clustered data, the criteria for successful bootstraps used in the literature and extending the theory from functions of the sample mean to include functions of the between and within sums of squares and non-parametric bootstraps to include model-based bootstraps. We determine that the consistency of variance estimates for a bootstrap method depends on the choice of model with the residual bootstrap giving consistency under the transformation model whereas the cluster bootstrap gives consistent estimates under both the transformation and the random-effect model. In addition we note that the criteria based on the distribution of the bootstrap observations are not really useful in assessing consistency.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Field_Welsh_2007_Bootstrapping clustered data.pdf}
}

@article{fieuwsJointModellingMultivariate2004,
  title = {Joint Modelling of Multivariate Longitudinal Profiles: Pitfalls of the Random-Effects Approach},
  shorttitle = {Joint Modelling of Multivariate Longitudinal Profiles},
  author = {Fieuws, Steffen and Verbeke, Geert},
  year = {2004},
  journal = {Statistics in Medicine},
  volume = {23},
  number = {20},
  pages = {3093--3104},
  issn = {1097-0258},
  doi = {10/c6b2ts},
  urldate = {2021-09-23},
  abstract = {Due to its flexibility, the random-effects approach for the joint modelling of multivariate longitudinal profiles received a lot of attention in recent publications. In this approach different mixed models are joined by specifying a common distribution for their random-effects. Parameter estimates of this common distribution can then be used to evaluate the relation between the different responses. Using bivariate longitudinal measurements on pure-tone hearing thresholds, it will be shown that such a random-effects approach can yield misleading results for evaluating this relationship. Copyright {\copyright} 2004 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {joint modelling,mixed models,multivariate longitudinal profiles},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fieuws_Verbeke_2004_Joint modelling of multivariate longitudinal profiles.pdf;/Users/nseewald/Zotero/storage/DAZ7A767/sim.html}
}

@article{fieuwsPairwiseFittingMixed2006,
  title = {Pairwise {{Fitting}} of {{Mixed Models}} for the {{Joint Modeling}} of {{Multivariate Longitudinal Profiles}}},
  author = {Fieuws, Steffen and Verbeke, Geert},
  year = {2006},
  journal = {Biometrics},
  volume = {62},
  number = {2},
  pages = {424--431},
  issn = {1541-0420},
  doi = {10/bftk23},
  urldate = {2021-09-20},
  abstract = {A mixed model is a flexible tool for joint modeling purposes, especially when the gathered data are unbalanced. However, computational problems due to the dimension of the joint covariance matrix of the random effects arise as soon as the number of outcomes and/or the number of used random effects per outcome increases. We propose a pairwise approach in which all possible bivariate models are fitted, and where inference follows from pseudo-likelihood arguments. The approach is applicable for linear, generalized linear, and nonlinear mixed models, or for combinations of these. The methodology will be illustrated for linear mixed models in the analysis of 22-dimensional, highly unbalanced, longitudinal profiles of hearing thresholds.},
  langid = {english},
  keywords = {Correlated curves,Joint modeling,Mixed models,Multivariate longitudinal profiles,Pseudo likelihood},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fieuws_Verbeke_2006_Pairwise Fitting of Mixed Models for the Joint Modeling of Multivariate.pdf;/Users/nseewald/Zotero/storage/XBQV38L3/j.1541-0420.2006.00507.html}
}

@article{finnPALOMA2PrimaryResults2016,
  title = {{{PALOMA-2}}: {{Primary}} Results from a Phase {{III}} Trial of Palbociclib ({{P}}) with Letrozole ({{L}}) Compared with Letrozole Alone in Postmenopausal Women with {{ER}}+/{{HER2}}-- Advanced Breast Cancer ({{ABC}}).},
  shorttitle = {{{PALOMA-2}}},
  author = {Finn, Richard S. and Martin, Miguel and Rugo, Hope S. and Jones, Stephen E. and Im, Seock-Ah and Gelmon, Karen A. and Harbeck, Nadia and Lipatov, Oleg N. and Walshe, Janice Maria and Moulder, Stacy L. and Gauthier, Eric Roland and Lu, Dongrui (Ray) and Randolph, Sophia and Di{\'e}ras, V{\'e}ronique and Slamon, Dennis J.},
  year = {2016},
  month = may,
  journal = {Journal of Clinical Oncology},
  volume = {34},
  number = {15\_suppl},
  pages = {507--507},
  publisher = {Wolters Kluwer},
  issn = {0732-183X},
  doi = {10.1200/JCO.2016.34.15_suppl.507},
  urldate = {2024-02-06},
  abstract = {507 Background: Hormonal therapy (HT) is the mainstay for patients (pts) with ER+ BC. P, a cyclin-dependent kinase 4/6 inhibitor, blocks growth of ER+/HER2-- BC preclinical models. In PALOMA-1, an open-label Ph 2 trial, addition of P to L improved median PFS vs L alone (20.2 months [mo] vs 10.2 mo) in pts with first-line ER+/HER2-- ABC with acceptable safety, leading to accelerated FDA approval. PALOMA-2 is a randomized double-blind Ph 3 trial designed to confirm these results. Methods: 666 postmenopausal pts with no prior systemic therapy for ABC were randomized 2:1 to receive P (oral 125 mg/d; 3 wks on/1 wk off) + L (2.5 mg/d continuously) or PLB + L every 28 days until disease progression, consent withdrawal or death. Pts were stratified by disease site, disease-free interval from end of (neo)adjuvant therapy, and prior HT (yes/no). Primary endpoint: investigator-assessed PFS; key secondary endpoints: overall survival (OS), objective response rate (ORR), clinical benefit rate (CBR=CR + PR + SD {$\geq$}24 wks), patient-reported outcomes and safety. Tumor assessments were every 12 wks. 347 events were needed with 90\% power to detect a hazard ratio (HR) {$\leq$}0.69 in favor of P+L (1-sided {$\alpha$}=0.025). Results: By 26 Feb 2016, 331 PFS events occurred. Baseline characteristics were well balanced. Median PFS was 24.8 mo (P+L) vs 14.5 mo (PLB+L) (HR=0.58 [0.46--0.72], P{$<$}0.000001). ORR was improved with P+L (42.1\% vs 34.7\%, P=0.031; 55.3\% vs 44.4\% in pts with measurable disease [P=0.013]). CBR was 84.9\% vs 70.3\% (P{$<$}.0001). Common adverse events (AEs; all grades) with P+L vs PLB+L were neutropenia (79.5\% vs 6.3\%), fatigue (37.4\% vs 27.5\%), nausea (35.1\% vs 26.1\%), arthralgia (33.3\% vs 33.8\%) and alopecia (32.9\% vs 15.8\%). Most common severity seen was G3 for neutropenia (56.1\%) and G1 for the other AEs. Febrile neutropenia was seen only with P+L (2.5\%). Permanent discontinuation due to AEs was 9.7\% (P+L) vs 5.9\% (PLB+L). OS data are immature; final OS analysis is pending. Conclusion: PALOMA-2 expands and confirms the significant clinical benefit and safety of P+L in ER+/HER2-- ABC pts who had not received prior systemic therapy for their advanced disease. Clinical trial information: NCT01740427.}
}

@article{firpoSyntheticControlMethod2018,
  title = {Synthetic {{Control Method}}: {{Inference}}, {{Sensitivity Analysis}} and {{Confidence Sets}}},
  shorttitle = {Synthetic {{Control Method}}},
  author = {Firpo, Sergio and Possebom, Vitor},
  year = {2018},
  month = sep,
  journal = {Journal of Causal Inference},
  volume = {6},
  number = {2},
  publisher = {De Gruyter},
  issn = {2193-3685},
  doi = {10.1515/jci-2016-0026},
  urldate = {2022-01-01},
  abstract = {We extend the inference procedure for the synthetic control method in two ways. First, we propose parametric weights for the p-value that includes the equal weights benchmark of Abadie et\,al. [1]. By changing the value of this parameter, we can analyze the sensitivity of the test's result to deviations from the equal weights benchmark. Second, we modify the RMSPE statistic to test any sharp null hypothesis , including, as a specific case, the null hypothesis of no effect whatsoever analyzed by Abadie et\,al. [1]. Based on this last extension, we invert the test statistic to estimate confidence sets that quickly show the point-estimates' precision, and the test's significance and robustness. We also extend these two tools to other test statistics and to problems with multiple outcome variables or multiple treated units. Furthermore, in a Monte Carlo experiment, we find that the RMSPE statistic has good properties with respect to size, power and robustness. Finally, we illustrate the usefulness of our proposed tools by reanalyzing the economic impact of ETA's terrorism in the Basque Country, studied first by Abadie and Gardeazabal [2] and Abadie et\,al. [3].},
  langid = {english},
  keywords = {Confidence Sets,Hypothesis Testing,Sensitivity Analysis,Synthetic Control Estimator},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Firpo_Possebom_2018_Synthetic Control Method.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Firpo_Possebom_2018_Synthetic Control Method2.pdf}
}

@article{fisherFiducialArgumentStatistical1935,
  title = {The {{Fiducial Argument}} in {{Statistical Inference}}},
  author = {Fisher, R. A.},
  year = {1935},
  month = dec,
  journal = {Annals of Eugenics},
  volume = {6},
  number = {4},
  pages = {391--398},
  issn = {20501420},
  doi = {10.1111/j.1469-1809.1935.tb02120.x},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fisher_1935_The Fiducial Argument in Statistical Inference.pdf}
}

@article{fisherMathematicalFoundationsTheoretical1922,
  title = {On the {{Mathematical Foundations}} of {{Theoretical Statistics}}},
  author = {Fisher, R. A.},
  year = {1922},
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {22},
  eprint = {91208},
  eprinttype = {jstor},
  pages = {309--368},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fisher_1922_On the Mathematical Foundations of Theoretical Statistics.pdf}
}

@article{fisherStatisticalMethodsScientific1955,
  title = {Statistical {{Methods}} and {{Scientific Induction}}},
  author = {Fisher, Ronald},
  year = {1955},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {17},
  number = {1},
  eprint = {2983785},
  eprinttype = {jstor},
  pages = {69--78},
  abstract = {THE attempt to reinterpret the common tests of significance used in scientific research as though they constituted some kind of acceptance procedure and led to "decisions" in Wald's sense, originated in several misapprehensions and has led, apparently, to several more.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fisher_1955_Statistical Methods and Scientific Induction.pdf}
}

@article{fisherUseOnesidedTests1991,
  title = {The Use of One-Sided Tests in Drug Trials: An Fda Advisory Committee Member's Perspective},
  shorttitle = {The Use of One-Sided Tests in Drug Trials},
  author = {Fisher, Lloyd D.},
  year = {1991},
  month = jan,
  journal = {Journal of Biopharmaceutical Statistics},
  volume = {1},
  number = {1},
  pages = {151--156},
  publisher = {Taylor \& Francis},
  issn = {1054-3406},
  doi = {10.1080/10543409108835012},
  urldate = {2024-06-17},
  abstract = {The use of one-sided or two-sided tests in drug trials to evaluate new compounds is considered. For drugs that may be tested against placebos, with two positive trials required (as in the United States), it is argued that from both a regulatory and pharmaceutical industry perspective, one-sided tests at the 0.05 significance level are appropriate. In situations where only one trial against placebo may be done (for example, survival trials), one-sided tests at the 0.025 level are appropriate in many cases. For active control trials it is argued that two-sided tests are usually appropriate.},
  pmid = {1844686},
  keywords = {Drug approval,One-sided test,Two-sided test},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fisher_1991_The_use_of_one-sided_tests_in_drug_trials.pdf}
}

@book{fitzmauriceAppliedLongitudinalAnalysis2011,
  title = {Applied Longitudinal Analysis},
  author = {Fitzmaurice, Garrett M. and Laird, Nan M. and Ware, James H.},
  year = {2011},
  series = {Wiley Series in Probability and Statistics},
  edition = {2nd ed},
  publisher = {Wiley},
  address = {Hoboken, N.J},
  abstract = {"Since the publication of the first edition, the authors have solicited feedback from both the instructors who use the book as a text for their courses as well as the researchers who use the book as a resource for their research. Thus, the improved Second Edition of Applied Longitudinal Analysis features many additions and revisions based on the feedback of readers, making it the go-to reference for applied use in public health, epidemiology, and pharmaceutical sciences"--},
  isbn = {978-0-470-38027-7},
  lccn = {QA278 .F575 2011},
  keywords = {nosource},
  annotation = {OCLC: ocn708358043},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fitzmaurice et al_2011_Applied longitudinal analysis.pdf}
}

@article{fitzmauricePrimerLongitudinalData2008,
  title = {A {{Primer}} in {{Longitudinal Data Analysis}}},
  author = {Fitzmaurice, Garrett M. and Ravichandran, Caitlin},
  year = {2008},
  month = nov,
  journal = {Circulation},
  volume = {118},
  number = {19},
  pages = {2005--2010},
  issn = {0009-7322, 1524-4539},
  doi = {10/ch8mc2},
  urldate = {2020-02-06},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fitzmaurice_Ravichandran_2008_A Primer in Longitudinal Data Analysis.pdf}
}

@article{fitzsimonsSmartDesignResponse2015,
  title = {A Smart Design: {{Response}} to Reinforcement-Based Treatment Intensity among Pregnant, Drug-Dependent Women},
  shorttitle = {A Smart Design},
  author = {Fitzsimons, Heather and Tuten, Michelle and O'Grady, Kevin and Chisolm, Margaret S. and Jones, Hendree E.},
  year = {2015},
  month = nov,
  journal = {Drug and Alcohol Dependence},
  volume = {156},
  pages = {e69},
  issn = {0376-8716},
  doi = {10/gfn9pt},
  urldate = {2018-12-08},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fitzsimons et al_2015_A smart design.pdf;/Users/nseewald/Zotero/storage/JK347FR3/S0376871615014933.html}
}

@article{fitzsimonsSmartDesignResponse2015a,
  title = {A Smart Design: {{Response}} to Reinforcement-Based Treatment Intensity among Pregnant, Drug-Dependent Women},
  shorttitle = {A Smart Design},
  author = {Fitzsimons, Heather and Tuten, Michelle and O'Grady, Kevin and Chisolm, Margaret S. and Jones, Hendree E.},
  year = {2015},
  month = nov,
  journal = {Drug and Alcohol Dependence},
  volume = {156},
  pages = {e69},
  issn = {0376-8716},
  doi = {10.1016/j.drugalcdep.2015.07.1106},
  urldate = {2024-08-20},
  file = {/Users/nseewald/Zotero/storage/M8V8F7UN/S0376871615014933.html}
}

@article{fjeldsoeIterativeDevelopmentMobileMums2012,
  title = {Iterative Development of {{MobileMums}}: A Physical Activity Intervention for Women with Young Children},
  shorttitle = {Iterative Development of {{MobileMums}}},
  author = {Fjeldsoe, Brianna S and Miller, Yvette D and O'Brien, Jasmine L and Marshall, Alison L},
  year = {2012},
  journal = {International Journal of Behavioral Nutrition and Physical Activity},
  volume = {9},
  number = {1},
  pages = {151},
  issn = {1479-5868},
  doi = {10.1186/1479-5868-9-151},
  urldate = {2018-10-12},
  abstract = {Background: To describe the iterative development process and final version of `MobileMums': a physical activity intervention for women with young children ({$<$}5 years) delivered primarily via mobile telephone (mHealth) short messaging service (SMS). Methods: MobileMums development followed the five steps outlined in the mHealth development and evaluation framework: 1) conceptualization (critique of literature and theory); 2) formative research (focus groups, n= 48); 3) pre-testing (qualitative pilot of intervention components, n= 12); 4) pilot testing (pilot RCT, n= 88); and, 5) qualitative evaluation of the refined intervention (n= 6). Results: Key findings identified throughout the development process that shaped the MobileMums program were the need for: behaviour change techniques to be grounded in Social Cognitive Theory; tailored SMS content; two-way SMS interaction; rapport between SMS sender and recipient; an automated software platform to generate and send SMS; and, flexibility in location of a face-to-face delivered component. Conclusions: The final version of MobileMums is flexible and adaptive to individual participant's physical activity goals, expectations and environment. MobileMums is being evaluated in a community-based randomised controlled efficacy trial (ACTRN12611000481976).},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fjeldsoe et al_2012_Iterative development of MobileMums.pdf}
}

@misc{FlexibleImputationMissing,
  title = {Flexible {{Imputation}} of {{Missing Data}}, {{Second Edition}} {\textbar} {{Stef}} van {{Buuren}}},
  urldate = {2024-02-15},
  howpublished = {https://www.taylorfrancis.com/pdfviewer/},
  file = {/Users/nseewald/Zotero/storage/S32YFD4P/pdfviewer.html}
}

@article{flexonEffectCannabisLaws2019,
  title = {The Effect of Cannabis Laws on Opioid Use},
  author = {Flexon, Jamie L. and Stolzenberg, Lisa and D'Alessio, Stewart J.},
  year = {2019},
  month = dec,
  journal = {International Journal of Drug Policy},
  volume = {74},
  pages = {152--159},
  issn = {0955-3959},
  doi = {10.1016/j.drugpo.2019.09.013},
  urldate = {2024-05-03},
  abstract = {Background Many Americans rely on opioids at varying dosages to help ameliorate their suffering. However, empirical evidence is mounting that opioids are ineffective at controlling non-cancer related chronic pain, and many argue the strategies meant to relieve patient suffering are contributing to the growing opioid epidemic. Concurrently, several states now allow the use of medical cannabis to treat a variety of medical conditions, including chronic pain. Needing more exploration is the impact of cannabis laws on general opioid reliance and whether chronic pain sufferers are opting to use cannabis medicinally instead of opioids. Methods This study investigates the effect of Medical Marijuana Laws (MML)s on opioid use and misuse controlling for a number of relevant factors using data from several years of the National Survey on Drug Use and Health and multivariate logistic regression and longitudinal analysis strategies. Results Results provide evidence that MMLs may be effective at reducing opioid reliance as survey respondents living in states with medical cannabis legislation are much less apt to report using opioid analgesics than people living in states without such laws, net other factors. Results further indicate that the presence of medicinal cannabis legislation appears to have no influence over opioid misuse. Conclusion MMLs may ultimately serve to attenuate the consequences of opioid overreliance.},
  keywords = {Cannabis,Chronic pain management,Medical Marijuana Laws,Opioids},
  file = {/Users/nseewald/Zotero/storage/WK2FDKAU/S0955395919302567.html}
}

@article{flynnModelingTumorGrowth2023,
  title = {Modeling {{Tumor Growth Using Partly Conditional Survival Models}}: {{A Case Study}} in {{Colorectal Cancer}}},
  shorttitle = {Modeling {{Tumor Growth Using Partly Conditional Survival Models}}},
  author = {Flynn, Jessica R. and Curry, Michael and Zhao, Binsheng and Yang, Hao and Dercle, Laurent and Fojo, Antonio Tito and Connors, Dana E. and Schwartz, Lawrence H. and G{\"o}nen, Mithat and Moskowitz, Chaya S.},
  year = {2023},
  month = sep,
  journal = {JCO Clinical Cancer Informatics},
  number = {7},
  pages = {e2200203},
  publisher = {Wolters Kluwer},
  doi = {10.1200/CCI.22.00203},
  urldate = {2024-03-13},
  abstract = {Purpose There are multiple approaches to modeling the relationship between longitudinal tumor measurements obtained from serial imaging and overall survival. Many require strong assumptions that are untestable and debatable. We illustrate how to apply a novel, more flexible approach, the partly conditional (PC) survival model, using images acquired during a phase III, randomized clinical trial in colorectal cancer as an example. Methods PC survival approaches were used to model longitudinal volumetric computed tomography data of 1,025 patients in the completed VELOUR trial, which evaluated adding aflibercept to infusional fluorouracil, leucovorin, and irinotecan for treating metastatic colorectal cancer. PC survival modeling is a semiparametric approach to estimating associations of longitudinal measurements with time-to-event outcomes. Overall survival was our outcome. Covariates included baseline tumor burden, change in tumor burden from baseline to each follow-up time, and treatment. Both unstratified and time-stratified models were investigated. Results Without making assumptions about the distribution of the tumor growth process, we characterized associations between the change in tumor burden and survival. This change was significantly associated with survival (hazard ratio [HR], 1.04; 95\% CI, 1.02 to 1.05; P {$<$} .001), suggesting that aflibercept works at least in part by altering the tumor growth trajectory. We also found baseline tumor size prognostic for survival even when accounting for the change in tumor burden over time (HR, 1.02; 95\% CI, 1.01 to 1.02; P {$<$} .001). Conclusion The PC modeling approach offers flexible characterization of associations between longitudinal covariates, such as serially assessed tumor burden, and survival time. It can be applied to a variety of data of this nature and used as clinical trials are ongoing to incorporate new disease assessment information as it is accumulated, as indicated by an example from colorectal cancer.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Flynn et al_2023_Modeling Tumor Growth Using Partly Conditional Survival Models.pdf}
}

@article{flynnModelingTumorGrowth2023a,
  title = {Modeling {{Tumor Growth Using Partly Conditional Survival Models}}: {{A Case Study}} in {{Colorectal Cancer}}},
  shorttitle = {Modeling {{Tumor Growth Using Partly Conditional Survival Models}}},
  author = {Flynn, Jessica R. and Curry, Michael and Zhao, Binsheng and Yang, Hao and Dercle, Laurent and Fojo, Antonio Tito and Connors, Dana E. and Schwartz, Lawrence H. and G{\"o}nen, Mithat and Moskowitz, Chaya S.},
  year = {2023},
  month = sep,
  journal = {JCO Clinical Cancer Informatics},
  number = {7},
  pages = {e2200203},
  publisher = {Wolters Kluwer},
  doi = {10.1200/CCI.22.00203},
  urldate = {2024-03-13},
  abstract = {Purpose There are multiple approaches to modeling the relationship between longitudinal tumor measurements obtained from serial imaging and overall survival. Many require strong assumptions that are untestable and debatable. We illustrate how to apply a novel, more flexible approach, the partly conditional (PC) survival model, using images acquired during a phase III, randomized clinical trial in colorectal cancer as an example. Methods PC survival approaches were used to model longitudinal volumetric computed tomography data of 1,025 patients in the completed VELOUR trial, which evaluated adding aflibercept to infusional fluorouracil, leucovorin, and irinotecan for treating metastatic colorectal cancer. PC survival modeling is a semiparametric approach to estimating associations of longitudinal measurements with time-to-event outcomes. Overall survival was our outcome. Covariates included baseline tumor burden, change in tumor burden from baseline to each follow-up time, and treatment. Both unstratified and time-stratified models were investigated. Results Without making assumptions about the distribution of the tumor growth process, we characterized associations between the change in tumor burden and survival. This change was significantly associated with survival (hazard ratio [HR], 1.04; 95\% CI, 1.02 to 1.05; P {$<$} .001), suggesting that aflibercept works at least in part by altering the tumor growth trajectory. We also found baseline tumor size prognostic for survival even when accounting for the change in tumor burden over time (HR, 1.02; 95\% CI, 1.01 to 1.02; P {$<$} .001). Conclusion The PC modeling approach offers flexible characterization of associations between longitudinal covariates, such as serially assessed tumor burden, and survival time. It can be applied to a variety of data of this nature and used as clinical trials are ongoing to incorporate new disease assessment information as it is accumulated, as indicated by an example from colorectal cancer.}
}

@article{fortneyStudyPromoteInnovation,
  title = {Study to {{Promote Innovation}} in {{Rural Integrated Telepsychiatry}} ({{SPIRIT}}): {{Rationale}} and Design of a Randomized Comparative Effectiveness Trial of Managing Complex Psychiatric Disorders in Rural Primary Care Clinics},
  author = {Fortney, John C. and Heagerty, Patrick J. and Bauer, Amy M. and Cerimele, Joseph M. and Kaysen, Debra and Pfeiffer, Paul N. and Zielinski, Melissa J. and Pyne, Jeffrey M. and Bowen, Deb and Russo, Joan and Ferro, Lori and Moore, Danna and Nolan, J.P. and Fee, Florence C. and Heral, Tammy and {Freyholtz-London}, Jode and McDonald, Bernadette and Mullins, Jeremey and Hafer, Erin and Solberg, Leif and Un{\"u}tzer, J{\"u}rgen},
  journal = {Contemporary Clinical Trials},
  doi = {10/ggcxpc},
  abstract = {{$<$}h3 class="u-h4 u-margin-m-top u-margin-xs-bottom"{$>$}Objective{$<$}/h3{$><$}p{$>$}Managing complex psychiatric disorders like PTSD and bipolar disorder is challenging in Federally Qualified Health Centers (FQHCs) delivering care to U.S residents living in underserved rural areas. This protocol paper describes SPIRIT, a pragmatic comparative effectiveness trial designed to compare two approaches to managing PTSD and bipolar disorder in FQHCs.{$<$}/p{$><$}h3 class="u-h4 u-margin-m-top u-margin-xs-bottom"{$>$}Interventions{$<$}/h3{$><$}p{$>$}Treatment comparators are: 1) Telepsychiatry Collaborative Care, which integrates consulting telepsychiatrists into primary care teams, and 2) Telepsychiatry Enhanced Referral, where telepsychiatrists and telepsychologists treat patients directly.{$<$}/p{$><$}h3 class="u-h4 u-margin-m-top u-margin-xs-bottom"{$>$}Methods{$<$}/h3{$><$}p{$>$}Because Telepsychiatry Enhanced Referral is an adaptive intervention, a Sequential, Multiple Assignment, Randomized Trial design is used. Twenty-four FQHC clinics without on-site psychiatrists or psychologists are participating in the trial. The target sample is 1000 patients screening positive for PTSD and/or bipolar disorder who are not already engaged in pharmacotherapy with a mental health specialist. Intervention fidelity is measured but not controlled. Patient treatment engagement is measured but not required, and intent-to-treat analysis will be used. Survey questions measure treatment engagement and effectiveness. The Short-Form 12 Mental Health Component Summary (SF-12 MCS) is the primary outcome.{$<$}/p{$><$}h3 class="u-h4 u-margin-m-top u-margin-xs-bottom"{$>$}Results{$<$}/h3{$><$}p{$>$}To date, 34\% of those enrolled ({$<$}em{$>$}n{$<$}/em{$>$}\,=\,924) are racial/ethnic minorities, 82\% are not fully employed, 68\% are Medicaid enrollees, 7\% are uninsured, and 62\% live in poverty. Mental health related quality of life (SF-12 MCS) is 2.5 standard deviations below the national mean.{$<$}/p{$><$}h3 class="u-h4 u-margin-m-top u-margin-xs-bottom"{$>$}Discussion{$<$}/h3{$><$}p{$>$}We hypothesize that patients randomized to Telepsychiatry Collaborative Care will have better outcomes than those randomized to Telepsychiatry Enhanced Referral because a higher proportion will engage in evidence-based treatment.{$<$}/p{$>$}},
  keywords = {nosource}
}

@article{foxSMARTUseMedications2024,
  title = {{{SMART}} Use of Medications for the Treatment of Adolescent Severe Obesity: {{A}} Sequential Multiple Assignment Randomized Trial Protocol},
  shorttitle = {{{SMART}} Use of Medications for the Treatment of Adolescent Severe Obesity},
  author = {Fox, Claudia K. and Vock, David M. and Sherwood, Nancy E. and Gross, Amy C. and Ryder, Justin R. and Bensignor, Megan O. and Bomberg, Eric M. and Sunni, Muna and Bramante, Carolyn T. and Jacobs, Nina and Raatz, Sarah J. and Kelly, Aaron S.},
  year = {2024},
  month = mar,
  journal = {Contemporary Clinical Trials},
  volume = {138},
  pages = {107444},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2024.107444},
  urldate = {2024-10-11},
  abstract = {Background Severe obesity is a complex, chronic disease affecting nearly 9\% of adolescents in the U.S. Although the current mainstay of treatment is lifestyle therapy, pediatric clinical practice guidelines recommend the addition of adjunct anti-obesity medication (AOM), such as phentermine and topiramate. However, guidance regarding when adjunct AOM should be started and how AOM should be used is unclear. Furthermore, an inherent limitation of current treatment guidelines is their ``one-size-fits-all'' approach, which does not account for the heterogeneous nature of obesity and high degree of patient variability in response to all interventions. Methods This paper describes the study design and methods of a sequential multiple assignment randomized trial (SMART), ``SMART Use of Medications for the Treatment of Adolescent Severe Obesity.'' The trial will examine 1) when to start AOM (specifically phentermine) in adolescents who are not responding to lifestyle therapy and 2) how to modify AOM when there is a sub-optimal response to the initial pharmacological intervention (specifically, for phentermine non-responders, is it better to add topiramate to phentermine or switch to topiramate monotherapy). Critically, participant characteristics that may differentially affect response to treatment will be assessed and evaluated as potential moderators of intervention efficacy. Conclusion Data from this study will be used to inform the development of an adaptive intervention for the treatment of adolescent severe obesity that includes empirically-derived decision rules regarding when and how to use AOM. Future research will test this adaptive intervention against standard ``one-size-fits-all'' treatments.},
  keywords = {_tablet,Adolescent,Lifestyle therapy,Obesity,Phentermine,Protocol,SMART,Topiramate},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fox_et_al_2024_SMART_use_of_medications_for_the_treatment_of_adolescent_severe_obesity.pdf;/Users/nseewald/Zotero/storage/3LKFJV7R/S1551714424000181.html}
}

@article{foxSMARTUseMedications2024a,
  title = {{{SMART}} Use of Medications for the Treatment of Adolescent Severe Obesity: {{A}} Sequential Multiple Assignment Randomized Trial Protocol},
  shorttitle = {{{SMART}} Use of Medications for the Treatment of Adolescent Severe Obesity},
  author = {Fox, Claudia K. and Vock, David M. and Sherwood, Nancy E. and Gross, Amy C. and Ryder, Justin R. and Bensignor, Megan O. and Bomberg, Eric M. and Sunni, Muna and Bramante, Carolyn T. and Jacobs, Nina and Raatz, Sarah J. and Kelly, Aaron S.},
  year = {2024},
  month = mar,
  journal = {Contemporary Clinical Trials},
  volume = {138},
  pages = {107444},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2024.107444},
  urldate = {2024-10-16},
  abstract = {Background Severe obesity is a complex, chronic disease affecting nearly 9\% of adolescents in the U.S. Although the current mainstay of treatment is lifestyle therapy, pediatric clinical practice guidelines recommend the addition of adjunct anti-obesity medication (AOM), such as phentermine and topiramate. However, guidance regarding when adjunct AOM should be started and how AOM should be used is unclear. Furthermore, an inherent limitation of current treatment guidelines is their ``one-size-fits-all'' approach, which does not account for the heterogeneous nature of obesity and high degree of patient variability in response to all interventions. Methods This paper describes the study design and methods of a sequential multiple assignment randomized trial (SMART), ``SMART Use of Medications for the Treatment of Adolescent Severe Obesity.'' The trial will examine 1) when to start AOM (specifically phentermine) in adolescents who are not responding to lifestyle therapy and 2) how to modify AOM when there is a sub-optimal response to the initial pharmacological intervention (specifically, for phentermine non-responders, is it better to add topiramate to phentermine or switch to topiramate monotherapy). Critically, participant characteristics that may differentially affect response to treatment will be assessed and evaluated as potential moderators of intervention efficacy. Conclusion Data from this study will be used to inform the development of an adaptive intervention for the treatment of adolescent severe obesity that includes empirically-derived decision rules regarding when and how to use AOM. Future research will test this adaptive intervention against standard ``one-size-fits-all'' treatments.},
  keywords = {Adolescent,Lifestyle therapy,Obesity,Phentermine,Protocol,SMART,Topiramate},
  file = {/Users/nseewald/Zotero/storage/G45CMEJI/S1551714424000181.html}
}

@article{frangakisPrincipalStratificationCausal2002,
  title = {Principal {{Stratification}} in {{Causal Inference}}},
  author = {Frangakis, Constantine E. and Rubin, Donald B.},
  year = {2002},
  month = mar,
  journal = {Biometrics},
  volume = {58},
  number = {1},
  pages = {21--29},
  issn = {0006341X},
  doi = {10/c26zhj},
  urldate = {2018-11-20},
  abstract = {Many scientific problems require that treatment comparisons be adjusted for posttreatment variables, but the estimands underlying standard methods are not causal effects. To address this deficiency, we propose a general framework for comparing treatments adjusting for posttreatment variables that yields principal effects based on principal stratification. Principal stratification with respect to a posttreatment variable is a cross-classification of subjects defined by the joint potential values of that posttreatment variable under each of the treatments being compared. Principal effects are causal effects within a principal stratum. The key property of principal strata is that they are not affected by treatment assignment and therefore can be used just as any pretreatment covariate, such as age category. As a result, the central property of our principal effects is that they are always causal effects and do not suffer from the complications of standard posttreatment-adjusted estimands. We discuss briefly that such principal causal effects are the link between three recent applications with adjustment for posttreatment variables: (i) treatment noncompliance, (ii) missing outcomes (dropout) following treatment noncompliance, and (iii) censoring by death. We then attack the problem of surrogate or biomarker endpoints, where we show, using principal causal effects, that all current definitions of surrogacy, even when perfectly true, do not generally have the desired interpretation as causal effects of treatment on outcome. We go on to formulate estimands based on principal stratification and principal causal effects and show their superiority.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Frangakis_Rubin_2002_Principal Stratification in Causal Inference.pdf}
}

@article{freedmanSoCalledHuberSandwich2006,
  title = {On {{The So-Called}} ``{{Huber Sandwich Estimator}}'' and ``{{Robust Standard Errors}}''},
  author = {Freedman, David A},
  year = {2006},
  month = nov,
  journal = {The American Statistician},
  volume = {60},
  number = {4},
  pages = {299--302},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/000313006X152207},
  urldate = {2022-01-28},
  abstract = {The ``Huber Sandwich Estimator'' can be used to estimate the variance of the MLE when the underlying model is incorrect. If the model is nearly correct, so are the usual standard errors, and robustification is unlikely to help much. On the other hand, if the model is seriously in error, the sandwich may help on the variance side, but the parameters being estimated by the MLE are likely to be meaningless---except perhaps as descriptive statistics.},
  keywords = {Maximum likelihood,Specification error,White's correction},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Freedman_2006_On The So-Called “Huber Sandwich Estimator” and “Robust Standard Errors”.pdf;/Users/nseewald/Zotero/storage/N2SCT6KE/000313006X152207.html}
}

@article{freyaldenhovenPreeventTrendsPanel2019,
  ids = {freyaldenhovenPreeventTrendsPanel2019a},
  title = {Pre-Event {{Trends}} in the {{Panel Event-Study Design}}},
  author = {Freyaldenhoven, Simon and Hansen, Christian and Shapiro, Jesse M.},
  year = {2019},
  month = sep,
  journal = {American Economic Review},
  volume = {109},
  number = {9},
  pages = {3307--3338},
  issn = {0002-8282},
  doi = {10/gf7gjt},
  urldate = {2021-06-27},
  abstract = {We consider a linear panel event-study design in which unobserved confounds may be related both to the outcome and to the policy variable of interest. We provide sufficient conditions to identify the causal effect of the policy by exploiting covariates related to the policy only through the confounds. Our model implies a set of moment equations that are linear in parameters. The effect of the policy can be estimated by 2SLS, and causal inference is valid even when endogeneity leads to pre-event trends ("pre-trends") in the outcome. Alternative approaches perform poorly in our simulations.},
  langid = {english},
  keywords = {Single Equation Models,Single Variables: Panel Data Models,Spatio-temporal Models Single Equation Models: Single Variables: Instrumental Variables (IV) Estimation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Freyaldenhoven et al_2019_Pre-event Trends in the Panel Event-Study Design.pdf;/Users/nseewald/Zotero/storage/3XQRGWDW/articles.html;/Users/nseewald/Zotero/storage/JP7K2CEA/articles.html}
}

@article{friedenReducingRisksRelief2016,
  title = {Reducing the {{Risks}} of {{Relief}} --- {{The CDC Opioid-Prescribing Guideline}}},
  author = {Frieden, Thomas R. and Houry, Debra},
  year = {2016},
  month = apr,
  journal = {New England Journal of Medicine},
  volume = {374},
  number = {16},
  pages = {1501--1504},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMp1515917},
  urldate = {2023-06-09},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Frieden_Houry_2016_Reducing the Risks of Relief — The CDC Opioid-Prescribing Guideline.pdf}
}

@book{friedmanFundamentalsClinicalTrials2010,
  title = {Fundamentals of Clinical Trials},
  author = {Friedman, Lawrence M. and Furberg, Curt and DeMets, David L.},
  year = {2010},
  edition = {4th ed},
  publisher = {Springer},
  address = {New York},
  isbn = {978-1-4419-1585-6},
  lccn = {R853.C55 F75 2010},
  keywords = {nosource}
}

@article{friedmanMultivariateAdaptiveRegression1991,
  title = {Multivariate {{Adaptive Regression Splines}}},
  author = {Friedman, Jerome H.},
  year = {1991},
  month = mar,
  journal = {The Annals of Statistics},
  volume = {19},
  number = {1},
  pages = {1--67},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176347963},
  urldate = {2024-12-16},
  abstract = {A new method is presented for flexible regression modeling of high dimensional data. The model takes the form of an expansion in product spline basis functions, where the number of basis functions as well as the parameters associated with each one (product degree and knot locations) are automatically determined by the data. This procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties. Unlike recursive partitioning, however, this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables. In addition, the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions.},
  keywords = {62H30,62J02,65D07,65D10,65D15,68T05,68T10,90A19,93C35,93E11,93E14,AID,CART,multivariable function approximation,multivariate smoothing,Nonparametric multiple regression,recursive partitioning,splines,statistical learning neural networks},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Friedman - 1991 - Multivariate Adaptive Regression Splines.pdf}
}

@article{frisonLinearlyDivergentTreatment1997,
  title = {Linearly Divergent Treatment Effects in Clinical Trials with Repeated Measures: Efficient Analysis Using Summary Statistics},
  shorttitle = {Linearly Divergent Treatment Effects in Clinical Trials with Repeated Measures},
  author = {Frison, Lars J. and Pocock, Stuart J.},
  year = {1997},
  journal = {Statistics in Medicine},
  volume = {16},
  number = {24},
  pages = {2855--2872},
  issn = {1097-0258},
  doi = {10/cfrgvr},
  urldate = {2020-01-17},
  abstract = {In many randomized clinical trials with repeated measures of a response variable one anticipates a linear divergence over time in the difference between treatments. This paper explores how to make an efficient choice of analysis based on individual patient summary statistics. With the objective of estimating the mean rate of treatment divergence the simplest choice of summary statistic is the regression coefficient of response on time for each subject (SLOPE). The gains in statistical efficiency imposed by adjusting for the observed pre-treatment levels, or even better the estimated intercepts, are clarified. In the process, we develop the optimal linear summary statistic for any repeated measures design with assumed known covariance structure and shape of true mean treatment difference over time. Statistical power considerations are explored and an example from an asthma trial is used to illustrate the main points. {\copyright} 1997 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 1997 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Frison_Pocock_1997_Linearly divergent treatment effects in clinical trials with repeated measures.pdf;/Users/nseewald/Zotero/storage/NFFEP3XK/(SICI)1097-0258(19971230)16242855AID-SIM7493.0.html}
}

@article{fritzRequiredSampleSize2007,
  title = {Required {{Sample Size}} to {{Detect}} the {{Mediated Effect}}},
  author = {Fritz, Matthew S. and MacKinnon, David P.},
  year = {2007},
  month = mar,
  journal = {Psychological Science},
  volume = {18},
  number = {3},
  pages = {233--239},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.2007.01882.x},
  urldate = {2023-05-12},
  abstract = {Mediation models are widely used, and there are many tests of the mediated effect. One of the most common questions that researchers have when planning mediation studies is, ``How many subjects do I need to achieve adequate power when testing for mediation?'' This article presents the necessary sample sizes for six of the most common and the most recommended tests of mediation for various combinations of parameters, to provide a guide for researchers when designing studies or applying for grants.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fritz_MacKinnon_2007_Required Sample Size to Detect the Mediated Effect.pdf}
}

@article{fritzSequentialMultipleAssignmentRandomized2020,
  title = {A {{Sequential Multiple-Assignment Randomized Trial}} ({{SMART}}) for {{Stepped Care Management}} of {{Low Back Pain}} in the {{Military Health System}}: {{A Trial Protocol}}},
  shorttitle = {A {{Sequential Multiple-Assignment Randomized Trial}} ({{SMART}}) for {{Stepped Care Management}} of {{Low Back Pain}} in the {{Military Health System}}},
  author = {Fritz, Julie M and Rhon, Daniel I and Teyhen, Deydre S and Kean, Jacob and Vanneman, Megan E and Garland, Eric L and Lee, Ian E and Thorp, Richard E and Greene, Tom H},
  year = {2020},
  month = dec,
  journal = {Pain Medicine},
  volume = {21},
  number = {Supplement\_2},
  pages = {S73-S82},
  issn = {1526-2375},
  doi = {10.1093/pm/pnaa338},
  urldate = {2021-04-09},
  abstract = {The Defense Health Agency has prioritized system-level pain management initiatives within the Military Health System (MHS), with low back pain as one of the key focus areas. A stepped care model focused on nonpharmacologic treatment to promote self-management is recommended. Implementation of stepped care is complicated by lack of information on the most effective nonpharmacologic strategies and how to sequence and tailor the various available options. The Sequential Multiple-Assignment Randomization Trial for Low Back Pain (SMART LBP) is a multisite pragmatic trial using a SMART design to assess the effectiveness of nonpharmacologic treatments for chronic low back pain.This SMART trial has two treatment phases. Participants from three military treatment facilities are randomized to 6~weeks of phase I treatment, receiving either physical therapy (PT) or Army Medicine's holistic Move2Health (M2H) program in a package specific to low back pain. Nonresponders to treatment in phase I are again randomized to phase II treatment of combined M2H + PT or mindfulness-based treatment using the Mindfulness-Oriented Recovery Enhancement (MORE) program. The primary outcome is the Patient-Reported Outcomes Measurement Information System pain interference computer-adapted test score.This trial is part of an initiative funded by the National Institutes of Health, Veterans Affairs, and the Department of Defense to establish a national infrastructure for effective system-level management of chronic pain with a focus on nonpharmacologic treatments. The results of this study will provide important information on nonpharmacologic care for chronic LBP in the MHS embedded within a stepped care framework.},
  keywords = {to-read},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fritz et al_2020_A Sequential Multiple-Assignment Randomized Trial (SMART) for Stepped Care.pdf;/Users/nseewald/Zotero/storage/5B27XCRC/6033496.html;/Users/nseewald/Zotero/storage/XZXR9XNH/6033496.html}
}

@article{fryBirdsFeatherFlock2021,
  title = {Birds of a Feather Flock Together: {{Comparing}} Controlled Pre--Post Designs},
  shorttitle = {Birds of a Feather Flock Together},
  author = {Fry, Carrie E. and Hatfield, Laura A.},
  year = {2021},
  journal = {Health Services Research},
  volume = {56},
  number = {5},
  pages = {942--952},
  issn = {1475-6773},
  doi = {10.1111/1475-6773.13697},
  urldate = {2024-06-05},
  abstract = {Objective To formalize comparative interrupted time series (CITS) using the potential outcomes framework; compare two version of CITS---a standard linear version and one that adds postperiod group-by-time parameters---to two versions of difference-in-differences (DID)---a standard version with time fixed effects and one that adds group-specific pretrends; and reanalyze three previously published papers using these models. Data Sources Outcome data for reanalyses come from two counties' jail booking and release data, Medicaid prescription drug rebate data from the Centers for Medicare and Medicaid Services (CMS), and acute hepatitis C incidence from the Centers for Disease Control and Prevention. Study Design DID and CITS were compared using potential outcomes, and reanalyses were conducted using the four described pre--post study designs. Data Collection/Extraction Methods Data from county jails were provided by sheriffs. Data from CMS are publicly available. Data for the third reanalysis were provided by the authors of the original study. Principal Findings Though written differently and preferred by different research communities, the general version of CITS and DID with group-specific pretrends are the same: they yield the same counterfactuals and identify the same treatment effects. In a reanalysis with evidence of divergent preperiod trends, failing to account for this in standard DID led to an 84\% smaller effect estimate than the more flexible models. In a second reanalysis with evidence of nonlinear outcome trends, failing to account for this in linear CITS led to a 28\% smaller effect estimate than the more flexible models. Conclusion We recommend detailing a causal model for treatment selection and outcome generation and the required counterfactuals before choosing an analytical approach. The more flexible versions of DID and CITS can accommodate features often found in real data, namely, nonlinearities and divergent preperiod outcome trends.},
  langid = {english},
  keywords = {_tablet,econometrics,evaluation design and research,observational data/quasi-experiments,program evaluation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fry_Hatfield_2021_Birds of a feather flock together.pdf;/Users/nseewald/Zotero/storage/FA5BTMAL/1475-6773.html}
}

@misc{FullArticleSMART,
  title = {Full Article: {{SMART Binary}}: {{New Sample Size Planning Resources}} for {{SMART Studies}} with {{Binary Outcome Measurements}}},
  urldate = {2024-08-19},
  howpublished = {https://www.tandfonline.com/doi/full/10.1080/00273171.2023.2229079?casa\_token=Vs3uS0peccAAAAAA\%3AukIiIQChEo-gl0LP6F-4ZY1iC22H9tfe-hR1XEWwRCYEOCAknFnXB8CIVmJqajIsmyjLA0UARposAw}
}

@article{fuProgramLungCancer2017,
  title = {Program for Lung Cancer Screening and Tobacco Cessation: {{Study}} Protocol of a Sequential, Multiple Assignment, Randomized Trial},
  shorttitle = {Program for Lung Cancer Screening and Tobacco Cessation},
  author = {Fu, Steven S. and Rothman, Alexander J. and Vock, David M. and Lindgren, Bruce and Almirall, Daniel and Begnaud, Abbie and Melzer, Anne and Schertz, Kelsey and Glaeser, Susan and Hammett, Patrick and Joseph, Anne M.},
  year = {2017},
  month = sep,
  journal = {Contemporary Clinical Trials},
  volume = {60},
  pages = {86--95},
  issn = {15517144},
  doi = {10.1016/j.cct.2017.07.002},
  urldate = {2018-10-12},
  abstract = {PCS) subscale of the Short Form Health Survey (SF-12) [26]. Medical co-morbidities will be evaluated using the Charlson Index [27]. Pain will be measured by the pain intensity, interference with enjoyment of life, and interference with general activity (PEG) scale, which includes one pain intensity and two pain interference questions [28]. Chronic obstructive pulmonary disease (COPD) symptoms will be assessed using the five-item Chronic Obstructive Pulmonary Disease Population Screener (COPD-PS) [29].},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fu et al_2017_Program for lung cancer screening and tobacco cessation.pdf}
}

@article{fuTargetTrialEmulation2023,
  title = {Target {{Trial Emulation}} to {{Improve Causal Inference}} from {{Observational Data}}: {{What}}, {{Why}}, and {{How}}?},
  shorttitle = {Target {{Trial Emulation}} to {{Improve Causal Inference}} from {{Observational Data}}},
  author = {Fu, Edouard L.},
  year = {2023},
  journal = {Journal of the American Society of Nephrology},
  pages = {10.1681/ASN.0000000000000152},
  issn = {1046-6673},
  doi = {10.1681/ASN.0000000000000152},
  urldate = {2023-07-25},
  abstract = {Target trial emulation has drastically improved the quality of observational studies investigating the effects of interventions. Its ability to prevent avoidable biases that have plagued many observational analyses has contributed to its recent popularity. This review explains what target trial emulation is, why it should be the standard approach for causal observational studies that investigate interventions, and how to do a target trial emulation analysis. We discuss the merits of target trial emulation compared with often used, but biased analyses, as well as potential caveats, and provide clinicians and researchers with the tools to better interpret results from observational studies investigating the effects of interventions.},
  langid = {american},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Fu_2023_Target Trial Emulation to Improve Causal Inference from Observational Data.pdf;/Users/nseewald/Zotero/storage/SUAY6BVN/target_trial_emulation_to_improve_causal_inference.132.html}
}

@article{gailTestingQualitativeInteractions1985,
  title = {Testing for Qualitative Interactions between Treatment Effects and Patient Subsets.},
  author = {Gail, M and Simon, R},
  year = {1985},
  journal = {Biometrics},
  volume = {41},
  number = {2},
  pages = {361--372},
  issn = {0006-341X (Print) 0006-341X (Linking)},
  doi = {10.2307/2530862},
  abstract = {Evaluation of evidence that treatment efficacy varies substantially among different subsets of patients is an important feature of the analysis of large clinical trials. Qualitative or crossover interactions are said to occur when one treatment is superior for some subsets of patients and the alternative treatment is superior for other subsets. A non-crossover interaction arises when there is variation in the magnitude, but not in the direction, of treatment effects among subsets. Some authors use the term quantitative interaction to mean non-crossover interaction. Non-crossover interactions are usually of less clinical importance than qualitative interactions, which often have major therapeutic significance. A likelihood ratio test is developed to test for qualitative interactions. Exact critical values are determined and tabulated.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gail_Simon_1985_Testing for qualitative interactions between treatment effects and patient.pdf}
}

@article{galbraithGuidelinesDesignClinical2002,
  title = {Guidelines for the Design of Clinical Trials with Longitudinal Outcomes},
  author = {Galbraith, Sally and {M.Stat} and Marschner, Ian C.},
  year = {2002},
  month = jun,
  journal = {Controlled Clinical Trials},
  volume = {23},
  number = {3},
  pages = {257--273},
  issn = {01972456},
  doi = {10/d2cjjr},
  urldate = {2020-06-26},
  abstract = {A common objective of longitudinal clinical trials is to compare rates of change in a continuous response variable between two groups. The power realized for such a study is a function of both the number of people recruited and the planned number of measurements for each participant. By varying these two quantities in opposite directions, power can be kept at the desired level. We consider the problem of how best to choose the sample size and frequency of measurement, with a view to minimizing either the total number of measurements or the cost of a study. Some general guidelines are first developed for the situation in which all participants have complete observations. In practice, however, longitudinal studies often suffer from dropout, where a participant leaves the study permanently so that no further observations are possible. We therefore consider the impact of unanticipated dropout on power and also ways of allowing for dropout at the design stage. Based on our results, we propose some general design guidelines for longitudinal trials comparing rates of change when dropout is present. {\copyright} 2002 Elsevier Science Inc. All rights reserved.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Galbraith et al_2002_Guidelines for the design of clinical trials with longitudinal outcomes.pdf}
}

@article{gaoEstimatingCorrelationMultivariate2017,
  title = {Estimating Correlation between Multivariate Longitudinal Data in the Presence of Heterogeneity},
  author = {Gao, Feng and Philip Miller, J. and Xiong, Chengjie and Luo, Jingqin and Beiser, Julia A. and Chen, Ling and Gordon, Mae O.},
  year = {2017},
  month = aug,
  journal = {BMC Medical Research Methodology},
  volume = {17},
  number = {1},
  pages = {124},
  issn = {1471-2288},
  doi = {10/gbtn3s},
  urldate = {2021-09-20},
  abstract = {Estimating correlation coefficients among outcomes is one of the most important analytical tasks in epidemiological and clinical research. Availability of multivariate longitudinal data presents a unique opportunity to assess joint evolution of outcomes over time. Bivariate linear mixed model (BLMM) provides a versatile tool with regard to assessing correlation. However, BLMMs often assume that all individuals are drawn from a single homogenous population where the individual trajectories are distributed smoothly around population average.},
  keywords = {Bivariate linear mixed model (BLMM),Correlation,Heterogeneity,Multivariate longitudinal data},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gao et al_2017_Estimating correlation between multivariate longitudinal data in the presence.pdf;/Users/nseewald/Zotero/storage/DZUYMJFW/s12874-017-0398-1.html}
}

@article{garciaCoxRegressionExclusion2016,
  title = {Cox Regression with Exclusion Frequency-Based Weights to Identify Neuroimaging Markers Relevant to {{Huntington}}'s Disease Onset},
  author = {Garcia, Tanya P. and M{\"u}ller, Samuel},
  year = {2016},
  month = dec,
  journal = {The Annals of Applied Statistics},
  volume = {10},
  number = {4},
  pages = {2130--2156},
  publisher = {Institute of Mathematical Statistics},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/16-AOAS967},
  urldate = {2024-04-12},
  abstract = {Biomedical studies of neuroimaging and genomics collect large amounts of data on a small subset of subjects so as to not miss informative predictors. An important goal is identifying those predictors that provide better visualization of the data and that could serve as cost-effective measures for future clinical trials. Identifying such predictors is challenging, however, when the predictors are naturally interrelated and the response is a failure time prone to censoring. We propose to handle these challenges with a novel variable selection technique. Our approach casts the problem into several smaller dimensional settings and extracts from this intermediary step the relative importance of each predictor through data-driven weights called exclusion frequencies. The exclusion frequencies are used as weights in a weighted Lasso, and results yield low false discovery rates and a high geometric mean of sensitivity and specificity. We illustrate the method's advantages over existing ones in an extensive simulation study, and use the method to identify relevant neuroimaging markers associated with Huntington's disease onset.},
  keywords = {Exclusion frequency,Model selection,neuroimaging,proportional hazards model,weighted lasso},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Garcia_Müller_2016_Cox_regression_with_exclusion_frequency-based_weights_to_identify_neuroimaging.pdf}
}

@inproceedings{gatysImageStyleTransfer2016,
  title = {Image {{Style Transfer Using Convolutional Neural Networks}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gatys, L. A. and Ecker, A. S. and Bethge, M.},
  year = {2016},
  month = jun,
  pages = {2414--2423},
  doi = {10/gfpg66},
  abstract = {Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gatys et al_2016_Image Style Transfer Using Convolutional Neural Networks.pdf;/Users/nseewald/Zotero/storage/TQ2AK8GR/7780634.html}
}

@book{gelmanBayesianDataAnalysis2004,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Rubin, Donald B.},
  year = {2004},
  series = {Texts in Statistical Science},
  edition = {2nd ed},
  publisher = {Chapman \& Hall/CRC},
  address = {Boca Raton, Fla},
  isbn = {978-1-58488-388-3},
  lccn = {QA279.5 .B386 2004},
  keywords = {Bayesian statistical decision theory},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gelman et al_2004_Bayesian data analysis.pdf}
}

@article{gelmanModelsAssumptionsModel2001,
  title = {Models, {{Assumptions}} and {{Model Checking}} in {{Ecological Regressions}}},
  author = {Gelman, Andrew and Park, David K. and Ansolabehere, Stephen and Price, Phillip N. and Minnite, Lorraine C.},
  year = {2001},
  month = jan,
  journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
  volume = {164},
  number = {1},
  pages = {101--118},
  issn = {0964-1998},
  doi = {10.1111/1467-985X.00190},
  urldate = {2025-01-21},
  abstract = {Ecological regression is based on assumptions that are untestable from aggregate data. However, these assumptions seem more questionable in some applications than in others. There has been some research on implicit models of individual data underlying aggregate ecological regression modelling. We discuss ways in which these implicit models can be checked from aggregate data. We also explore the differences in applications of ecological regressions in two examples: estimating the effect of radon on lung cancer in the United States and estimating voting patterns for different ethnic groups in New York City.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gelman et al. - 2001 - Models, Assumptions and Model Checking in Ecological Regressions.pdf;/Users/nseewald/Zotero/storage/LTHH5W5A/7102286.html}
}

@article{gelmanWhyWeUsually2012,
  title = {Why {{We}} ({{Usually}}) {{Don}}'t {{Have}} to {{Worry About Multiple Comparisons}}},
  author = {Gelman, Andrew and Hill, Jennifer and Yajima, Masanao},
  year = {2012},
  month = apr,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {5},
  number = {2},
  pages = {189--211},
  issn = {1934-5747, 1934-5739},
  doi = {10.1080/19345747.2011.618213},
  urldate = {2018-10-18},
  abstract = {Applied researchers often find themselves making statistical inferences in settings that would seem to require multiple comparisons adjustments. We challenge the Type I error paradigm that underlies these corrections. Moreover we posit that the problem of multiple comparisons can disappear entirely when viewed from a hierarchical Bayesian perspective. We propose building multilevel models in the settings where multiple comparisons arise. Multilevel models perform partial pooling (shifting estimates toward each other), whereas classical procedures typically keep the centers of intervals stationary, adjusting for multiple comparisons by making the intervals wider (or, equivalently, adjusting the p values corresponding to intervals of fixed width). Thus, multilevel models address the multiple comparisons problem and also yield more efficient estimates, especially in settings with low group-level variation, which is where multiple comparisons are a particular concern.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gelman et al_2012_Why We (Usually) Don't Have to Worry About Multiple Comparisons.pdf}
}

@book{gentleMatrixAlgebraTheory2007,
  title = {Matrix Algebra: Theory, Computations, and Applications in Statistics},
  shorttitle = {Matrix Algebra},
  author = {Gentle, James E.},
  year = {2007},
  series = {Springer Texts in Statistics},
  publisher = {Springer},
  address = {New York, N.Y. ; [London]},
  isbn = {978-0-387-70872-0},
  lccn = {QA188 .G56 2007},
  keywords = {Matrices,Problems exercises etc},
  annotation = {OCLC: ocn123374514},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gentle_2007_Matrix algebra.pdf}
}

@article{gertnerNaloxoneAccessLaws2018,
  title = {Do Naloxone Access Laws Increase Outpatient Naloxone Prescriptions? {{Evidence}} from {{Medicaid}}},
  shorttitle = {Do Naloxone Access Laws Increase Outpatient Naloxone Prescriptions?},
  author = {Gertner, Alex K. and Domino, Marisa Elena and Davis, Corey S.},
  year = {2018},
  month = sep,
  journal = {Drug and Alcohol Dependence},
  volume = {190},
  pages = {37--41},
  issn = {0376-8716},
  doi = {10.1016/j.drugalcdep.2018.05.014},
  urldate = {2022-06-16},
  abstract = {Background Naloxone is a prescription medication that can quickly and effectively reverse opioid overdose. Medicaid is a major payer of substance use disorder services, and Medicaid beneficiaries experience especially high rates of opioid overdose. As opioid overdose rates have risen sharply, every state has modified its laws to make naloxone easier to access. The aim of this paper is to determine whether implementation of different provisions of naloxone access laws led to increased naloxone dispensing financed by Medicaid. Methods We reviewed naloxone legislation passed by every state between 2007 and 2016. We used the Medicaid State Drug Utilization dataset to examine the effect of different types of state naloxone access law provisions, separately and as a whole, on the number of outpatient naloxone prescriptions reimbursed by Medicaid from 2007 to 2016. We included state-level covariates in our models that may be correlated with naloxone utilization in Medicaid and passage of naloxone access laws. Results We found that the presence of any naloxone law was significantly associated with increases in outpatient naloxone reimbursed through Medicaid. Laws containing standing order provisions were most consistently associated with increases in naloxone dispensing across models. Standing order provisions led on average to an increase of approximately 33 naloxone prescriptions per state-quarter, which is equivalent to 74\% of the average number of naloxone prescriptions per state-quarter. Conclusions Naloxone access laws, particularly those with standing order provisions, appear to be an effective policy approach to increasing naloxone access among Medicaid beneficiaries.},
  langid = {english},
  keywords = {Fixed effects model,Law,Medicaid,Naloxone},
  file = {/Users/nseewald/Zotero/storage/K5Z8FGSQ/S0376871618303296.html}
}

@article{ghoshAncillaryStatisticsReview2010,
  ids = {ghoshAncillaryStatisticsReview},
  title = {Ancillary {{Statistics}}: {{A Review}}},
  shorttitle = {{{ANCILLARY STATISTICS}}},
  author = {Ghosh, M. and Reid, N. and Fraser, D. A. S.},
  year = {2010},
  journal = {Statistica Sinica},
  volume = {20},
  number = {4},
  eprint = {24309506},
  eprinttype = {jstor},
  pages = {1309--1332},
  publisher = {Institute of Statistical Science, Academia Sinica},
  issn = {1017-0405},
  urldate = {2020-12-11},
  abstract = {In a parametric statistical model, a function of the data is said to be ancillary if its distribution does not depend on the parameters in the model. The concept of ancillary statistics is one of R. A. Fisher's fundamental contributions to statistical inference. Fisher motivated the principle of conditioning on ancillary statistics by an argument based on relevant subsets, and by a closely related argument on recovery of information. Conditioning can also be used to reduce the dimension of the data to that of the parameter of interest, and conditioning on ancillary statistics ensures that no information about the parameter is lost in this reduction. The present review article is an attempt to illustrate various aspects of the use of ancillarity in statistical inference. Both exact and asymptotic theory are considered. Without any claim of completeness, we have made a modest attempt to crystalize many of the basic ideas in the literature.},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ghosh et al_2010_Ancillary Statistics.pdf}
}

@article{ghoshNoninferiorityEquivalenceTests2020,
  ids = {ghoshNonInferiorityEquivalenceTests2017},
  title = {Noninferiority and Equivalence Tests in Sequential, Multiple Assignment, Randomized Trials ({{SMARTs}})},
  author = {Ghosh, Palash and {Nahum-Shani}, Inbal and Spring, Bonnie and Chakraborty, Bibhas},
  year = {2020},
  month = apr,
  journal = {Psychological Methods},
  volume = {25},
  number = {2},
  eprint = {1705.01772},
  pages = {182--205},
  publisher = {American Psychological Association},
  issn = {1082-989X},
  doi = {10/ggtmgv},
  urldate = {2020-05-01},
  abstract = {Adaptive interventions (AIs) are increasingly popular in the behavioral sciences. An AI is a sequence of decision rules that specify for whom and under what conditions different intervention options should be offered, in order to address the changing needs of individuals as they progress over time. The sequential, multiple assignment, randomized trial (SMART) is a novel trial design that was developed to aid in empirically constructing effective AIs. The sequential randomizations in a SMART often yield multiple AIs that are embedded in the trial by design. Many SMARTs are motivated by scientific questions pertaining to the comparison of such embedded AIs. Existing data analytic methods and sample size planning resources for SMARTs are suitable only for superiority testing, namely for testing whether one embedded AI yields better primary outcomes on average than another. This calls for noninferiority/equivalence testing methods, because AIs are often motivated by the need to deliver support/care in a less costly or less burdensome manner, while still yielding benefits that are equivalent or noninferior to those produced by a more costly/burdensome standard of care. Here, we develop data-analytic methods and sample-size formulas for SMARTs testing the noninferiority or equivalence of one AI over another. Sample size and power considerations are discussed with supporting simulations, and online resources for sample size planning are provided. A simulated data analysis shows how to test noninferiority and equivalence hypotheses with SMART data. For illustration, we use an example from a SMART in the area of health psychology aiming to develop an AI for promoting weight loss among overweight/obese adults. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ghosh et al_2020_Noninferiority and equivalence tests in sequential, multiple assignment,.pdf;/Users/nseewald/Zotero/storage/YZ7F7TTY/1705.html}
}

@article{gibbonsAdvancesAnalysisLongitudinal2010,
  title = {Advances in {{Analysis}} of {{Longitudinal Data}}},
  author = {Gibbons, Robert D. and Hedeker, Donald and DuToit, Stephen},
  year = {2010},
  month = mar,
  journal = {Annual Review of Clinical Psychology},
  volume = {6},
  number = {1},
  pages = {79--107},
  issn = {1548-5943, 1548-5951},
  doi = {10.1146/annurev.clinpsy.032408.153550},
  urldate = {2020-12-11},
  abstract = {In this review, we explore recent developments in the area of linear and nonlinear generalized mixed-effects regression models and various alternatives, including generalized estimating equations for analysis of longitudinal data. Methods are described for continuous and normally distributed as well as categorical (binary, ordinal, nominal) and count (Poisson) variables. Extensions of the model to three and four levels of clustering, multivariate outcomes, and incorporation of design weights are also described. Linear and nonlinear models are illustrated using an example involving a study of the relationship between mood and smoking.},
  langid = {english},
  keywords = {Generalized estimating equations,LOGISTIC REGRESSION,marginal,maximum likelihood,mixed-effects models,multilevel,poisson regression},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gibbons et al_2010_Advances in Analysis of Longitudinal Data.pdf}
}

@article{gigantiMultipleImputationVarianceEstimation2020,
  title = {Multiple-{{Imputation Variance Estimation}} in {{Studies With Missing}} or {{Misclassified Inclusion Criteria}}},
  author = {Giganti, Mark J and Shepherd, Bryan E},
  year = {2020},
  month = dec,
  journal = {American Journal of Epidemiology},
  volume = {189},
  number = {12},
  pages = {1628--1632},
  issn = {0002-9262},
  doi = {10.1093/aje/kwaa153},
  urldate = {2024-02-14},
  abstract = {In observational studies using routinely collected data, a variable with a high level of missingness or misclassification may determine whether an observation is included in the analysis. In settings where inclusion criteria are assessed after imputation, the popular multiple-imputation variance estimator proposed by Rubin (``Rubin's rules'' (RR)) is biased due to incompatibility between imputation and analysis models. While alternative approaches exist, most analysts are not familiar with them. Using partially validated data from a human immunodeficiency virus cohort, we illustrate the calculation of an imputation variance estimator proposed by Robins and Wang (RW) in a scenario where the study exclusion criteria are based on a variable that must be imputed. In this motivating example, the corresponding imputation variance estimate for the log odds was 29\% smaller using the RW estimator than using the RR estimator. We further compared these 2 variance estimators with a simulation study which showed that coverage probabilities of 95\% confidence intervals based on the RR estimator were too high and became worse as more observations were imputed and more subjects were excluded from the analysis. The RW imputation variance estimator performed much better and should be employed when there is incompatibility between imputation and analysis models. We provide analysis code to aid future analysts in implementing this method.},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Giganti_Shepherd_2020_Multiple-Imputation_Variance_Estimation_in_Studies_With_Missing_or.pdf;/Users/nseewald/Zotero/storage/RWFA8865/5873641.html}
}

@article{girlingStatisticalEfficiencyOptimal2016,
  title = {Statistical Efficiency and Optimal Design for Stepped Cluster Studies under Linear Mixed Effects Models},
  author = {Girling, Alan J. and Hemming, Karla},
  year = {2016},
  journal = {Statistics in Medicine},
  volume = {35},
  number = {13},
  pages = {2149--2166},
  issn = {1097-0258},
  doi = {10.1002/sim.6850},
  urldate = {2024-10-17},
  abstract = {In stepped cluster designs the intervention is introduced into some (or all) clusters at different times and persists until the end of the study. Instances include traditional parallel cluster designs and the more recent stepped-wedge designs. We consider the precision offered by such designs under mixed-effects models with fixed time and random subject and cluster effects (including interactions with time), and explore the optimal choice of uptake times. The results apply both to cross-sectional studies where new subjects are observed at each time-point, and longitudinal studies with repeat observations on the same subjects. The efficiency of the design is expressed in terms of a `cluster-mean correlation' which carries information about the dependency-structure of the data, and two design coefficients which reflect the pattern of uptake-times. In cross-sectional studies the cluster-mean correlation combines information about the cluster-size and the intra-cluster correlation coefficient. A formula is given for the `design effect' in both cross-sectional and longitudinal studies. An algorithm for optimising the choice of uptake times is described and specific results obtained for the best balanced stepped designs. In large studies we show that the best design is a hybrid mixture of parallel and stepped-wedge components, with the proportion of stepped wedge clusters equal to the cluster-mean correlation. The impact of prior uncertainty in the cluster-mean correlation is considered by simulation. Some specific hybrid designs are proposed for consideration when the cluster-mean correlation cannot be reliably estimated, using a minimax principle to ensure acceptable performance across the whole range of unknown values. {\copyright} 2016 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  copyright = {{\copyright} 2016 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {cluster studies,intra-cluster correlation,optimal design,stepped-wedge designs},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Girling and Hemming - 2016 - Statistical efficiency and optimal design for stepped cluster studies under linear mixed effects mod.pdf;/Users/nseewald/Zotero/storage/LLWM3WSF/sim.html}
}

@article{glenComputingDistributionProduct2004,
  title = {Computing the Distribution of the Product of Two Continuous Random Variables},
  author = {Glen, Andrew G. and Leemis, Lawrence M. and Drew, John H.},
  year = {2004},
  month = jan,
  journal = {Computational Statistics \& Data Analysis},
  volume = {44},
  number = {3},
  pages = {451--464},
  issn = {0167-9473},
  doi = {10/djs6wb},
  urldate = {2020-05-26},
  abstract = {We present an algorithm for computing the probability density function of the product of two independent random variables, along with an implementation of the algorithm in a computer algebra system. We combine this algorithm with the earlier work on transformations of random variables to create an automated algorithm for convolutions of random variables. Some examples demonstrate the algorithm's application.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Glen et al_2004_Computing the distribution of the product of two continuous random variables.pdf;/Users/nseewald/Zotero/storage/5ZFRTNHM/S0167947302002347.html}
}

@article{goetghebeurFormulatingCausalQuestions2020,
  title = {Formulating Causal Questions and Principled Statistical Answers},
  author = {Goetghebeur, Els and {le Cessie}, Saskia and De Stavola, Bianca and Moodie, Erica EM and Waernbaum, Ingeborg and {initiative}, "on behalf of" the topic group Causal Inference (TG7) of the STRATOS},
  year = {2020},
  journal = {Statistics in Medicine},
  volume = {39},
  pages = {4922--4948},
  doi = {10.1002/SIM.8741},
  abstract = {{$<$}p{$>$}Although review papers on causal inference methods are now available, there is a lack of introductory overviews on \emph{what} they can render and on the guiding criteria for choosing one particular method. This tutorial gives an overview in situations where an exposure of interest is set at a chosen baseline ("point exposure") and the target outcome arises at a later time point. We first phrase relevant causal questions and make a case for being specific about the possible exposure levels involved and the populations for which the question is relevant. Using the potential outcomes framework, we describe principled definitions of causal effects and of estimation approaches classified according to whether they invoke the no unmeasured confounding assumption (including outcome regression and propensity score-based methods) or an instrumental variable with added assumptions. We mainly focus on continuous outcomes and causal average treatment effects. We discuss interpretation, challenges, and potential pitfalls and illustrate application using a "simulation learner," that mimics the effect of various breastfeeding interventions on a child's later development. This involves a typical simulation component with generated exposure, covariate, and outcome data inspired by a randomized intervention study. The simulation learner further generates various (linked) exposure types with a set of possible values per observation unit, from which observed as well as potential outcome data are generated. It thus provides true values of several causal effects. R code for data generation and analysis is available on  {$<$}a class="linkBehavior" href="www.ofcaus.org"{$>$}www.ofcaus.org{$<$}/a{$>$}, where SAS and Stata code for analysis is also provided.{$<$}/p{$>$}},
  keywords = {nosource,Researcher App},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Goetghebeur et al_2020_Formulating causal questions and principled statistical answers.pdf}
}

@book{golubMatrixComputations2013,
  title = {Matrix Computations},
  author = {Golub, Gene H. and Van Loan, Charles F.},
  year = {2013},
  series = {Johns {{Hopkins}} Studies in the Mathematical Sciences},
  edition = {Fourth edition},
  publisher = {The Johns Hopkins University Press},
  address = {Baltimore},
  isbn = {978-1-4214-0794-4},
  lccn = {QA188 .G65 2013},
  keywords = {Data processing,Matrices},
  annotation = {OCLC: ocn824733531},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Golub_Van Loan_2013_Matrix computations.pdf}
}

@article{goodman-baconDifferenceindifferencesVariationTreatment2021,
  title = {Difference-in-Differences with Variation in Treatment Timing},
  author = {{Goodman-Bacon}, Andrew},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  volume = {225},
  number = {2},
  pages = {254--277},
  issn = {03044076},
  doi = {10/gk55kj},
  urldate = {2021-07-02},
  abstract = {The canonical difference-in-differences (DD) estimator contains two time periods, "pre" and "post", and two groups, "treatment" and "control". Most DD applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period DD estimators in the data. A causal interpretation of twoway fixed effects DD estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.},
  langid = {english},
  keywords = {Difference-in-differences,Treatment effect heterogeneity,Two-way fixed effects,Variation in treatment timing},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Goodman-Bacon_2021_Difference-in-differences with variation in treatment timing.pdf;/Users/nseewald/Zotero/storage/33GZRIGI/S0304407621001445.html}
}

@article{goodmanPracticalImprovementsContinual1995,
  title = {Some Practical Improvements in the Continual Reassessment Method for Phase {{I}} Studies},
  author = {Goodman, Steven N. and Zahurak, Marianna L. and Piantadosi, Steven},
  year = {1995},
  journal = {Statistics in Medicine},
  volume = {14},
  number = {11},
  pages = {1149--1161},
  issn = {1097-0258},
  doi = {10.1002/sim.4780141102},
  urldate = {2024-02-12},
  abstract = {The Continual Reassessment Method (CRM) is a Bayesian phase I design whose purpose is to estimate the maximum tolerated dose of a drug that will be used in subsequent phase II and III studies. Its acceptance has been hindered by the greater duration of CRM designs compared to standard methods, as well as by concerns with excessive experimentation at high dosage levels, and with more frequent and severe toxicity. This paper presents the results of a simulation study in which one assigns more than one subject at a time to each dose level, and each dose increase is limited to one level. We show that these modifications address all of the most serious criticisms of the CRM, reducing the duration of the trial by 50--67 per cent, reducing toxicity incidence by 20--35 per cent, and lowering toxicity severity. These are achieved with minimal effects on accuracy. Most important, based on our experience at our institution, such modifications make the CRM acceptable to clinical investigators.},
  copyright = {Copyright {\copyright} 1995 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Goodman_et_al_1995_Some_practical_improvements_in_the_continual_reassessment_method_for_phase_I.pdf;/Users/nseewald/Zotero/storage/I3PX7CT7/sim.html}
}

@misc{goodrichTruncatedMultivariateNormal2017,
  title = {Truncated {{Multivariate Normal Variates}} in {{Stan}}},
  author = {Goodrich, Ben},
  year = {2017},
  month = may,
  urldate = {2022-06-28},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Goodrich_2017_Truncated Multivariate Normal Variates in Stan.pdf}
}

@article{gosaviReinforcementLearningTutorial2009,
  title = {Reinforcement Learning: {{A}} Tutorial Survey and Recent Advances},
  author = {Gosavi, Abhijit},
  year = {2009},
  journal = {INFORMS Journal on Computing},
  volume = {21},
  number = {2},
  pages = {178--192},
  issn = {10919856},
  doi = {10.1287/ijoc.1080.0305},
  abstract = {In the last few years, reinforcement learning (RL), also called adaptive\${\textbackslash}backslash\$n(or approximate) dynamic programming, has emerged as a powerful tool\${\textbackslash}backslash\$nfor solving complex sequential decision-making problems in control\${\textbackslash}backslash\$ntheory. Although seminal research in this area was performed in the\${\textbackslash}backslash\$nartificial intelligence (AI) community, more recently it has attracted\${\textbackslash}backslash\$nthe attention of optimization theorists because of several noteworthy\${\textbackslash}backslash\$nsuccess stories from operations management. It is on large-scale\${\textbackslash}backslash\$nand complex problems of dynamic optimization, in particular the Markov\${\textbackslash}backslash\$ndecision problem (MDP) and its variants, that the power of RL becomes\${\textbackslash}backslash\$nmore obvious. It has been known for many years that on large-scale\${\textbackslash}backslash\$nMDPs, the curse of dimensionality and the curse of modeling render\${\textbackslash}backslash\$nclassical dynamic programming (DP) ineffective. The excitement in\${\textbackslash}backslash\$nRL stems from its direct attack on these curses, which allows it\${\textbackslash}backslash\$nto solve problems that were considered intractable via classical\${\textbackslash}backslash\$nDP in the past. The success of RL is due to its strong mathematical\${\textbackslash}backslash\$nroots in the principles of DP, Monte Carlo simulation, function approximation,\${\textbackslash}backslash\$nand AI. Topics treated in some detail in this survey are temporal\${\textbackslash}backslash\$ndifferences, Q-learning, semi-MDPs, and stochastic games. Several\${\textbackslash}backslash\$nrecent advances in RL, e.g., policy gradients and hierarchical RL,\${\textbackslash}backslash\$nare covered along with references. Pointers to numerous examples\${\textbackslash}backslash\$nof applications are provided. This overview is aimed at uncovering\${\textbackslash}backslash\$nthe mathematical roots of this science so that readers gain a clear\${\textbackslash}backslash\$nunderstanding of the core concepts and are able to use them in their\${\textbackslash}backslash\$nown research. The survey points to more than 100 references from\${\textbackslash}backslash\$nthe literature.},
  pmid = {17255001},
  keywords = {Artificial intelligence,Dynamic programming,Reinforcement learning,Simulation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gosavi_2009_Reinforcement learning.pdf}
}

@article{gouldTeenagersAttitudesSeeking2006,
  title = {Teenagers' {{Attitudes}} about {{Seeking Help}} from {{Telephone Crisis Services}} ({{Hotlines}})},
  author = {Gould, Madelyn S. and Greenberg, Ted and Munfakh, Jimmie Lou Harris and Kleinman, Marjorie and Lubell, Keri},
  year = {2006},
  month = dec,
  journal = {Suicide and Life-Threatening Behavior},
  volume = {36},
  number = {6},
  pages = {601--613},
  issn = {0363-0234},
  doi = {10.1521/suli.2006.36.6.601},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gould et al_2006_Teenagers' Attitudes about Seeking Help from Telephone Crisis Services.pdf}
}

@article{granthamTimeParameterizationsCluster2020,
  ids = {granthamTimeParameterizationsCluster},
  title = {Time {{Parameterizations}} in {{Cluster Randomized Trial Planning}}},
  author = {Grantham, Kelsey L. and Forbes, Andrew B. and Heritier, Stephane and Kasza, Jessica},
  year = {2020},
  month = apr,
  journal = {The American Statistician},
  volume = {74},
  number = {2},
  pages = {184--189},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1623072},
  urldate = {2020-12-11},
  abstract = {Models for cluster randomized trials conducted over multiple time periods should account for underlying temporal trends. However, in practice there is often limited knowledge or data available to inform the choice of time parameterization of these trends, or to anticipate the implications of this choice on trial planning. In this article, we establish a sufficient condition for when the choice of time parameterization does not affect the form of the variance of the treatment effect estimator, thereby simplifying the planning of these trials.},
  keywords = {nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Grantham et al_2020_Time Parameterizations in Cluster Randomized Trial Planning.pdf;/Users/nseewald/Zotero/storage/9PD2PY29/00031305.2019.html}
}

@article{graylingWebApplicationDesign2020,
  title = {A Web Application for the Design of Multi-Arm Clinical Trials},
  author = {Grayling, Michael J. and Wason, James MS.},
  year = {2020},
  month = dec,
  journal = {BMC Cancer},
  volume = {20},
  number = {1},
  pages = {80},
  issn = {1471-2407},
  doi = {10.1186/s12885-020-6525-0},
  urldate = {2020-06-05},
  abstract = {Background: Multi-arm designs provide an effective means of evaluating several treatments within the same clinical trial. Given the large number of treatments now available for testing in many disease areas, it has been argued that their utilisation should increase. However, for any given clinical trial there are numerous possible multi-arm designs that could be used, and choosing between them can be a difficult task. This task is complicated further by a lack of available easy-to-use software for designing multi-arm trials. Results: To aid the wider implementation of multi-arm clinical trial designs, we have developed a web application for sample size calculation when using a variety of popular multiple comparison corrections. Furthermore, the application supports sample size calculation to control several varieties of power, as well as the determination of optimised arm-wise allocation ratios. It is built using the Shiny package in the R programming language, is free to access on any device with an internet browser, and requires no programming knowledge to use. It incorporates a variety of features to make it easier to use, including help boxes and warning messages. Using design parameters motivated by a recently completed phase II oncology trial, we demonstrate that the application can effectively determine and evaluate complex multi-arm trial designs. Conclusions: The application provides the core information required by statisticians and clinicians to review the operating characteristics of a chosen multi-arm clinical trial design. The range of designs supported by the application is broader than other currently available software solutions. Its primary limitation, particularly from a regulatory agency point of view, is its lack of validation. However, we present an approach to efficiently confirming its results via simulation.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Grayling_Wason_2020_A web application for the design of multi-arm clinical trials.pdf}
}

@article{greenlandInterpretationChoiceEffect1987,
  title = {Interpretation and Choice of Effect Measures in Epidemiologic Analyses},
  author = {Greenland, Sander},
  year = {1987},
  month = may,
  journal = {American Journal of Epidemiology},
  volume = {125},
  number = {5},
  pages = {761--768},
  issn = {1476-6256, 0002-9262},
  doi = {10.1093/oxfordjournals.aje.a114593},
  urldate = {2025-01-29},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Greenland - 1987 - Interpretation and choice of effect measures in epidemiologic analyses.pdf}
}

@misc{greiferChoosingCausalEstimand2023,
  title = {Choosing the {{Causal Estimand}} for {{Propensity Score Analysis}} of {{Observational Studies}}},
  author = {Greifer, Noah and Stuart, Elizabeth A.},
  year = {2023},
  month = jul,
  number = {arXiv:2106.10577},
  eprint = {2106.10577},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.10577},
  urldate = {2023-08-17},
  abstract = {Matching and weighting methods for observational studies involve the choice of an estimand, the causal effect with reference to a specific target population. Commonly used estimands include the average treatment effect in the treated (ATT), the average treatment effect in the untreated (ATU), the average treatment effect in the population (ATE), and the average treatment effect in the overlap (i.e., equipoise population; ATO). Each estimand has its own assumptions, interpretation, and statistical methods that can be used to estimate it. This article provides guidance on selecting and interpreting an estimand to help medical researchers correctly implement statistical methods used to estimate causal effects in observational studies and to help audiences correctly interpret the results and limitations of these studies. The interpretations of the estimands resulting from regression and instrumental variable analyses are also discussed. Choosing an estimand carefully is essential for making valid inferences from the analysis of observational data and ensuring results are replicable and useful for practitioners.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Greifer_Stuart_2023_Choosing the Causal Estimand for Propensity Score Analysis of Observational.pdf;/Users/nseewald/Zotero/storage/5LJV4T6X/2106.html}
}

@article{griffinEstimatingCausalEffects2014,
  title = {Estimating the Causal Effects of Cumulative Treatment Episodes for Adolescents Using Marginal Structural Models and Inverse Probability of Treatment Weighting},
  author = {Griffin, Beth Ann and Ramchand, Rajeev and Almirall, Daniel and Slaughter, Mary E. and Burgette, Lane F. and McCaffery, Daniel F.},
  year = {2014},
  month = mar,
  journal = {Drug and Alcohol Dependence},
  volume = {136},
  pages = {69--78},
  issn = {0376-8716},
  doi = {10.1016/j.drugalcdep.2013.12.017},
  urldate = {2022-02-16},
  abstract = {Background Substance use treatment is rarely a one-time event for individuals with substance use disorders. Sustained reductions in substance use and its related symptoms may result from multiple treatment episodes. Methods We use a marginal structural model with inverse-probability-of-treatment weighting to estimate the causal effects of cumulative treatment experiences over a period of 9 months on drug use at the end of 1-year among 2870 adolescents receiving care in community-based treatment settings. During the 9 months, adolescents move in and out of outpatient and residential treatment with periods where they only receive biological drug screening (BDS) or no treatment at all. The use of inverse-probability-of-treatment weighting reduces confounding bias due to observed baseline and time-varying measures over the course of follow-up; weights were estimated using generalized boosted models. Results Each additional period of treatment (representing at least one day, 1 session, or 1 BDS during the 90 day period between follow-up visits) yielded reductions in average substance use frequency at 1-year relative to no treatment during the 90-day period. For residential treatment it was a 16\% decrease (95\% CI=-27\%, -7\%), for outpatient treatment it was a 9\% decrease (95\% CI=-18\%, -0\%), and for BDS (with no additional outpatient or residential treatment) it was an 11\% decrease (95\% CI=-20\%, -3\%). Conclusions Using robust statistical methods, we find promising (albeit preliminary) evidence that additional periods of outpatient and residential treatment, as well as biological drug screening, lead to reductions in substance use outcomes at one year.},
  langid = {english},
  keywords = {Adolescent treatment,Cumulative treatment effects,Generalized boosted model,Inverse-probability-of-treatment weights,Marginal structural model,notion,Time-varying confounding},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Griffin et al_2014_Estimating the causal effects of cumulative treatment episodes for adolescents.pdf;/Users/nseewald/Zotero/storage/KGQT2B9E/S0376871613005334.html}
}

@article{griffinMethodologicalConsiderationsEstimating2023,
  title = {Methodological Considerations for Estimating Policy Effects in the Context of Co-Occurring Policies},
  author = {Griffin, Beth Ann and Schuler, Megan S. and Pane, Joseph and Patrick, Stephen W. and Smart, Rosanna and Stein, Bradley D. and Grimm, Geoffrey and Stuart, Elizabeth A.},
  year = {2023},
  month = jun,
  journal = {Health Services and Outcomes Research Methodology},
  volume = {23},
  number = {2},
  pages = {149--165},
  issn = {1572-9400},
  doi = {10.1007/s10742-022-00284-w},
  urldate = {2023-11-20},
  abstract = {Understanding how best to estimate state-level policy effects is important, and several unanswered questions remain, particularly about the ability of statistical models to disentangle the effects of concurrently enacted policies. In practice, many policy evaluation studies do not attempt to control for effects of co-occurring policies, and this issue has not received extensive attention in the methodological literature to date. In this study, we utilized Monte Carlo simulations to assess the impact of co-occurring policies on the performance of commonly-used statistical models in state policy evaluations. Simulation conditions varied effect sizes of the co-occurring policies and length of time between policy enactment dates, among other factors. Outcome data (annual state-specific opioid mortality rate per 100,000) were obtained from 1999 to 2016 National Vital Statistics System (NVSS) Multiple Cause of Death mortality files, thus yielding longitudinal annual state-level data over 18~years from 50 states. When co-occurring policies are ignored (i.e., omitted from the analytic model), our results demonstrated that high relative bias ({$>$}\,82\%) arises, particularly when policies are enacted in rapid succession. Moreover, as expected, controlling for all co-occurring policies will effectively mitigate the threat of confounding bias; however, effect estimates may be relatively imprecise (i.e., larger variance) when policies are enacted in near succession. Our findings highlight several key methodological issues regarding co-occurring policies in the context of opioid-policy research yet also generalize more broadly to evaluation of other state-level policies, such as policies related to firearms or COVID-19, showcasing the need to think critically about co-occurring policies that are likely to influence the outcome when specifying analytic models.},
  langid = {english},
  keywords = {Clustered policies,Concurrent policies,Difference-in-differences,Opioid,Policy evaluations,Simulation,State-level policy},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Griffin et al_2023_Methodological considerations for estimating policy effects in the context of.pdf}
}

@article{griffinMethodologicalConsiderationsEstimating2023a,
  title = {Methodological Considerations for Estimating Policy Effects in the Context of Co-Occurring Policies},
  author = {Griffin, Beth Ann and Schuler, Megan S. and Pane, Joseph and Patrick, Stephen W. and Smart, Rosanna and Stein, Bradley D. and Grimm, Geoffrey and Stuart, Elizabeth A.},
  year = {2023},
  month = jun,
  journal = {Health Services and Outcomes Research Methodology},
  volume = {23},
  number = {2},
  pages = {149--165},
  issn = {1572-9400},
  doi = {10.1007/s10742-022-00284-w},
  urldate = {2025-03-22},
  abstract = {Understanding how best to estimate state-level policy effects is important, and several unanswered questions remain, particularly about the ability of statistical models to disentangle the effects of concurrently enacted policies. In practice, many policy evaluation studies do not attempt to control for effects of co-occurring policies, and this issue has not received extensive attention in the methodological literature to date. In this study, we utilized Monte Carlo simulations to assess the impact of co-occurring policies on the performance of commonly-used statistical models in state policy evaluations. Simulation conditions varied effect sizes of the co-occurring policies and length of time between policy enactment dates, among other factors. Outcome data (annual state-specific opioid mortality rate per 100,000) were obtained from 1999 to 2016 National Vital Statistics System (NVSS) Multiple Cause of Death mortality files, thus yielding longitudinal annual state-level data over 18~years from 50 states. When co-occurring policies are ignored (i.e., omitted from the analytic model), our results demonstrated that high relative bias ({$>$}\,82\%) arises, particularly when policies are enacted in rapid succession. Moreover, as expected, controlling for all co-occurring policies will effectively mitigate the threat of confounding bias; however, effect estimates may be relatively imprecise (i.e., larger variance) when policies are enacted in near succession. Our findings highlight several key methodological issues regarding co-occurring policies in the context of opioid-policy research yet also generalize more broadly to evaluation of other state-level policies, such as policies related to firearms or COVID-19, showcasing the need to think critically about co-occurring policies that are likely to influence the outcome when specifying analytic models.},
  langid = {english},
  keywords = {Clustered policies,Concurrent policies,Difference-in-differences,Opioid,Policy evaluations,Simulation,State-level policy},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Griffin et al. - 2023 - Methodological considerations for estimating policy effects in the context of co-occurring policies.pdf}
}

@article{griffinMovingClassicDifferenceindifferences2021,
  title = {Moving beyond the Classic Difference-in-Differences Model: A Simulation Study Comparing Statistical Methods for Estimating Effectiveness of State-Level Policies},
  shorttitle = {Moving beyond the Classic Difference-in-Differences Model},
  author = {Griffin, Beth Ann and Schuler, Megan S. and Stuart, Elizabeth A. and Patrick, Stephen and McNeer, Elizabeth and Smart, Rosanna and Powell, David and Stein, Bradley D. and Schell, Terry L. and Pacula, Rosalie Liccardo},
  year = {2021},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {21},
  number = {1},
  pages = {1--19},
  publisher = {BioMed Central},
  issn = {1471-2288},
  doi = {10.1186/s12874-021-01471-y},
  urldate = {2022-02-07},
  abstract = {Reliable evaluations of state-level policies are essential for identifying effective policies and informing policymakers' decisions. State-level policy evaluations commonly use a difference-in-differences (DID) study design; yet within this framework, statistical model specification varies notably across studies. More guidance is needed about which set of statistical models perform best when estimating how state-level policies affect outcomes. Motivated by applied state-level opioid policy evaluations, we implemented an extensive simulation study to compare the statistical performance of multiple variations of the two-way fixed effect models traditionally used for DID under a range of simulation conditions. We also explored the performance of autoregressive (AR) and GEE models. We simulated policy effects on annual state-level opioid mortality rates and assessed statistical performance using various metrics, including directional bias, magnitude bias, and root mean squared error. We also reported Type I error rates and the rate of correctly rejecting the null hypothesis (e.g., power), given the prevalence of frequentist null hypothesis significance testing in the applied literature. Most linear models resulted in minimal bias. However, non-linear models and population-weighted versions of classic linear two-way fixed effect and linear GEE models yielded considerable bias (60 to 160\%). Further, root mean square error was minimized by linear AR models when we examined crude mortality rates and by negative binomial models when we examined raw death counts. In the context of frequentist hypothesis testing, many models yielded high Type I error rates and very low rates of correctly rejecting the null hypothesis ({$<$} 10\%), raising concerns of spurious conclusions about policy effectiveness in the opioid literature. When considering performance across models, the linear AR models were optimal in terms of directional bias, root mean squared error, Type I error, and correct rejection rates. The findings highlight notable limitations of commonly used statistical models for DID designs, which are widely used in opioid policy studies and in state policy evaluations more broadly. In contrast, the optimal model we identified--the AR model--is rarely used in state policy evaluation. We urge applied researchers to move beyond the classic DID paradigm and adopt use of AR models.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Difference-in-differences,notion,Opioid,Overdose,Policy evaluations,Simulation,State-level policy},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Griffin et al_2021_Moving beyond the classic difference-in-differences model.pdf;/Users/nseewald/Zotero/storage/73PFYNT7/s12874-021-01471-y.html}
}

@article{grindstaffNoOneWants2019,
  title = {``{{No One Wants}} to {{Believe It}}'': {{Manifestations}} of {{White Privilege}} in a {{STEM-Focused College}}},
  shorttitle = {``{{No One Wants}} to {{Believe It}}''},
  author = {Grindstaff, Kelly and Mascarenhas, Michael},
  year = {2019},
  month = apr,
  journal = {Multicultural Perspectives},
  volume = {21},
  number = {2},
  pages = {102--111},
  publisher = {Routledge},
  issn = {1521-0960},
  doi = {10/ghgb6x},
  urldate = {2020-10-22},
  abstract = {The lagging behind of underrepresented minority (URM) students in higher education, and particularly in the STEM fields, is well documented. In this paper we draw on critical race theory in education to frame and present counter-narratives of URM students in STEM fields, to explicate the function of the interactions that occur between these students and their (mostly White) instructors and peers. Focus group interviews with URM students (and staff) at a STEM focused college identify three ways in which White privilege is enacted through these interactions: in group projects; in cheating accusations; and in the grading process. Our participants illuminate particular manifestations of White privilege in STEM classrooms and on campus, and we place these within the context of ``colorblind'' changes in higher education in the U.S.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Grindstaff_Mascarenhas_2019_“No One Wants to Believe It”.pdf;/Users/nseewald/Zotero/storage/NYPNFT64/15210960.2019.html}
}

@article{guallarSeeingPositiveNegative2023,
  title = {Seeing the {{Positive}} in {{Negative Studies}}},
  author = {Guallar, Eliseo and Goodman, Steven N. and Localio, A. Russell and {Stephens-Shields}, Alisa J. and Laine, Christine},
  year = {2023},
  month = apr,
  journal = {Annals of Internal Medicine},
  volume = {176},
  number = {4},
  pages = {561--562},
  publisher = {American College of Physicians},
  issn = {0003-4819},
  doi = {10.7326/M23-0576},
  urldate = {2023-07-28},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Guallar et al_2023_Seeing the Positive in Negative Studies.pdf}
}

@article{gunasekaraFixedEffectsAnalysis2014,
  title = {Fixed Effects Analysis of Repeated Measures Data},
  author = {Gunasekara, Fiona Imlach and Richardson, Ken and Carter, Kristie and Blakely, Tony},
  year = {2014},
  month = feb,
  journal = {International Journal of Epidemiology},
  volume = {43},
  number = {1},
  pages = {264--269},
  issn = {0300-5771},
  doi = {10.1093/ije/dyt221},
  urldate = {2024-07-12},
  abstract = {The analysis of repeated measures or panel data allows control of some of the biases which plague other observational studies, particularly unmeasured confounding. When this bias is suspected, and the research question is: `Does a change in an exposure cause a change in the outcome?', a fixed effects approach can reduce the impact of confounding by time-invariant factors, such as the unmeasured characteristics of individuals. Epidemiologists familiar with using mixed models may initially presume that specifying a random effect (intercept) for every individual in the study is an appropriate method. However, this method uses information from both the within-individual/unit exposure-outcome association and the between-individual/unit exposure-outcome association. Variation between individuals may introduce confounding bias into mixed model estimates, if unmeasured time-invariant factors are associated with both the exposure and the outcome. Fixed effects estimators rely only on variation within individuals and hence are not affected by confounding from unmeasured time-invariant factors. The reduction in bias using a fixed effects model may come at the expense of precision, particularly if there is little change in exposures over time. Neither fixed effects nor mixed models control for unmeasured time-varying confounding or reverse causation.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gunasekara_et_al_2014_Fixed_effects_analysis_of_repeated_measures_data.pdf;/Users/nseewald/Zotero/storage/IHS9RRUA/732283.html}
}

@article{gunlicks-stoesselPilotSMARTDeveloping2016,
  title = {A {{Pilot SMART}} for {{Developing}} an {{Adaptive Treatment Strategy}} for {{Adolescent Depression}}},
  author = {{Gunlicks-Stoessel}, Meredith and Mufson, Laura and Westervelt, Ana and Almirall, Daniel and Murphy, Susan A.},
  year = {2016},
  month = jul,
  journal = {Journal of Clinical Child \& Adolescent Psychology},
  volume = {45},
  number = {4},
  pages = {480--494},
  issn = {1537-4416, 1537-4424},
  doi = {10/ghpbrv},
  urldate = {2020-12-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gunlicks-Stoessel et al_2016_A Pilot SMART for Developing an Adaptive Treatment Strategy for Adolescent.pdf}
}

@article{guptaGeneralizedSkewNormal2004,
  title = {Generalized Skew Normal Model},
  author = {Gupta, Ramesh C. and Gupta, Rameshwar D.},
  year = {2004},
  month = dec,
  journal = {Test},
  volume = {13},
  number = {2},
  pages = {501--524},
  issn = {1133-0686, 1863-8260},
  doi = {10.1007/BF02595784},
  urldate = {2019-01-07},
  abstract = {The skew normal distribution proposed by Azzalini (1985) can be a suitable model for the analysis of data exhibiting a unimodal density function having some skewness present, a structure often occurring in data analysis. In this paper, we study a generalization ofthe basic Azzalini model proposed by Balakrishnan, as a discussant of Arnold and Beaver (2002). The basic structural properties of the model inch ding the reliability properties are presented. Estimation and testing of hypothesis oI the skew parameter are discussed. Some comparisons oI the models in terms of mean, variance and skewness are provided. Two data sets are analyzed.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Gupta_Gupta_2004_Generalized skew normal model.pdf}
}

@article{hadouxSequentialDecisionMakingNonstationary,
  title = {Sequential {{Decision-Making}} under {{Non-stationary Environments}} via {{Sequential Change-point Detection}}},
  author = {Hadoux, Emmanuel and Beynier, Aur{\'e}lie and Weng, Paul},
  pages = {11},
  abstract = {Reinforcement Learning (RL) has been mainly interested in computing an optimal policy for an agent acting in a stationary environment. However, in many real world decision problems the assumption on the stationarity does not hold. One can view a non-stationary environment as a set of contexts (also called modes or modules) where a context corresponds to a possible stationary dynamics of the environment. Even most approaches assume that the number of modes is known, a RL method - Reinforcement Learning with Context Detection (RLCD)- has been recently proposed to learn an a pirori unknown set of contexts and detect context changes. In this paper, we propose a new approach by adapting the tools developed in statistics and more precisely in sequential analysis for detecting an environmental change. Our approach is thus more theoretically founded and necessitates less parameters than RLCD. We also show that our parameters are easier to interpret and therefore easier to tune. Finally, we show experimentally that our approach outperforms the current methods on several application problems.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hadoux et al_Sequential Decision-Making under Non-stationary Environments via Sequential.pdf}
}

@article{hajian-tilakiComparisonThreeMethods2002,
  title = {Comparison of {{Three Methods}} for {{Estimating}} the {{Standard Error}} of the {{Area}} under the {{Curve}} in {{ROC Analysis}} of {{Quantitative Data}}},
  author = {{Hajian-Tilaki}, Karim O. and Hanley, James A.},
  year = {2002},
  month = nov,
  journal = {Academic Radiology},
  volume = {9},
  number = {11},
  pages = {1278--1285},
  issn = {10766332},
  doi = {10.1016/S1076-6332(03)80561-5},
  urldate = {2024-08-30},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hajian-Tilaki_Hanley_2002_Comparison_of_Three_Methods_for_Estimating_the_Standard_Error_of_the_Area_under.pdf}
}

@article{halekohPackageGeepackGeneralized2006,
  title = {The {{R Package}} Geepack for {{Generalized Estimating Equations}}},
  author = {Halekoh, Ulrich and H{\o}jsgaard, S{\o}ren and Yan, Jun},
  year = {2006},
  journal = {Journal of Statistical Software},
  volume = {15},
  number = {2},
  issn = {1548-7660},
  doi = {10.18637/jss.v015.i02},
  urldate = {2018-10-12},
  abstract = {This paper describes the core features of the R package geepack, which implements the generalized estimating equations (GEE) approach for fitting marginal generalized linear models to clustered data. Clustered data arise in many applications such as longitudinal data and repeated measures. The GEE approach focuses on models for the mean of the correlated observations within clusters without fully specifying the joint distribution of the observations. It has been widely used in statistical practice. This paper illustrates the application of the GEE approach with geepack through an example of clustered binary data.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Halekoh et al_2006_The R Package geepack for Generalized Estimating Equations.pdf}
}

@incollection{hallAdaptiveInterventionDesigns2019,
  title = {Adaptive {{Intervention Designs}} in {{Substance Use Prevention}}},
  booktitle = {Prevention of {{Substance Use}}},
  author = {Hall, Kelly L. and {Nahum-Shani}, Inbal and August, Gerald J. and Patrick, Megan E. and Murphy, Susan A. and Almirall, Daniel},
  editor = {Sloboda, Zili and Petras, Hanno and Robertson, Elizabeth and Hingson, Ralph},
  year = {2019},
  series = {Advances in {{Prevention Science}}},
  pages = {263--280},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-00627-3_17},
  urldate = {2020-12-12},
  abstract = {An adaptive preventive intervention design approach is an intervention design that uses baseline or ongoing information about an individual to make subsequent preventive intervention decisions. Often, the individuals and families served by substance use preventive interventions are widely heterogeneous both in background (e.g., initial risk prior to intervention) and in response to the intervention (e.g., intervention adherence or engagement, or change in risk). Adaptive preventive interventions have the potential to reduce the risk of substance use and abuse for a greater number of individuals, relative to one-intervention-for-all approaches to prevention, because they respond to this heterogeneity. An important framework in prevention science distinguishes between universal, selective, and indicated intervention tiers or components. The first contribution of this chapter is to illustrate different types of adaptive preventive interventions and describe how they fit within the universal, selective, and indicated framework. Specifically, we will illustrate, via four examples, how adaptive preventive interventions may lead to a sequence of interventions within or across these categories. The second contribution of this chapter responds to the growing interest in the use of sequential multiple assignment randomized trial (SMART) designs as a tool for addressing open scientific questions that prevention scientists confront when developing adaptive preventive interventions.},
  isbn = {978-3-030-00627-3},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hall et al_2019_Adaptive Intervention Designs in Substance Use Prevention.pdf}
}

@article{hallExtendedGeneralizedEstimating1998,
  title = {Extended {{Generalized Estimating Equations}} for {{Clustered Data}}},
  author = {Hall, Daniel B. and Severini, Thomas A.},
  year = {1998},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {93},
  number = {444},
  pages = {1365--1375},
  issn = {0162-1459, 1537-274X},
  doi = {10/gf443t},
  urldate = {2019-07-18},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hall_Severini_1998_Extended Generalized Estimating Equations for Clustered Data.pdf}
}

@article{hamakerNoTimePresent2017,
  title = {No {{Time Like}} the {{Present}}: {{Discovering}} the {{Hidden Dynamics}} in {{Intensive Longitudinal Data}}},
  shorttitle = {No {{Time Like}} the {{Present}}},
  author = {Hamaker, Ellen L. and Wichers, Marieke},
  year = {2017},
  month = feb,
  journal = {Current Directions in Psychological Science},
  volume = {26},
  number = {1},
  pages = {10--15},
  issn = {0963-7214, 1467-8721},
  doi = {10/f9r4kc},
  urldate = {2018-12-08},
  abstract = {There has been a strong increase in the number of studies based on intensive longitudinal data, such as those obtained with experience sampling and daily diaries. These data contain a wealth of information regarding the dynamics of processes as they unfold within individuals over time. In this article, we discuss how combining intensive longitudinal data with either time-series analysis, which consists of modeling the temporal dependencies in the data for a single individual, or dynamic multilevel modeling, which consists of using a time-series model at Level 1 to describe the within-person process while allowing for individual differences in the parameters of these processes at Level 2, has led to new insights in clinical psychology. In addition, we discuss several methodological and statistical challenges that researchers face when they are interested in studying the dynamics of psychological processes using intensive longitudinal data.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hamaker_Wichers_2017_No Time Like the Present.pdf}
}

@article{hamasakiDesignDataMonitoring2018,
  title = {Design, Data Monitoring and Analysis of Clinical Trials with Co-Primary Endpoints: A Review},
  shorttitle = {Design, Data Monitoring and Analysis of Clinical Trials with Co-Primary Endpoints},
  author = {Hamasaki, Toshimitsu and Evans, Scott R. and Asakura, Koko},
  year = {2018},
  journal = {Journal of biopharmaceutical statistics},
  volume = {28},
  number = {1},
  pages = {28--51},
  issn = {1054-3406},
  doi = {10.1080/10543406.2017.1378668},
  urldate = {2024-08-08},
  abstract = {We review the design, data monitoring, and analyses of clinical trials with co-primary endpoints. Recently developed methods for fixed-sample and group-sequential settings are described. Practical considerations are discussed and guidance for the application of these methods is provided.},
  pmcid = {PMC6135538},
  pmid = {29083951},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hamasaki_et_al_2018_Design,_data_monitoring_and_analysis_of_clinical_trials_with_co-primary.pdf}
}

@article{hamburgPathPersonalizedMedicine2010,
  title = {The {{Path}} to {{Personalized Medicine}}},
  author = {Hamburg, Margaret A},
  year = {2010},
  journal = {n engl j med},
  pages = {4},
  doi = {10.1056/NEJMp1006304},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hamburg_2010_The Path to Personalized Medicine.pdf}
}

@article{hamptonDeeplyTailoringAdaptive2021,
  title = {Deeply {{Tailoring Adaptive Interventions}}: {{Enhancing Knowledge Generation}} of {{SMARTs}} in {{Special Education}}},
  shorttitle = {Deeply {{Tailoring Adaptive Interventions}}},
  author = {Hampton, Lauren H. and Chow, Jason C.},
  year = {2021},
  month = jul,
  journal = {Remedial and Special Education},
  pages = {07419325211030669},
  publisher = {SAGE Publications Inc},
  issn = {0741-9325},
  doi = {10/gmdmpv},
  urldate = {2021-10-14},
  abstract = {Special educators serve a diverse population of students with unique strengths and needs, and adaptive interventions that account for individual differences before and during the intervention are an important tool to moving the field toward more individualized practices. The purpose of this article is to detail the conceptualization and application of tailoring the sequential multiple assignment randomized trial (SMART) design approach to developing and evaluating deeply tailored adaptive interventions through the application of secondary analyses to account for individual differences at multiple time points. This conceptual paper provides an overview beyond the basic SMART design components by describing the tactics, design options, and analyses currently available to further refine a SMART study into a more personalized intervention to account for individual differences at multiple points throughout the intervention and individual response to treatment.},
  langid = {english},
  keywords = {adaptive interventions,SMART,tailoring variables},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hampton_Chow_2021_Deeply Tailoring Adaptive Interventions.pdf;/Users/nseewald/Zotero/storage/WDDHLV2P/07419325211030669.html}
}

@article{hamptonReportingDesignConsiderations2023,
  title = {Reporting and {{Design Considerations}} for {{SMART Behavioral Science Research}}},
  author = {Hampton, Lauren H. and Chow, Jason C. and Bhat, Bethany Hamilton and Roberts, Greg},
  year = {2023},
  month = dec,
  journal = {Educational Psychology Review},
  volume = {35},
  number = {4},
  pages = {117},
  issn = {1573-336X},
  doi = {10.1007/s10648-023-09837-y},
  urldate = {2024-12-17},
  abstract = {Sequential Multiple Assignment Randomized Trials (SMARTs) are increasing in popularity in behavior science research with over 130 federally funded studies currently active. SMARTs use multiple randomizations to experimentally evaluate the impact of different decisions during an ongoing adaptive intervention. While many aspects of SMARTs are similar to more traditional single-randomization trials, the additional complexity and variability introduced during subsequent randomization requires specificity in reporting. Due to the increase in the application and reporting of SMARTs, this paper serves as a guide to support transparent reporting of adaptive intervention research. Further, these guidelines provide detail to ensure that future meta-analyses examining SMARTs can be robust. We provide recommendations for reporting and how reporting in SMARTs warrant additional detail above and beyond a traditional randomized trial.},
  langid = {english},
  keywords = {Adaptive interventions,Methods,Reporting,Sequential Multiple Assignment Randomized Trials},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hampton et al. - 2023 - Reporting and Design Considerations for SMART Behavioral Science Research.pdf}
}

@article{hancockSimulationMethodsTeaching,
  title = {Simulation {{Methods}} for {{Teaching Sampling Distributions}}: {{Should Hands-on Activities Precede}} the {{Computer}}?},
  author = {Hancock, Stacey A. and Rummerfield, Wendy},
  journal = {Journal of Statistics Education},
  doi = {10/ggpbnz},
  abstract = {Sampling distributions are fundamental to an understanding of statistical inference, yet research shows that students in introductory statistics courses tend to have multiple misconceptions of this important concept. A common instructional method used to address these misconceptions is computer simulation, often preceded by hands-on simulation activities. However, the results on computer simulation activities' effects on student understanding of sampling distributions, and if hands-on simulation activities are necessary, are mixed. In this article, we describe an empirical intervention study in which each of eight discussion sections of an introductory statistics course at a large research university was assigned to one of two in-class activity sequences on sampling distributions: one consisting of computer simulation activities preceded by hands-on simulation using dice, cards, or tickets, and the other comprised of computer simulation alone with the same time-on-task. Using a longitudinal model of changes in standardized exam scores across three exams, we found significant evidence that students who took part in a hands-on activity before computer simulation had better improvement from the first midterm to the final exam, on average, compared to those who only did computer simulations. {$<$}a class="ext-link" href="https://doi.org/10.1080/10691898.2020.1720551"{$>$}Supplementary materials{$<$}/a{$>$} for this article are available online.},
  keywords = {nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hancock_Rummerfield_Simulation Methods for Teaching Sampling Distributions.pdf}
}

@article{handClassifierTechnologyIllusion2006,
  title = {Classifier {{Technology}} and the {{Illusion}} of {{Progress}}},
  author = {Hand, David J.},
  year = {2006},
  month = feb,
  journal = {Statistical Science},
  volume = {21},
  number = {1},
  pages = {1--14},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/088342306000000060},
  urldate = {2022-08-17},
  abstract = {A great many tools have been developed for supervised classification, ranging from early methods such as linear discriminant analysis through to modern developments such as neural networks and support vector machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. This paper argues that these comparisons often fail to take into account important aspects of real problems, so that the apparent superiority of more sophisticated methods may be something of an illusion. In particular, simple methods typically yield performance almost as good as more sophisticated methods, to the extent that the difference in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm.},
  keywords = {empirical comparisons,error rate,flat maximum effect,misclassification rate,population drift,principle of parsimony,problem uncertainty,selectivity bias,simplicity,Supervised classification},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hand_2006_Classifier Technology and the Illusion of Progress.pdf;/Users/nseewald/Zotero/storage/8G5MMSV8/088342306000000060.html}
}

@article{hanleyStatisticalAnalysisCorrelated2003,
  title = {Statistical {{Analysis}} of {{Correlated Data Using Generalized Estimating Equations}}: {{An Orientation}}},
  shorttitle = {Statistical {{Analysis}} of {{Correlated Data Using Generalized Estimating Equations}}},
  author = {Hanley, J. A.},
  year = {2003},
  month = feb,
  journal = {American Journal of Epidemiology},
  volume = {157},
  number = {4},
  pages = {364--375},
  issn = {00029262},
  doi = {10/dwqrv3},
  urldate = {2019-11-29},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hanley_2003_Statistical Analysis of Correlated Data Using Generalized Estimating Equations.pdf}
}

@article{hansfordDevelopmentTrAnsparentReportinG2023,
  title = {Development of the {{TrAnsparent ReportinG}} of Observational Studies {{Emulating}} a {{Target}} Trial ({{TARGET}}) Guideline},
  author = {Hansford, Harrison J. and Cashin, Aidan G. and Jones, Matthew D. and Swanson, Sonja A. and Islam, Nazrul and Dahabreh, Issa J. and Dickerman, Barbra A. and Egger, Matthias and {Garcia-Albeniz}, Xavier and Golub, Robert M. and Lodi, Sara and {Moreno-Betancur}, Margarita and Pearson, Sallie-Anne and Schneeweiss, Sebastian and Sterne, Jonathan and Sharp, Melissa K. and Stuart, Elizabeth A. and Hernan, Miguel A. and Lee, Hopin and McAuley, James H.},
  year = {2023},
  month = sep,
  journal = {BMJ Open},
  volume = {13},
  number = {9},
  pages = {e074626},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2023-074626},
  urldate = {2024-01-17},
  abstract = {Background Observational studies are increasingly used to inform health decision-making when randomised trials are not feasible, ethical or timely. The target trial approach provides a framework to help minimise common biases in observational studies that aim to estimate the causal effect of interventions. Incomplete reporting of studies using the target trial framework limits the ability for clinicians, researchers, patients and other decision-makers to appraise, synthesise and interpret findings to inform clinical and public health practice and policy. This paper describes the methods that we will use to develop the TrAnsparent ReportinG of observational studies Emulating a Target trial (TARGET) reporting guideline. Methods/design The TARGET reporting guideline will be developed in five stages following recommended guidance. The first stage will identify target trial reporting practices by systematically reviewing published studies that explicitly emulated a target trial. The second stage will identify and refine items to be considered for inclusion in the TARGET guideline by consulting content experts using sequential online surveys. The third stage will prioritise and consolidate key items to be included in the TARGET guideline at an in-person consensus meeting of TARGET investigators. The fourth stage will produce and pilot-test both the TARGET guideline and explanation and elaboration document with relevant stakeholders. The fifth stage will disseminate the TARGET guideline and resources via journals, conferences and courses. Ethics and dissemination Ethical approval for the survey has been attained (HC220536). The TARGET guideline will be disseminated widely in partnership with stakeholders to maximise adoption and improve reporting of these studies.},
  chapter = {Epidemiology},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published by BMJ.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See:~https://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {37699620},
  keywords = {_tablet_modified,EPIDEMIOLOGY,Retrospective Studies,STATISTICS & RESEARCH METHODS},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hansford et al_2023_Development of the TrAnsparent ReportinG of observational studies Emulating a.pdf}
}

@article{hansfordReportingObservationalStudies2023,
  title = {Reporting of {{Observational Studies Explicitly Aiming}} to {{Emulate Randomized Trials}}: {{A Systematic Review}}},
  shorttitle = {Reporting of {{Observational Studies Explicitly Aiming}} to {{Emulate Randomized Trials}}},
  author = {Hansford, Harrison J. and Cashin, Aidan G. and Jones, Matthew D. and Swanson, Sonja A. and Islam, Nazrul and Douglas, Susan R. G. and Rizzo, Rodrigo R. N. and Devonshire, Jack J. and Williams, Sam A. and Dahabreh, Issa J. and Dickerman, Barbra A. and Egger, Matthias and {Garcia-Albeniz}, Xabier and Golub, Robert M. and Lodi, Sara and {Moreno-Betancur}, Margarita and Pearson, Sallie-Anne and Schneeweiss, Sebastian and Sterne, Jonathan A. C. and Sharp, Melissa K. and Stuart, Elizabeth A. and Hern{\'a}n, Miguel A. and Lee, Hopin and McAuley, James H.},
  year = {2023},
  month = sep,
  journal = {JAMA Network Open},
  volume = {6},
  number = {9},
  pages = {e2336023},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2023.36023},
  urldate = {2024-04-03},
  abstract = {Observational (nonexperimental) studies that aim to emulate a randomized trial (ie, the target trial) are increasingly informing medical and policy decision-making, but it is unclear how these studies are reported in the literature. Consistent reporting is essential for quality appraisal, evidence synthesis, and translation of evidence to policy and practice.To assess the reporting of observational studies that explicitly aimed to emulate a target trial.We searched Medline, Embase, PsycINFO, and Web of Science for observational studies published between March 2012 and October 2022 that explicitly aimed to emulate a target trial of a health or medical intervention. Two reviewers double-screened and -extracted data on study characteristics, key predefined components of the target trial protocol and its emulation (eligibility criteria, treatment strategies, treatment assignment, outcome[s], follow-up, causal contrast[s], and analysis plan), and other items related to the target trial emulation.A total of 200 studies that explicitly aimed to emulate a target trial were included. These studies included 26 subfields of medicine, and 168 (84\%) were published from January 2020 to October 2022. The aim to emulate a target trial was explicit in 70 study titles (35\%). Forty-three studies (22\%) reported use of a published reporting guideline (eg, Strengthening the Reporting of Observational Studies in Epidemiology). Eighty-five studies (43\%) did not describe all key items of how the target trial was emulated and 113 (57\%) did not describe the protocol of the target trial and its emulation.In this systematic review of 200 studies that explicitly aimed to emulate a target trial, reporting of how the target trial was emulated was inconsistent. A reporting guideline for studies explicitly aiming to emulate a target trial may improve the reporting of the target trial protocols and other aspects of these emulation attempts.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hansford et al_2023_Reporting of Observational Studies Explicitly Aiming to Emulate Randomized.pdf;/Users/nseewald/Zotero/storage/5HYCFKDM/2809945.html}
}

@article{hardtEqualityOpportunitySupervised,
  title = {Equality of {{Opportunity}} in {{Supervised Learning}}},
  author = {Hardt, Moritz and Price, Eric},
  pages = {9},
  abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hardt_Price_Equality of Opportunity in Supervised Learning.pdf}
}

@article{harmonValidationAtHomeDirect2021,
  title = {Validation of an {{At-Home Direct Antigen Rapid Test}} for {{COVID-19}}},
  author = {Harmon, Alexander and Chang, Celina and Salcedo, Nol and Sena, Brena and Herrera, Bobby Brooke and Bosch, Irene and Holberger, Laura E.},
  year = {2021},
  month = aug,
  journal = {JAMA Network Open},
  volume = {4},
  number = {8},
  pages = {e2126931},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2021.26931},
  urldate = {2024-07-19},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Harmon_et_al_2021_Validation_of_an_At-Home_Direct_Antigen_Rapid_Test_for_COVID-19.pdf;/Users/nseewald/Zotero/storage/W9Y25YTN/2783550.html}
}

@article{harozDesigningClinicalDecision2021,
  title = {Designing a {{Clinical Decision Support Tool That Leverages Machine Learning}} for {{Suicide Risk Prediction}}: {{Development Study}} in {{Partnership With Native American Care Providers}}},
  shorttitle = {Designing a {{Clinical Decision Support Tool That Leverages Machine Learning}} for {{Suicide Risk Prediction}}},
  author = {Haroz, Emily E. and Grubin, Fiona and Goklish, Novalene and Pioche, Shardai and Cwik, Mary and Barlow, Allison and Waugh, Emma and Usher, Jason and Lenert, Matthew C. and Walsh, Colin G.},
  year = {2021},
  month = sep,
  journal = {JMIR Public Health and Surveillance},
  volume = {7},
  number = {9},
  pages = {e24377},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/24377},
  urldate = {2022-08-17},
  abstract = {Background: Machine learning algorithms for suicide risk prediction have been developed with notable improvements in accuracy. Implementing these algorithms to enhance clinical care and reduce suicide has not been well studied. Objective: This study aims to design a clinical decision support tool and appropriate care pathways for community-based suicide surveillance and case management systems operating on Native American reservations. Methods: Participants included Native American case managers and supervisors (N=9) who worked on suicide surveillance and case management programs on 2 Native American reservations. We used in-depth interviews to understand how case managers think about and respond to suicide risk. The results from interviews informed a draft clinical decision support tool, which was then reviewed with supervisors and combined with appropriate care pathways. Results: Case managers reported acceptance of risk flags based on a predictive algorithm in their surveillance system tools, particularly if the information was available in a timely manner and used in conjunction with their clinical judgment. Implementation of risk flags needed to be programmed on a dichotomous basis, so the algorithm could produce output indicating high versus low risk. To dichotomize the continuous predicted probabilities, we developed a cutoff point that favored specificity, with the understanding that case managers' clinical judgment would help increase sensitivity. Conclusions: Suicide risk prediction algorithms show promise, but implementation to guide clinical care remains relatively elusive. Our study demonstrates the utility of working with partners to develop and guide the operationalization of risk prediction algorithms to enhance clinical care in a community setting.},
  copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in the Journal of Medical Internet Research...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Haroz et al_2021_Designing a Clinical Decision Support Tool That Leverages Machine Learning for.pdf;/Users/nseewald/Zotero/storage/9UEJUZMW/e24377.html}
}

@article{harozReachingThoseHighest2020,
  title = {Reaching {{Those}} at {{Highest Risk}} for {{Suicide}}: {{Development}} of a {{Model Using Machine Learning Methods}} for Use {{With Native American Communities}}},
  shorttitle = {Reaching {{Those}} at {{Highest Risk}} for {{Suicide}}},
  author = {Haroz, Emily E. and Walsh, Colin G. and Goklish, Novalene and Cwik, Mary F. and O'Keefe, Victoria and Barlow, Allison},
  year = {2020},
  journal = {Suicide and Life-Threatening Behavior},
  volume = {50},
  number = {2},
  pages = {422--436},
  issn = {1943-278X},
  doi = {10.1111/sltb.12598},
  urldate = {2022-08-25},
  abstract = {Objective Suicide prevention is a major priority in Native American communities. We used machine learning with community-based suicide surveillance data to better identify those most at risk. Method This study leverages data from the Celebrating Life program operated by the White Mountain Apache Tribe in Arizona and in partnership with Johns Hopkins University. We examined N = 2,390 individuals with a validated suicide-related event between 2006 and 2017. Predictors included 73 variables (e.g., demographics, educational history, past mental health, and substance use). The outcome was suicide attempt 6, 12, and 24 months after an initial event. We tested four algorithmic approaches using cross-validation. Results Area under the curves ranged from AUC = 0.81 (95\% CI {\textpm} 0.08) for the decision tree classifiers to AUC = 0.87 (95\% CI {\textpm} 0.04) for the ridge regression, results that were considerably higher than a past suicide attempt (AUC = 0.57; 95\% CI {\textpm} 0.08). Selecting a cutoff value based on risk concentration plots yielded 0.88 sensitivity, 0.72 specificity, and a positive predictive value of 0.12 for detecting an attempt 24 months postindex event. Conclusion These models substantially improved our ability to determine who was most at risk in this community. Further work is needed including developing clinical guidance and external validation.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Haroz et al_2020_Reaching Those at Highest Risk for Suicide.pdf;/Users/nseewald/Zotero/storage/FU966N93/sltb.html}
}

@article{hartmanSequentialMultipleAssignment2024,
  title = {A Sequential, Multiple Assignment, Randomized Trial Design with a Tailoring Function},
  author = {Hartman, Holly and Schipper, Matthew and Kidwell, Kelley},
  year = {2024},
  month = jul,
  journal = {Statistics in Medicine},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1097-0258},
  doi = {10.1002/sim.10161},
  urldate = {2024-07-20},
  abstract = {We present a trial design for sequential multiple assignment randomized trials (SMARTs) that use a tailoring function instead of a binary tailoring variable allowing for simultaneous development of t...},
  langid = {english}
}

@article{hartmanSequentialMultipleAssignment2024a,
  title = {A Sequential, Multiple Assignment, Randomized Trial Design with a Tailoring Function},
  author = {Hartman, Holly and Schipper, Matthew and Kidwell, Kelley},
  year = {2024},
  journal = {Statistics in Medicine},
  volume = {43},
  number = {21},
  pages = {4055--4072},
  issn = {1097-0258},
  doi = {10.1002/sim.10161},
  urldate = {2024-12-17},
  abstract = {We present a trial design for sequential multiple assignment randomized trials (SMARTs) that use a tailoring function instead of a binary tailoring variable allowing for simultaneous development of the tailoring variable and estimation of dynamic treatment regimens (DTRs). We apply methods for developing DTRs from observational data: tree-based regression learning and Q-learning. We compare this to a balanced randomized SMART with equal re-randomization probabilities and a typical SMART design where re-randomization depends on a binary tailoring variable and DTRs are analyzed with weighted and replicated regression. This project addresses a gap in clinical trial methodology by presenting SMARTs where second stage treatment is based on a continuous outcome removing the need for a binary tailoring variable. We demonstrate that data from a SMART using a tailoring function can be used to efficiently estimate DTRs and is more flexible under varying scenarios than a SMART using a tailoring variable.},
  copyright = {{\copyright} 2024 The Author(s). Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {clinical trials,dynamic treatment regimens,Q-learning,SMARTs,tailoring function,tailoring variable,tree based reinforcement learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hartman et al. - 2024 - A sequential, multiple assignment, randomized trial design with a tailoring function.pdf;/Users/nseewald/Zotero/storage/CPQ5F7S4/sim.html}
}

@book{hartungStatisticalMetaanalysisApplications2008,
  title = {Statistical Meta-Analysis with Applications},
  author = {Hartung, Joachim and Knapp, Guido and Sinha, Bimal K.},
  year = {2008},
  publisher = {Wiley},
  address = {Hoboken, N.J.},
  abstract = {"Statistical Meta-Analysis with Applications presents the necessary statistical methodologies that allow readers to tackle the four main stages of meta-analysis: problem formulation, data collection, data evaluation, and data analysis and interpretation. Combining the authors' expertise on the topic with a wealth of up-to-date information, this book successfully introduces the essential statistical practices for making thorough and accurate discoveries across a wide array of diverse fields, such as business, public health, biostatistics, and environmental studies."--Jacket},
  isbn = {978-0-470-38633-0},
  langid = {english},
  annotation = {OCLC: 263431183},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hartung et al_2008_Statistical meta-analysis with applications.pdf}
}

@article{hartwigNotePartialOrdering1978,
  title = {A Note on the Partial Ordering of Positive Semi-Definite Matrices},
  author = {Hartwig, Robert E.},
  year = {1978},
  month = jan,
  journal = {Linear and Multilinear Algebra},
  volume = {6},
  number = {3},
  pages = {223--226},
  issn = {0308-1087, 1563-5139},
  doi = {10/dnpjtd},
  urldate = {2019-04-09},
  langid = {english},
  keywords = {linear algebra},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hartwig_1978_A note on the partial ordering of positive semi-definite matrices.pdf}
}

@article{hashimotoFairnessDemographicsRepeated2018,
  title = {Fairness {{Without Demographics}} in {{Repeated Loss Minimization}}},
  author = {Hashimoto, Tatsunori B. and Srivastava, Megha and Namkoong, Hongseok and Liang, Percy},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.08010 [cs, stat]},
  eprint = {1806.08010},
  primaryclass = {cs, stat},
  urldate = {2019-01-25},
  abstract = {Machine learning models (e.g., speech recognizers) are usually trained to minimize average loss, which results in representation disparity---minority groups (e.g., non-native speakers) contribute less to the training objective and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even make initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,No DOI found,Statistics - Machine Learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hashimoto et al_2018_Fairness Without Demographics in Repeated Loss Minimization.pdf}
}

@article{hatfieldIntroductoryCoursesDisproportionately2022,
  title = {Do Introductory Courses Disproportionately Drive Minoritized Students out of {{STEM}} Pathways?},
  author = {Hatfield, Neil and Brown, Nathanial and Topaz, Chad M},
  year = {2022},
  month = sep,
  journal = {PNAS Nexus},
  volume = {1},
  number = {4},
  pages = {pgac167},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgac167},
  urldate = {2022-10-05},
  abstract = {Diversifying science, technology, engineering, and mathematics (STEM) requires a critical examination of institutional structures at every educational level. In higher education, performance in core introductory courses required for STEM degrees is strongly associated with degree completion. Leveraging a multi-institutional database, we examine nearly 110,000 student records from six large, public, research-intensive universities in order to assess whether these introductory courses disproportionately weed out underrepresented minority (URM) students. We find that the association between low performance in an introductory STEM class and failure to obtain a STEM degree is stronger for URM students than for other students, even after controlling for academic preparation in high school and intent to obtain a STEM degree. To facilitate interpretation of our multivariate logistic regression model, and to highlight the dire situation in higher education, we also calculate predicted probabilities of STEM degree attainment for students of various demographics. The probability of obtaining a STEM degree for a STEM-intending white male student with average academic preparation who receives grades of C or better in all introductory courses is 48\%. In contrast, for an otherwise similar URM female student, the probability is merely 35\%. If these students receive less than a C in even one introductory STEM course, the probabilities drop to 33\% and 21\%, respectively.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hatfield et al_2022_Do introductory courses disproportionately drive minoritized students out of.pdf;/Users/nseewald/Zotero/storage/JCEK4ZD6/6706685.html}
}

@article{hausmanSpecificationTestsEconometrics1978,
  title = {Specification {{Tests}} in {{Econometrics}}},
  author = {Hausman, J. A.},
  year = {1978},
  journal = {Econometrica},
  volume = {46},
  number = {6},
  eprint = {1913827},
  eprinttype = {jstor},
  pages = {1251--1271},
  publisher = {[Wiley, Econometric Society]},
  issn = {0012-9682},
  doi = {10.2307/1913827},
  urldate = {2021-07-14},
  abstract = {Using the result that under the null hypothesis of no misspecification an asymptotically efficient estimator must have zero asymptotic covariance with its difference from a consistent but asymptotically inefficient estimator, specification tests are devised for a number of model specifications in econometrics. Local power is calculated for small departures from the null hypothesis. An instrumental variable test as well as tests for a time series cross section model and the simultaneous equation model are presented. An empirical model provides evidence that unobserved individual factors are present which are not orthogonal to the included right-hand-side variable in a common econometric specification of an individual wage equation.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hausman_1978_Specification Tests in Econometrics.pdf}
}

@book{hayesClusterRandomisedTrials2017,
  title = {Cluster Randomised Trials},
  author = {Hayes, Richard J. and Moulton, Lawrence H.},
  year = {2017},
  series = {Chapman \& {{Hall}} - {{CRC}} Biostatistics Series},
  edition = {2nd ed},
  publisher = {CRC press},
  address = {Boca Raton},
  isbn = {978-1-4987-2822-5 978-1-315-37028-6},
  langid = {english},
  lccn = {610.724}
}

@article{heagertyMarginalRegressionModels1996,
  title = {Marginal {{Regression Models}} for {{Clustered Ordinal Measurements}}},
  author = {Heagerty, Patrick J. and Zeger, Scott L.},
  year = {1996},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {91},
  number = {435},
  pages = {1024--1036},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1996.10476973},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Heagerty_Zeger_1996_Marginal Regression Models for Clustered Ordinal Measurements.pdf}
}

@article{heagertySurvivalModelPredictive2005,
  title = {Survival {{Model Predictive Accuracy}} and {{ROC Curves}}},
  author = {Heagerty, Patrick J. and Zheng, Yingye},
  year = {2005},
  journal = {Biometrics},
  volume = {61},
  number = {1},
  pages = {92--105},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.2005.030814.x},
  urldate = {2024-03-14},
  abstract = {The predictive accuracy of a survival model can be summarized using extensions of the proportion of variation explained by the model, or R2, commonly used for continuous response models, or using extensions of sensitivity and specificity, which are commonly used for binary response models. In this article we propose new time-dependent accuracy summaries based on time-specific versions of sensitivity and specificity calculated over risk sets. We connect the accuracy summaries to a previously proposed global concordance measure, which is a variant of Kendall's tau. In addition, we show how standard Cox regression output can be used to obtain estimates of time-dependent sensitivity and specificity, and time-dependent receiver operating characteristic (ROC) curves. Semiparametric estimation methods appropriate for both proportional and nonproportional hazards data are introduced, evaluated in simulations, and illustrated using two familiar survival data sets.},
  langid = {english},
  keywords = {Cox regression,Discrimination,Prediction,Sensitivity,Specificity},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Heagerty_Zheng_2005_Survival Model Predictive Accuracy and ROC Curves.pdf;/Users/nseewald/Zotero/storage/H6PIXZJZ/j.0006-341X.2005.030814.html}
}

@article{heagertyTimeDependentROCCurves2000,
  title = {Time-{{Dependent ROC Curves}} for {{Censored Survival Data}} and a {{Diagnostic Marker}}},
  author = {Heagerty, Patrick J. and Lumley, Thomas and Pepe, Margaret S.},
  year = {2000},
  journal = {Biometrics},
  volume = {56},
  number = {2},
  pages = {337--344},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.2000.00337.x},
  urldate = {2023-10-25},
  abstract = {Summary. ROC curves are a popular method for displaying sensitivity and specificity of a continuous marker, X, for a binary disease variable, D. However, many disease outcomes are time dependent, D(t, and ROC curves that vary as a function of time may be mire appropriate. A common examples of a time-dependent variable is vital status, where D(t) = 1 if a patient has died prior to time t and zero otherwise. We propose summarizing the discrimination potential of a marker X, measured at baseline (t= 0), by calculating ROC Curves for cumulative disease or death incidence by time t, which we denote as ROC(t). A typical complexity with survival data is that observations may be censored. Two ROC curve estimators are proposed that can accommodate censored data. A simple estimator is based on using the Kaplan-Meier estimated for each possible subset X {$>$} c. However, this estimator does not guarantee the necessary condition that sensitivity and specificity are monotone in X. An alternative estimator that does guarantee monotonicity is based on a nearest neighbor estimator for the bivariate distribution function of (X, T), where T represents survival time (Akritas, M. J., 1994, Annals of Statistics22, 1299--1327). We present an example where ROC(t) is used to compare a standard and a modified flow cytometry measurement for predicting survival after detection of breast cancer and an example where the ROC(t) curve displays the impact of modifying eligibility criteria for sample size and power in HIV prevention trials.},
  langid = {english},
  keywords = {Accuracy,Discrimination,Kaplan-Meier estimator,Kernel smoothing,Sensitivity,Specificity},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Heagerty et al_2000_Time-Dependent ROC Curves for Censored Survival Data and a Diagnostic Marker.pdf;/Users/nseewald/Zotero/storage/48HGKEBD/j.0006-341X.2000.00337.html}
}

@article{heckmanPolicyRelevantTreatmentEffects2001,
  title = {Policy-{{Relevant Treatment Effects}}},
  author = {Heckman, James J. and Vytlacil, Edward},
  year = {2001},
  month = may,
  journal = {American Economic Review},
  volume = {91},
  number = {2},
  pages = {107--111},
  issn = {0002-8282},
  doi = {10.1257/aer.91.2.107},
  urldate = {2025-03-22},
  abstract = {Policy-Relevant Treatment Effects by James J. Heckman and Edward Vytlacil. Published in volume 91, issue 2, pages 107-111 of American Economic Review, May 2001},
  langid = {english}
}

@article{hedekerSampleSizeEstimation1999,
  ids = {hedekerSampleSizeEstimation},
  title = {Sample {{Size Estimation}} for {{Longitudinal Designs}} with {{Attrition}}: {{Comparing Time-Related Contrasts Between Two Groups}}},
  author = {Hedeker, Donald and Gibbons, Robert D and Waternaux, Christine},
  year = {1999},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {24},
  number = {1},
  pages = {70--93},
  langid = {english},
  keywords = {Multiple DOI},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hedeker et al_1999_Sample Size Estimation for Longitudinal Designs with Attrition.pdf}
}

@article{heEvaluatingMisclassificationEffects2022,
  title = {Evaluating {{Misclassification Effects}} on {{Single Sequential Treatment}} in {{Sequential Multiple Assignment Randomized Trial}} ({{SMART}}) {{Designs}}},
  author = {He, Jun and McClish, Donna K. and Sabo, Roy T.},
  year = {2022},
  month = aug,
  journal = {Statistics in Biopharmaceutical Research},
  volume = {14},
  number = {3},
  pages = {306--313},
  publisher = {ASA Website},
  issn = {null},
  doi = {10.1080/19466315.2021.1883472},
  urldate = {2024-12-17},
  abstract = {Sequential multiple assignment randomized trial designs tailor individual treatment by rerandomizing participants to subsequent therapies based on their response to initial treatment. Misclassification of participant responses to initial treatment can lead to inappropriate treatment assignment and thus impact the final outcome. The aim of this study is to derive a series of formulas for quantifying potential misclassification effects on the mean, variance, and statistical inference of a single sequential treatment (SST) effect with continuous outcome. Relative bias is expressed as a function of sensitivity, specificity, and the probability of being true responders. Results show that misclassification can introduce bias to the estimated treatment effect. Though the magnitude of bias varies, there are a few general conclusions: (1) for any fixed sensitivity (or specificity) the relative bias of the mean of responders (or nonresponders) always approaches 0 in a monotonic nonlinear pattern as specificity (or sensitivity) increases; (2) the relative bias of SST variance always has nonmonotone nonlinear relationship with sensitivity or specificity; (3) the SST variance under misclassification is always over-estimated. Furthermore, the results show that misclassification can affect statistical inference, with power exhibiting either monotonic or nonmonotonic patterns and resulting in either under- or over-estimation.},
  keywords = {Misclassification,Relative bias,Sequential multiple assignment randomized trial design,Simulation,Single sequential treatment},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/He et al. - 2022 - Evaluating Misclassification Effects on Single Sequential Treatment in Sequential Multiple Assignmen.pdf}
}

@article{heidariMoralFrameworkUnderstanding2018,
  title = {A {{Moral Framework}} for {{Understanding}} of {{Fair ML}} through {{Economic Models}} of {{Equality}} of {{Opportunity}}},
  author = {Heidari, Hoda and Loi, Michele and Gummadi, Krishna P. and Krause, Andreas},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.03400 [cs, econ, stat]},
  eprint = {1809.03400},
  primaryclass = {cs, econ, stat},
  urldate = {2018-10-12},
  abstract = {Equality of opportunity (EOP) is an extensively studied conception of fairness in political philosophy. In this work, we map recently proposed notions of algorithmic fairness to economic models of EOP. We formally show that through our proposed mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and also interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new, more general family of measures for algorithmic fairness. We empirically show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences on the subjects' welfare.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Economics - Theoretical Economics,No DOI found,Statistics - Machine Learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Heidari et al_2018_A Moral Framework for Understanding of Fair ML through Economic Models of.pdf}
}

@article{heiligPharmacologicalTreatmentAlcohol2006,
  title = {Pharmacological Treatment of Alcohol Dependence: {{Target}} Symptoms and Target Mechanisms},
  shorttitle = {Pharmacological Treatment of Alcohol Dependence},
  author = {Heilig, M and Egli, M},
  year = {2006},
  month = sep,
  journal = {Pharmacology \& Therapeutics},
  volume = {111},
  number = {3},
  pages = {855--876},
  issn = {01637258},
  doi = {10/cfs7df},
  urldate = {2020-12-12},
  langid = {english},
  keywords = {nosource}
}

@article{heilImprovedPostoperativeOutcomes2023,
  title = {Improved {{Postoperative Outcomes}} after {{Prehabilitation}} for {{Colorectal Cancer Surgery}} in {{Older Patients}}: {{An Emulated Target Trial}}},
  shorttitle = {Improved {{Postoperative Outcomes}} after {{Prehabilitation}} for {{Colorectal Cancer Surgery}} in {{Older Patients}}},
  author = {Heil, Thea C. and Verdaasdonk, Emiel G. G. and Maas, Huub A. A. M. and {van Munster}, Barbara C. and Rikkert, Marcel G. M. Olde and {de Wilt}, Johannes H. W. and Melis, Ren{\'e} J. F.},
  year = {2023},
  month = jan,
  journal = {Annals of Surgical Oncology},
  volume = {30},
  number = {1},
  pages = {244--254},
  issn = {1534-4681},
  doi = {10.1245/s10434-022-12623-9},
  urldate = {2024-01-30},
  abstract = {The aim of this study was to assess the effect of a multimodal prehabilitation program on perioperative outcomes in colorectal cancer patients with a higher postoperative complication risk, using an emulated target trial (ETT) design.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Heil_et_al_2023_Improved_Postoperative_Outcomes_after_Prehabilitation_for_Colorectal_Cancer_annotated.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Heil_et_al_2023_Improved_Postoperative_Outcomes_after_Prehabilitation_for_Colorectal_Cancer.pdf}
}

@article{hemmingAnalysisClusterRandomised2017,
  title = {Analysis of Cluster Randomised Stepped Wedge Trials with Repeated Cross-Sectional Samples},
  author = {Hemming, Karla and Taljaard, Monica and Forbes, Andrew},
  year = {2017},
  month = mar,
  journal = {Trials},
  volume = {18},
  number = {1},
  pages = {101},
  issn = {1745-6215},
  doi = {10.1186/s13063-017-1833-7},
  urldate = {2025-01-14},
  abstract = {The stepped wedge cluster randomised trial (SW-CRT) is increasingly being used to evaluate policy or service delivery interventions. However, there is a dearth of trials literature addressing analytical approaches to the SW-CRT. Perhaps as a result, a significant number of published trials have major methodological shortcomings, including failure to adjust for secular trends at the analysis stage. Furthermore, the commonly used analytical framework proposed by Hussey and Hughes makes several assumptions.},
  langid = {english},
  keywords = {Analysis,Cluster randomised trial,Secular trends,Stepped wedge},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hemming et al. - 2017 - Analysis of cluster randomised stepped wedge trials with repeated cross-sectional samples.pdf}
}

@article{hemmingModelingClusteringTreatment2018,
  title = {Modeling Clustering and Treatment Effect Heterogeneity in Parallel and Stepped-Wedge Cluster Randomized Trials},
  author = {Hemming, Karla and Taljaard, Monica and Forbes, Andrew},
  year = {2018},
  journal = {Statistics in Medicine},
  volume = {37},
  number = {6},
  pages = {883--898},
  issn = {1097-0258},
  doi = {10.1002/sim.7553},
  urldate = {2025-02-11},
  abstract = {Cluster randomized trials are frequently used in health service evaluation. It is common practice to use an analysis model with a random effect to allow for clustering at the analysis stage. In designs where clusters are exposed to both control and treatment conditions, it may be of interest to examine treatment effect heterogeneity across clusters. In designs where clusters are not exposed to both control and treatment conditions, it can also be of interest to allow heterogeneity in the degree of clustering between arms. These two types of heterogeneity are related. It has been proposed in both parallel cluster trials, stepped-wedge, and other cross-over designs that this heterogeneity can be allowed for by incorporating additional random effect(s) into the model. Here, we show that the choice of model parameterization needs careful consideration as some parameterizations for additional heterogeneity induce unnecessary or implausible assumptions. We suggest more appropriate parameterizations, discuss their relative advantages, and demonstrate the implications of these model choices using a real example of a parallel cluster trial and a simulated stepped-wedge trial.},
  copyright = {{\copyright} 2018 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {cluster randomized trial,ICC,stepped-wedge,treatment effect heterogeneity},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hemming et al. - 2018 - Modeling clustering and treatment effect heterogeneity in parallel and stepped-wedge cluster randomi.pdf;/Users/nseewald/Zotero/storage/6VUUX9H4/sim.html}
}

@article{hemmingReflectionModernMethods2020,
  title = {Reflection on Modern Methods: When Is a Stepped-Wedge Cluster Randomized Trial a Good Study Design Choice?},
  shorttitle = {Reflection on Modern Methods},
  author = {Hemming, Karla and Taljaard, Monica},
  year = {2020},
  month = jun,
  journal = {International Journal of Epidemiology},
  volume = {49},
  number = {3},
  pages = {1043--1052},
  issn = {0300-5771},
  doi = {10.1093/ije/dyaa077},
  urldate = {2023-09-01},
  abstract = {The stepped-wedge cluster randomized trial (SW-CRT) involves the sequential transition of clusters (such as hospitals, public health units or communities) from control to intervention conditions in a randomized order. The use of the SW-CRT is growing rapidly. Yet the SW-CRT is at greater risks of bias compared with the conventional parallel cluster randomized trial (parallel-CRT). For this reason, the CONSORT extension for SW-CRTs requires that investigators provide a clear justification for the choice of study design. In this paper, we argue that all other things being equal, the SW-CRT is at greater risk of bias due to misspecification of the secular trends at the analysis stage. This is particularly problematic for studies randomizing a small number of heterogeneous clusters. We outline the potential conditions under which an SW-CRT might be an appropriate choice. Potentially appropriate and often overlapping justifications for conducting an SW-CRT include: (i) the SW-CRT provides a means to conduct a randomized evaluation which otherwise would not be possible; (ii) the SW-CRT facilitates cluster recruitment as it enhances the acceptability of a randomized evaluation either to cluster gatekeepers or other stakeholders; (iii) the SW-CRT is the only feasible design due to pragmatic and logistical constraints (for example the roll-out of a scare resource); and (iv) the SW-CRT has increased statistical power over other study designs (which will include situations with a limited number of clusters). As the number of arguments in favour of an SW-CRT increases, the likelihood that the benefits of using the SW-CRT, as opposed to a parallel-CRT, outweigh its risks also increases. We argue that the mere popularity and novelty of the SW-CRT should not be a factor in its adoption. In situations when a conventional parallel-CRT is feasible, it is likely to be the preferred design.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hemming_Taljaard_2020_Reflection on modern methods.pdf;/Users/nseewald/Zotero/storage/ZPSAGJ27/5835358.html}
}

@article{hemmingTutorialSampleSize2020,
  title = {A Tutorial on Sample Size Calculation for Multiple-Period Cluster Randomized Parallel, Cross-over and Stepped-Wedge Trials Using the {{Shiny CRT Calculator}}},
  author = {Hemming, Karla and Kasza, Jessica and Hooper, Richard and Forbes, Andrew and Taljaard, Monica},
  year = {2020},
  month = jun,
  journal = {International Journal of Epidemiology},
  volume = {49},
  number = {3},
  pages = {979--995},
  issn = {0300-5771, 1464-3685},
  doi = {10.1093/ije/dyz237},
  urldate = {2023-05-09},
  abstract = {Abstract             It has long been recognized that sample size calculations for cluster randomized trials require consideration of the correlation between multiple observations within the same cluster. When measurements are taken at anything other than a single point in time, these correlations depend not only on the cluster but also on the time separation between measurements and additionally, on whether different participants (cross-sectional designs) or the same participants (cohort designs) are repeatedly measured. This is particularly relevant in trials with multiple periods of measurement, such as the cluster cross-over and stepped-wedge designs, but also to some degree in parallel designs. Several papers describing sample size methodology for these designs have been published, but this methodology might not be accessible to all researchers. In this article we provide a tutorial on sample size calculation for cluster randomized designs with particular emphasis on designs with multiple periods of measurement and provide a web-based tool, the Shiny CRT Calculator, to allow researchers to easily conduct these sample size calculations. We consider both cross-sectional and cohort designs and allow for a variety of assumed within-cluster correlation structures. We consider cluster heterogeneity in treatment effects (for designs where treatment is crossed with cluster), as well as individually randomized group-treatment trials with differential clustering between arms, for example designs where clustering arises from interventions being delivered in groups. The calculator will compute power or precision, as a function of cluster size or number of clusters, for a wide variety of designs and correlation structures. We illustrate the methodology and the flexibility of the Shiny CRT Calculator using a range of examples.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hemming et al_2020_A tutorial on sample size calculation for multiple-period cluster randomized.pdf}
}

@misc{hengDesignBasedCausalInference2023,
  title = {Design-{{Based Causal Inference}} with {{Missing Outcomes}}: {{Missingness Mechanisms}}, {{Imputation-Assisted Randomization Tests}}, and {{Covariate Adjustment}}},
  shorttitle = {Design-{{Based Causal Inference}} with {{Missing Outcomes}}},
  author = {Heng, Siyu and Zhang, Jiawei and Feng, Yang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.18556},
  eprint = {2310.18556},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.18556},
  urldate = {2023-11-30},
  abstract = {Design-based causal inference is one of the most widely used frameworks for testing causal null hypotheses or inferring about causal parameters from experimental or observational data. The most significant merit of design-based causal inference is that its statistical validity only comes from the study design (e.g., randomization design) and does not require assuming any outcome-generating distributions or models. Although immune to model misspecification, design-based causal inference can still suffer from other data challenges, among which missingness in outcomes is a significant one. However, compared with model-based causal inference, outcome missingness in design-based causal inference is much less studied, largely due to the challenge that design-based causal inference does not assume any outcome distributions/models and, therefore, cannot directly adopt any existing model-based approaches for missing data. To fill this gap, we systematically study the missing outcomes problem in design-based causal inference. First, we use the potential outcomes framework to clarify the minimal assumption (concerning the outcome missingness mechanism) needed for conducting finite-population-exact randomization tests for the null effect (i.e., Fisher's sharp null) and that needed for constructing finite-population-exact confidence sets with missing outcomes. Second, we propose a general framework called ``imputation and re-imputation" for conducting finite-population-exact randomization tests in design-based causal studies with missing outcomes. Our framework can incorporate any existing outcome imputation algorithms and meanwhile guarantee finite-population-exact type-I error rate control. Third, we extend our framework to conduct covariate adjustment in an exact randomization test with missing outcomes and to construct finite-population-exact confidence sets with missing outcomes.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Heng et al_2023_Design-Based Causal Inference with Missing Outcomes.pdf;/Users/nseewald/Zotero/storage/YRQV5Y4Q/2310.html}
}

@article{hengstDiscoveringHierarchyReinforcement2002,
  title = {Discovering Hierarchy in Reinforcement Learning with {{HEXQ}}},
  author = {Hengst, Bernhard},
  year = {2002},
  journal = {Icml},
  number = {1},
  pages = {243--250},
  doi = {10.1.1.9.5839},
  abstract = {An open problem in reinforcement learning is discovering hierarchical structure. HEXQ, an algorithm which automatically attempts to decompose and solve a model-free fac- tored MDP hierarchically is described. By searching for aliased Markov sub-space re- gions based on the state variables the algo- rithm uses temporal and state abstraction to construct a hierarchy of interlinked smaller MDPs.},
  keywords = {Invalid DOI},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hengst_2002_Discovering hierarchy in reinforcement learning with HEXQ.pdf}
}

@book{hernanCausalInferenceWhat2020,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hern{\'a}n, Miguel A. and Robins, James M.},
  year = {2020},
  publisher = {Chapman \& Hall/CRC},
  address = {Boca Raton},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hernán_Robins_2020_Causal Inference.pdf}
}

@article{hernanCWordScientificEuphemisms2018,
  title = {The {{C-Word}}: {{Scientific Euphemisms Do Not Improve Causal Inference From Observational Data}}},
  shorttitle = {The {{C-Word}}},
  author = {Hern{\'a}n, Miguel A.},
  year = {2018},
  month = may,
  journal = {American Journal of Public Health},
  volume = {108},
  number = {5},
  pages = {616--619},
  publisher = {American Public Health Association},
  issn = {0090-0036},
  doi = {10.2105/AJPH.2018.304337},
  urldate = {2022-01-27},
  abstract = {Causal inference is a core task of science. However, authors and editors often refrain from explicitly acknowledging the causal goal of research projects; they refer to causal effect estimates as associational estimates. This commentary argues that using the term ``causal'' is necessary to improve the quality of observational research. Specifically, being explicit about the causal objective of a study reduces ambiguity in the scientific question, errors in the data analysis, and excesses in the interpretation of the results.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hernán_2018_The C-Word.pdf}
}

@article{hernanDiscussionStatisticalIssues2005,
  title = {Discussion on "{{Statistical Issues Arising}} in the {{Women}}'s {{Health Initiative}}"},
  author = {Hern{\'a}n, Miguel A. and Robins, James M. and Garc{\'i}a Rodr{\'i}guez, Luis A.},
  year = {2005},
  month = dec,
  journal = {Biometrics},
  volume = {61},
  number = {4},
  pages = {922--930},
  issn = {0006-341X},
  doi = {10.1111/j.0006-341X.2005.454_7.x},
  urldate = {2024-07-10},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hernán et al_2005_Discussion on Statistical Issues Arising in the Women's Health Initiative.pdf;/Users/nseewald/Zotero/storage/N3QW27IW/7296195.html}
}

@article{hernanEstimatingCausalEffect2002,
  title = {Estimating the Causal Effect of Zidovudine on {{CD4}} Count with a Marginal Structural Model for Repeated Measures: {{MARGINAL STRUCTURAL MODEL FOR REPEATED MEASURES}}},
  shorttitle = {Estimating the Causal Effect of Zidovudine on {{CD4}} Count with a Marginal Structural Model for Repeated Measures},
  author = {Hern{\'a}n, Miguel A. and Brumback, Babette A. and Robins, James M.},
  year = {2002},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {21},
  number = {12},
  pages = {1689--1709},
  issn = {02776715},
  doi = {10.1002/sim.1144},
  urldate = {2018-10-12},
  abstract = {Even in the absence of unmeasured confounding factors or model misspeci{\"y}cation, standard methods for estimating the causal e ect of a time-varying treatment on the mean of a repeated measures outcome (for example, GEE regression) may be biased when there are time-dependent variables that are simultaneously confounders of the e ect of interest and are predicted by previous treatment. In contrast, the recently developed marginal structural models (MSMs) can provide consistent estimates of causal e ects when unmeasured confounding and model misspeci{\"y}cation are absent. We describe an MSM for repeated measures that parameterizes the marginal means of counterfactual outcomes corresponding to prespeci{\"y}ed treatment regimes. The parameters of MSMs are estimated using a new class of estimators -- inverse-probability of treatment weighted estimators. We used an MSM to estimate the e ect of zidovudine therapy on mean CD4 count among HIV-infected men in the Multicenter AIDS Cohort Study. We estimated a potential expected increase of 5:4 (95 per cent con{\"y}dence interval -1:8; 12:7) CD4 lymphocytes= l per additional study visit while on zidovudine therapy. We also explain the theory and implementation of MSMs for repeated measures data and draw upon a simple example to illustrate the basic ideas. Copyright ? 2002 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hernán et al_2002_Estimating the causal effect of zidovudine on CD4 count with a marginal.pdf}
}

@article{hernanInstrumentsCausalInference2006,
  title = {Instruments for {{Causal Inference}}: {{An Epidemiologist}}'s {{Dream}}?},
  shorttitle = {Instruments for {{Causal Inference}}},
  author = {Hern{\'a}n, Miguel A. and Robins, James M.},
  year = {2006},
  journal = {Epidemiology},
  volume = {17},
  number = {4},
  eprint = {20486236},
  eprinttype = {jstor},
  pages = {360--372},
  publisher = {Lippincott Williams \& Wilkins},
  issn = {1044-3983},
  urldate = {2022-04-29},
  abstract = {The use of instrumental variable (IV) methods is attractive because, even in the presence of unmeasured confounding, such methods may consistently estimate the average causal effect of an exposure on an outcome. However, for this consistent estimation to be achieved, several strong conditions must hold. We review the definition of an instrumental variable, describe the conditions required to obtain consistent estimates of causal effects, and explore their implications in the context of a recent application of the instrumental variables approach. We also present (1) a description of the connection between 4 causal models-counterfactuals, causal directed acyclic graphs, nonparametric structural equation models, and linear structural equation models--that have been used to describe instrumental variables methods; (2) a unified presentation of IV methods for the average causal effect in the study population through structural mean models; and (3) a discussion and new extensions of instrumental variables methods based on assumptions of monotonicity.}
}

@article{hernanMarginalStructuralModels2000,
  title = {Marginal {{Structural Models}} to {{Estimate}} the {{Causal Effect}} of {{Zidovudine}} on the {{Survival}} of {{HIV-Positive Men}}:},
  shorttitle = {Marginal {{Structural Models}} to {{Estimate}} the {{Causal Effect}} of {{Zidovudine}} on the {{Survival}} of {{HIV-Positive Men}}},
  author = {Hern{\'a}n, Miguel {\'A}ngel and Brumback, Babette and Robins, James M.},
  year = {2000},
  month = sep,
  journal = {Epidemiology},
  volume = {11},
  number = {5},
  pages = {561--570},
  issn = {1044-3983},
  doi = {10.1097/00001648-200009000-00012},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hernán et al_2000_Marginal Structural Models to Estimate the Causal Effect of Zidovudine on the.pdf}
}

@article{hernanObservationalStudiesAnalyzed2008,
  title = {Observational {{Studies Analyzed Like Randomized Experiments}}: {{An Application}} to {{Postmenopausal Hormone Therapy}} and {{Coronary Heart Disease}}},
  shorttitle = {Observational {{Studies Analyzed Like Randomized Experiments}}},
  author = {Hern{\'a}n, Miguel A. and Alonso, Alvaro and Logan, Roger and Grodstein, Francine and Michels, Karin B. and Willett, Walter C. and Manson, JoAnn E. and Robins, James M.},
  year = {2008},
  journal = {Epidemiology},
  volume = {19},
  number = {6},
  eprint = {25662633},
  eprinttype = {jstor},
  pages = {766--779},
  publisher = {Lippincott Williams \& Wilkins},
  issn = {1044-3983},
  urldate = {2024-03-02},
  abstract = {Background: The Women's Health Initiative randomized trial found greater coronary heart disease (CHD) risk in women assigned to estrogen/progestin therapy than in those assigned to placebo. Observational studies had previously suggested reduced CHD risk in hormone users. Methods: Using data from the observational Nurses' Health Study, we emulated the design and intention-to-treat (ITT) analysis of the randomized trial. The observational study was conceptualized as a sequence of "trials," in which eligible women were classified as initiators or noninitiators of estrogen/progestin therapy. Results: The ITT hazard ratios (HRs) (95\% confidence intervals) of CHD for initiators versus noninitiators were 1.42 (0.92--2.20) for the first 2 years, and 0.96 (0.78--1.18) for the entire follow-up. The ITT HRs were 0.84 (0.61--1.14) in women within 10 years of menopause, and 1.12 (0.84--1.48) in the others (P value for interaction = 0.08). These ITT estimates are similar to those from the Women's Health Initiative. Because the ITT approach causes severe treatment misclassification, we also estimated adherence-adjusted effects by inverse probability weighting. The HRs were 1.61 (0.97--2.66) for the first 2 years, and 0.98 (0.66--1.49) for the entire follow-up. The HRs were 0.54 (0.19--1.51) in women within 10 years after menopause, and 1.20 (0.78--1.84) in others (P value for interaction = 0.01). We also present comparisons between these estimates and previously reported Nurses' Health Study estimates. Conclusions: Our findings suggest that the discrepancies between the Women's Health Initiative and Nurses' Health Study ITT estimates could be largely explained by differences in the distribution of time since menopause and length of follow-up.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hernán_et_al_2008_Observational_Studies_Analyzed_Like_Randomized_Experiments.pdf}
}

@article{hernanSpecifyingTargetTrial2016,
  title = {Specifying a Target Trial Prevents Immortal Time Bias and Other Self-Inflicted Injuries in Observational Analyses},
  author = {Hern{\'a}n, Miguel A. and Sauer, Brian C. and {Hern{\'a}ndez-D{\'i}az}, Sonia and Platt, Robert and Shrier, Ian},
  year = {2016},
  month = nov,
  journal = {Journal of Clinical Epidemiology},
  volume = {79},
  pages = {70--75},
  issn = {0895-4356},
  doi = {10/f9ff7n},
  urldate = {2021-08-17},
  abstract = {Many analyses of observational data are attempts to emulate a target trial. The emulation of the target trial may fail when researchers deviate from simple principles that guide the design and analysis of randomized experiments. We review a framework to describe and prevent biases, including immortal time bias, that result from a failure to align start of follow-up, specification of eligibility, and treatment assignment. We review some analytic approaches to avoid these problems in comparative effectiveness or safety research.},
  langid = {english},
  keywords = {Comparative effectiveness research,Immortal time bias,Observational,Selection bias,studies,Target trial,Time zero},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hernán et al_2016_Specifying a target trial prevents immortal time bias and other self-inflicted.pdf;/Users/nseewald/Zotero/storage/6T2HVWFJ/S0895435616301366.html}
}

@article{hernanTargetTrialEmulation2022,
  title = {Target {{Trial Emulation}}: {{A Framework}} for {{Causal Inference From Observational Data}}},
  shorttitle = {Target {{Trial Emulation}}},
  author = {Hern{\'a}n, Miguel A. and Wang, Wei and Leaf, David E.},
  year = {2022},
  month = dec,
  journal = {JAMA},
  volume = {328},
  number = {24},
  pages = {2446--2447},
  issn = {0098-7484},
  doi = {10.1001/jama.2022.21383},
  urldate = {2023-07-28},
  abstract = {Quantifying the effect of a treatment on a clinical outcome---causal inference---requires the comparison of outcomes under different courses of action. For example, to quantify the effect of tocilizumab on mortality in critically ill patients with COVID-19, the mortality risk could be compared between a group of patients administered tocilizumab and a group who are not. Ideally, eligible patients would be assigned to these groups at random. The key advantage of such a randomized trial is that both groups are expected to be comparable, and thus any differences in mortality can be attributed to tocilizumab rather than to prognostic differences between the groups.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hernán et al_2022_Target Trial Emulation.pdf;/Users/nseewald/Zotero/storage/8MLA7V4X/2799678.html}
}

@article{hernanUsingBigData2016,
  title = {Using {{Big Data}} to {{Emulate}} a {{Target Trial When}} a {{Randomized Trial Is Not Available}}},
  author = {Hern{\'a}n, Miguel A. and Robins, James M.},
  year = {2016},
  month = apr,
  journal = {American Journal of Epidemiology},
  volume = {183},
  number = {8},
  pages = {758--764},
  issn = {0002-9262},
  doi = {10/f8m6xn},
  urldate = {2021-07-02},
  abstract = {Ideally, questions about comparative effectiveness or safety would be answered using an appropriately designed and conducted randomized experiment. When we cannot conduct a randomized experiment, we analyze observational data. Causal inference from large observational databases (big data) can be viewed as an attempt to emulate a randomized experiment---the target experiment or target trial---that would answer the question of interest. When the goal is to guide decisions among several strategies, causal analyses of observational data need to be evaluated with respect to how well they emulate a particular target trial. We outline a framework for comparative effectiveness research using big data that makes the target trial explicit. This framework channels counterfactual theory for comparing the effects of sustained treatment strategies, organizes analytic approaches, provides a structured process for the criticism of observational studies, and helps avoid common methodologic pitfalls.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hernán_Robins_2016_Using Big Data to Emulate a Target Trial When a Randomized Trial Is Not.pdf;/Users/nseewald/Zotero/storage/J4KCJQTL/1739860.html}
}

@article{hertzEvidenceAssociationSNPs2016,
  title = {Evidence for Association of {{SNPs}} in {{ABCB1}} and {{CBR3}}, but Not {{RAC2}}, {{NCF4}}, {{SLC28A3}} or {{TOP2B}}, with Chronic Cardiotoxicity in a Cohort of Breast Cancer Patients Treated with Anthracyclines.},
  author = {Hertz, Daniel L. and Caram, Megan V. and Kidwell, Kelley M. and Thibert, Jacklyn N. and Gersch, Christina and Seewald, Nicholas J. and Smerage, Jeffrey and Rubenfire, Melvyn and Henry, N. Lynn and Cooney, Kathleen A. and Leja, Monika and Griggs, Jennifer J. and Rae, James M.},
  year = {2016},
  journal = {Pharmacogenomics},
  volume = {17},
  number = {3},
  eprint = {26799497},
  eprinttype = {pubmed},
  pages = {231--240},
  issn = {1744-8042},
  doi = {10.2217/pgs.15.162},
  abstract = {AIMS: Validation of associations for SNPs in RAC2, NCF4 and SLC28A3, identification of a novel association with a TOP2B SNP and screening 23 SNPs putatively relevant to anthracycline-induced cardiotoxicity.\${\textbackslash}backslash\$n\${\textbackslash}backslash\$nPATIENTS \& METHODS: A total of 166 breast cancer patients treated with doxorubicin underwent echocardiogram, including 19 cases with systolic dysfunction (ejection fraction {\textbackslash}textless55\%) and 147 controls. Four high priority SNPs were tested in the primary analysis, with appropriate statistical correction, and 23 additional SNPs were screened in an uncorrected secondary analysis.\${\textbackslash}backslash\$n\${\textbackslash}backslash\$nRESULTS: Previously reported associations for RAC2, NCF4 and SLC28A3 could not be validated and a novel association with TOP2B was not discovered in this cohort (all p {\textbackslash}textgreater 0.05), likely due to inadequate power. Two SNPs were identified in the uncorrected secondary analysis including a protective SNP in ABCB1 (3435C{\textbackslash}textgreaterT, p = 0.049) and a risk allele in CBR3 (V244M, p = 0.012).\${\textbackslash}backslash\$n\${\textbackslash}backslash\$nCONCLUSION: The associations reported in prior publications and those discovered in this secondary analysis require further replication in independent cohorts.},
  copyright = {All rights reserved},
  pmid = {26799497},
  keywords = {Adult,article,breast cancer,cancer patient,cancer staging,carbonyl reductase,carbonyl reductase 3,cardiotoxicity,cell protein,cohort analysis,concentrative nucleoside transporter 3,controlled study,cross-sectional study,DNA topoisomerase (ATP hydrolysing),DNA topoisomerase (ATP hydrolysing) beta,doxorubicin,echocardiography,Female,genetic analysis,genetic association,genotype,heart ejection fraction,human,major clinical study,multidrug resistance protein 1,observational study,pharmacogenetics,protein NCF4,Rac2 protein,secondary analysis,single nucleotide polymorphism,systolic dysfunction,unclassified drug},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hertz et al_2016_Evidence for association of SNPs in ABCB1 and CBR3, but not RAC2, NCF4, SLC28A3.pdf}
}

@article{hertzPolymorphismsDrugmetabolizingEnzymes2016,
  title = {Polymorphisms in Drug-Metabolizing Enzymes and Steady-State Exemestane Concentration in Postmenopausal Patients with Breast Cancer},
  author = {Hertz, D L and Kidwell, K M and Seewald, Nicholas J and Gersch, C L and Desta, Z and Flockhart, D A and Storniolo, A-M and Stearns, V and Skaar, T C and Hayes, D F and Henry, N L and Rae, J M},
  year = {2016},
  journal = {The Pharmacogenomics Journal},
  number = {November 2015},
  pages = {1--7},
  issn = {1470-269X},
  doi = {10.1038/tpj.2016.60},
  copyright = {All rights reserved},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hertz et al_2016_Polymorphisms in drug-metabolizing enzymes and steady-state exemestane.pdf}
}

@article{hibbardLIBERTISMARTStudy2018,
  title = {{{LIBERTI}}: {{A SMART}} Study in Plastic Surgery},
  shorttitle = {{{LIBERTI}}},
  author = {Hibbard, Jonathan C and Friedstat, Jonathan S and Thomas, Sonia M and Edkins, Renee E and Hultman, C Scott and Kosorok, Michael R},
  year = {2018},
  month = jun,
  journal = {Clinical Trials},
  volume = {15},
  number = {3},
  pages = {286--293},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774518762435},
  urldate = {2018-10-12},
  abstract = {Background/aims: Laser treatment of burns scars is considered by some providers to be standard of care. However, there is little evidence-based research as to the true benefit. A number of factors hinder evaluation of the benefit of laser treatment. These include significant heterogeneity in patient response and possible delayed effects from the laser treatment. Moreover, laser treatments are often provided sequentially using different types of equipment and settings, so there are effectively a large number of overall treatment options that need to be compared. We propose a trial capable of coping with these issues and that also attempts to take advantage of the heterogeneous response in order to estimate optimal treatment plans personalized to each individual patient. It will be the first large-scale randomized trial to compare the effectiveness of laser treatments for burns scars and, to our knowledge, the very first example of the utility of a Sequential Multiple Assignment Randomized Trial in plastic surgery. Methods: We propose using a Sequential Multiple Assignment Randomized Trial design to investigate the effect of various permutations of laser treatment on hypertrophic burn scars. We will compare and test hypotheses regarding laser treatment effects at a general population level. Simultaneously, we hope to use the data generated to discover possible beneficial personalized treatment plans, tailored to individual patient characteristics. Results: We show that the proposed trial has good power to detect laser treatment effect at the overall population level, despite comparing a large number of treatment combinations. The trial will simultaneously provide high-quality data appropriate for estimating precision-medicine treatment rules. We detail population-level comparisons of interest and corresponding sample size calculations. We provide simulations to suggest the power of the trial to detect laser effect and also the possible benefits of personalization of laser treatment to individual characteristics. Conclusion: We propose, to our knowledge, the first use of a Sequential Multiple Assignment Randomized Trial in surgery. The trial is rigorously designed so that it is reasonably straightforward to implement and powered to answer general overall questions of interest. The trial is also designed to provide data that are suitable for the estimation of beneficial precision-medicine treatment rules that depend both on individual patient characteristics and on-going real-time patient response to treatment.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hibbard et al_2018_LIBERTI.pdf}
}

@article{hiekeConditionalSurvivalUseful2015,
  title = {Conditional {{Survival}}: {{A Useful Concept}} to {{Provide Information}} on {{How Prognosis Evolves}} over {{Time}}},
  shorttitle = {Conditional {{Survival}}},
  author = {Hieke, Stefanie and Kleber, Martina and K{\"o}nig, Christine and Engelhardt, Monika and Schumacher, Martin},
  year = {2015},
  month = mar,
  journal = {Clinical Cancer Research},
  volume = {21},
  number = {7},
  pages = {1530--1536},
  issn = {1078-0432},
  doi = {10.1158/1078-0432.CCR-14-2154},
  urldate = {2024-02-03},
  abstract = {Conditional survival (CS) is defined as the probability of surviving further t years, given that a patient has already survived s years after the diagnosis of a chronic disease. It is the simplest form of a dynamic prediction in which other events in the course of the disease or biomarker values measured up to time s can be incorporated. CS has attracted attention in recent years either in an absolute or relative form where the latter is based on a comparison with an age-adjusted normal population being highly relevant from a public health perspective. In its absolute form, CS constitutes the quantity of major interest in a clinical context. Given a clinical cohort of patients with a particular type of cancer, absolute CS can be estimated by conditional Kaplan--Meier estimates in strata defined, for example, by age and disease stage or by a conditional version of the Cox and other regression models for time-to-event data. CS can be displayed as a function of the prediction time s in parametric as well as nonparametric fashion. We illustrate the use of absolute CS in a large clinical cohort of patients with multiple myeloma. For investigating CS, it is necessary to ensure almost complete long-term follow-up of the patients enrolled in the clinical cohort and to consider potential age--stage migration as well as changing treatment modalities over time. CS provides valuable and relevant information on how prognosis develops over time. It also serves as a starting point for identifying factors related to long-term survival. Clin Cancer Res; 21(7); 1530--6. {\copyright}2015 AACR.},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hieke_et_al_2015_Conditional_Survival.pdf;/Users/nseewald/Zotero/storage/NSLZHH5M/Conditional-Survival-A-Useful-Concept-to-Provide.html}
}

@article{hinkleyEfficiencyRobustStandard1991,
  title = {Efficiency of Robust Standard Errors for Regression Coefficients},
  author = {Hinkley, D.V. and Wang, S.},
  year = {1991},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {20},
  number = {1},
  pages = {1--11},
  issn = {0361-0926, 1532-415X},
  doi = {10/c4vqwn},
  urldate = {2019-08-10},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hinkley_Wang_1991_Efficiency of robust standard errors for regression coefficients.pdf}
}

@article{hokstadFailureIntensityProcess1997,
  title = {The Failure Intensity Process and the Formulation of Reliability and Maintenance Models},
  author = {Hokstad, Per},
  year = {1997},
  month = oct,
  journal = {Reliability Engineering \& System Safety},
  volume = {58},
  number = {1},
  pages = {69--82},
  issn = {09518320},
  doi = {10.1016/S0951-8320(97)00053-7},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hokstad_1997_The failure intensity process and the formulation of reliability and.pdf}
}

@article{hoMatchingNonparametricPreprocessing2007,
  title = {Matching as {{Nonparametric Preprocessing}} for {{Reducing Model Dependence}} in {{Parametric Causal Inference}}},
  author = {Ho, Daniel E. and Imai, Kosuke and King, Gary and Stuart, Elizabeth A.},
  year = {2007/ed},
  journal = {Political Analysis},
  volume = {15},
  number = {3},
  pages = {199--236},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10/ctfsg5},
  urldate = {2021-07-20},
  abstract = {Although published works rarely include causal estimates from more than a few model specifications, authors usually choose the presented estimates from numerous trial runs readers never see. Given the often large variation in estimates across choices of control variables, functional forms, and other modeling assumptions, how can researchers ensure that the few estimates presented are accurate or representative? How do readers know that publications are not merely demonstrations that it is possible to find a specification that fits the author's favorite hypothesis? And how do we evaluate or even define statistical properties like unbiasedness or mean squared error when no unique model or estimator even exists? Matching methods, which offer the promise of causal inference with fewer assumptions, constitute one possible way forward, but crucial results in this fast-growing methodological literature are often grossly misinterpreted. We explain how to avoid these misinterpretations and propose a unified approach that makes it possible for researchers to preprocess data with matching (such as with the easy-to-use software we offer) and then to apply the best parametric techniques they would have used anyway. This procedure makes parametric models produce more accurate and considerably less model-dependent causal inferences.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ho et al_2007_Matching as Nonparametric Preprocessing for Reducing Model Dependence in.pdf;/Users/nseewald/Zotero/storage/7YQDS5AE/4D7E6D07C9727F5A604E5C9FCCA2DD21.html}
}

@misc{HomePerelmanSchool,
  title = {Home {\textbar} {{Perelman School}} of {{Medicine}} {\textbar} {{Perelman School}} of {{Medicine}} at the {{University}} of {{Pennsylvania}}},
  urldate = {2024-11-20},
  howpublished = {https://www.med.upenn.edu/psom/}
}

@article{hoodAssociationBRCA122024,
  title = {Association of {{BRCA1}}/2 Pathogenic Variants with Primary Tumor Location and Metastatic Organotropism in Pancreatic Adenocarcinoma.},
  author = {Hood, Ryan and Rathore, Srishti and Seewald, Nicholas J. and Reiss, Kim Anna},
  year = {2024},
  month = jun,
  journal = {Journal of Clinical Oncology},
  volume = {42},
  number = {16\_suppl},
  pages = {4151--4151},
  publisher = {Wolters Kluwer},
  issn = {0732-183X},
  doi = {10.1200/JCO.2024.42.16_suppl.4151},
  urldate = {2024-11-22},
  abstract = {4151 Background: Pancreatic cancers (PDACs) with BRCA1/2 pathogenic variants (PVs) are prognostically favorable and predict response to platinum-based treatment. Recent data suggest that these mutations may be more strongly associated with cancers arising from the body/tail (B/T) compared to the head of the pancreas (HOP). Primary tumors of the B/T are known to frequently metastasize to the peritoneum, which is thought to be largely due to anatomic location. We hypothesized that BRCA1/2 PVs may interact with tumor location in determining sites of metastatic spread. Methods: We conducted a retrospective study using a database of patients with BRCA1/2 PVs and PDAC as well as wildtype (WT) control patients matched for age, sex, year and stage at diagnosis. Demographic and clinical data were compared by Fisher's exact test. Adjusted odds ratios for each site of metastasis or somatic variant were estimated using logistic regression with backwards stepwise AIC minimization. Cases with diagnosed metastases and available next generation somatic sequencing were included in the respective logistic regression models. Results: Patients with BRCA1/2 PVs were more likely to have masses in the B/T compared to control patients [51.6\% vs 31.0\%; p-value {$<$} 0.001]. After adjusting for age, sex, platinum-based chemotherapy, curative-intent surgical resection, and primary location, patients with BRCA1/2 PVs were less likely to develop peritoneal metastases compared to control patients (see table, row 3). Additionally, for patients with B/T masses, the rate of peritoneal spread was significantly lower in those with BRCA1/2PVs compared to those without (OR: 0.41 [0.20, 0.83]). We did not observe a significant association between BRCA1/2 PV and peritoneal spread in patients with HOP masses (OR: 1.09 [0.48, 2.5]). Furthermore, patients with BRCA1/2PVs were significantly less likely to have a TP53 mutation compared to controls (51\% vs 71\%; p = 0.038). This association persisted after adjusting for the variables described above (see table, row 3). No significant differences were found for mutations in KRAS (83\% vs 86\%; p = 0.60), SMAD4 (16\% vs 13\%; p = 0.59), or CDKN2A (19\% vs 29\%; p = 0.13). Conclusions: Our data suggest that patients with BRCA1/2-related PDAC are more likely to present with B/T masses compared to WT patients, are less likely to have TP53 mutations and are less likely to have peritoneal spread of disease. These data support that the prognostic benefit of BRCA-related PDAC may be linked to a favorable somatic mutation profile. ~	Liver	Lungs	Peritoneum	TP53 OR1 [CI] Group	1.64 [0.93, 2.95]	1.00 [0.61, 1.62]	0.79 [0.48, 1.28]	0.50 [0.25, 0.96] OR2 [CI] Model 1 + Age, Sex, Platinum, Surgery	1.56 [0.84, 2.94]	0.97 [0.59, 1.59]	0.76 [0.46, 1.24]	0.50 [0.25, 0.96] OR3 [CI] Model 2 + Primary Location	1.56 [0.84, 2.94]	0.92 [0.55, 1.50]	0.43 [0.21, 0.89]	0.25 [0.07, 0.74] P-value OR3	0.161	0.728	0.025	0.018 EXPAND TABLE OPEN IN VIEWER}
}

@article{hortonMuchAdoNothing2007,
  title = {Much {{Ado About Nothing}}: {{A Comparison}} of {{Missing Data Methods}} and {{Software}} to {{Fit Incomplete Data Regression Models}}},
  author = {Horton, Nicholas J and Kleinman, Ken P},
  year = {2007},
  month = feb,
  journal = {The American Statistician},
  volume = {61},
  number = {1},
  pages = {79--90},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/000313007X172556},
  urldate = {2024-01-05},
  abstract = {Missing data are a recurring problem that can cause bias or lead to inefficient analyses. Statistical methods to address missingness have been actively pursued in recent years, including imputation, likelihood, and weighting approaches. Each approach is more complicated when there are many patterns of missing values, or when both categorical and continuous random variables are involved. Implementations of routines to incorporate observations with incomplete variables in regression models are now widely available. We review these routines in the context of a motivating example from a large health services research dataset. While there are still limitations to the current implementations, and additional efforts are required of the analyst, it is feasible to incorporate partially observed values, and these methods should be used in practice.},
  pmid = {17401454},
  keywords = {_tablet,Conditional Gaussian,Health services research,Maximum likelihood,Multiple imputation,Psychiatric epidemiology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Horton_Kleinman_2007_Much_Ado_About_Nothing.pdf}
}

@article{howardTableDiscussionBayesian1998,
  title = {The 2 {\texttimes} 2 {{Table}}: {{A Discussion}} from a {{Bayesian Viewpoint}}},
  author = {Howard, J V},
  year = {1998},
  journal = {Statistical Science},
  volume = {13},
  number = {4},
  pages = {351--367},
  doi = {10.1214/ss/1028905830},
  abstract = {The 2 {\texttimes} 2 table is used as a vehicle for discussing different approaches to statistical inference. Several of these approaches (both classical and Bayesian) are compared, and difficulties with them are highlighted. More frequent use of one-sided tests is advocated. Given independent samples from two binomial distributions, and taking independent Jeffreys priors, we note that the posterior probability that the proportion of successes in the first population is larger than in the second can be estimated from the standard (uncorrected) chi-square significance level. An exact formula for this probability is derived. However, we argue that usually it will be more appropriate to use dependent priors, and we suggest a particular ``standard prior'' for the 2 {\texttimes} 2 table. For small numbers of observations this is more conservative than Fisher's exact test, but it is less conservative for larger sample sizes. Several examples are given.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Howard_1998_The 2 × 2 Table.pdf}
}

@article{howeLimitationInverseProbabilityofCensoring2011,
  title = {Limitation of {{Inverse Probability-of-Censoring Weights}} in {{Estimating Survival}} in the {{Presence}} of {{Strong Selection Bias}}},
  author = {Howe, Chanelle J. and Cole, Stephen R. and Chmiel, Joan S. and Mu{\~n}oz, Alvaro},
  year = {2011},
  month = mar,
  journal = {American Journal of Epidemiology},
  volume = {173},
  number = {5},
  pages = {569--577},
  issn = {0002-9262},
  doi = {10.1093/aje/kwq385},
  urldate = {2024-07-18},
  abstract = {In time-to-event analyses, artificial censoring with correction for induced selection bias using inverse probability-of-censoring weights can be used to 1) examine the natural history of a disease after effective interventions are widely available, 2) correct bias due to noncompliance with fixed or dynamic treatment regimens, and 3) estimate survival in the presence of competing risks. Artificial censoring entails censoring participants when they meet a predefined study criterion, such as exposure to an intervention, failure to comply, or the occurrence of a competing outcome. Inverse probability-of-censoring weights use measured common predictors of the artificial censoring mechanism and the outcome of interest to determine what the survival experience of the artificially censored participants would be had they never been exposed to the intervention, complied with their treatment regimen, or not developed the competing outcome. Even if all common predictors are appropriately measured and taken into account, in the context of small sample size and strong selection bias, inverse probability-of-censoring weights could fail because of violations in assumptions necessary to correct selection bias. The authors used an example from the Multicenter AIDS Cohort Study, 1984--2008, regarding estimation of long-term acquired immunodeficiency syndrome-free survival to demonstrate the impact of violations in necessary assumptions. Approaches to improve correction methods are discussed.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Howe et al_2011_Limitation of Inverse Probability-of-Censoring Weights in Estimating Survival.pdf;/Users/nseewald/Zotero/storage/NHI7B24T/88993.html}
}

@article{hsuSampleSizeComputation1988,
  title = {Sample Size Computation for Designing Multiple Comparison Experiments},
  author = {Hsu, Jason C.},
  year = {1988},
  month = aug,
  journal = {Computational Statistics \& Data Analysis},
  volume = {7},
  number = {1},
  pages = {79--91},
  issn = {01679473},
  doi = {10.1016/0167-9473(88)90017-5},
  urldate = {2022-04-01},
  abstract = {This article considers sample size computation for designing multiple comparisons experiments. We propose sample size should be computed so that multiple comparisons confidence intervals will cover the true parameters and be sufficiently narrow with a guaranteed high probability. Appropriate formulas and computer implementation are provided for Tukey's method of all-pairwise multiple comparisons (MCA), multiple comparisons with the best (MCB) as proposed by the author, and Dunnett's method of multiple comparisons with a control (MCC). Our sample size computation is then compared with the usual computation based on the power of the F-test. An advantage of our method over the usual power-of-test method is that our method guarantees a high probability of correct multiple comparisons inference, while the latter does not, because the probability of rejecting a false null hypothesis includes the probability of directional error (inferring a treatment to be better than another when it is in fact worse). The graphical nature of our computer implementation also makes sensitivity analysis immediate.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hsu_1988_Sample size computation for designing multiple comparison experiments.pdf}
}

@article{huangStudyGeneralizedSkewnormal2013,
  title = {A Study of Generalized Skew-Normal Distribution},
  author = {Huang, Wen-Jang and Su, Nan-Cheng and Gupta, Arjun K.},
  year = {2013},
  month = oct,
  journal = {Statistics},
  volume = {47},
  number = {5},
  pages = {942--953},
  issn = {0233-1888},
  doi = {10.1080/02331888.2012.697164},
  urldate = {2019-01-10},
  abstract = {Following the paper by Genton and Loperfido [Generalized skew-elliptical distributions and their quadratic forms, Ann. Inst. Statist. Math. 57 (2005), pp. 389--401], we say that Z has a generalized skew-normal distribution, if its probability density function (p.d.f.) is given by f(z)=2{$\varphi$} p (z; {$\xi$}, {\textohm}){$\pi$} (z-{$\xi$}), z{$\in\mathbb{R}$} p , where {$\varphi$} p ({$\cdot$}; {$\xi$}, {\textohm}) is the p-dimensional normal p.d.f. with location vector {$\xi$} and scale matrix {\textohm}, {$\xi\in\mathbb{R}$} p , {\textohm}{$>$}0, and {$\pi$} is a skewing function from {$\mathbb{R}$} p to {$\mathbb{R}$}, that is 0{$\leq\pi$} (z){$\leq$}1 and {$\pi$} (-z)=1-{$\pi$} (z), {$\forall$} z{$\in\mathbb{R}$} p . First the distribution of linear transformations of Z are studied, and some moments of Z and its quadratic forms are derived. Next we obtain the joint moment-generating functions (m.g.f.'s) of linear and quadratic forms of Z and then investigate conditions for their independence. Finally explicit forms for the above distributions, m.g.f.'s and moments are derived when {$\pi$} (z)={$\kappa$} ({$\alpha\prime$}z), where {$\alpha\in\mathbb{R}$} p and {$\kappa$} is the normal, Laplace, logistic or uniform distribution function.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Huang et al_2013_A study of generalized skew-normal distribution.pdf;/Users/nseewald/Zotero/storage/9VDE6VJT/02331888.2012.html}
}

@article{huBiasDiversityII,
  title = {Bias and {{Diversity II}}: {{Fairness}} at {{Equilibrium}} in the {{Labor Market}}},
  author = {Hu, Lily},
  pages = {10},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hu_Bias and Diversity II.pdf}
}

@article{huDisparateEffectsStrategic2018,
  title = {The {{Disparate Effects}} of {{Strategic Manipulation}}},
  author = {Hu, Lily and Immorlica, Nicole and Vaughan, Jennifer Wortman},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.08646 [cs, stat]},
  eprint = {1808.08646},
  primaryclass = {cs, stat},
  urldate = {2018-10-22},
  abstract = {When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Previous models of agent responsiveness, termed ``strategic manipulation,'' have analyzed the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to ``trick'' a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm, is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to better capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of potential interventions in which a learner can subsidize members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off---even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's ``quality'' when agents' capacities to adaptively respond differ.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hu et al_2018_The Disparate Effects of Strategic Manipulation.pdf}
}

@article{huEstimatingCausalEffects2023,
  title = {Estimating the Causal Effects of Multiple Intermittent Treatments with Application to {{COVID-19}}},
  author = {Hu, Liangyuan and Ji, Jiayi and Joshi, Himanshu and Scott, Erick R and Li, Fan},
  year = {2023},
  month = nov,
  journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume = {72},
  number = {5},
  pages = {1162--1186},
  issn = {0035-9254},
  doi = {10.1093/jrsssc/qlad076},
  urldate = {2024-12-12},
  abstract = {To draw real-world evidence about the comparative effectiveness of multiple time-varying treatments on patient survival, we develop a joint marginal structural survival model and a novel weighting strategy to account for time-varying confounding and censoring. Our methods formulate complex longitudinal treatments with multiple start/stop switches as the recurrent events with discontinuous intervals of treatment eligibility. We derive the weights in continuous time to handle a complex longitudinal data set without the need to discretise or artificially align the measurement times. We further use machine learning models designed for censored survival data with time-varying covariates and the kernel function estimator of the baseline intensity to efficiently estimate the continuous-time weights. Our simulations demonstrate that the proposed methods provide better bias reduction and nominal coverage probability when analysing observational longitudinal survival data with irregularly spaced time intervals, compared to conventional methods that require aligned measurement time points. We apply the proposed methods to a large-scale COVID-19 data set to estimate the causal effects of several COVID-19 treatments on the composite of in-hospital mortality and intensive care unit (ICU) admission relative to findings from randomised trials.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hu et al. - 2023 - Estimating the causal effects of multiple intermittent treatments with application to COVID-19.pdf;/Users/nseewald/Zotero/storage/C9XZGPGV/7249916.html}
}

@article{huiEvaluationDiagnosticTests1998,
  title = {Evaluation of Diagnostic Tests without Gold Standards},
  author = {Hui, Siu L and Zhou, Xiao H},
  year = {1998},
  month = aug,
  journal = {Statistical Methods in Medical Research},
  volume = {7},
  number = {4},
  pages = {354--370},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/096228029800700404},
  urldate = {2025-01-27},
  abstract = {This paper reviews statistical methods developed to estimate the sensitivity and specificity of screening or diagnostic tests when the fallible tests are not evaluated against a gold standard. It gives a brief summary of the earlier historical developments and focuses on the more recent methods. It covers Bayesian approaches and longitudinal studies with repeated testing. In particular, it reviews the procedures that do not require the assumption of independence between tests conditional on the true disease status.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hui and Zhou - 1998 - Evaluation of diagnostic tests without gold standards.pdf}
}

@article{huitfeldtComparativeEffectivenessResearch2016,
  title = {Comparative {{Effectiveness Research Using Observational Data}}: {{Active Comparators}} to {{Emulate Target Trials}} with {{Inactive Comparators}}},
  shorttitle = {Comparative {{Effectiveness Research Using Observational Data}}},
  author = {Huitfeldt, Anders and Hernan, Miguel A. and Kalager, Mette and Robins, James M.},
  year = {2016},
  month = oct,
  journal = {eGEMs},
  volume = {4},
  number = {1},
  pages = {1234},
  publisher = {Ubiquity Press},
  issn = {2327-9214},
  doi = {10/gjzmxz},
  urldate = {2021-07-28},
  abstract = {Introduction: Because a comparison of noninitiators and initiators of treatment may be hopelessly confounded, guidelines for the conduct of observational research often recommend using an ``active'' comparator group consisting of people who initiate a treatment other than the medication of interest. In this paper, we discuss the conditions under which this approach is valid if the goal is to emulate a trial with an inactive comparator. Identification of Effects: We provide conditions under which a target trial in a subpopulation can be validly emulated from observational data, using an active comparator that is known or believed to be inactive for the outcome of interest. The average treatment effect in the population as a whole is not identified, but under certain conditions this approach can be used to emulate a trial in the subset of individuals who were treated with the treatment of interest, in the subset of individuals who were treated with the treatment of interest but not with the comparator, or in the subset of individuals who were treated with both the treatment of interest and the active comparator. The Plausibility of the Comparability Conditions: We discuss whether the required conditions can be expected to hold in pharmacoepidemiologic research, with a particular focus on whether the conditions are plausible in situations where the standard analysis fails due to unmeasured confounding by access to health care or health seeking behaviors. Discussion: The conditions discussed in this paper may at best be approximately true. Investigators using active comparator designs to emulate trials with inactive comparators should exercise caution.},
  pmcid = {PMC5108633},
  pmid = {27891526},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Huitfeldt et al_2016_Comparative Effectiveness Research Using Observational Data.pdf;G\:\\My Drive\\Papers\\Huitfeldt_et_al_2016_Comparative_Effectiveness_Research_Using_Observational_Data2.pdf;/Users/nseewald/Zotero/storage/4WKG7ZMI/2327-9214.1234.html}
}

@article{hulmeChallengesEstimatingEffectiveness2023,
  title = {Challenges in {{Estimating}} the {{Effectiveness}} of {{COVID-19 Vaccination Using Observational Data}}},
  author = {Hulme, William J. and Williamson, Elizabeth and Horne, Elsie M.F. and Green, Amelia and McDonald, Helen I. and Walker, Alex J. and Curtis, Helen J. and Morton, Caroline E. and MacKenna, Brian and Croker, Richard and Mehrkar, Amir and Bacon, Seb and Evans, David and Inglesby, Peter and Davy, Simon and Bhaskaran, Krishnan and Schultze, Anna and Rentsch, Christopher T. and Tomlinson, Laurie and Douglas, Ian J. and Evans, Stephen J.W. and Smeeth, Liam and Palmer, Tom and Goldacre, Ben and Hern{\'a}n, Miguel A. and Sterne, Jonathan A.C.},
  year = {2023},
  month = may,
  journal = {Annals of Internal Medicine},
  volume = {176},
  number = {5},
  pages = {685--693},
  publisher = {American College of Physicians},
  issn = {0003-4819},
  doi = {10.7326/M21-4269},
  urldate = {2023-11-15},
  abstract = {The COVID-19 vaccines were developed and rigorously evaluated in randomized trials during 2020. However, important questions, such as the magnitude and duration of protection, their effectiveness against new virus variants, and the effectiveness of booster vaccination, could not be answered by randomized trials and have therefore been addressed in observational studies. Analyses of observational data can be biased because of confounding and because of inadequate design that does not consider the evolution of the pandemic over time and the rapid uptake of vaccination. Emulating a hypothetical ``target trial'' using observational data assembled during vaccine rollouts can help manage such potential sources of bias. This article describes 2 approaches to target trial emulation. In the sequential approach, on each day, eligible persons who have not yet been vaccinated are matched to a vaccinated person. The single-trial approach sets a single baseline at the start of the rollout and considers vaccination as a time-varying variable. The nature of the confounding depends on the analysis strategy: Estimating ``per-protocol'' effects (accounting for vaccination of initially unvaccinated persons after baseline) may require adjustment for both baseline and ``time-varying'' confounders. These issues are illustrated by using observational data from 2\,780\,931 persons in the United Kingdom aged 70 years or older to estimate the effect of a first dose of a COVID-19 vaccine. Addressing the issues discussed in this article should help authors of observational studies provide robust evidence to guide clinical and policy decisions.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hulme et al_2023_Challenges in Estimating the Effectiveness of COVID-19 Vaccination Using.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hulme et al_2023_Challenges in Estimating the Effectiveness of COVID-19 Vaccination Using2.pdf;/Users/nseewald/Zotero/storage/R3SRBNKE/M21-4269.html}
}

@article{huShorttermInterventionLongterm2018,
  title = {A {{Short-term Intervention}} for {{Long-term Fairness}} in the {{Labor Market}}},
  author = {Hu, Lily and Chen, Yiling},
  year = {2018},
  journal = {Proceedings of the 2018 World Wide Web Conference on World Wide Web  - WWW '18},
  eprint = {1712.00064},
  pages = {1389--1398},
  doi = {10.1145/3178876.3186044},
  urldate = {2018-10-22},
  abstract = {The persistence of racial inequality in the U.S. labor market against a general backdrop of formal equality of opportunity is a troubling phenomenon that has significant ramifications on the design of hiring policies. In this paper, we show that current group disparate outcomes may be immovable even when hiring decisions are bound by an input-output notion of ``individual fairness.'' Instead, we construct a dynamic reputational model of the labor market that illustrates the reinforcing nature of asymmetric outcomes resulting from groups' divergent accesses to resources and as a result, investment choices. To address these disparities, we adopt a dual labor market composed of a Temporary Labor Market (TLM), in which firms' hiring strategies are constrained to ensure statistical parity of workers granted entry into the pipeline, and a Permanent Labor Market (PLM), in which firms hire top performers as desired. Individual worker reputations produce externalities for their group; the corresponding feedback loop raises the collective reputation of the initially disadvantaged group via a TLM fairness intervention that need not be permanent. We show that such a restriction on hiring practices induces an equilibrium that, under particular market conditions, Pareto-dominates those arising from strategies that statistically discriminate or employ a ``group-blind'' criterion. The enduring nature of equilibria that are both inequitable and Pareto suboptimal suggests that fairness interventions beyond procedural checks of hiring decisions will be of critical importance in a world where machines play a greater role in the employment process.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hu_Chen_2018_A Short-term Intervention for Long-term Fairness in the Labor Market.pdf}
}

@article{husseyDesignAnalysisStepped2007,
  title = {Design and Analysis of Stepped Wedge Cluster Randomized Trials},
  author = {Hussey, Michael A. and Hughes, James P.},
  year = {2007},
  month = feb,
  journal = {Contemporary Clinical Trials},
  volume = {28},
  number = {2},
  pages = {182--191},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2006.05.007},
  urldate = {2024-10-21},
  abstract = {Cluster randomized trials (CRT) are often used to evaluate therapies or interventions in situations where individual randomization is not possible or not desirable for logistic, financial or ethical reasons. While a significant and rapidly growing body of literature exists on CRTs utilizing a ``parallel'' design (i.e. I clusters randomized to each treatment), only a few examples of CRTs using crossover designs have been described. In this article we discuss the design and analysis of a particular type of crossover CRT -- the stepped wedge -- and provide an example of its use.},
  keywords = {Cluster randomized trial,Prevention trials,Stepped wedge design},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hussey and Hughes - 2007 - Design and analysis of stepped wedge cluster randomized trials.pdf;/Users/nseewald/Zotero/storage/L8INERDJ/S1551714406000632.html}
}

@article{huWelfareDistributionalImpacts,
  title = {Welfare and {{Distributional Impacts}} of {{Fair Classification}}},
  author = {Hu, Lily and Chen, Yiling},
  pages = {5},
  abstract = {Current methodologies in machine learning analyze the effects of various statistical parity notions of fairness primarily in light of their impacts on predictive accuracy and vendor utility loss. In this paper, we propose a new framework for interpreting the effects of fairness criteria by converting the constrained loss minimization problem into a social welfare maximization problem. This translation moves a classifier and its output into utility space where individuals, groups, and society atlarge experience different welfare changes due to classification assignments. Under this characterization, predictions and fairness constraints are seen as shaping societal welfare and distribution and revealing individuals' implied welfare weights in society---weights that may then be interpreted through a fairness lens. The social welfare formulation of the fairness problem brings to the fore concerns of distributive justice that have always had a central albeit more implicit role in standard algorithmic fairness approaches.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Hu_Chen_Welfare and Distributional Impacts of Fair Classification.pdf}
}

@article{imaiMatchingMethodsCausal2023,
  title = {Matching {{Methods}} for {{Causal Inference}} with {{Time-Series Cross-Sectional Data}}},
  author = {Imai, Kosuke and Kim, In Song and Wang, Erik H.},
  year = {2023},
  journal = {American Journal of Political Science},
  volume = {67},
  number = {3},
  pages = {587--605},
  issn = {1540-5907},
  doi = {10.1111/ajps.12685},
  urldate = {2023-12-11},
  abstract = {Matching methods improve the validity of causal inference by reducing model dependence and offering intuitive diagnostics. Although they have become a part of the standard tool kit across disciplines, matching methods are rarely used when analysing time-series cross-sectional data. We fill this methodological gap. In the proposed approach, we first match each treated observation with control observations from other units in the same time period that have an identical treatment history up to the prespecified number of lags. We use standard matching and weighting methods to further refine this matched set so that the treated and matched control observations have similar covariate values. Assessing the quality of matches is done by examining covariate balance. Finally, we estimate both short-term and long-term average treatment effects using the difference-in-differences estimator, accounting for a time trend. We illustrate the proposed methodology through simulation and empirical studies. An open-source software package is available for implementing the proposed methods.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Imai_et_al_2023_Matching_Methods_for_Causal_Inference_with_Time-Series_Cross-Sectional_Data.pdf;/Users/nseewald/Zotero/storage/JM88DTG9/ajps.html}
}

@article{imaiRobustEstimationInverse2015,
  title = {Robust {{Estimation}} of {{Inverse Probability Weights}} for {{Marginal Structural Models}}},
  author = {Imai, Kosuke and Ratkovic, Marc},
  year = {2015},
  journal = {Journal of the American Statistical Association},
  volume = {110},
  number = {June 2015},
  pages = {37--41},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.956872},
  abstract = {Marginal structural models (MSMs) are becoming increasingly popular among applied re- searchers as a tool to make causal inference from longitudinal data. Unlike standard regression models, MSMs can adjust for time-dependent observed confounders while avoiding post-treatment bias. Despite their theoretical appeal, a main practical challenge of MSMs is the difficulty in es- timating inverse probability weights. Previous studies have found that MSMs can be highly sensitive to model misspecification of treatment assignment model even when the number of time periods is moderate. The effect of misspecification can propagate across time periods because inverse probability weights used for MSMs are typically based on the product of propensity score estimated separately at each time period. To address this problem, we introduce the Covariate Balancing Propensity Score (CBPS) methodology, which estimates the inverse probability weights such that the resulting covariate balance is optimized. We provide a small scale simulation study, which suggests that the CBPS significantly improves the empirical performance of MSMs by making the treatment assignment model robust to misspecification.},
  keywords = {Causal inference,covariate balancing propensity score,inverse propensity score,observational studies,sequential ignorability,time-dependent treatments,weighting},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Imai_Ratkovic_2015_Robust Estimation of Inverse Probability Weights for Marginal Structural Models.pdf}
}

@article{imaiUseTwoWayFixed2021,
  title = {On the {{Use}} of {{Two-Way Fixed Effects Regression Models}} for {{Causal Inference}} with {{Panel Data}}},
  author = {Imai, Kosuke and Kim, In Song},
  year = {2021},
  month = jul,
  journal = {Political Analysis},
  volume = {29},
  number = {3},
  pages = {405--415},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10/gjgvvd},
  urldate = {2021-07-05},
  abstract = {The two-way linear fixed effects regression (2FE) has become a default method for estimating causal effects from panel data. Many applied researchers use the 2FE estimator to adjust for unobserved unit-specific and time-specific confounders at the same time. Unfortunately, we demonstrate that the ability of the 2FE model to simultaneously adjust for these two types of unobserved confounders critically relies upon the assumption of linear additive effects. Another common justification for the use of the 2FE estimator is based on its equivalence to the difference-in-differences estimator under the simplest setting with two groups and two time periods. We show that this equivalence does not hold under more general settings commonly encountered in applied research. Instead, we prove that the multi-period difference-in-differences estimator is equivalent to the weighted 2FE estimator with some observations having negative weights. These analytical results imply that in contrast to the popular belief, the 2FE estimator does not represent a design-based, nonparametric estimation strategy for causal inference. Instead, its validity fundamentally rests on the modeling assumptions.},
  langid = {english},
  keywords = {difference-in-differences,longitudinal data,matching,unobserved confounding,weighted least squares},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Imai_Kim_2021_On the Use of Two-Way Fixed Effects Regression Models for Causal Inference with.pdf;/Users/nseewald/Zotero/storage/EX4ZCAFX/F10006D0210407C5F9C7CAC1EEE3EF0D.html}
}

@article{imaiWhenShouldWe2019,
  title = {When {{Should We Use Unit Fixed Effects Regression Models}} for {{Causal Inference}} with {{Longitudinal Data}}?},
  author = {Imai, Kosuke and Kim, In Song},
  year = {2019},
  journal = {American Journal of Political Science},
  volume = {63},
  number = {2},
  pages = {467--490},
  issn = {1540-5907},
  doi = {10/gfwnpf},
  urldate = {2021-07-07},
  abstract = {Many researchers use unit fixed effects regression models as their default methods for causal inference with longitudinal data. We show that the ability of these models to adjust for unobserved time-invariant confounders comes at the expense of dynamic causal relationships, which are permitted under an alternative selection-on-observables approach. Using the nonparametric directed acyclic graph, we highlight two key causal identification assumptions of unit fixed effects models: Past treatments do not directly influence current outcome, and past outcomes do not affect current treatment. Furthermore, we introduce a new nonparametric matching framework that elucidates how various unit fixed effects models implicitly compare treated and control observations to draw causal inference. By establishing the equivalence between matching and weighted unit fixed effects estimators, this framework enables a diverse set of identification strategies to adjust for unobservables in the absence of dynamic causal relationships between treatment and outcome variables. We illustrate the proposed methodology through its application to the estimation of GATT membership effects on dyadic trade volume.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Imai_Kim_2019_When Should We Use Unit Fixed Effects Regression Models for Causal Inference.pdf;/Users/nseewald/Zotero/storage/PE9ZAJGD/ajps.html}
}

@article{imbensRegressionDiscontinuityDesigns2008,
  title = {Regression Discontinuity Designs: {{A}} Guide to Practice},
  shorttitle = {Regression Discontinuity Designs},
  author = {Imbens, Guido W. and Lemieux, Thomas},
  year = {2008},
  month = feb,
  journal = {Journal of Econometrics},
  series = {The Regression Discontinuity Design: {{Theory}} and Applications},
  volume = {142},
  number = {2},
  pages = {615--635},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2007.05.001},
  urldate = {2024-06-09},
  abstract = {In regression discontinuity (RD) designs for evaluating causal effects of interventions, assignment to a treatment is determined at least partly by the value of an observed covariate lying on either side of a fixed threshold. These designs were first introduced in the evaluation literature by Thistlewaite and Campbell [1960. Regression-discontinuity analysis: an alternative to the ex-post Facto experiment. Journal of Educational Psychology 51, 309--317] With the exception of a few unpublished theoretical papers, these methods did not attract much attention in the economics literature until recently. Starting in the late 1990s, there has been a large number of studies in economics applying and extending RD methods. In this paper we review some of the practical and theoretical issues in implementation of RD methods.},
  keywords = {Nonparametric estimation,Regression discontinuity,Treatment effects},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Imbens_Lemieux_2008_Regression_discontinuity_designs3.pdf;/Users/nseewald/Zotero/storage/PKVXRWTR/S0304407607001091.html}
}

@article{inczeHeterogeneousStateCannabis2021,
  title = {Heterogeneous {{State Cannabis Policies}}: {{Potential Implications}} for {{Patients}} and {{Health Care Professionals}}},
  shorttitle = {Heterogeneous {{State Cannabis Policies}}},
  author = {Incze, Michael A. and Kelley, A. Taylor and Singer, Phillip M.},
  year = {2021},
  month = dec,
  journal = {JAMA},
  volume = {326},
  number = {23},
  pages = {2363--2364},
  issn = {0098-7484},
  doi = {10.1001/jama.2021.21182},
  urldate = {2023-08-16},
  abstract = {As of November 2021, cannabis has become legalized for medical purposes in 36 states and the District of Columbia. Eighteen states also authorize nonmedical use of cannabis by adults. The rapid expansion of access to cannabis has coincided with increased use across a number of demographic groups. In 2019, an estimated 48 million people in the US reported using cannabis, a 60\% increase from 2002. Among adults aged 65 years and older, the prevalence of cannabis use increased 10-fold between 2006 and 2018, from approximately 0.4\% to more than 4\%. Yet amid the increasing availability and use of cannabis nationwide, regulatory policies and sources of information for patients remain fragmented and inconsistent.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Incze et al_2021_Heterogeneous State Cannabis Policies.pdf;/Users/nseewald/Zotero/storage/KKU9T954/2786653.html}
}

@article{inkerNewCreatinineCystatin2021,
  title = {New {{Creatinine-}} and {{Cystatin C}}--{{Based Equations}} to {{Estimate GFR}} without {{Race}}},
  author = {Inker, Lesley A. and Eneanya, Nwamaka D. and Coresh, Josef and Tighiouart, Hocine and Wang, Dan and Sang, Yingying and Crews, Deidra C. and Doria, Alessandro and Estrella, Michelle M. and Froissart, Marc and Grams, Morgan E. and Greene, Tom and Grubb, Anders and Gudnason, Vilmundur and Guti{\'e}rrez, Orlando M. and Kalil, Roberto and Karger, Amy B. and Mauer, Michael and Navis, Gerjan and Nelson, Robert G. and Poggio, Emilio D. and Rodby, Roger and Rossing, Peter and Rule, Andrew D. and Selvin, Elizabeth and Seegmiller, Jesse C. and Shlipak, Michael G. and Torres, Vicente E. and Yang, Wei and Ballew, Shoshana H. and Couture, Sara J. and Powe, Neil R. and Levey, Andrew S.},
  year = {2021},
  month = nov,
  journal = {New England Journal of Medicine},
  volume = {385},
  number = {19},
  pages = {1737--1749},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMoa2102953},
  urldate = {2022-08-19},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Inker et al_2021_New Creatinine- and Cystatin C–Based Equations to Estimate GFR without Race.pdf}
}

@book{instituteofmedicineuscommitteeonadvancingpainresearchcareandeducationRelievingPainAmerica2011,
  title = {Relieving {{Pain}} in {{America}}: {{A Blueprint}} for {{Transforming Prevention}}, {{Care}}, {{Education}}, and {{Research}}},
  shorttitle = {Relieving {{Pain}} in {{America}}},
  author = {{Institute of Medicine (US) Committee on Advancing Pain Research, Care, andEducation}},
  year = {2011},
  series = {The {{National Academies Collection}}: {{Reports}} Funded by {{National Institutes}} of {{Health}}},
  publisher = {National Academies Press (US)},
  address = {Washington (DC)},
  urldate = {2024-05-01},
  abstract = {The Department of Health and Human Services, National Institutes of Health, has requested that the IOM (Institute of Medicine) convene the ad hoc committee to address the current state of the science with respect to pain research, care, and education; and explore approaches to advance the field. Specifically, the committee will: Review and quantify the public health significance of pain, including the adequacy of assessment, diagnosis, treatment, and management of acute and chronic pain in the United States. This effort will take a comprehensive view of chronic pain as a biological, biobehavioral, and societal condition. Identify barriers to appropriate pain care and strategies to reduce such barriers, including exploring the importance of individualized approaches to diagnosis and treatment of pain. Identify demographic groups and special populations, including older adults, individuals with co-morbidities, and cognitive impairment, that may be disparately undertreated for pain, and discuss related research needs, barriers particularly associated with these demographic groups, and opportunities to reduce such barriers. Identify and discuss what scientific tools and technologies are available, what strategies can be employed to enhance training of pain researchers, and what interdisciplinary research approaches will be necessary in the short and long term to advance basic, translational, and clinical pain research and improve the assessment, diagnosis, treatment, and management of pain. Discuss opportunities for public--private partnerships in the support and conduct of pain research, care, and education.},
  copyright = {Copyright {\copyright} 2011, National Academy of Sciences.},
  isbn = {978-0-309-21484-1},
  langid = {english},
  lccn = {NBK91497},
  pmid = {22553896}
}

@article{InversevarianceWeighting2024,
  title = {Inverse-Variance Weighting},
  year = {2024},
  month = may,
  journal = {Wikipedia},
  urldate = {2024-07-31},
  abstract = {In statistics, inverse-variance weighting is a method of aggregating two or more random variables to minimize the variance of the weighted average. Each random variable is weighted in inverse proportion to its variance (i.e., proportional to its precision). Given a sequence of independent observations yi with variances {$\sigma$}i2, the inverse-variance weighted average is given by                                                               y               {\textasciicircum}                                          =                                                                 {$\sum$}                                    i                                                                y                                    i                                                                /                                               {$\sigma$}                                    i                                                     2                                                                                          {$\sum$}                                    i                                               1                                /                                               {$\sigma$}                                    i                                                     2                                                                          .                 \{{\textbackslash}displaystyle \{{\textbackslash}hat \{y\}\}=\{{\textbackslash}frac \{{\textbackslash}sum \_\{i\}y\_\{i\}/{\textbackslash}sigma \_\{i\}{\textasciicircum}\{2\}\}\{{\textbackslash}sum \_\{i\}1/{\textbackslash}sigma \_\{i\}{\textasciicircum}\{2\}\}\}.\}    The inverse-variance weighted average has the least variance among all weighted averages, which can be calculated as                        V         a         r         (                                                y               {\textasciicircum}                                          )         =                                 1                                             {$\sum$}                                    i                                               1                                /                                               {$\sigma$}                                    i                                                     2                                                                          .                 \{{\textbackslash}displaystyle Var(\{{\textbackslash}hat \{y\}\})=\{{\textbackslash}frac \{1\}\{{\textbackslash}sum \_\{i\}1/{\textbackslash}sigma \_\{i\}{\textasciicircum}\{2\}\}\}.\}    If the variances of the measurements are all equal, then the inverse-variance weighted average becomes the simple average. Inverse-variance weighting is typically used in statistical meta-analysis or sensor fusion to combine the results from independent measurements.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1223841432},
  file = {/Users/nseewald/Zotero/storage/G7Y9IPHV/Inverse-variance_weighting.html}
}

@article{jacobAssessingUseAggregate2014,
  title = {Assessing the {{Use}} of {{Aggregate Data}} in the {{Evaluation}} of {{School-Based Interventions}}: {{Implications}} for {{Evaluation Research}} and {{State Policy Regarding Public-Use Data}}},
  shorttitle = {Assessing the {{Use}} of {{Aggregate Data}} in the {{Evaluation}} of {{School-Based Interventions}}},
  author = {Jacob, Robin T. and Goddard, Roger D. and Kim, Eun Sook},
  year = {2014},
  month = mar,
  journal = {Educational Evaluation and Policy Analysis},
  volume = {36},
  number = {1},
  pages = {44--66},
  publisher = {American Educational Research Association},
  issn = {0162-3737},
  doi = {10.3102/0162373713485814},
  urldate = {2022-02-03},
  abstract = {It is often difficult and costly to obtain individual-level student achievement data, yet, researchers are frequently reluctant to use school-level achievement data that are widely available from state websites. We argue that public-use aggregate school-level achievement data are, in fact, sufficient to address a wide range of evaluation questions and the use of this data is more appropriate than commonly thought. Specifically, we explore (a) when point estimates and standard errors differ between models that use individual student-level data and those that use aggregate school-level data, (b) the potential for conducting subgroup and nonexperimental analyses with aggregate data, and (c) the metrics that are currently available in state public-use data sets and the implications these have for analyses.},
  langid = {english},
  keywords = {educational evaluation,multilevel modeling,notion,school-level data},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Jacob et al_2014_Assessing the Use of Aggregate Data in the Evaluation of School-Based.pdf}
}

@techreport{jacobPracticalGuideRegression2012,
  title = {A {{Practical Guide}} to {{Regression Discontinuity}}},
  author = {Jacob, Robin T. and Zhu, Pei and Somers, Marie-Andr{\'e}e and Bloom, Howard},
  year = {2012},
  month = aug,
  address = {New York},
  institution = {MDRC},
  urldate = {2024-06-10},
  abstract = {Regression discontinuity (RD) analysis is a rigorous nonexperimental approach that can be used to estimate program impacts in situations in which candidates are selected for treatment based on whether their value for a numeric rating exceeds a designated threshold or cut-point.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Jacob_et_al_2012_A_Practical_Guide_to_Regression_Discontinuity.pdf;/Users/nseewald/Zotero/storage/IZ99XSPL/practical-guide-regression-discontinuity.html}
}

@book{jamesIntroductionStatisticalLearning2013,
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  series = {Springer {{Texts}} in {{Statistics}}},
  volume = {103},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4614-7138-7},
  urldate = {2022-03-17},
  isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/James et al_2013_An Introduction to Statistical Learning.pdf}
}

@book{jamesIntroductionStatisticalLearning2021,
  title = {An Introduction to Statistical Learning: With Applications in {{R}}},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2021},
  series = {Springer Texts in Statistics},
  edition = {Second edition},
  publisher = {Springer},
  address = {New York, NY},
  isbn = {978-1-0716-1417-4 978-1-0716-1420-4},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/James_et_al_2021_An_introduction_to_statistical_learning.pdf}
}

@article{jonesHeroinUseHeroin2013,
  title = {Heroin Use and Heroin Use Risk Behaviors among Nonmedical Users of Prescription Opioid Pain Relievers -- {{United States}}, 2002--2004 and 2008--2010},
  author = {Jones, Christopher M.},
  year = {2013},
  month = sep,
  journal = {Drug and Alcohol Dependence},
  volume = {132},
  number = {1-2},
  pages = {95--100},
  issn = {03768716},
  doi = {10.1016/j.drugalcdep.2013.01.007},
  urldate = {2024-05-01},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@article{jonesReinforcementbasedTreatmentImproves2011,
  title = {Reinforcement-Based Treatment Improves the Maternal Treatment and Neonatal Outcomes of Pregnant Patients Enrolled in Comprehensive Care Treatment},
  author = {Jones, Hendr{\'e}e E. and O'Grady, Kevin E. and Tuten, Michelle},
  year = {2011 May-Jun},
  journal = {The American Journal on Addictions},
  volume = {20},
  number = {3},
  pages = {196--204},
  issn = {1521-0391},
  doi = {10.1111/j.1521-0391.2011.00119.x},
  abstract = {This randomized clinical trial examined the efficacy of comprehensive usual care (UC) alone (n = 42) or enhanced by reinforcement-based treatment (RBT) (n = 47) to produce improved treatment outcomes, maternal delivery, and neonatal outcomes in pregnant women with opioid and/or cocaine substance use disorders. RBT participants spent, on average, 32.6 days longer in treatment (p {$<$} .001) and almost six times longer in recovery housing than did UC participants (p = .01). There were no significant differences between the RBT and UC conditions in proportion of participants testing positive for any illegal substance. Neonates in the RBT condition spent 1.3 fewer days hospitalized after birth than UC condition neonates (p = .03), although the two conditions did not differ significantly in neonatal gestational age at delivery, birth weight, or number of days hospitalized. Integrating RBT into a rich array of comprehensive care treatment components may be a promising approach to increase maternal treatment retention and reduce neonatal length of hospital stay.},
  langid = {english},
  pmcid = {PMC3084548},
  pmid = {21477047},
  keywords = {Adult,Behavior Therapy,Birth Weight,Cocaine-Related Disorders,Combined Modality Therapy,Comprehensive Health Care,Delivery Obstetric,Female,Gestational Age,Hospitalization,Housing,Humans,Infant Newborn,Models Statistical,Opioid-Related Disorders,Outcome and Process Assessment Health Care,Pregnancy,Pregnancy Complications,Pregnancy Outcome,Severity of Illness Index,Treatment Outcome},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Jones et al_2011_Reinforcement-based treatment improves the maternal treatment and neonatal.pdf}
}

@article{jungSampleSizeEstimation2003,
  title = {Sample Size Estimation for {{GEE}} Method for Comparing Slopes in Repeated Measurements Data},
  author = {Jung, Sin-Ho and Ahn, Chul},
  year = {2003},
  journal = {Statistics in Medicine},
  volume = {22},
  number = {8},
  pages = {1305--1315},
  issn = {1097-0258},
  doi = {10/dbr2m9},
  urldate = {2021-04-13},
  abstract = {Sample size calculation is an important component at the design stage of clinical trials. Controlled clinical trials often use a repeated measurement design in which individuals are randomly assigned to treatment groups and followed-up for measurements at intervals across a treatment period of fixed duration. In studies with repeated measurements, one of the popular primary interests is the comparison of the rates of change in a response variable between groups. Statistical models for calculating sample sizes for repeated measurement designs often fail to take into account the impact of missing data correctly. In this paper we propose to use the generalized estimating equation (GEE) method in comparing the rates of change in repeated measurements and introduce closed form formulae for sample size and power that can be calculated using a scientific calculator. Since the sample size formula is based on asymptotic theory, we investigate the performance of the estimated sample size in practical settings through simulations. Copyright {\copyright} 2003 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {AR(1),compound symmetry,Compound symmetry,independent missing,Independent missing,independent working correlation,Independent working correlation,missing completely at random,Missing completely at random,monotone missing,Monotone missing},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Jung_Ahn_2003_Sample size estimation for GEE method for comparing slopes in repeated.pdf;/Users/nseewald/Zotero/storage/4ZCN5YZN/sim.html;/Users/nseewald/Zotero/storage/SZPP7CGW/sim.html}
}

@article{justicePreparingGraduateStudents2020,
  title = {Preparing {{Graduate Students}} to {{Teach Statistics}}: {{A Review}} of {{Research}} and {{Ten Practical Recommendations}}},
  shorttitle = {Preparing {{Graduate Students}} to {{Teach Statistics}}},
  author = {Justice, Nicola},
  year = {2020},
  month = oct,
  journal = {Journal of Statistics Education},
  volume = {0},
  number = {0},
  pages = {1--10},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10/ghn84z},
  urldate = {2020-12-11},
  abstract = {Many statistics departments in institutions throughout the world hire graduate students to teach and assist with the teaching of undergraduate and graduate-level statistics courses. As many of these graduate student instructors and graduate teaching assistants (GTAs) have little or no previous experience teaching statistics, statistics departments are faced with the challenge of preparing their graduate students for teaching roles. Articles have been written sharing various departments' strategies for GTA training and development programs, however, articles are often not supported by empirical research. This article provides a review of empirical research regarding graduate students' preparation for teaching---first focusing on graduate students in statistics, specifically, and second offering what can be learned from studies of graduate students in other disciplines. We conclude with ten research-based recommendations for preparing graduate students to teach statistics, along with practical ideas for how to implement them.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Justice_2020_Preparing Graduate Students to Teach Statistics.pdf;/Users/nseewald/Zotero/storage/FMXR9HNS/10691898.2020.html}
}

@inproceedings{kadakiaAssociationsTreatmentemergentSymptoms2015,
  title = {Associations between Treatment-Emergent Symptoms and Early Discontinuation of Aromatase Inhibitor ({{AI}}) Therapy.},
  booktitle = {Journal of {{Clinical Oncology}}},
  author = {Kadakia, Kunal C. and Snyder, Claire Frances and Kidwell, Kelley M and Seewald, Nicholas J. and Storniolo, Anna Maria and Flockhart, David A. and Carpenter, Janet S and Hayes, Daniel F. and Stearns, Vered and Henry, Norah Lynn},
  year = {2015},
  month = may,
  volume = {33},
  pages = {e20745-e20745},
  doi = {10.1200/jco.2015.33.15_suppl.e20745},
  urldate = {2024-10-22},
  langid = {english}
}

@inproceedings{kadakiaCrossoverOneAromatase2016,
  title = {Crossover from One Aromatase Inhibitor ({{AI}}) to Another in the {{Exemestane}} and {{Letrozole Pharmacogenetics}} ({{ELPh}}) Trial},
  booktitle = {Journal of {{Clinical Oncology}}},
  author = {Kadakia, Kunal C. and Kidwell, Kelley M. and Seewald, Nicholas J. and Synder, Claire Frances and Flockhart, David A. and Carpenter, Janet S and Otte, Julie L. and Hayes, Daniel F. and Storniolo, Anna Maria and Stearns, Vered and Henry, Norah Lynn},
  year = {2016},
  month = jan,
  volume = {34},
  pages = {158--158},
  doi = {10.1200/jco.2016.34.3_suppl.158},
  langid = {english}
}

@article{kadakiaPatientreportedOutcomesEarly2016,
  title = {Patient-Reported Outcomes and Early Discontinuation in Aromatase Inhibitor-Treated Postmenopausal Women with Early Stage Breast Cancer},
  author = {Kadakia, K.C. Kunal C and Snyder, C.F. Claire F and Kidwell, Kelley M K.M. and Seewald, N.J. Nicholas J and Flockhart, D.A. David A and Skaar, T.C. Todd C and Desta, Zereunesay and Rae, James M and Otte, Julie L and Carpenter, J.S. Janet S and Storniolo, A.M. Anna M and Hayes, Daniel F D.F. and Stearns, Vered and Henry, N Lynn and Lynn Henry, N.},
  year = {2016},
  month = may,
  journal = {The Oncologist},
  volume = {21},
  number = {5},
  pages = {539--546},
  issn = {1549-490X},
  doi = {10.1634/theoncologist.2015-0349},
  abstract = {BACKGROUND Early discontinuation of aromatase inhibitors (AIs) is common and leads to poor outcomes but is challenging to predict. In the Exemestane and Letrozole Pharmacogenetics trial, a high rate of early discontinuation due to intolerance was observed. We hypothesized that early changes in patient-reported outcomes (PROs) predict AI discontinuation and that biochemical factors are associated with changes in PROs. PATIENTS AND METHODS Postmenopausal women with early-stage breast cancer enrolled in a prospective randomized trial of exemestane versus letrozole completed questionnaires at baseline and serially over 24 months to assess overall quality of life (EuroQOL Visual Analog Scale [VAS]); mood; and multiple symptoms, including a musculoskeletal symptom cluster. A joint mixed-effects/survival model was used to estimate the effect of the change in PROs on AI discontinuation. Associations between biochemical factors and change in PROs were examined. RESULTS A total of 490 patients were analyzed. Worsening of EuroQOL VAS and the musculoskeletal cluster were associated with the highest risk for early discontinuation (hazard ratio [HR], 2.77 [95\% confidence interval (CI), 2.72-2.81; p = .015]; HR, 4.39 [95\% CI, 2.40-8.02; p {\textbackslash}textless .0001], respectively). Pharmacokinetics and estrogen metabolism were not consistently associated with change in PRO measures. No clinically significant differences in any PRO between AIs were observed. CONCLUSION Changes in PROs early during AI therapy were associated with treatment discontinuation. Identification of these changes could be used to target interventions in patients at high risk for early discontinuation. IMPLICATIONS FOR PRACTICE Early changes in patient-reported outcomes (PROs) can predict nonpersistence to aromatase inhibitor therapy. If used in clinical practice, PROs might identify women at highest risk for early discontinuation and allow for interventions to improve tolerance before significant toxicities develop. Further research is needed to improve capturing PROs in routine clinical practice.},
  copyright = {All rights reserved},
  pmid = {27009936},
  keywords = {[Aromatase inhibitors,Aromatase inhibitors,Early discontinuation,Pati,Patient-reported outcomes,Quality of life},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kadakia et al_2016_Patient-reported outcomes and early discontinuation in aromatase.pdf}
}

@article{kadakiaProspectiveAssessmentPatientreported2017,
  title = {Prospective Assessment of Patient-Reported Outcomes and Estradiol and Drug Concentrations in Patients Experiencing Toxicity from Adjuvant Aromatase Inhibitors},
  author = {Kadakia, Kunal C. and Kidwell, Kelley M. and Seewald, Nicholas J. and Snyder, Claire F. and Storniolo, Anna Maria and Otte, Julie L. and Flockhart, David A. and Hayes, Daniel F. and Stearns, Vered and Henry, N. Lynn},
  year = {2017},
  month = jul,
  journal = {Breast Cancer Research and Treatment},
  volume = {164},
  number = {2},
  pages = {411--419},
  issn = {0167-6806, 1573-7217},
  doi = {10.1007/S10549-017-4260-2},
  urldate = {2018-11-17},
  abstract = {Purpose Aromatase inhibitors (AI), which decrease circulating estradiol concentrations in post-menopausal women, are associated with toxicities that limit adherence. Approximately one-third of patients will tolerate a different AI after not tolerating the first. We report the effect of crossover from exemestane to letrozole or vice versa on patient-reported outcomes (PROs) and whether the success of crossover is due to lack of estrogen suppression.},
  copyright = {All rights reserved},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kadakia et al_2017_Prospective assessment of patient-reported outcomes and estradiol and drug.pdf}
}

@article{kaelblingReinforcementLearningSurvey1996,
  title = {Reinforcement Learning: {{A}} Survey},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
  year = {1996},
  journal = {Journal of Artificial Intelligence Research},
  volume = {4},
  pages = {237--285},
  issn = {10769757},
  doi = {10.1613/jair.301},
  abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
  pmid = {17255001},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kaelbling et al_1996_Reinforcement learning.pdf}
}

@article{kahanRisksRewardsCovariate2014,
  title = {The Risks and Rewards of Covariate Adjustment in Randomized Trials: An Assessment of 12 Outcomes from 8 Studies},
  shorttitle = {The Risks and Rewards of Covariate Adjustment in Randomized Trials},
  author = {Kahan, Brennan C and Jairath, Vipul and Dor{\'e}, Caroline J and Morris, Tim P},
  year = {2014},
  journal = {Trials},
  volume = {15},
  number = {1},
  pages = {139},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-15-139},
  urldate = {2018-10-12},
  abstract = {Background: Adjustment for prognostic covariates can lead to increased power in the analysis of randomized trials. However, adjusted analyses are not often performed in practice. Methods: We used simulation to examine the impact of covariate adjustment on 12 outcomes from 8 studies across a range of therapeutic areas. We assessed (1) how large an increase in power can be expected in practice; and (2) the impact of adjustment for covariates that are not prognostic. Results: Adjustment for known prognostic covariates led to large increases in power for most outcomes. When power was set to 80\% based on an unadjusted analysis, covariate adjustment led to a median increase in power to 92.6\% across the 12 outcomes (range 80.6 to 99.4\%). Power was increased to over 85\% for 8 of 12 outcomes, and to over 95\% for 5 of 12 outcomes. Conversely, the largest decrease in power from adjustment for covariates that were not prognostic was from 80\% to 78.5\%. Conclusions: Adjustment for known prognostic covariates can lead to substantial increases in power, and should be routinely incorporated into the analysis of randomized trials. The potential benefits of adjusting for a small number of possibly prognostic covariates in trials with moderate or large sample sizes far outweigh the risks of doing so, and so should also be considered.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kahan et al_2014_The risks and rewards of covariate adjustment in randomized trials.pdf}
}

@article{kaisersAddressingEnvironmentNonStationarity2016,
  title = {Addressing {{Environment Non-Stationarity}} by {{Repeating Q-learning}}},
  author = {Kaisers, Michael},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  pages = {1--31},
  issn = {1532-4435},
  abstract = {Q-learning (QL) is a popular reinforcement learning algorithm that is guaranteed to converge to optimal policies in Markov decision processes. However, QL exhibits an artifact: in expectation, the effective rate of updating the value of an action depends on the probability of choosing that action. In other words, there is a tight coupling between the learning dynamics and underlying execution policy. This coupling can cause performance degradation in noisy non-stationary environments. Here, we introduce Repeated Update Q-learning (RUQL), a learning algorithm that resolves the undesirable artifact of Q-learning while maintaining simplicity. We analyze the similarities and differences between RUQL, QL, and the closest state-of-the-art algorithms theoretically. Our analysis shows that RUQL maintains the convergence guarantee of QL in stationary environments, while relaxing the coupling between the execution policy and the learning dynamics. Experimental results confirm the theoretical insights and show how RUQL outperforms both QL and the closest state-of-the-art algorithms in noisy non-stationary environments.},
  keywords = {multi-agent learning,No DOI found,non-stationary environ-,Q-learning,Reinforcement learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kaisers_2016_Addressing Environment Non-Stationarity by Repeating Q-learning.pdf}
}

@book{kalbfleischStatisticalAnalysisFailure2002,
  title = {The Statistical Analysis of Failure Time Data},
  author = {Kalbfleisch, J. D. and Prentice, Ross L.},
  year = {2002},
  series = {Wiley Series in Probability and Statistics},
  edition = {2nd ed},
  publisher = {J. Wiley},
  address = {Hoboken, N.J},
  isbn = {978-0-471-36357-6},
  lccn = {QA276 .K215 2002},
  keywords = {Failure time data analysis,Regression analysis,Survival analysis (Biometry)},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kalbfleisch_Prentice_2002_The statistical analysis of failure time data.pdf}
}

@article{kaltenthalerAcceptabilityPatientsComputerized2008,
  title = {The Acceptability to Patients of Computerized Cognitive Behaviour Therapy for Depression: A Systematic Review},
  shorttitle = {The Acceptability to Patients of Computerized Cognitive Behaviour Therapy for Depression},
  author = {Kaltenthaler, E. and Sutcliffe, P. and Parry, G. and Beverley, C. and Rees, A. and Ferriter, M.},
  year = {2008},
  month = nov,
  journal = {Psychological Medicine},
  volume = {38},
  number = {11},
  pages = {1521--1530},
  publisher = {Cambridge University Press},
  issn = {1469-8978, 0033-2917},
  doi = {10/cshqg4},
  urldate = {2021-04-09},
  abstract = {BackgroundCognitive behaviour therapy (CBT) is widely used to treat depression. However, CBT is not always available to patients because of a shortage of therapists and long waiting times. Computerized CBT (CCBT) is one of several alternatives currently available to treat patients with depression. Evidence of its clinical effectiveness has led to programs being used increasingly within the UK and elsewhere. However, little information is available regarding the acceptability of CCBT to patients.MethodA systematic review of sources of information on acceptability to patients of CCBT for depression.ResultsSources of information on acceptability included: recruitment rates, patient drop-outs and patient-completed questionnaires. We identified 16 studies of CCBT for the treatment of depression that provided at least some information on these sources. Limited information was provided on patient take-up rates and recruitment methods. Drop-out rates were comparable to other forms of treatment. Take-up rates, when reported, were much lower. Six of the 16 studies included specific questions on patient acceptability or satisfaction although information was only provided for those who had completed treatment. Several studies have reported positive expectancies and high satisfaction in routine care CCBT services for those completing treatment.ConclusionsTrials of CCBT should include more detailed information on patient recruitment methods, drop-out rates and reasons for dropping out. It is important that well-designed surveys and qualitative studies are included alongside trials to determine levels and determinants of patient acceptability.},
  langid = {english},
  keywords = {CCBT,computer programs,patient acceptability},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kaltenthaler et al_2008_The acceptability to patients of computerized cognitive behaviour therapy for.pdf;/Users/nseewald/Zotero/storage/A56ZMHTK/CE9A413B842B63BE61D97FCC072ABFE6.html}
}

@article{kasariCommunicationInterventionsMinimally2014,
  ids = {kasariCommunicationInterventionsMinimally2014a},
  title = {Communication Interventions for Minimally Verbal Children with Autism: {{A}} Sequential Multiple Assignment Randomized Trial},
  author = {Kasari, Connie and Kaiser, Ann and Goods, Kelly and Nietfeld, Jennifer and Mathy, Pamela and Landa, Rebecca and Murphy, Susan A. and Almirall, Daniel},
  year = {2014},
  journal = {Journal of the American Academy of Child and Adolescent Psychiatry},
  volume = {53},
  number = {6},
  pages = {635--646},
  issn = {15275418},
  doi = {10.1016/j.jaac.2014.01.019},
  abstract = {Objective This study tested the effect of beginning treatment with a speech-generating device (SGD) in the context of a blended, adaptive treatment design for improving spontaneous, communicative utterances in school-aged, minimally verbal children with autism. Method A total of 61 minimally verbal children with autism, aged 5 to 8 years, were randomized to a blended developmental/behavioral intervention (JASP+EMT) with or without the augmentation of a SGD for 6 months with a 3-month follow-up. The intervention consisted of 2 stages. In stage 1, all children received 2 sessions per week for 3 months. Stage 2 intervention was adapted (by increased sessions or adding the SGD) based on the child's early response. The primary outcome was the total number of spontaneous communicative utterances; secondary measures were the total number of novel words and total comments from a natural language sample. Results Primary aim results found improvements in spontaneous communicative utterances, novel words, and comments that all favored the blended behavioral intervention that began by including an SGD (JASP+EMT+SGD) as opposed to spoken words alone (JASP+EMT). Secondary aim results suggest that the adaptive intervention beginning with JASP+EMT+SGD and intensifying JASP+EMT+SGD for children who were slow responders led to better posttreatment outcomes. Conclusion Minimally verbal school-aged children can make significant and rapid gains in spoken spontaneous language with a novel, blended intervention that focuses on joint engagement and play skills and incorporates an SGD. Future studies should further explore the tailoring design used in this study to better understand children's response to treatment. Clinical trial registration information - Developmental and Augmented Intervention for Facilitating Expressive Language (CCNIA); http://clinicaltrials.gov/; NCT01013545.},
  pmid = {24839882},
  keywords = {autism spectrum disorders,communication intervention,minimally verbal,school-aged,SMART design},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kasari et al_2014_Communication interventions for minimally verbal children with autism.pdf}
}

@article{kaszaImpactNonuniformCorrelation2019,
  title = {Impact of Non-Uniform Correlation Structure on Sample Size and Power in Multiple-Period Cluster Randomised Trials},
  author = {Kasza, J and Hemming, K and Hooper, R and Matthews, Jns and Forbes, Ab},
  year = {2019},
  month = mar,
  journal = {Statistical Methods in Medical Research},
  volume = {28},
  number = {3},
  pages = {703--716},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280217734981},
  urldate = {2023-06-09},
  abstract = {Stepped wedge and cluster randomised crossover trials are examples of cluster randomised designs conducted over multiple time periods that are being used with increasing frequency in health research. Recent systematic reviews of both of these designs indicate that the within-cluster correlation is typically taken account of in the analysis of data using a random intercept mixed model, implying a constant correlation between any two individuals in the same cluster no matter how far apart in time they are measured: within-period and between-period intra-cluster correlations are assumed to be identical. Recently proposed extensions allow the within- and between-period intra-cluster correlations to differ, although these methods require that all between-period intra-cluster correlations are identical, which may not be appropriate in all situations. Motivated by a proposed intensive care cluster randomised trial, we propose an alternative correlation structure for repeated cross-sectional multiple-period cluster randomised trials in which the between-period intra-cluster correlation is allowed to decay depending on the distance between measurements. We present results for the variance of treatment effect estimators for varying amounts of decay, investigating the consequences of the variation in decay on sample size planning for stepped wedge, cluster crossover and multiple-period parallel-arm cluster randomised trials. We also investigate the impact of assuming constant betweenperiod intra-cluster correlations instead of decaying between-period intra-cluster correlations. Our results indicate that in certain design configurations, including the one corresponding to the proposed trial, a correlation decay can have an important impact on variances of treatment effect estimators, and hence on sample size and power. An R Shiny app allows readers to interactively explore the impact of correlation decay.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kasza et al_2019_Impact of non-uniform correlation structure on sample size and power in.pdf}
}

@article{kaszaInferenceTreatmentEffect2019,
  title = {Inference for the Treatment Effect in Multiple-Period Cluster Randomised Trials When Random Effect Correlation Structure Is Misspecified},
  author = {Kasza, Jessica and Forbes, Andrew B},
  year = {2019},
  month = nov,
  journal = {Statistical Methods in Medical Research},
  volume = {28},
  number = {10-11},
  pages = {3112--3122},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280218797151},
  urldate = {2024-09-03},
  abstract = {Multiple-period cluster randomised trials, such as stepped wedge or cluster cross-over trials, are being conducted with increasing frequency. In the design and analysis of these trials, it is necessary to specify the form of the within-cluster correlation structure, and a common assumption is that the correlation between the outcomes of any pair of subjects within a cluster is identical. More complex models that allow for correlations within a cluster to decay over time have recently been suggested. However, most software packages cannot fit these models. As a result, practitioners may choose a simpler model. We analytically examine the impact of incorrectly omitting a decay in correlation on the variance of the treatment effect estimator and show that misspecification of the within-cluster correlation structure can lead to incorrect conclusions regarding estimated treatment effects for stepped wedge and cluster crossover trials.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kasza_Forbes_2019_Inference for the treatment effect in multiple-period cluster randomised trials.pdf}
}

@article{kauermannNoteEfficiencySandwich2001,
  title = {A {{Note}} on the {{Efficiency}} of {{Sandwich Covariance Matrix Estimation}}},
  author = {Kauermann, G{\"o}ran and Carroll, Raymond J.},
  year = {2001},
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {456},
  eprint = {3085907},
  eprinttype = {jstor},
  pages = {1387--1396},
  issn = {0162-1459},
  doi = {10/dnmrrw},
  urldate = {2019-08-09},
  abstract = {The sandwich estimator, also known as robust covariance matrix estimator, heteroscedasticity-consistent covariance matrix estimate, or empirical covariance matrix estimator, has achieved increasing use in the econometric literature as well as with the growing popularity of generalized estimating equations. Its virtue is that it provides consistent estimates of the covariance matrix for parameter estimates even when the fitted parametric model fails to hold or is not even specified. Surprisingly though, there has been little discussion of properties of the sandwich method other than consistency. We investigate the sandwich estimator in quasi-likelihood models asymptotically, and in the linear case analytically. We show that under certain circumstances when the quasi-likelihood model is correct, the sandwich estimate is often far more variable than the usual parametric variance estimate. The increased variance is a fixed feature of the method and the price that one pays to obtain consistency even when the parametric model fails or when there is heteroscedasticity. We show that the additional variability directly affects the coverage probability of confidence intervals constructed from sandwich variance estimates. In fact, the use of sandwich variance estimates combined with t-distribution quantiles gives confidence intervals with coverage probability falling below the nominal value. We propose an adjustment to compensate for this fact.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kauermann_Carroll_2001_A Note on the Efficiency of Sandwich Covariance Matrix Estimation.pdf}
}

@misc{KbUnknownData,
  title = {Kb:Unknown Data Error [{{Zotero Documentation}}]},
  urldate = {2022-02-03},
  howpublished = {https://www.zotero.org/support/kb/unknown\_data\_error},
  keywords = {notion},
  file = {/Users/nseewald/Zotero/storage/EYI6N4DD/unknown_data_error.html}
}

@book{keenerTheoreticalStatisticsTopics2010,
  title = {Theoretical Statistics: Topics for a Core Course},
  shorttitle = {Theoretical Statistics},
  author = {Keener, Robert W.},
  year = {2010},
  series = {Springer Texts in Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-93838-7},
  lccn = {QA276 .K246 2010},
  keywords = {nosource},
  annotation = {OCLC: ocn688477588},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Keener_2010_Theoretical statistics.pdf}
}

@article{keldersPsychometricEvaluationTWente2020,
  title = {Psychometric {{Evaluation}} of the {{TWente Engagement}} with {{Ehealth Technologies Scale}} ({{TWEETS}}): {{Evaluation Study}}},
  shorttitle = {Psychometric {{Evaluation}} of the {{TWente Engagement}} with {{Ehealth Technologies Scale}} ({{TWEETS}})},
  author = {Kelders, Saskia Marion and Kip, Hanneke and Greeff, Japie},
  year = {2020},
  month = oct,
  journal = {Journal of Medical Internet Research},
  volume = {22},
  number = {10},
  pages = {e17757},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10/gmwr4t},
  urldate = {2021-09-23},
  abstract = {Background: Engagement emerges as a predictor for the effectiveness of digital health interventions. However, a shared understanding of engagement is missing. Therefore, a new scale has been developed that proposes a clear definition and creates a tool to measure it. The TWente Engagement with Ehealth Technologies Scale (TWEETS) is based on a systematic review and interviews with engaged health app users. It defines engagement as a combination of behavior, cognition, and affect. Objective: This paper aims to evaluate the psychometric properties of the TWEETS. In addition, a comparison is made with the experiential part of the Digital Behavior Change Intervention Engagement Scale (DBCI-ES-Ex), a scale that showed some issues in previous psychometric analyses. Methods: In this study, 288 participants were asked to use any step counter app on their smartphones for 2 weeks. They completed online questionnaires at 4 time points: T0=baseline, T1=after 1 day, T2=1 week, and T3=2 weeks. At T0, demographics and personality (conscientiousness and intellect/imagination) were assessed; at T1-T3, engagement, involvement, enjoyment, subjective usage, and perceived behavior change were included as measures that are theoretically related to our definition of engagement. Analyses focused on internal consistency,  reliability, and the convergent, divergent, and predictive validity of both engagement scales. Convergent validity was assessed by correlating the engagement scales with involvement, enjoyment, and subjective usage; divergent validity was assessed by correlating the engagement scales with personality; and predictive validity was assessed by regression analyses using engagement to predict perceived behavior change at later time points. Results: The Cronbach alpha values of the TWEETS were .86, .86, and .87 on T1, T2, and T3, respectively. Exploratory factor analyses indicated that a 1-factor structure best fits the data. The TWEETS is moderately to strongly correlated with involvement and enjoyment (theoretically related to cognitive and affective engagement, respectively; P\&lt;.001). Correlations between the TWEETS and frequency of use were nonsignificant or small, and differences between adherers and nonadherers on the TWEETS were significant (P\&lt;.001). Correlations between personality and the TWEETS were nonsignificant. The TWEETS at T1 was predictive of perceived behavior change at T3, with an explained variance of 16\%. The psychometric properties of the TWEETS and the DBCI-ES-Ex seemed comparable in some aspects (eg, internal consistency), and in other aspects, the TWEETS seemed somewhat superior (divergent and predictive validity). Conclusions: The TWEETS performs quite well as an engagement measure with high internal consistency, reasonable test-retest reliability and convergent validity, good divergent validity, and reasonable predictive validity. As the psychometric quality of a scale is a reflection of how closely a scale matches the conceptualization of a concept, this paper is also an attempt to conceptualize and define engagement as a unique concept, providing a first step toward an acceptable standard of defining and measuring engagement.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kelders et al_2020_Psychometric Evaluation of the TWente Engagement with Ehealth Technologies.pdf;/Users/nseewald/Zotero/storage/6YSXK3UJ/e17757.html}
}

@book{kendiHowBeAntiracist2019,
  title = {How to Be an Antiracist},
  author = {Kendi, Ibram X.},
  year = {2019},
  edition = {First},
  publisher = {One World},
  address = {New York},
  isbn = {978-0-525-50928-8},
  lccn = {E184.A1 K344 2019},
  keywords = {nosource}
}

@article{kennedy-hendricksOpioidOverdoseDeaths2016,
  title = {Opioid {{Overdose Deaths}} and {{Florida}}'s {{Crackdown}} on {{Pill Mills}}},
  author = {{Kennedy-Hendricks}, Alene and Richey, Matthew and McGinty, Emma E. and Stuart, Elizabeth A. and Barry, Colleen L. and Webster, Daniel W.},
  year = {2016},
  month = feb,
  journal = {American Journal of Public Health},
  volume = {106},
  number = {2},
  pages = {291--297},
  publisher = {American Public Health Association},
  issn = {0090-0036},
  doi = {10/f8hgps},
  urldate = {2021-07-01},
  abstract = {Objectives. We examined the effect on opioid overdose mortality of Florida state laws and law enforcement operations targeting ``pill mills.''Methods. We collected 2003 to 2012 mortality data from the Florida Department of Health and the North Carolina State Center for Health Statistics (the comparison state) to estimate changes in the rates of death from prescription opioid, heroin, or any opioid overdose.Results. Florida's actions were associated with an estimated 1029 lives saved from prescription opioid overdose over a 34-month period. Estimated reductions in deaths grew over the intervention period, with rates per 100\,000 population that were 0.6 lower in 2010, 1.8 lower in 2011, and 3.0 lower in 2012 than what would have been expected had the changes in mortality rate trends in Florida been the same as changes in trends in North Carolina. Florida's mortality rates from heroin and total opioid overdose were also lower than anticipated relative to changes in trends in North Carolina.Conclusions. Findings from this study indicate that laws regulating pain clinics and enforcement of these laws may, in combination, reduce opioid overdose deaths.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kennedy-Hendricks et al_2016_Opioid Overdose Deaths and Florida’s Crackdown on Pill Mills.pdf;/Users/nseewald/Zotero/storage/BUI6TYK6/AJPH.2015.html}
}

@article{kennedy-shafferSampleSizeEstimation2020,
  title = {Sample Size Estimation for Stratified Individual and Cluster Randomized Trials with Binary Outcomes},
  author = {{Kennedy-Shaffer}, Lee and Hughes, Michael D.},
  year = {2020},
  journal = {Statistics in Medicine},
  volume = {39},
  number = {10},
  pages = {1489--1513},
  issn = {1097-0258},
  doi = {10.1002/sim.8492},
  urldate = {2024-11-26},
  abstract = {Individual randomized trials (IRTs) and cluster randomized trials (CRTs) with binary outcomes arise in a variety of settings and are often analyzed by logistic regression (fitted using generalized estimating equations for CRTs). The effect of stratification on the required sample size is less well understood for trials with binary outcomes than for continuous outcomes. We propose easy-to-use methods for sample size estimation for stratified IRTs and CRTs and demonstrate the use of these methods for a tuberculosis prevention CRT currently being planned. For both IRTs and CRTs, we also identify the ratio of the sample size for a stratified trial vs a comparably powered unstratified trial, allowing investigators to evaluate how stratification will affect the required sample size when planning a trial. For CRTs, these can be used when the investigator has estimates of the within-stratum intracluster correlation coefficients (ICCs) or by assuming a common within-stratum ICC. Using these methods, we describe scenarios where stratification may have a practically important impact on the required sample size. We find that in the two-stratum case, for both IRTs and for CRTs with very small cluster sizes, there are unlikely to be plausible scenarios in which an important sample size reduction is achieved when the overall probability of a subject experiencing the event of interest is low. When the probability of events is not small, or when cluster sizes are large, however, there are scenarios where practically important reductions in sample size result from stratification.},
  langid = {english},
  keywords = {cluster randomized trials,design effect,generalized estimating equations,intracluster correlation coefficient,sample size,stratification},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kennedy-Shaffer and Hughes - 2020 - Sample size estimation for stratified individual and cluster randomized trials with binary outcomes.pdf;/Users/nseewald/Zotero/storage/9KEMQ5LI/sim.html}
}

@article{kenwardShouldBaselineBe2010,
  title = {Should Baseline Be a Covariate or Dependent Variable in Analyses of Change from Baseline in Clinical Trials? By {{G}}. {{F}}. {{Liu}}, {{K}}. {{Lu}}, {{R}}. {{Mogg}}, {{M}}. {{Mallick}} and {{D}}. {{V}}. {{Mehrotra}}, {{Statistics}} in {{Medicine}} 2009; 28:2509--2530},
  shorttitle = {Should Baseline Be a Covariate or Dependent Variable in Analyses of Change from Baseline in Clinical Trials?},
  author = {Kenward, Michael G. and White, Ian R. and Carpenter, James R.},
  year = {2010},
  journal = {Statistics in Medicine},
  volume = {29},
  number = {13},
  pages = {1455--1456},
  issn = {1097-0258},
  doi = {10/fg3s3s},
  urldate = {2020-01-23},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kenward et al_2010_Should baseline be a covariate or dependent variable in analyses of change from.pdf;/Users/nseewald/Zotero/storage/P4N7VEC4/sim.html}
}

@article{keoghCausalInferenceSurvival2023,
  title = {Causal Inference in Survival Analysis Using Longitudinal Observational Data: {{Sequential}} Trials and Marginal Structural Models},
  shorttitle = {Causal Inference in Survival Analysis Using Longitudinal Observational Data},
  author = {Keogh, Ruth H. and Gran, Jon Michael and Seaman, Shaun R. and Davies, Gwyneth and Vansteelandt, Stijn},
  year = {2023},
  journal = {Statistics in Medicine},
  volume = {42},
  number = {13},
  pages = {2191--2225},
  issn = {1097-0258},
  doi = {10.1002/sim.9718},
  urldate = {2024-07-12},
  abstract = {Longitudinal observational data on patients can be used to investigate causal effects of time-varying treatments on time-to-event outcomes. Several methods have been developed for estimating such effects by controlling for the time-dependent confounding that typically occurs. The most commonly used is marginal structural models (MSM) estimated using inverse probability of treatment weights (IPTW) (MSM-IPTW). An alternative, the sequential trials approach, is increasingly popular, and involves creating a sequence of ``trials'' from new time origins and comparing treatment initiators and non-initiators. Individuals are censored when they deviate from their treatment assignment at the start of each ``trial'' (initiator or noninitiator), which is accounted for using inverse probability of censoring weights. The analysis uses data combined across trials. We show that the sequential trials approach can estimate the parameters of a particular MSM. The causal estimand that we focus on is the marginal risk difference between the sustained treatment strategies of ``always treat'' vs ``never treat.'' We compare how the sequential trials approach and MSM-IPTW estimate this estimand, and discuss their assumptions and how data are used differently. The performance of the two approaches is compared in a simulation study. The sequential trials approach, which tends to involve less extreme weights than MSM-IPTW, results in greater efficiency for estimating the marginal risk difference at most follow-up times, but this can, in certain scenarios, be reversed at later time points and relies on modelling assumptions. We apply the methods to longitudinal observational data from the UK Cystic Fibrosis Registry to estimate the effect of dornase alfa on survival.},
  copyright = {{\copyright} 2023 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {cystic fibrosis,inverse probability weighting,marginal structural model,registries,sequential trials,survival,target trials,time-dependent confounding},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Keogh_et_al_2023_Causal_inference_in_survival_analysis_using_longitudinal_observational_data.pdf;/Users/nseewald/Zotero/storage/NDFH2M39/sim.html}
}

@article{khanPerspectivesCannabisSubstitute2019,
  title = {Perspectives on Cannabis as a Substitute for Opioid Analgesics},
  author = {Khan, Sara P and Pickens, Thomas A and Berlau, Daniel J},
  year = {2019},
  month = mar,
  journal = {Pain Management},
  volume = {9},
  number = {2},
  pages = {191--203},
  publisher = {Future Medicine},
  issn = {1758-1869},
  doi = {10.2217/pmt-2018-0051},
  urldate = {2024-05-01},
  abstract = {With the opioid epidemic reaching new heights in the USA, it has become critical to find suitable alternatives to opioids. Cannabis, an antinociceptive, is a strong contender to help patients reduce their opioid usage. A growing literature has been examining the complex effects cannabis has on pain relief and on opioid usage; whether it is a substitute for opioids or increases their use. This review explores the studies that compare cannabis-opioid interactions and presents some challenges of cannabis research and usage. The practical clinical pharmacology of cannabis as an analgesic, including the route of administration, safety and pharmacokinetics, are discussed to address the concerns, as well as possible solutions, of cannabis as a pain reliever.},
  keywords = {_tablet,analgesia,marijuana,mu-opioid receptor agonists,opioid-sparing effect,synthetic CB2 agonist},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Khan_et_al_2019_Perspectives_on_cannabis_as_a_substitute_for_opioid_analgesics.pdf}
}

@article{kidwellDesignAnalysisConsiderations2018,
  title = {Design and Analysis Considerations for Comparing Dynamic Treatment Regimens with Binary Outcomes from Sequential Multiple Assignment Randomized Trials},
  author = {Kidwell, Kelley M. and Seewald, Nicholas J. and Tran, Qui and Kasari, Connie and Almirall, Daniel},
  year = {2018},
  month = jul,
  journal = {Journal of Applied Statistics},
  volume = {45},
  number = {9},
  pages = {1628--1651},
  issn = {0266-4763, 1360-0532},
  doi = {10/hh5r},
  urldate = {2018-10-12},
  abstract = {In behavioral, educational and medical practice, interventions are often personalized over time using strategies that are based on individual behaviors and characteristics and changes in symptoms, severity, or adherence that are a result of one's treatment. Such strategies that more closely mimic real practice, are known as dynamic treatment regimens (DTRs). A sequential multiple assignment randomized trial (SMART) is a multi-stage trial design that can be used to construct effective DTRs. This article reviews a simple to use `weighted and replicated' estimation technique for comparing DTRs embedded in a SMART design using logistic regression for a binary, end-of-study outcome variable. Based on a Wald test that compares two embedded DTRs of interest from the `weighted and replicated' regression model, a sample size calculation is presented with a corresponding user-friendly applet to aid in the process of designing a SMART. The analytic models and sample size calculations are presented for three of the more commonly used two-stage SMART designs. Simulations for the sample size calculation show the empirical power reaches expected levels. A data analysis example with corresponding code is presented in the appendix using data from a SMART developing an effective DTR in autism.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kidwell et al_2018_Design and analysis considerations for comparing dynamic treatment regimens.pdf}
}

@article{kidwellSequentialMultipleAssignment2018,
  title = {Sequential, {{Multiple Assignment}}, {{Randomized Trial Designs}} in {{Immuno-oncology Research}}},
  author = {Kidwell, Kelley M. and Postow, Michael A. and Panageas, Katherine S.},
  year = {2018},
  month = feb,
  journal = {Clinical Cancer Research},
  volume = {24},
  number = {4},
  pages = {730--736},
  issn = {1078-0432, 1557-3265},
  doi = {10.1158/1078-0432.CCR-17-1355},
  urldate = {2024-08-16},
  abstract = {Abstract             Clinical trials investigating immune checkpoint inhibitors have led to the approval of anti--CTLA-4 (cytotoxic T-lymphocyte antigen-4), anti--PD-1 (programmed death-1), and anti--PD-L1 (PD-ligand 1) drugs by the FDA for numerous tumor types. In the treatment of metastatic melanoma, combinations of checkpoint inhibitors are more effective than single-agent inhibitors, but combination immunotherapy is associated with increased frequency and severity of toxicity. There are questions about the use of combination immunotherapy or single-agent anti--PD-1 as initial therapy and the number of doses of either approach required to sustain a response. In this article, we describe a novel use of sequential, multiple assignment, randomized trial (SMART) design to evaluate immune checkpoint inhibitors to find treatment regimens that adapt within an individual based on intermediate response and lead to the longest overall survival. We provide a hypothetical example SMART design for BRAF wild-type metastatic melanoma as a framework for investigating immunotherapy treatment regimens. We compare implementing a SMART design to implementing multiple traditional randomized clinical trials. We illustrate the benefits of a SMART over traditional trial designs and acknowledge the complexity of a SMART. SMART designs may be an optimal way to find treatment strategies that yield durable response, longer survival, and lower toxicity. Clin Cancer Res; 24(4); 730--6. {\copyright}2017 AACR.},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/8UDN9JZX/Kidwell et al. - 2018 - Sequential, Multiple Assignment, Randomized Trial .pdf}
}

@article{kidwellSMARTDesignsCancer2014,
  title = {{{SMART}} Designs in Cancer Research: {{Past}}, Present, and Future},
  shorttitle = {{{SMART}} Designs in Cancer Research},
  author = {Kidwell, Kelley M},
  year = {2014},
  month = aug,
  journal = {Clinical Trials},
  volume = {11},
  number = {4},
  pages = {445--456},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774514525691},
  urldate = {2018-10-12},
  abstract = {Background Cancer affects millions of people worldwide each year. Patients require sequences of treatment based on their response to previous treatments to combat cancer and fight metastases. Physicians provide treatment based on clinical characteristics, changing over time. Guidelines for these individualized sequences of treatments are known as dynamic treatment regimens (DTRs) where the initial treatment and subsequent modifications depend on the response to previous treatments, disease progression, and other patient characteristics or behaviors. To provide evidence-based DTRs, the Sequential Multiple Assignment Randomized Trial (SMART) has emerged over the past few decades. Purpose To examine and learn from past SMARTs investigating cancer treatment options, to discuss potential limitations preventing the widespread use of SMARTs in cancer research, and to describe courses of action to increase the implementation of SMARTs and collaboration between statisticians and clinicians. Conclusion There have been SMARTs investigating treatment questions in areas of cancer, but the novelty and perceived complexity has limited its use. By building bridges between statisticians and clinicians, clarifying research objectives, and furthering methods work, there should be an increase in SMARTs addressing relevant cancer treatment questions. Within any area of cancer, SMARTs develop DTRs that can guide treatment decisions over the disease history and improve patient outcomes.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kidwell_2014_SMART designs in cancer research.pdf}
}

@article{kidwellWeightedLogrankStatistic2013,
  title = {Weighted Log-Rank Statistic to Compare Shared-Path Adaptive Treatment Strategies},
  author = {Kidwell, Kelley M. and Wahed, Abdus S.},
  year = {2013},
  month = apr,
  journal = {Biostatistics},
  volume = {14},
  number = {2},
  pages = {299--312},
  issn = {1465-4644},
  doi = {10/gfppsw},
  urldate = {2018-12-14},
  abstract = {Abstract.  Adaptive treatment strategies (ATSs) more closely mimic the reality of a physician's prescription process where the physician prescribes a medication},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kidwell_Wahed_2013_Weighted log-rank statistic to compare shared-path adaptive treatment strategies.pdf;/Users/nseewald/Zotero/storage/HSWR594N/377435.html}
}

@article{kilbourneAdaptiveSchoolbasedImplementation2018,
  title = {Adaptive {{School-based Implementation}} of {{CBT}} ({{ASIC}}): Clustered-{{SMART}} for Building an Optimized Adaptive Implementation Intervention to Improve Uptake of Mental Health Interventions in Schools},
  shorttitle = {Adaptive {{School-based Implementation}} of {{CBT}} ({{ASIC}})},
  author = {Kilbourne, Amy M. and Smith, Shawna N. and Choi, Seo Youn and Koschmann, Elizabeth and Liebrecht, Celeste and Rusch, Amy and Abelson, James L. and Eisenberg, Daniel and Himle, Joseph A. and Fitzgerald, Kate and Almirall, Daniel},
  year = {2018},
  month = dec,
  journal = {Implementation Science},
  volume = {13},
  number = {1},
  pages = {119},
  issn = {1748-5908},
  doi = {10/gd7jt2},
  urldate = {2021-03-22},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kilbourne et al_2018_Adaptive School-based Implementation of CBT (ASIC).pdf}
}

@article{kilbourneClusterRandomizedAdaptive2013,
  title = {Cluster Randomized Adaptive Implementation Trial Comparing a Standard versus Enhanced Implementation Intervention to Improve Uptake of an Effective Re-Engagement Program for Patients with Serious Mental Illness},
  author = {Kilbourne, Amy M. and Abraham, Kristen M. and Goodrich, David E. and Bowersox, Nicholas W. and Almirall, Daniel and Lai, Zongshan and Nord, Kristina M.},
  year = {2013},
  month = dec,
  journal = {Implementation Science},
  volume = {8},
  number = {1},
  issn = {1748-5908},
  doi = {10.1186/1748-5908-8-136},
  urldate = {2018-10-12},
  abstract = {Discussion: Adaptive implementation designs consisting of a sequence of decision rules that are tailored based on a site's uptake of an effective program may produce more relevant, rapid, and generalizable results by more quickly validating or rejecting new implementation strategies, thus enhancing the efficiency and sustainability of implementation research and potentially leading to the rollout of more cost-efficient implementation strategies. Trial registration: Current Controlled Trials ISRCTN21059161.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kilbourne et al_2013_Cluster randomized adaptive implementation trial comparing a standard versus.pdf}
}

@article{kilbourneLifeGoalsCollaborative2012,
  title = {Life {{Goals Collaborative Care}} for {{Patients With Bipolar Disorder}} and {{Cardiovascular Disease Risk}}},
  author = {Kilbourne, Amy M. and Goodrich, David E. and Lai, Zongshan and Clogston, Julia and Waxmonsky, Jeanette and Bauer, Mark S.},
  year = {2012},
  month = dec,
  journal = {Psychiatric Services},
  volume = {63},
  number = {12},
  pages = {1234--1238},
  issn = {1075-2730, 1557-9700},
  doi = {10.1176/appi.ps.201100528},
  urldate = {2024-08-20},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/2PXZRNAR/Kilbourne et al. - 2012 - Life Goals Collaborative Care for Patients With Bi.pdf}
}

@article{kilbourneProtocolAdaptiveImplementation2014,
  title = {Protocol: {{Adaptive Implementation}} of {{Effective Programs Trial}} ({{ADEPT}}): Cluster Randomized {{SMART}} Trial Comparing a Standard versus Enhanced Implementation Strategy to Improve Outcomes of a Mood Disorders Program},
  shorttitle = {Protocol},
  author = {Kilbourne, Amy M. and Almirall, Daniel and Eisenberg, Daniel and Waxmonsky, Jeanette and Goodrich, David E. and Fortney, John C. and Kirchner, JoAnn E. and Solberg, Leif I. and Main, Deborah and Bauer, Mark S. and Kyle, Julia and Murphy, Susan A. and Nord, Kristina M. and Thomas, Marshall R.},
  year = {2014},
  month = sep,
  journal = {Implementation Science},
  volume = {9},
  number = {1},
  pages = {132},
  issn = {1748-5908},
  doi = {10/f6q9fc},
  urldate = {2019-12-01},
  abstract = {Despite the availability of psychosocial evidence-based practices (EBPs), treatment and outcomes for persons with mental disorders remain suboptimal. Replicating Effective Programs (REP), an effective implementation strategy, still resulted in less than half of sites using an EBP. The primary aim of this cluster randomized trial is to determine, among sites not initially responding to REP, the effect of adaptive implementation strategies that begin with an External Facilitator (EF) or with an External Facilitator plus an Internal Facilitator (IF) on improved EBP use and patient outcomes in 12 months.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kilbourne et al_2014_Protocol.pdf;/Users/nseewald/Zotero/storage/XLSQ58CG/s13012-014-0132-x.html}
}

@article{kimCollectiveReputationDynamics2018,
  title = {Collective {{Reputation}} and the {{Dynamics}} of {{Statistical Discrimination}}},
  author = {Kim, Young-Chul and Loury, Glenn C.},
  year = {2018},
  month = feb,
  journal = {International Economic Review},
  volume = {59},
  number = {1},
  pages = {3--18},
  issn = {1468-2354},
  doi = {10.1111/iere.12260},
  urldate = {2018-10-22},
  abstract = {Economists have developed theoretical models identifying self-fulfilling expectations as an important source of statistical discrimination practices. The static models dominating the literature, however, may leave the false impression that a bad equilibrium is as fragile as a ``bubble'' and can burst at any moment when expectations flip. By developing a dynamic version of the model, we clarify the limits of expectations-related fragility. Even if group members can coordinate their expectations about future employer behavior, a group with a poor initial collective reputation may still be unable to recover its reputation, implying that the once-developed discriminatory outcomes can be long-standing.},
  copyright = {{\copyright} (2017) by the Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kim_Loury_2018_Collective Reputation and the Dynamics of Statistical Discrimination.pdf;/Users/nseewald/Zotero/storage/MU8996FL/iere.html}
}

@book{kingdonAgendasAlternativesPublic2003,
  title = {Agendas, Alternatives, and Public Policies},
  author = {Kingdon, John W.},
  year = {2003},
  series = {Longman Classics in Political Science},
  edition = {2nd ed},
  publisher = {Longman},
  address = {New York},
  isbn = {978-0-321-12185-1},
  lccn = {JK274 .K62 2003},
  keywords = {Policy sciences,Political planning,Politics and government,United States}
}

@article{kirkpatrickApplicationsHealthyEating2018,
  title = {Applications of the {{Healthy Eating Index}} for {{Surveillance}}, {{Epidemiology}}, and {{Intervention Research}}: {{Considerations}} and {{Caveats}}},
  shorttitle = {Applications of the {{Healthy Eating Index}} for {{Surveillance}}, {{Epidemiology}}, and {{Intervention Research}}},
  author = {Kirkpatrick, Sharon I. and Reedy, Jill and {Krebs-Smith}, Susan M. and Pannucci, TusaRebecca E. and Subar, Amy F. and Wilson, Magdalena M. and Lerman, Jennifer L. and Tooze, Janet A.},
  year = {2018},
  month = sep,
  journal = {Journal of the Academy of Nutrition and Dietetics},
  volume = {118},
  number = {9},
  pages = {1603--1621},
  issn = {2212-2672},
  doi = {10.1016/j.jand.2018.05.020},
  urldate = {2025-02-13},
  abstract = {The Healthy Eating Index (HEI) is a measure of diet quality that can be used to examine alignment of dietary patterns with the Dietary Guidelines for Americans. The HEI is made up of multiple adequacy and moderation components, most of which are expressed relative to energy intake (ie, as densities) for the purpose of calculating scores. Due to these characteristics and the complexity of dietary intake data more broadly, calculating and using HEI scores can involve unique statistical considerations and, depending on the particular application, intensive computational methods. The objective of this article is to review potential applications of the HEI, including those relevant to surveillance, epidemiology, and intervention research, and to summarize available guidance for appropriate analysis and interpretation. Steps in calculating HEI scores are reviewed and statistical methods described. Consideration of salient issues in the calculation and interpretation of scores can help researchers avoid common pitfalls and reviewers ensure that articles reporting on the use of the HEI include sufficient details such that the work is comprehensible and replicable, with the overall goal of contributing to knowledge on dietary patterns and health among Americans.},
  keywords = {Diet indexes,Diet quality,Dietary patterns,Healthy Eating Index,Statistical modeling},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kirkpatrick et al. - 2018 - Applications of the Healthy Eating Index for Surveillance, Epidemiology, and Intervention Research.pdf;/Users/nseewald/Zotero/storage/DU7TE6MV/S2212267218308372.html}
}

@article{kishimotoAntiracistPedagogyFaculty2018,
  title = {Anti-Racist Pedagogy: From Faculty's Self-Reflection to Organizing within and beyond the Classroom},
  shorttitle = {Anti-Racist Pedagogy},
  author = {Kishimoto, Kyoko},
  year = {2018},
  month = jul,
  journal = {Race Ethnicity and Education},
  volume = {21},
  number = {4},
  pages = {540--554},
  publisher = {Routledge},
  issn = {1361-3324},
  doi = {10/gcz7ng},
  urldate = {2020-06-05},
  abstract = {This article is a synthesis of my own work as well as a critical reading of the key literature in anti-racist pedagogy. Its purpose is to define anti-racist pedagogy and what applying this to courses and the fullness of our professional lives entails. I argue that faculty need to be aware of their social position, but more importantly, to begin and continue critical self-reflection in order to effectively implement anti-racist pedagogy, which has three components: (1) incorporating the topics of race and inequality into course content, (2) teaching from an anti-racist pedagogical approach, and (3) anti-racist organizing within the campus and linking our efforts to the surrounding community. In other words, anti-racist pedagogy is an organizing effort for institutional and social change that is much broader than teaching in the classroom.},
  keywords = {teaching},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kishimoto_2018_Anti-racist pedagogy.pdf}
}

@article{klasnjaEfficacyContextuallyTailored2018,
  title = {Efficacy of {{Contextually Tailored Suggestions}} for {{Physical Activity}}: {{A Micro-randomized Optimization Trial}} of {{HeartSteps}}},
  author = {Klasnja, Predrag and Smith, Shawna and Seewald, Nicholas J. and Lee, Andy and Hall, Kelly and Luers, Brook and Hekler, Eric B. and Murphy, Susan A.},
  year = {2018},
  journal = {Annals of Behavioral Medicine},
  pages = {10},
  doi = {10.1093/abm/kay067},
  abstract = {Background HeartSteps is an mHealth intervention that encourages regular walking via activity suggestions tailored to the individuals' current context.},
  copyright = {All rights reserved},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Klasnja et al_2018_Efficacy of Contextually Tailored Suggestions for Physical Activity.pdf}
}

@article{klasnjaMicrorandomizedTrialsExperimental2015,
  title = {Microrandomized Trials: {{An}} Experimental Design for Developing Just-in-Time Adaptive Interventions.},
  shorttitle = {Microrandomized Trials},
  author = {Klasnja, Predrag and Hekler, Eric B. and Shiffman, Saul and Boruvka, Audrey and Almirall, Daniel and Tewari, Ambuj and Murphy, Susan A.},
  year = {2015},
  month = dec,
  journal = {Health Psychology},
  volume = {34},
  number = {Suppl},
  pages = {1220--1228},
  issn = {1930-7810, 0278-6133},
  doi = {10.1037/hea0000305},
  urldate = {2018-10-12},
  abstract = {Objective: This article presents an experimental design, the microrandomized trial, developed to support optimization of just-in-time adaptive interventions (JITAIs). JITAIs are mHealth technologies that aim to deliver the right intervention components at the right times and locations to optimally support individuals' health behaviors. Microrandomized trials offer a way to optimize such interventions by enabling modeling of causal effects and time-varying effect moderation for individual intervention components within a JITAI. Method: The article describes the microrandomized trial design, enumerates research questions that this experimental design can help answer, and provides an overview of the data analyses that can be used to assess the causal effects of studied intervention components and investigate time-varying moderation of those effects. Results: Microrandomized trials enable causal modeling of proximal effects of the randomized intervention components and assessment of time-varying moderation of those effects. Conclusion: Microrandomized trials can help researchers understand whether their interventions are having intended effects, when and for whom they are effective, and what factors moderate the interventions' effects, enabling creation of more effective JITAIs.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Klasnja et al_2015_Microrandomized trials.pdf}
}

@article{kleinAnalyzingSurvivalCurves2007,
  title = {Analyzing Survival Curves at a Fixed Point in Time},
  author = {Klein, John P. and Logan, Brent and Harhoff, Mette and Andersen, Per Kragh},
  year = {2007},
  journal = {Statistics in Medicine},
  volume = {26},
  number = {24},
  pages = {4505--4519},
  issn = {1097-0258},
  doi = {10.1002/sim.2864},
  urldate = {2024-09-24},
  abstract = {A common problem encountered in many medical applications is the comparison of survival curves. Often, rather than comparison of the entire survival curves, interest is focused on the comparison at a fixed point in time. In most cases, the naive test based on a difference in the estimates of survival is used for this comparison. In this note, we examine the performance of alternatives to the naive test. These include tests based on a number of transformations of the survival function and a test based on a generalized linear model for pseudo-observations. The type I errors and power of these tests for a variety of sample sizes are compared by a Monte Carlo study. We also discuss how these tests may be extended to situations where the data are stratified. The pseudo-value approach is also applicable in more detailed regression analysis of the survival probability at a fixed point in time. The methods are illustrated on a study comparing survival for autologous and allogeneic bone marrow transplants. Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {censored data,generalized linear models,Kaplan-Meier estimators,pseudo-value approach,variance stabilizing transformation},
  file = {/Users/nseewald/Zotero/storage/MAWLXCAU/sim.html}
}

@article{kloekOLSEstimationModel1981,
  title = {{{OLS Estimation}} in a {{Model Where}} a {{Microvariable}} Is {{Explained}} by {{Aggregates}} and {{Contemporaneous Disturbances}} Are {{Equicorrelated}}},
  author = {Kloek, T.},
  year = {1981},
  journal = {Econometrica},
  volume = {49},
  number = {1},
  eprint = {1911134},
  eprinttype = {jstor},
  pages = {205--207},
  publisher = {[Wiley, Econometric Society]},
  issn = {0012-9682},
  doi = {10.2307/1911134},
  urldate = {2022-03-17},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kloek_1981_OLS Estimation in a Model Where a Microvariable is Explained by Aggregates and.pdf}
}

@article{kolodnyPrescriptionOpioidHeroin2015,
  title = {The {{Prescription Opioid}} and {{Heroin Crisis}}: {{A Public Health Approach}} to an {{Epidemic}} of {{Addiction}}},
  shorttitle = {The {{Prescription Opioid}} and {{Heroin Crisis}}},
  author = {Kolodny, Andrew and Courtwright, David T. and Hwang, Catherine S. and Kreiner, Peter and Eadie, John L. and Clark, Thomas W. and Alexander, G. Caleb},
  year = {2015},
  month = mar,
  journal = {Annual Review of Public Health},
  volume = {36},
  number = {Volume 36, 2015},
  pages = {559--574},
  publisher = {Annual Reviews},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev-publhealth-031914-122957},
  urldate = {2024-05-01},
  abstract = {Public health authorities have described, with growing alarm, an unprecedented increase in morbidity and mortality associated with use of opioid pain relievers (OPRs). Efforts to address the opioid crisis have focused mainly on reducing nonmedical OPR use. Too often overlooked, however, is the need for preventing and treating opioid addiction, which occurs in both medical and nonmedical OPR users. Overprescribing of OPRs has led to a sharp increase in the prevalence of opioid addiction, which in turn has been associated with a rise in overdose deaths and heroin use. A multifaceted public health approach that utilizes primary, secondary, and tertiary opioid addiction prevention strategies is required to effectively reduce opioid-related morbidity and mortality. We describe the scope of this public health crisis, its historical context, contributing factors, and lines of evidence indicating the role of addiction in exacerbating morbidity and mortality, and we provide a framework for interventions to address the epidemic of opioid addiction.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kolodny_et_al_2015_The_Prescription_Opioid_and_Heroin_Crisis.pdf;/Users/nseewald/Zotero/storage/RDM88S5E/annurev-publhealth-031914-122957.html}
}

@article{komiyamaComparingFairnessCriteria2018,
  title = {Comparing {{Fairness Criteria Based}} on {{Social Outcome}}},
  author = {Komiyama, Junpei and Shimao, Hajime},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.05112 [cs, stat]},
  eprint = {1806.05112},
  primaryclass = {cs, stat},
  urldate = {2018-10-12},
  abstract = {Fairness in algorithmic decision-making processes is attracting increasing concern. When an algorithm is applied to human-related decisionmaking an estimator solely optimizing its predictive power can learn biases on the existing data, which motivates us the notion of fairness in machine learning. while several different notions are studied in the literature, little studies are done on how these notions affect the individuals. We demonstrate such a comparison between several policies induced by well-known fairness criteria, including the color-blind (CB), the demographic parity (DP), and the equalized odds (EO). We show that the EO is the only criterion among them that removes group-level disparity. Empirical studies on the social welfare and disparity of these policies are conducted.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Komiyama_Shimao_2018_Comparing Fairness Criteria Based on Social Outcome.pdf}
}

@article{koningIdentificationChildrenRisk2019,
  title = {Identification of Children at Risk for Mental Health Problems in Primary Care---{{Development}} of a Prediction Model with Routine Health Care Data},
  author = {Koning, Nynke R. and B{\"u}chner, Frederike L. and Vermeiren, Robert R. J. M. and Crone, Mathilde R. and Numans, Mattijs E.},
  year = {2019},
  month = oct,
  journal = {EClinicalMedicine},
  volume = {15},
  pages = {89--97},
  issn = {2589-5370},
  doi = {10.1016/j.eclinm.2019.09.007},
  urldate = {2022-08-19},
  abstract = {Background Despite being common and having long lasting effects, mental health problems in children are often under-recognised and under-treated. Improving early identification is important in order to provide adequate, timely treatment. We aimed to develop prediction models for the one-year risk of a first recorded mental health problem in children attending primary care. Methods We carried out a population-based cohort study based on readily available routine healthcare data anonymously extracted from electronic medical records of 76 general practice centers in the Leiden area, the Netherlands. We included all patients aged 1--19 years on 31 December 2016 without prior mental health problems. Multilevel logistic regression analyses were used to predict the one-year risk of a first recorded mental health problem. Potential predictors were characteristics related to the child, family and healthcare use. Model performance was assessed by examining measures of discrimination and calibration. Findings Data from 70,000 children were available. A mental health problem was recorded in 27{$\bullet$}7\% of patients during the period 2007--2017. Age independent predictors were somatic complaints, more than two GP visits in the previous year, one or more laboratory test and one or more referral/contact with other healthcare professional in the previous year. Other predictors and their effects differed between age groups. Model performance was moderate (c-statistic 0.62--0.63), while model calibration was good. Interpretation This study is a first promising step towards developing prediction models for identifying children at risk of a first mental health problem to support primary care practice by using routine healthcare data. Data enrichment from other available sources regarding e.g. school performance and family history could improve model performance. Further research is needed to externally validate our models and to establish whether we are able to improve under-recognition of mental health problems.},
  langid = {english},
  keywords = {Children,Identification,Mental health,Primary care},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Koning et al_2019_Identification of children at risk for mental health problems in primary.pdf;/Users/nseewald/Zotero/storage/CFWB9ITQ/S2589537019301816.html}
}

@article{kontopantelisRegressionBasedQuasiexperimental2015,
  title = {Regression Based Quasi-Experimental Approach When Randomisation Is Not an Option: Interrupted Time Series Analysis},
  shorttitle = {Regression Based Quasi-Experimental Approach When Randomisation Is Not an Option},
  author = {Kontopantelis, Evangelos and Doran, Tim and Springate, David A. and Buchan, Iain and Reeves, David},
  year = {2015},
  month = jun,
  journal = {BMJ},
  volume = {350},
  pages = {h2750},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj.h2750},
  urldate = {2022-04-29},
  abstract = {{$<$}p{$>$}Interrupted time series analysis is a quasi-experimental design that can evaluate an intervention effect, using longitudinal data. The advantages, disadvantages, and underlying assumptions of various modelling approaches are discussed using published examples {$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {{\copyright} Kontopantelis et al 2015. This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {26058820},
  file = {C\:\\Users\\nseew\\Google Drive\\Papers\\Kontopantelis_et_al_2015_Regression_based_quasi-experimental_approach_when_randomisation_is_not_an_option.pdf;/Users/nseewald/Zotero/storage/AQXX9JEF/bmj.h2750.html}
}

@article{korSequentialMultipleAssignment2023,
  title = {Sequential Multiple Assignment Randomised Controlled Trial Protocol for Developing an Adaptive Intervention to Improve Depressive Symptoms among Family Caregivers of People with Dementia},
  author = {Kor, Patrick Pui Kin and Chou, Kee Lee and Zarit, Steven H and Gallagher, Dolores and Galante, Julieta and Wong, Samuel Y S and Cheung, Daphne Sze Ki and Leung, Angela Y M and Chu, Leung-Wing},
  year = {2023},
  month = sep,
  journal = {BMJ Open},
  volume = {13},
  number = {9},
  pages = {e072410},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2023-072410},
  urldate = {2024-10-22},
  abstract = {Objectives               Family caregivers of people with dementia (FC-of-PWD) suffer from a high level of stress and depressive symptoms, which usually require different interventions at different stages. Although some standalone interventions such as behavioural activation (BA) and mindfulness practice (MP) have been shown to be potentially effective at reducing depressive symptoms, the best sequence and combination of these interventions for caregivers are unknown. This study aims to develop and identify a two-stage adaptive intervention with prespecified rules guiding whether, how or when to offer different interventions initially/over time to reduce depressive symptoms in FG-of-PWD.                                         Methods               A sequential multiple assignment randomised trial design will be adopted. 272 FG-of-PWD with mild to moderate depressive symptoms will be recruited from the community. Four two-stage, embedded adaptive interventions involving BA and MP of different sequences and dosages (eg, 8 weeks of BA followed by booster sessions for responders and 8 weeks of MP for non-responders) will be assigned to the participants following a set of decision rules. The primary outcomes will be depressive symptoms (measured using the Patient Health Questionnaire-9), assessed after the second stage of the intervention. Other outcomes, such as positive aspects of caregiving (measured using the Positive Aspects of Caregiving Scale), sleep quality (measured using the Pittsburgh Sleep Quality Index) and time points will also be assessed. The analyses will follow the intention-to-treat principle. Several process indicators (eg, engagement in meaningful activities and level of mindfulness) will also be assessed. The findings will have strong implications for the further development of psychosocial adaptive interventions to reduce depressive symptoms among FC-of-PWD.                                         Ethics and dissemination               This study has received ethical approval from the Human Research Ethics Committee at The Hong Kong Polytechnic University (HSEARS20211223001). The findings will be presented at academic conferences and submitted to peer-reviewed journals for publication.                                         Trial registration number                                NCT05634317                 .},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kor et al. - 2023 - Sequential multiple assignment randomised controlled trial protocol for developing an adaptive inter.pdf}
}

@book{kosorokAdaptiveTreatmentStrategies2015,
  title = {Adaptive {{Treatment Strategies}} in {{Practice}}: {{Planning Trials}} and {{Analyzing Data}} for {{Personalized Medicine}}},
  shorttitle = {Adaptive {{Treatment Strategies}} in {{Practice}}},
  editor = {Kosorok, Michael R. and Moodie, Erica E. M.},
  year = {2015},
  month = dec,
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, PA},
  doi = {10.1137/1.9781611974188},
  urldate = {2018-10-12},
  isbn = {978-1-61197-417-1},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kosorok_Moodie_2015_Adaptive Treatment Strategies in Practice.pdf}
}

@inproceedings{kotzThreatTaxonomyMHealth2011,
  title = {A Threat Taxonomy for {{mHealth}} Privacy},
  booktitle = {2011 {{Third International Conference}} on {{Communication Systems}} and {{Networks}} ({{COMSNETS}} 2011)},
  author = {Kotz, D},
  year = {2011},
  month = jan,
  pages = {1--6},
  publisher = {IEEE},
  address = {Bangalore},
  doi = {10.1109/COMSNETS.2011.5716518},
  urldate = {2018-10-12},
  abstract = {Networked mobile devices have great potential to enable individuals (and their physicians) to better monitor their health and to manage medical conditions. In this paper, we examine the privacy-related threats to these so-called mHealth technologies. We develop a taxonomy of the privacy-related threats, and discuss some of the technologies that could support privacy-sensitive mHealth systems. We conclude with a brief summary of research challenges.},
  isbn = {978-1-4244-8952-7},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kotz_2011_A threat taxonomy for mHealth privacy.pdf}
}

@article{koUpfrontSequentialRandomizations2012,
  title = {Up-Front versus Sequential Randomizations for Inference on Adaptive Treatment Strategies},
  author = {Ko, Jin H. and Wahed, Abdus S.},
  year = {2012},
  journal = {Statistics in Medicine},
  volume = {31},
  number = {9},
  pages = {812--830},
  issn = {1097-0258},
  doi = {10.1002/sim.4473},
  urldate = {2022-08-26},
  abstract = {Adaptive treatment strategies are useful in the treatment of chronic diseases such as AIDS and cancer because they allow tailoring the treatment to a patient's need and disease status. We consider two randomization schemes for clinical trials that are commonly used to design studies comparing adaptive treatment strategies, namely, up-front randomization and sequential randomization. Up-front randomization is the classical method of randomization where patients are randomized at the beginning of the study to pre-specified treatment strategies. In sequentially randomized trials, patients are randomized sequentially to available treatment options over the duration of the therapy as they become eligible to receive subsequent treatments. We compare the efficiency and the power of the traditional up-front randomized trials with that of sequentially randomized trials designed for comparing adaptive treatment strategies based on a continuous outcome. The analytical and simulation results indicate that, when properly analyzed, sequentially randomized trials are more efficient and powerful than up-front randomized trials. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {adaptive treatment strategies,inverse probability weighting,sequential randomization,two-stage randomization design},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ko_Wahed_2012_Up-front versus sequential randomizations for inference on adaptive treatment.pdf;/Users/nseewald/Zotero/storage/426AV66A/sim.html}
}

@article{kraemerStrategyUseSoft1989,
  title = {A Strategy to Use Soft Data Effectively in Randomized Controlled Clinical Trials},
  author = {Kraemer, Helena C. and Thiemann, Sue},
  year = {1989},
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {57},
  number = {1},
  pages = {148--154},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2117(Electronic),0022-006X(Print)},
  doi = {10/b5kb34},
  abstract = {Soft data are defined as measures having substantial intrasubject variability due to errors of measurement or to the inconsistency of subjects' responses. Such data are often important measures of response in randomized clinical trials. In this context, we show that using an intensive design and the slope of response on time as the outcome measure (a) maximizes sample retention and (b) decreases within-group variability, thus (c) maximizing the power of test procedures without requiring increased sample sizes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Clinical Trials,Experimental Methods,Experimentation,Statistical Data,Treatment},
  file = {/Users/nseewald/Zotero/storage/V3UHLMD8/1989-24684-001.html}
}

@article{kravetsStatisticalConsiderationsSoftware2024,
  title = {Statistical {{Considerations}} and {{Software}} for {{Designing Sequential}}, {{Multiple Assignment}}, {{Randomized Trials}} ({{SMART}}) with a {{Survival Final Endpoint}}},
  author = {Kravets, Sasha and Ruppert, Amy S. and Jacobson, Sawyer B. and {Le-Rademacher}, Jennifer G. and Mandrekar, Sumithra J.},
  year = {2024},
  month = jul,
  journal = {Journal of Biopharmaceutical Statistics},
  volume = {34},
  number = {4},
  pages = {539--552},
  publisher = {Taylor \& Francis},
  issn = {1054-3406},
  doi = {10.1080/10543406.2023.2233616},
  urldate = {2024-08-28},
  abstract = {Sequential, multiple assignment, randomized trial (SMART) designs are appropriate for comparing adaptive treatment interventions, in which intermediate outcomes (called tailoring variables) guide subsequent treatment decisions for individual patients. Within a SMART design, patients may be re-randomized to subsequent treatments following the outcomes of their intermediate assessments. In this paper, we provide an overview of statistical considerations necessary to design and implement a two-stage SMART design with a binary tailoring variable and a survival final endpoint. A chronic lymphocytic leukemia trial with a final endpoint of progression-free survival is used as an example for the simulations to assess how design parameters, including, choice of randomization ratios for each stage of randomization, and response rates of the tailoring variable affect the statistical power. We assess the choice of weights from restricted re-randomization on data analyses and appropriate hazard rate assumptions. Specifically, for a given first-stage therapy and prior to the tailoring variable assessment, we assume equal hazard rates for all patients randomized to a treatment arm. After the tailoring variable assessment, individual hazard rates are assumed for each intervention path. Simulation studies demonstrate that the response rate of the binary tailoring variable impacts power as it directly impacts the distribution of patients. We also confirm that when the first stage randomization is 1:1, it is not necessary to consider the first stage randomization ratio when applying the weights. We provide an R-shiny application for obtaining power for a given sample size for SMART designs.},
  pmid = {37434437},
  keywords = {Adaptive treatment regimen,Rshiny,Sequential multiple assignment randomized trial (SMART),Survival analysis}
}

@article{kumarMobileHealthRevolutionizing2013,
  title = {Mobile {{Health}}: {{Revolutionizing Healthcare Through Transdisciplinary Research}}},
  shorttitle = {Mobile {{Health}}},
  author = {Kumar, S. and Nilsen, W. and Pavel, M. and Srivastava, M.},
  year = {2013},
  month = jan,
  journal = {Computer},
  volume = {46},
  number = {1},
  pages = {28--35},
  issn = {0018-9162},
  doi = {10.1109/MC.2012.392},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kumar et al_2013_Mobile Health.pdf}
}

@article{kumarMobileHealthTechnology2013,
  title = {Mobile {{Health Technology Evaluation}}},
  author = {Kumar, Santosh and Nilsen, Wendy J. and Abernethy, Amy and Atienza, Audie and Patrick, Kevin and Pavel, Misha and Riley, William T. and Shar, Albert and Spring, Bonnie and {Spruijt-Metz}, Donna and Hedeker, Donald and Honavar, Vasant and Kravitz, Richard and Craig Lefebvre, R. and Mohr, David C. and Murphy, Susan A. and Quinn, Charlene and Shusterman, Vladimir and Swendeman, Dallas},
  year = {2013},
  month = aug,
  journal = {American Journal of Preventive Medicine},
  volume = {45},
  number = {2},
  pages = {228--236},
  issn = {07493797},
  doi = {10.1016/j.amepre.2013.03.017},
  urldate = {2018-10-12},
  abstract = {Creative use of new mobile and wearable health information and sensing technologies (mHealth) has the potential to reduce the cost of health care and improve well-being in numerous ways. These applications are being developed in a variety of domains, but rigorous research is needed to examine the potential, as well as the challenges, of utilizing mobile technologies to improve health outcomes. Currently, evidence is sparse for the efficacy of mHealth. Although these technologies may be appealing and seemingly innocuous, research is needed to assess when, where, and for whom mHealth devices, apps, and systems are efficacious.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kumar et al_2013_Mobile Health Technology Evaluation.pdf}
}

@article{kvaloyEstimationInferenceNonparametric2003,
  title = {Estimation and {{Inference}} in {{Nonparametric Cox-models}}: {{Time Transformation Methods}}},
  shorttitle = {Estimation and {{Inference}} in {{Nonparametric Cox-models}}},
  author = {Kval{\o}y, Jan Terje and Lindqvist, Bo Henry},
  year = {2003},
  month = jul,
  journal = {Computational Statistics},
  volume = {18},
  number = {2},
  pages = {205--221},
  issn = {1613-9658},
  doi = {10.1007/s001800300141},
  urldate = {2024-02-20},
  abstract = {In this paper generalization of the Cox proportional hazards regression model to a completely nonparametric model with an unspecified smooth covariate function is studied. A class of methods for Cox-regression called time transformation methods are defined, and a new method for nonparametric Cox-regression in this class is in particular studied. It turns out that this method enjoys a number of useful properties.},
  langid = {english},
  keywords = {_tablet,Covariate order method,Kernel estimation,Local likelihood,Nonparametric hazard regression,Proportional hazard},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Kvaløy_Lindqvist_2003_Estimation_and_Inference_in_Nonparametric_Cox-models.pdf}
}

@article{laberAdaptiveConfidenceIntervals2011,
  title = {Adaptive {{Confidence Intervals}} for the {{Test Error}} in {{Classification}}},
  author = {Laber, Eric B. and Murphy, Susan A.},
  year = {2011},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {106},
  number = {495},
  pages = {904--913},
  issn = {0162-1459, 1537-274X},
  doi = {10/bx7qt7},
  urldate = {2019-03-14},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Laber_Murphy_2011_Adaptive Confidence Intervals for the Test Error in Classification.pdf}
}

@article{laberDynamicTreatmentRegimes2014,
  title = {Dynamic Treatment Regimes: {{Technical}} Challenges and Applications},
  shorttitle = {Dynamic Treatment Regimes},
  author = {Laber, Eric B. and Lizotte, Daniel J. and Qian, Min and Pelham, William E. and Murphy, Susan A.},
  year = {2014},
  journal = {Electronic Journal of Statistics},
  volume = {8},
  number = {1},
  pages = {1225--1272},
  issn = {1935-7524},
  doi = {10/gg29c8},
  urldate = {2020-12-13},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Laber et al_2014_Dynamic treatment regimes.pdf}
}

@article{lachinIntroductionSampleSize1981,
  title = {Introduction to Sample Size Determination and Power Analysis for Clinical Trials},
  author = {Lachin, John M.},
  year = {1981},
  month = jun,
  journal = {Controlled Clinical Trials},
  volume = {2},
  number = {2},
  pages = {93--113},
  issn = {01972456},
  doi = {10.1016/0197-2456(81)90001-5},
  urldate = {2018-10-12},
  abstract = {The importance of sample size evaluation in clinical trials is reviewed and a general method is presented from which specific equations are derived for sample size determination or the analysis of power for a wide variety of statistical procedures. The method is discussed and illustrated in relation to the t test, tests for proportions, tests of survival time, and tests for correlations as they commonly occur in clinical trials. Most of the specific equations reduce to a simple general form for which tables are presented.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lachin_1981_Introduction to sample size determination and power analysis for clinical trials.pdf}
}

@article{laforgiaAssociationSurpriseBillingLegislation2021,
  title = {Association of {{Surprise-Billing Legislation}} with {{Prices Paid}} to {{In-Network}} and {{Out-of-Network Anesthesiologists}} in {{California}}, {{Florida}}, and {{New York}}: {{An Economic Analysis}}},
  shorttitle = {Association of {{Surprise-Billing Legislation}} with {{Prices Paid}} to {{In-Network}} and {{Out-of-Network Anesthesiologists}} in {{California}}, {{Florida}}, and {{New York}}},
  author = {La Forgia, Ambar and Bond, Amelia M. and Braun, Robert Tyler and Kjaer, Klaus and Zhang, Manyao and Casalino, Lawrence P.},
  year = {2021},
  month = oct,
  journal = {JAMA Internal Medicine},
  volume = {181},
  number = {10},
  pages = {1324--1331},
  issn = {2168-6106},
  doi = {10.1001/jamainternmed.2021.4564},
  urldate = {2022-06-16},
  abstract = {Several states have passed surprise-billing legislation to protect patients from unanticipated out-of-network medical bills, yet little is known about how state laws influence out-of-network prices and whether spillovers exist to in-network prices.To identify any changes in prices paid to out-of-network anesthesiologists at in-network facilities and to in-network anesthesiologists before and after states passed surprise-billing legislation.This retrospective economic analysis used difference-in-differences methods to compare price changes before and after the passage of legislation in California, Florida, and New York, which passed comprehensive surprise-billing legislation between January 1, 2014, and December 31, 2017, to 45 states that did not. Commercial claims data from the Health Care Cost Institute were used to identify prices paid to anesthesiologists in hospital outpatient departments and ambulatory surgery centers. The final analytic sample comprised 2\,713\,913 anesthesia claims across the 3 treated states and the 45 control states.Temporal and state-level variation in exposure to surprise-billing legislation.The unit price (allowed amounts standardized per unit of service) paid to out-of-network anesthesiologists at in-network facilities and to in-network anesthesiologists.This retrospective economic analysis of 2 713 913 anesthesia claims found that after surprise-billing laws were passed in 3 states, the unit price paid to out-of-network anesthesiologists at in-network facilities decreased significantly in 2 of them: California, -\$12.71 (95\% CI, -\$25.70 to -\$0.27; P\,=\,.05) and Florida, -\$35.67 (95\% CI, -\$46.27 to -\$25.07; P\,\&lt;\,.001). In New York, a decline in the overall out-of-network price was not statistically significant (-\$7.91; 95\% CI, -\$17.48 to -\$1.68; P\,=\,.10); however, by the fourth quarter of 2017, the decline was -\$41.28 (95\% CI, -\$70.24 to -\$12.33; P\,=\,.01). In-network prices decreased in California by -\$10.68 (95\% CI, -\$12.70 to -\$8.66; P\,\&lt;\,.001); in Florida, -\$3.18 (95\% CI, -\$5.17 to -\$1.19; P\,=\,.002); and in New York, -\$8.05 (95\% CI, -\$11.46 to -\$4.64; P\,\&lt;\,.001).This retrospective study found that prices paid to in-network and out-of-network anesthesiologists in hospital outpatient departments and ambulatory surgery centers decreased after the introduction of surprise-billing legislation, providing early insights into how prices may change under the federal No Surprises Act and in states that have recently passed their own legislation.},
  file = {/Users/nseewald/Zotero/storage/4ITTEZLU/2782816.html}
}

@article{lairdEstimatingRatesChange1990,
  title = {Estimating Rates of Change in Randomized Clinical Trials},
  author = {Laird, Nan M. and Wang, Fong},
  year = {1990},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {11},
  number = {6},
  pages = {405--419},
  issn = {0197-2456},
  doi = {10/dwqckp},
  urldate = {2020-01-14},
  abstract = {This article deals with the extension of the pretest-posttest clinical trial to the longitudinal data setting. We assume that a baseline (or pretest) measurement is taken on all individuals, who are then randomized, without regard to baseline values, to a treatment group. Repeated measurements are taken postrandomization at specified times. Our objective is to estimate the average rate of change (or slope) in the experimental groups and the differences in the slopes. Our focus is on the optimal use of the baseline measurements in the analysis. We contrast two different approaches:--- a multivariate one that regards the entire vector of responses (including the baseline) as random outcomes and a univariate one that uses each individual's least squares slope as an outcome. Our multivariate approach is essentially a generalization of Stanek's Seemingly Unrelated Regression (SUR) estimator for the pretest-posttest design (Am Stat 42:178--183, 1988). The multivariate approach is natural to apply in this setting, and optimal if the assumed model is correct. However, the most efficient estimator requires assuming that the baseline mean parameters are the same for all experimental groups. Although this assumption is reasonable in the randomized setting, the resulting multivariate estimator uses postrandomization data as a covariate; if the assumed linear model is not correct, this can lead to distortions in the estimated treatment effect. We propose instead a reduced form multivariate estimator that may be somewhat less efficient, but protects against model misspecification.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Laird_Wang_1990_Estimating rates of change in randomized clinical trials.pdf;/Users/nseewald/Zotero/storage/W6ULGR6A/019724569090018W.html}
}

@article{langkjaer-bainTroublingLegacyFrancis2019,
  title = {The Troubling Legacy of {{Francis Galton}}},
  author = {Langkj{\ae}r-Bain, Robert},
  year = {2019},
  journal = {Significance},
  volume = {16},
  number = {3},
  pages = {16--21},
  issn = {1740-9713},
  doi = {10/ggh82c},
  urldate = {2020-01-21},
  abstract = {Few figures stand taller in the history of statistics than Sir Francis Galton. But can we separate Galton's achievements in statistical science from his views on race and his association with eugenics? By Robert Langkj{\ae}r-Bain},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Langkjær‐Bain_2019_The troubling legacy of Francis Galton.pdf;/Users/nseewald/Zotero/storage/W9T5GPKS/j.1740-9713.2019.01275.html}
}

@article{lankenauBecomingMedicalMarijuana2018,
  title = {Becoming a Medical Marijuana User},
  author = {Lankenau, Stephen E. and Kioumarsi, Avat and Reed, Megan and McNeeley, Miles and Iverson, Ellen and Wong, Carolyn F.},
  year = {2018},
  month = feb,
  journal = {International Journal of Drug Policy},
  volume = {52},
  pages = {62--70},
  issn = {0955-3959},
  doi = {10.1016/j.drugpo.2017.11.018},
  urldate = {2024-05-01},
  abstract = {Background Since marijuana became legal for medical use in California in 1996, reasons for medical use among medical marijuana patients (MMP) have become increasingly well described in qualitative studies. However, few studies have detailed how the use of marijuana for medical purposes fits into the broader career trajectories of either becoming a marijuana user or becoming a MMP, including the social influences on medical use. Methods Young adult MMP (N=40) aged 18 to 26 years old were recruited in Los Angeles, CA in 2014-15 and administered a semi-structured interview that included questions focusing on marijuana use practices before and after becoming MMP. Results MMP were categorized into three trajectory groups: primarily medical users (n=30); primarily non-medical users (n=3); and medical users who transitioned to non-medical users (n=7). Most medical users discovered medicinal effects from marijuana in the context of non-medical use as adolescents prior to becoming MMP. Becoming a mature MMP followed interactions with dispensary staff or further self-exploration of medical uses and often involved a social process that helped confirm the legitimacy of medical use and identity as a medical user. In some cases, MMP transitioned back to non-medical users as health conditions improved or remained primarily non-medical users even after becoming MMP for reasons unrelated to health, e.g., protection against arrest. Conclusion Becoming a medical marijuana user was an important career trajectory that was influenced by early discoveries of effective medicinal use, interaction with proponents of medical use at dispensaries, experiences with new kinds of medical use, and the demands of particular health condition requiring more or less treatment with marijuana.},
  keywords = {Medical marijuana,Qualitative research,Young adults},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lankenau_et_al_2018_Becoming_a_medical_marijuana_user.pdf;/Users/nseewald/Zotero/storage/K4X5WHFU/S095539591730347X.html}
}

@article{lashleyWaitingInhaleReducing2020,
  title = {Waiting to {{Inhale}}: {{Reducing Stigma}} in the {{Medical Cannabis Industry}}},
  shorttitle = {Waiting to {{Inhale}}},
  author = {Lashley, Kisha and Pollock, Timothy G.},
  year = {2020},
  month = jun,
  journal = {Administrative Science Quarterly},
  volume = {65},
  number = {2},
  pages = {434--482},
  publisher = {SAGE Publications Inc},
  issn = {0001-8392},
  doi = {10.1177/0001839219851501},
  urldate = {2024-05-03},
  abstract = {When a new industry category is predicated on a product or activity subject to ``core'' stigma---meaning its very nature is stigmatized---the actors trying to establish it may struggle to gain the resources they need to survive and grow. To explain the process of reducing an industry category's stigma, we take an inductive approach to understanding how actors in the U.S. medical cannabis industry collectively attempted to create and disseminate a moral public image based on healing and patients' rights. We find that reducing category-level core stigma is a phased effort that takes place across different relational spaces. A moral agenda based on broadly acceptable values jumpstarts the process, and the industry then creates a new moral prototype reflecting these values that industry actors can identify with. Category members must publicly disidentify with the current, stigmatized prototypes and infuse the new moral prototype among their stakeholder audiences through their language and practices, creating emotional connections that lead to cognitive acceptance. This process is messy, as individual organizations often need to continue engaging in stigmatized behaviors to survive, even as they publicly disidentify with them. Our process model also identifies ways in which category emergence in core-stigmatized categories differs from the process for non-stigmatized categories.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lashley_Pollock_2020_Waiting_to_Inhale.pdf}
}

@article{lavergneCauchySchwarzInequalityExpectation2008,
  title = {A {{Cauchy-Schwarz}} Inequality for Expectation of Matrices},
  author = {Lavergne, Pascal},
  year = {2008},
  month = jan,
  journal = {Department of Economics, Simon Fraser University, Discussion Papers},
  abstract = {A generalization of the Cauchy-Schwarz inequality for expectations of matrices is proved.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lavergne_2008_A Cauchy-Schwarz inequality for expectation of matrices.pdf}
}

@article{lavoriAdaptiveTreatmentStrategies2008,
  title = {Adaptive {{Treatment Strategies}} in {{Chronic Disease}}},
  author = {Lavori, Philip W. and Dawson, Ree},
  year = {2008},
  month = feb,
  journal = {Annual Review of Medicine},
  volume = {59},
  number = {1},
  pages = {443--453},
  issn = {0066-4219, 1545-326X},
  doi = {10.1146/annurev.med.59.062606.122232},
  urldate = {2018-10-12},
  abstract = {An adaptive treatment strategy (ATS) is a rule for adapting a treatment plan to a patient's history of previous treatments and the response to those treatments. The ongoing management of chronic disease defines an ATS, which may be implicit and hidden or explicit and well-specified. The ATS is characterized by the use of intermediate, early markers of response to dynamically alter treatment decisions, in order to achieve a favorable ultimate outcome. We illustrate the ATS concept and describe how the effect of initial treatment decisions depends on the performance of subsequent decisions at later stages. We show how to compare two or more ATSs, or to determine an optimal ATS, using a sequential multiple assignment randomized (SMAR) trial. Designers of clinical trials might find the ATS concept useful in improving the efficiency and ecological relevance of clinical trials.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lavori_Dawson_2008_Adaptive Treatment Strategies in Chronic Disease.pdf}
}

@article{lavoriDesignTestingClinical2000,
  title = {A {{Design}} for {{Testing Clinical Strategies}}: {{Biased Adaptive}} within-{{Subject Randomization}}},
  author = {Lavori, Philip W. and Dawson, Ree},
  year = {2000},
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume = {163},
  number = {1},
  eprint = {2680506},
  eprinttype = {jstor},
  pages = {29--38},
  doi = {10.1111/1467-985X.00154},
  abstract = {We propose a method for assigning treatment in clinical trials, called the 'biased coin adaptive within-subject' (BCAWS) design: during the course of follow-up, the subject's response to a treatment is used to influence the future treatment, through a 'biased coin' algorithm. This design results in treatment patterns that are closer to actual clinical practice and may be more acceptable to patients with chronic disease than the usual fixed trial regimens, which often suffer from drop-out and non-adherence. In this work, we show how to use the BCAWS design to compare treatment strategies, and we provide a simple example to illustrate the method.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lavori_Dawson_2000_A Design for Testing Clinical Strategies.pdf}
}

@article{lavoriDynamicTreatmentRegimes2004,
  ids = {lavoriDynamicTreatmentRegimes2004a},
  title = {Dynamic Treatment Regimes: Practical Design Considerations},
  shorttitle = {Dynamic Treatment Regimes},
  author = {Lavori, Philip W and Dawson, Ree},
  year = {2004},
  month = feb,
  journal = {Clinical Trials},
  volume = {1},
  number = {1},
  pages = {9--20},
  issn = {17407745, 17407753},
  doi = {10.1191/1740774504cn002oa},
  urldate = {2020-07-09},
  abstract = {Background Clinical management of chronic disease requires a dynamic treatment regime (DTR): rules for choosing the new treatment based on the history of response to past treatments. Estimating and comparing the effects of DTRs from a sample of observed trajectories of treatment and outcome depends on the untestable assumption that new treatments are assigned independently of potential future responses to treatment, conditional on the history of treatments and response to date (``sequential ignorability''). In longitudinal observational studies, sequential ignorability must be assumed, while randomization of dynamic regimes can guarantee it. Methods Using several clinical examples, we describe the simplest randomized experimental designs for comparing DTRs. We begin by considering an initial treatment A and a second treatment B, and discuss how a dynamic treatment regime that starts with A and leads (sometimes) to B, might be compared to either xed treatment A or B. We also illustrate the problem of nding the optimal sequence of treatments in a DTR, when there are several choices. We describe and contrast two ways of incorporating randomization into studies to compare such regimes: baseline randomization among DTRs versus randomization at the decision points (sequentially randomized designs). Conclusions We discuss estimation and inference from both baseline randomized and sequentially randomized designs and conclude with a discussion of the differences between the experimental and observational approaches to optimizing and comparing dynamic treatment regimes. Clinical Trials 2004; 1: 9 -- 20. www.SCTjournal.com},
  langid = {english},
  pmid = {16281458},
  keywords = {*clinical protocols,*clinical trials topic,chronic disease,depressive disorder,Humans,myocardial infarction,nosource,outcome assessment (health care),random allocation,schizophrenia,th [therapy]},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lavori_Dawson_2004_Dynamic treatment regimes.pdf}
}

@article{lavoriFlexibleTreatmentStrategies2000,
  title = {Flexible Treatment Strategies in Chronic Disease: Clinical and Research Implications},
  shorttitle = {Flexible Treatment Strategies in Chronic Disease},
  author = {Lavori, Philip W and Dawson, Ree and Rush, A.John},
  year = {2000},
  month = sep,
  journal = {Biological Psychiatry},
  volume = {48},
  number = {6},
  pages = {605--614},
  issn = {00063223},
  doi = {10/b7j65z},
  urldate = {2019-07-03},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lavori et al_2000_Flexible treatment strategies in chronic disease.pdf}
}

@article{lavoriIntroductionDynamicTreatment2014,
  title = {Introduction to Dynamic Treatment Strategies and Sequential Multiple Assignment Randomization},
  author = {Lavori, Philip W. and Dawson, Ree},
  year = {2014},
  month = aug,
  journal = {Clinical Trials: Journal of the Society for Clinical Trials},
  volume = {11},
  number = {4},
  pages = {393--399},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774514527651},
  urldate = {2018-10-12},
  abstract = {Background In June 2013, a 1-day workshop on Dynamic Treatment Strategies (DTSs) and Sequential Multiple Assignment Randomized Trials (SMARTs) was held at the University of Pennsylvania in Philadelphia, Pennsylvania. These two linked topics have generated a great deal of interest as researchers have recognized the importance of comparing entire strategies for managing chronic disease. A number of articles emerged from that workshop. Purpose The purpose of this survey of the DTS/SMART methodology (which is taken from the introductory talk in the workshop) is to provide the reader the collected articles presented in this volume with sufficient background to appreciate the more detailed discussions in the articles. Methods The way that the DTS arises naturally in clinical practice is described, along with its connection to the well-known difficulties of interpreting the analysis by intention-to-treat. The SMART methodology for comparing DTS is described, and the basics of estimation and inference presented. Results The DTS/SMART methodology can be a flexible and practical way to optimize ongoing clinical decision making, providing evidence (based on randomization) for comparative effectiveness. Limitations The DTS/SMART methodology is not a solution for unstandardized study protocols. Conclusions The DTS/SMART methodology has growing relevance to comparative effectiveness research and the needs of the learning healthcare system. Clinical Trials 2014; 11: 393--399. http://ctj.sagepub.com},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lavori_Dawson_2014_Introduction to dynamic treatment strategies and sequential multiple assignment.pdf}
}

@article{lazzatiEffectBariatricSurgery2022,
  title = {Effect of Bariatric Surgery on Cancer Risk: Results from an Emulated Target Trial Using Population-Based Data},
  shorttitle = {Effect of Bariatric Surgery on Cancer Risk},
  author = {Lazzati, Andrea and Epaud, Salom{\'e} and Ortala, Matthieu and Katsahian, Sandrine and Lanoy, Emilie},
  year = {2022},
  month = may,
  journal = {British Journal of Surgery},
  volume = {109},
  number = {5},
  pages = {433--438},
  issn = {0007-1323},
  doi = {10.1093/bjs/znac003},
  urldate = {2023-07-25},
  abstract = {The impact of weight loss induced by bariatric surgery on cancer occurrence is controversial. To study the causal effect of bariatric surgery on cancer risk from an observational database, a target-trial emulation technique was used to mimic an RCT.Data on patients admitted between 2010 and 2019 with a diagnosis of obesity were extracted from a national hospital discharge database. Criteria for inclusion included eligibility criteria for bariatric surgery and the absence of cancer in the 2 years following inclusion. The intervention arms were bariatric surgery versus no surgery. Outcomes were the occurrence of any cancer and obesity-related cancer; cancers not related to obesity were used as negative controls.A total of 1 140 347 patients eligible for bariatric surgery were included in the study. Some 288 604 patients (25.3 per cent) underwent bariatric surgery. A total of 48 411 cancers were identified, including 4483 in surgical patients and 43 928 among patients who did not receive bariatric surgery. Bariatric surgery was associated with a decrease in the risk of obesity-related cancer (hazard ratio (HR) 0.89, 95 per cent c.i. 0.83 to 0.95), whereas no significant effect of surgery was identified with regard to cancers not related to obesity (HR 0.96, 0.91 to 1.01).When emulating a target trial from observational data, a reduction of 11 per cent in obesity-related cancer was found after bariatric surgery.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lazzati et al_2022_Effect of bariatric surgery on cancer risk.pdf;/Users/nseewald/Zotero/storage/332B5T89/6524038.html}
}

@article{lechnerEstimationCausalEffects2011,
  title = {The {{Estimation}} of {{Causal Effects}} by {{Difference-in-Difference Methods}}},
  author = {Lechner, Michael},
  year = {2011},
  month = nov,
  journal = {Foundations and Trends in Econometrics},
  volume = {4},
  number = {3},
  pages = {165--224},
  publisher = {Now Publishers, Inc.},
  issn = {1551-3076, 1551-3084},
  doi = {10.1561/0800000014},
  urldate = {2023-06-28},
  abstract = {The Estimation of Causal Effects by Difference-in-Difference Methods},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lechner_2011_The Estimation of Causal Effects by Difference-in-Difference Methods.pdf}
}

@article{leeComparisonTwoMetaAnalysis2016,
  title = {Comparison of {{Two Meta-Analysis Methods}}: {{Inverse-Variance-Weighted Average}} and {{Weighted Sum}} of {{Z-Scores}}},
  shorttitle = {Comparison of {{Two Meta-Analysis Methods}}},
  author = {Lee, Cue Hyunkyu and Cook, Seungho and Lee, Ji Sung and Han, Buhm},
  year = {2016},
  month = dec,
  journal = {Genomics \& Informatics},
  volume = {14},
  number = {4},
  pages = {173--180},
  publisher = {Korea Genome Organization},
  issn = {1598-866X, 2234-0742},
  doi = {10.5808/GI.2016.14.4.173},
  urldate = {2022-12-21},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lee et al_2016_Comparison of Two Meta-Analysis Methods.pdf}
}

@article{leeExactPostselectionInference2016,
  title = {Exact Post-Selection Inference, with Application to the Lasso},
  author = {Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor, Jonathan E.},
  year = {2016},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {44},
  number = {3},
  pages = {907--927},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/15-AOS1371},
  urldate = {2023-04-04},
  abstract = {We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the lasso to form valid confidence intervals for the selected coefficients and test whether all relevant variables have been included in the model.},
  keywords = {62E15,62F03,62J07,Confidence interval,hypothesis test,Lasso,Model selection},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lee et al_2016_Exact post-selection inference, with application to the lasso.pdf}
}

@article{leiSMARTDesignBuilding2012,
  title = {A "{{SMART}}" {{Design}} for {{Building Individualized Treatment Sequences}}},
  author = {Lei, Huitian and {Nahum-Shani}, Inbal and Lynch, Kevin and Oslin, David and Murphy, Susan A.},
  year = {2012},
  month = apr,
  journal = {Annual Review of Clinical Psychology},
  volume = {8},
  number = {1},
  pages = {21--48},
  issn = {1548-5943, 1548-5951},
  doi = {10.1146/annurev-clinpsy-032511-143152},
  urldate = {2018-10-12},
  abstract = {Interventions often involve a sequence of decisions. For example, clinicians frequently adapt the intervention to an individual's outcomes. Altering the intensity and type of intervention over time is crucial for many reasons, such as to obtain improvement if the individual is not responding or to reduce costs and burden when intensive treatment is no longer necessary. Adaptive interventions utilize individual variables (severity, preferences) to adapt the intervention and then dynamically utilize individual outcomes (response to treatment, adherence) to readapt the intervention. The Sequential Multiple Assignment Randomized Trial (SMART) provides high-quality data that can be used to construct adaptive interventions. We review the SMART and highlight its advantages in constructing and revising adaptive interventions as compared to alternative experimental designs. Selected examples of SMART studies are described and compared. A data analysis method is provided and illustrated using data from the Extending Treatment Effectiveness of Naltrexone SMART study.},
  langid = {english},
  keywords = {Humans,Mental Disorders,Mental Disorders: therapy,Patient-Centered Care,Patient-Centered Care: methods,Program Development,Program Development: methods,Program Evaluation,Program Evaluation: methods,Randomized Controlled Trials as Topic,Randomized Controlled Trials as Topic: methods,Research Design,United States},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lei et al_2012_A SMART Design for Building Individualized Treatment Sequences.pdf}
}

@article{lenthPracticalGuidelinesEffective2001,
  title = {Some {{Practical Guidelines}} for {{Effective Sample Size Determination}}},
  author = {Lenth, Russell V},
  year = {2001},
  month = aug,
  journal = {The American Statistician},
  volume = {55},
  number = {3},
  pages = {187--193},
  issn = {0003-1305, 1537-2731},
  doi = {10/b4523s},
  urldate = {2021-04-13},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lenth_2001_Some Practical Guidelines for Effective Sample Size Determination.pdf}
}

@article{leonSampleSizeRequirementsComparisons2004,
  title = {Sample-{{Size Requirements}} for {{Comparisons}} of {{Two Groups}} on {{Repeated Observations}} of a {{Binary Outcome}}},
  author = {Leon, Andrew C.},
  year = {2004},
  month = mar,
  journal = {Evaluation \& the Health Professions},
  volume = {27},
  number = {1},
  pages = {34--44},
  issn = {0163-2787, 1552-3918},
  doi = {10.1177/0163278703261198},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Leon_2004_Sample-Size Requirements for Comparisons of Two Groups on Repeated Observations.pdf}
}

@article{leTruerMulticulturalScience2019,
  title = {Towards a Truer Multicultural Science Education: How Whiteness Impacts Science Education},
  shorttitle = {Towards a Truer Multicultural Science Education},
  author = {Le, Paul T. and Matias, Cheryl E.},
  year = {2019},
  month = mar,
  journal = {Cultural Studies of Science Education},
  volume = {14},
  number = {1},
  pages = {15--31},
  issn = {1871-1510},
  doi = {10/ghfsqd},
  urldate = {2020-10-16},
  abstract = {The hope for multicultural, culturally competent, and diverse perspectives in science education falls short if theoretical considerations of whiteness are not entertained. Since whiteness is characterized as a hegemonic racial dominance that has become so natural it is almost invisible, this paper identifies how whiteness operates in science education such that it falls short of its goal for cultural diversity. Because literature in science education has yet to fully entertain whiteness ideology, this paper offers one of the first theoretical postulations. Drawing from the fields of education, legal studies, and sociology, this paper employs critical whiteness studies as both a theoretical lens and an analytic tool to re-interpret how whiteness might impact science education. Doing so allows the field to reconsider benign, routine, or normative practices and protocol that may influence how future scientists of Color experience the field. In sum, we seek to have the field consider the theoretical frames of whiteness and how it might influence how we engage in science education such that our hope for diversity never fully materializes.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Le_Matias_2019_Towards a truer multicultural science education.pdf}
}

@article{lewisNonlinearModelingTime1991,
  title = {Nonlinear {{Modeling}} of {{Time Series Using Multivariate Adaptive Regression Splines}} ({{MARS}})},
  author = {Lewis, P. A. W. and Stevens, J. G.},
  year = {1991},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {86},
  number = {416},
  pages = {864--877},
  publisher = {ASA Website},
  issn = {0162-1459},
  doi = {10.1080/01621459.1991.10475126},
  urldate = {2024-12-16},
  abstract = {Multivariate Adaptive Regression Splines (MARS) is a new methodology, due to Friedman, for nonlinear regression modeling. MARS can be conceptualized as a generalization of recursive partitioning that uses spline fitting in lieu of other simple fitting functions. Given a set of predictor variables, MARS fits a model in the form of an expansion in product spline basis functions of predictors chosen during a forward and backward recursive partitioning strategy. MARS produces continuous models for high-dimensional data that can have multiple partitions and predictor variable interactions. Predictor variable contributions and interactions in a MARS model may be analyzed using an ANOVA style decomposition. By letting the predictor variables in MARS be lagged values of a time series, one obtains a new method for nonlinear autoregressive threshold modeling of time series. A significant feature of this extension of MARS is its ability to produce models with limit cycles when modeling time series data that exhibit periodic behavior. In a physical context, limit cycles represent a stationary state of sustained oscillations, a satisfying behavior for any model of a time series with periodic behavior. Analysis of the yearly Wolf sunspot numbers with MARS appears to give an improvement over existing nonlinear threshold and bilinear models. A graphical representation for the models is given.},
  keywords = {ASTAR models,Limit cycles,Nonlinear time series models,Recursive partitioning,Threshold models,Wolf sunspot numbers},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lewis and Stevens - 1991 - Nonlinear Modeling of Time Series Using Multivariate Adaptive Regression Splines (MARS).pdf}
}

@article{liAnalysisLongitudinalData2006,
  title = {Analysis of Longitudinal Data with Missing Values},
  author = {Li, Jinhui},
  year = {2006},
  journal = {Trial},
  pages = {81--81},
  abstract = {A frequent problem in longitudinal studies is that subjects may miss some sched-uled visits or be assessed at self-selected points in time. As a result, observed outcome data may be highly unbalanced. Also, the availability of the data may be directly related to the outcome measure or some auxiliary factors that are related to the outcomes. This situation can be viewed as informative follow-up, as well as the one of informative intermittent missing data. Analysis without accounting for informative follow-up will produce biased estimates. Building on the work of Robins, Rotnitzky and Zhao (1995), we propose a class of inverse intensity of visit process weighted estimators in marginal regression models for longitudinal responses that may be observed in a continuous-time fashion. This allows us to handle arbitrary patterns of missing data as embedded in a subject's visit process. We derive the large sample distribution for our inverse visit intensity weighted estimators and investigate the finite sample behavior of our estimators with simulations. Our approach is also illustrated with a data set from a health services research study in which homeless people with mental illness were randomized to three different treatments and measures of homelessness and other auxiliary factors were recorded at the follow-up times that are not fixed by design.},
  keywords = {Markov Transition,Model,No DOI found,Nonignorable Missing Values.,Selection Model,Shared Parameter Model},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Li_2006_Analysis of longitudinal data with missing values.pdf}
}

@article{liangLongitudinalDataAnalysis1986,
  title = {Longitudinal Data Analysis Using Generalized Linear Models},
  author = {Liang, Kung-Yee and Zeger, Scott L},
  year = {1986},
  journal = {Biometrika},
  volume = {73},
  number = {1},
  pages = {13--22},
  doi = {10.1093/BIOMET/73.1.13},
  abstract = {This paper proposes an extension of generalized linear models to the analysis of longitudinal data. We introduce a class of estimating equations that give consistent estimates of the regression parameters and of their variance under mild assumptions about the time dependence. The estimating equations are derived without specifying the joint distribution of a subject's observations yet they reduce to the score equations for multivariate Gaussian outcomes. Asymptotic theory is presented for the general class of estimators. Specific cases in which we assume independence, m-dependence and exchangeable correlation structures from each subject are discussed. Efficiency of the proposed estimators in two simple situations is considered. The approach is closely related to quasi-likelihood.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liang_Zeger_1986_Longitudinal data analysis using generalized linear models.pdf}
}

@article{liangLongitudinalDataAnalysis2000,
  title = {Longitudinal {{Data Analysis}} of {{Continuous}} and {{Discrete Responses}} for {{Pre-Post Designs}}},
  author = {Liang, Kung-Yee and Zeger, Scott L.},
  year = {2000},
  journal = {Sankhy{\=a}: The Indian Journal of Statistics, Series B (1960-2002)},
  volume = {62},
  eprint = {25053123},
  eprinttype = {jstor},
  pages = {134--148},
  abstract = {The availability of biomarkers has led to the increasing adoption of the pre and post-randomization designs in clinical trials. In this paper we discuss the use of random effects models when the primary objective of the trial is to assess the efficacy of newly developed treatments in terms of progression of biomarkers over time. One issue of particular interest is how to best utilize the pre-randomization responses. We discuss the pros and cons of several inferential procedures including the conditional and full likelihood approaches. Throughout, these issues are illustrated by an analysis of data from a schizophrenic trial by the Janssen Research Foundation.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liang_Zeger_2000_Longitudinal Data Analysis of Continuous and Discrete Responses for Pre-Post.pdf}
}

@article{liangMedicalCannabisLegalization2018,
  title = {Medical Cannabis Legalization and Opioid Prescriptions: Evidence on {{US Medicaid}} Enrollees during 1993--2014},
  shorttitle = {Medical Cannabis Legalization and Opioid Prescriptions},
  author = {Liang, Di and Bao, Yuhua and Wallace, Mark and Grant, Igor and Shi, Yuyan},
  year = {2018},
  journal = {Addiction},
  volume = {113},
  number = {11},
  pages = {2060--2070},
  issn = {1360-0443},
  doi = {10.1111/add.14382},
  urldate = {2022-06-17},
  abstract = {Background and Aims While the United States has been experiencing an opioid epidemic, 29 states and Washington DC have legalized cannabis for medical use. This study examined whether state-wide medical cannabis legalization was associated with reduction in opioids received by Medicaid enrollees. Design Secondary data analysis of state-level opioid prescription records from 1993--2014 Medicaid State Drug Utilization Data. Linear time--series regressions assessed the associations between medical cannabis legalization and opioid prescriptions, controlling for state-level time-varying policy covariates (such as prescription drug monitoring programs) and socio-economic covariates (such as income). Setting United States. Participants Drug prescription records for patients enrolled in fee-for-service Medicaid programs that primarily provide health-care coverage to low-income and disabled people. Measurements The primary outcomes were population-adjusted number, dosage and Medicaid spending on opioid prescriptions. Outcomes for Schedule II opioids (e.g. hydrocodone, oxycodone) and Schedule III opioids (e.g. codeine) were analyzed separately. The primary policy variable of interest was the implementation of state-wide medical cannabis legalization. Findings For Schedule III opioid prescriptions, medical cannabis legalization was associated with a 29.6\% (P = 0.03) reduction in number of prescriptions, 29.9\% (P = 0.02) reduction in dosage and 28.8\% (P = 0.04) reduction in related Medicaid spending. No evidence was found to support the associations between medical cannabis legalization and Schedule II opioid prescriptions. Permitting medical cannabis dispensaries was not associated with Schedule II or Schedule III opioid prescriptions after controlling for medical cannabis legalization. It was estimated that, if all the states had legalized medical cannabis by 2014, Medicaid annual spending on opioid prescriptions would be reduced by 17.8 million dollars. Conclusion State-wide medical cannabis legalization appears to have been associated with reductions in both prescriptions and dosages of Schedule III (but not Schedule II) opioids received by Medicaid enrollees in the United States.},
  langid = {english},
  keywords = {Cannabis,legalization,medicaid,medical cannabis,opioid,opioid prescription,Prescription Drug Monitoring Program},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liang et al_2018_Medical cannabis legalization and opioid prescriptions.pdf;/Users/nseewald/Zotero/storage/DFWQPDYF/add.html}
}

@article{liangMultivariateRegressionAnalyses1992,
  title = {Multivariate Regression Analyses for Categorical Data},
  author = {Liang, Kung-Yee and Zeger, Scott L. and Qaqish, Bahjat},
  year = {1992},
  journal = {Journal of the Royal Statistical Society: Series B},
  volume = {54},
  number = {1},
  pages = {3--40},
  issn = {00359246},
  doi = {10.2307/2346101},
  abstract = {It is common to observe a vector of discrete and/or continuous responses in scientific problems where the objective is to characterize the dependence of each response on explanatory variables and to account for the association between the outcomes. The response vector can comprise repeated observations on one variable, as in longitudinal studies or genetic studies of families, or can include observations for different variables. This paper discusses a class of models for the marginal expectations of each response and for pairwise associations. The marginal models are contrasted with log-linear models. Two generalized estimating equation approaches are compared for parameter estimation. The first focuses on the regression parameters; the second simultaneously estimates the regression and association parameters. The robustness and efficiency of each is discussed. The methods are illustrated with analyses of two data sets from public health research.},
  pmid = {11682119},
  keywords = {efficiency,ESTIMATING FUNCTIONS,Invalid DOI,LOG-LINEAR MODELS,LOGISTIC REGRESSION,MARGINAL MODELS,MULTIVARIATE RESPONSE,PUBLIC HEALTH},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liang et al_1992_Multivariate regression analyses for categorical data.pdf}
}

@article{liaoJustinTimeNotToo2018,
  title = {Just-in-{{Time}} but {{Not Too Much}}: {{Determining Treatment Timing}} in {{Mobile Health}}},
  shorttitle = {Just-in-{{Time}} but {{Not Too Much}}},
  author = {Liao, Peng and Dempsey, Walter and Sarker, Hillol and Hossain, Syed Monowar and {al'Absi}, Mustafa and Klasnja, Predrag and Murphy, Susan},
  year = {2018},
  month = dec,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {2},
  number = {4},
  pages = {1--21},
  issn = {2474-9567},
  doi = {10.1145/3287057},
  urldate = {2022-04-28},
  abstract = {There is a growing scientific interest in the use and development of just-in-time adaptive interventions in mobile health. These mobile interventions typically involve treatments, such as reminders, activity suggestions and motivational messages, delivered via notifications on a smartphone or a wearable to help users make healthy decisions in the moment. To be effective in influencing health, the combination of the right treatment and right delivery time is likely critical. A variety of prediction/detection algorithms have been developed with the goal of pinpointing the best delivery times. The best delivery times might be times of greatest risk and/or times at which the user might be most receptive to the treatment notifications. In addition, to avoid over burdening users, there is of ten a constraint on the number of treatments that should be provided per time interval (e.g., day or week). Yet there may be many more times at which the user is predicted or detected to be at risk and/or receptive. The goal then is to spread treatment uniformly across all of these times. In this paper, we introduce a method that spreads the treatment uniformly across the delivery times. This method can also be used to provide data for learning whether the treatments are effective at the delivery times. This work is motivated by our work on two mobile health studies, a smoking cessation study and a physical activity study.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liao et al_2018_Just-in-Time but Not Too Much.pdf}
}

@article{liaoSampleSizeCalculations2016,
  title = {Sample Size Calculations for Micro-Randomized Trials in {{mHealth}}: {{Sample}} Size Calculations for Micro-Randomized Trials in {{mHealth}}},
  shorttitle = {Sample Size Calculations for Micro-Randomized Trials in {{mHealth}}},
  author = {Liao, Peng and Klasnja, Predrag and Tewari, Ambuj and Murphy, Susan A.},
  year = {2016},
  month = may,
  journal = {Statistics in Medicine},
  volume = {35},
  number = {12},
  pages = {1944--1971},
  issn = {02776715},
  doi = {10.1002/sim.6847},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liao et al_2016_Sample size calculations for micro-randomized trials in mHealth.pdf}
}

@article{liBalancingCovariatesPropensity2018,
  title = {Balancing {{Covariates}} via {{Propensity Score Weighting}}},
  author = {Li, Fan and Morgan, Kari Lock and Zaslavsky, Alan M.},
  year = {2018},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {521},
  pages = {390--400},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1260466},
  urldate = {2023-02-07},
  abstract = {Covariate balance is crucial for unconfounded descriptive or causal comparisons. However, lack of balance is common in observational studies. This article considers weighting strategies for balancing covariates. We define a general class of weights---the balancing weights---that balance the weighted distributions of the covariates between treatment groups. These weights incorporate the propensity score to weight each group to an analyst-selected target population. This class unifies existing weighting methods, including commonly used weights such as inverse-probability weights as special cases. General large-sample results on nonparametric estimation based on these weights are derived. We further propose a new weighting scheme, the overlap weights, in which each unit's weight is proportional to the probability of that unit being assigned to the opposite group. The overlap weights are bounded, and minimize the asymptotic variance of the weighted average treatment effect among the class of balancing weights. The overlap weights also possess a desirable small-sample exact balance property, based on which we propose a new method that achieves exact balance for means of any selected set of covariates. Two applications illustrate these methods and compare them with other approaches.},
  keywords = {Balancing weights,Causal inference,Clinical equipoise,Confounding,Exact balance,Overlap weights},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Li et al_2018_Balancing Covariates via Propensity Score Weighting.pdf}
}

@article{liComparisonAdaptiveTreatment2017,
  title = {Comparison of Adaptive Treatment Strategies Based on Longitudinal Outcomes in Sequential Multiple Assignment Randomized Trials},
  shorttitle = {Comparison of Adaptive Treatment Strategies Based on Longitudinal Outcomes in Sequential Multiple Assignment Randomized Trials},
  author = {Li, Zhiguo},
  year = {2017},
  month = feb,
  journal = {Statistics in Medicine},
  volume = {36},
  number = {3},
  pages = {403--415},
  issn = {02776715},
  doi = {10.1002/sim.7136},
  urldate = {2018-10-12},
  langid = {english},
  keywords = {10.1002/sim.7136 and adaptive treatment strategy,adaptive treatment strategy,data analysis,generalized estimating equation,generalized linear model,longitudinal,longitudinal data analysis,piecewise linear model,sequential multiple assignment randomized,Sequential multiple assignment randomized trial,trial},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Li_2017_Comparison of adaptive treatment strategies based on longitudinal outcomes in.pdf}
}

@article{liEssentialHistogram2020,
  title = {The Essential Histogram},
  author = {Li, Housen and Munk, Axel and Sieling, Hannes and Walther, Guenther},
  year = {2020},
  month = jun,
  journal = {Biometrika},
  volume = {107},
  number = {2},
  pages = {347--364},
  publisher = {Oxford Academic},
  issn = {0006-3444},
  doi = {10/ggzhbp},
  urldate = {2020-06-05},
  abstract = {Summary.  The histogram is widely used as a simple, exploratory way of displaying data, but it is usually not clear how to choose the number and size of the bin},
  langid = {english},
  keywords = {Miscellaneous},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Li et al_2020_The essential histogram.pdf;/Users/nseewald/Zotero/storage/UA3U3UPY/5733266.html}
}

@article{liMixedeffectsModelsDesign2021,
  title = {Mixed-Effects Models for the Design and Analysis of Stepped Wedge Cluster Randomized Trials: {{An}} Overview},
  shorttitle = {Mixed-Effects Models for the Design and Analysis of Stepped Wedge Cluster Randomized Trials},
  author = {Li, Fan and Hughes, James P and Hemming, Karla and Taljaard, Monica and Melnick, Edward R. and Heagerty, Patrick J},
  year = {2021},
  month = feb,
  journal = {Statistical Methods in Medical Research},
  volume = {30},
  number = {2},
  pages = {612--639},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280220932962},
  urldate = {2025-01-23},
  abstract = {The stepped wedge cluster randomized design has received increasing attention in pragmatic clinical trials and implementation science research. The key feature of the design is the unidirectional crossover of clusters from the control to intervention conditions on a staggered schedule, which induces confounding of the intervention effect by time. The stepped wedge design first appeared in the Gambia hepatitis study in the 1980s. However, the statistical model used for the design and analysis was not formally introduced until 2007 in an article by Hussey and Hughes. Since then, a variety of mixed-effects model extensions have been proposed for the design and analysis of these trials. In this article, we explore these extensions under a unified perspective. We provide a general model representation and regard various model extensions as alternative ways to characterize the secular trend, intervention effect, as well as sources of heterogeneity. We review the key model ingredients and clarify their implications for the design and analysis. The article serves as an entry point to the evolving statistical literatures on stepped wedge designs.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Li et al. - 2021 - Mixed-effects models for the design and analysis of stepped wedge cluster randomized trials An over.pdf}
}

@article{linAnalysisLongitudinalData2004,
  title = {Analysis of Longitudinal Data with Irregular, Outcome-Dependent Follow-Up},
  author = {Lin, Haiqun and Scharfstein, Daniel O. and Rosenheck, Robert A.},
  year = {2004},
  month = aug,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {66},
  number = {3},
  pages = {791--813},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2004.b5543.x},
  urldate = {2018-10-12},
  abstract = {A frequent problem in longitudinal studies is that subjects may miss scheduled visits or be assessed at self-selected points in time. As a result, observed outcome data may be highly unbalanced and the availability of the data may be directly related to the outcome measure and/or some auxiliary factors that are associated with the outcome. If the follow-up visit and outcome processes are correlated, then marginal regression analyses will produce biased estimates. Building on the work of Robins, Rotnitzky and Zhao, we propose a class of inverse intensity-of-visit process-weighted estimators in marginal regression models for longitudinal responses that may be observed in continuous time. This allows us to handle arbitrary patterns of missing data as embedded in a subject's visit process. We derive the large sample distribution for our inverse visit-intensity-weighted estimators and investigate their finite sample behaviour by simulation. Our approach is illustrated with a data set from a health services research study in which homeless people with mental illness were randomized to three different treatments and measures of homelessness (as percentage days homeless in the past 3 months) and other auxiliary factors were recorded at follow-up times that are not fixed by design.},
  langid = {english},
  keywords = {Dropout,Informative follow-up,Intermittently missingness,Longitudinal data,Visit process,Weighted generalized estimating equations},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lin et al_2004_Analysis of longitudinal data with irregular, outcome-dependent follow-up.pdf}
}

@article{lindenEvaluatingDiseaseManagement2006,
  title = {Evaluating Disease Management Programme Effectiveness: An Introduction to the Regression Discontinuity Design},
  shorttitle = {Evaluating Disease Management Programme Effectiveness},
  author = {Linden, Ariel and Adams, John L. and Roberts, Nancy},
  year = {2006},
  journal = {Journal of Evaluation in Clinical Practice},
  volume = {12},
  number = {2},
  pages = {124--131},
  issn = {1365-2753},
  doi = {10.1111/j.1365-2753.2005.00573.x},
  urldate = {2022-04-29},
  abstract = {Although disease management (DM) has been in existence for over a decade, there is still much uncertainty as to its effectiveness in improving health status and reducing medical cost. The main reason is that most programme evaluations typically follow weak observational study designs that are subject to bias, most notably selection bias and regression to the mean. The regression discontinuity (RD) design may be the best alternative to randomized studies for evaluating DM programme effectiveness. The most crucial element of the RD design is its use of a `cut-off' score on a pre-test measure to determine assignment to intervention or control. A valuable feature of this technique is that the pre-test measure does not have to be the same as the outcome measure, thus maximizing the programme's ability to use research-based practice guidelines, survey instruments and other tools to identify those individuals in greatest need of the programme intervention. Similarly, the cut-off score can be based on clinical understanding of the disease process, empirically derived, or resource-based. In the RD design, programme effectiveness is determined by a change in the pre--post relationship at the cut-off point. While the RD design is uniquely suitable for DM programme evaluation, its success will depend, in large part, on fundamental changes being made in the way DM programmes identify and assign individuals to the programme intervention.},
  langid = {english},
  keywords = {cut-off point,disease management,evaluation,observational study designs,regression discontinuity},
  file = {C\:\\Users\\nseew\\Google Drive\\Papers\\Linden_et_al_2006_Evaluating_disease_management_programme_effectiveness.pdf;/Users/nseewald/Zotero/storage/ZCWTFSHD/j.1365-2753.2005.00573.html}
}

@article{linMetaAnalysisGenomewideAssociation2009,
  title = {Meta-{{Analysis}} of {{Genome-wide Association Studies}} with {{Overlapping Subjects}}},
  author = {Lin, Dan-Yu and Sullivan, Patrick F.},
  year = {2009},
  month = dec,
  journal = {The American Journal of Human Genetics},
  volume = {85},
  number = {6},
  pages = {862--872},
  publisher = {Elsevier},
  issn = {0002-9297, 1537-6605},
  doi = {10.1016/j.ajhg.2009.11.001},
  urldate = {2023-10-27},
  langid = {english},
  pmid = {20004761},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/false;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lin_Sullivan_2009_Meta-Analysis of Genome-wide Association Studies with Overlapping Subjects.pdf}
}

@article{linScopingReviewCausal2021,
  title = {A Scoping Review of Causal Methods Enabling Predictions under Hypothetical Interventions},
  author = {Lin, Lijing and Sperrin, Matthew and Jenkins, David A. and Martin, Glen P. and Peek, Niels},
  year = {2021},
  month = feb,
  journal = {Diagnostic and Prognostic Research},
  volume = {5},
  number = {1},
  pages = {3},
  issn = {2397-7523},
  doi = {10.1186/s41512-021-00092-9},
  urldate = {2022-08-17},
  abstract = {The methods with which prediction models are usually developed mean that neither the parameters nor the predictions should be interpreted causally. For many applications, this is perfectly acceptable. However, when prediction models are used to support decision making, there is often a need for predicting outcomes under hypothetical interventions.},
  keywords = {Causal inference,Clinical prediction models,Counterfactual prediction,Statistical modeling},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lin et al_2021_A scoping review of causal methods enabling predictions under hypothetical.pdf;/Users/nseewald/Zotero/storage/TYMHXQU3/s41512-021-00092-9.html}
}

@article{lipkovichTutorialBiostatisticsDatadriven2017,
  title = {Tutorial in Biostatistics: Data-Driven Subgroup Identification and Analysis in Clinical Trials},
  shorttitle = {Tutorial in Biostatistics},
  author = {Lipkovich, Ilya and Dmitrienko, Alex and B. D'Agostino Sr., Ralph},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {1},
  pages = {136--196},
  issn = {1097-0258},
  doi = {10.1002/sim.7064},
  urldate = {2024-06-13},
  abstract = {It is well known that both the direction and magnitude of the treatment effect in clinical trials are often affected by baseline patient characteristics (generally referred to as biomarkers). Characterization of treatment effect heterogeneity plays a central role in the field of personalized medicine and facilitates the development of tailored therapies. This tutorial focuses on a general class of problems arising in data-driven subgroup analysis, namely, identification of biomarkers with strong predictive properties and patient subgroups with desirable characteristics such as improved benefit and/or safety. Limitations of ad-hoc approaches to biomarker exploration and subgroup identification in clinical trials are discussed, and the ad-hoc approaches are contrasted with principled approaches to exploratory subgroup analysis based on recent advances in machine learning and data mining. A general framework for evaluating predictive biomarkers and identification of associated subgroups is introduced. The tutorial provides a review of a broad class of statistical methods used in subgroup discovery, including global outcome modeling methods, global treatment effect modeling methods, optimal treatment regimes, and local modeling methods. Commonly used subgroup identification methods are illustrated using two case studies based on clinical trials with binary and survival endpoints. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {_tablet,biomarker analysis,clinical trials,data mining,exploratory subgroup analysis,multiplicity control},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lipkovich et al_2017_Tutorial in biostatistics.pdf;/Users/nseewald/Zotero/storage/ZY8X5C27/sim.html}
}

@article{lipsitzGeneralizedEstimatingEquations1991,
  title = {Generalized {{Estimating Equations}} for {{Correlated Binary Data}}: {{Using}} the {{Odds Ratio}} as a {{Measure}} of {{Association}}},
  shorttitle = {Generalized {{Estimating Equations}} for {{Correlated Binary Data}}},
  author = {Lipsitz, Stuart R. and Laird, Nan M. and Harrington, David P.},
  year = {1991},
  month = mar,
  journal = {Biometrika},
  volume = {78},
  number = {1},
  eprint = {2336905},
  eprinttype = {jstor},
  pages = {153},
  issn = {00063444},
  doi = {10.2307/2336905},
  urldate = {2018-10-12},
  abstract = {Moment methods for analyzing repeated binary responses have been proposed by Liang \& Zeger (1986), and extended by Prentice (1988). In their generalized estimating equations, both Liang \& Zeger (1986) and Prentice (1988) estimate the parameters associated with the expected value of an individual's vector of binary responses as well as the correlations between pairs of binary responses. Because the odds ratio has many desirable properties, and some investigators may find the odds ratio is easier to interpret, we discuss modelling the association between binary responses at pairs of times with the odds ratio. We then modify the estimating equations of Prentice to estimate the odds ratios. In simulations, the parameter estimates for the logistic regression model for the marginal probabilities appear slightly more efficient when using the odds ratio parameterization.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lipsitz et al_1991_Generalized Estimating Equations for Correlated Binary Data.pdf}
}

@article{lipsitzOneStepGeneralizedEstimating2017,
  title = {One-{{Step Generalized Estimating Equations With Large Cluster Sizes}}},
  author = {Lipsitz, Stuart and Fitzmaurice, Garrett and Sinha, Debajyoti and Hevelone, Nathanael and Hu, Jim and Nguyen, Louis L.},
  year = {2017},
  month = jul,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {3},
  pages = {734--737},
  issn = {1061-8600, 1537-2715},
  doi = {10/gfn5fd},
  urldate = {2018-12-07},
  abstract = {Medical studies increasingly involve a large sample of independent clusters, where the cluster sizes are also large. Our motivating example from the 2010 Nationwide Inpatient Sample (NIS) has 8,001,068 patients and 1049 clusters, with average cluster size of 7627. Consistent parameter estimates can be obtained naively assuming independence, which are inefficient when the intra-cluster correlation (ICC) is high. Efficient generalized estimating equations (GEE) incorporate the ICC and sum all pairs of observations within a cluster when estimating the ICC. For the 2010 NIS, there are 92.6 billion pairs of observations, making summation of pairs computationally prohibitive. We propose a one-step GEE estimator that (1) matches the asymptotic efficiency of the fully iterated GEE; (2) uses a simpler formula to estimate the ICC that avoids summing over all pairs; and (3) completely avoids matrix multiplications and inversions. These three features make the proposed estimator much less computationally intensive, especially with large cluster sizes. A unique contribution of this article is that it expresses the GEE estimating equations incorporating the ICC as a simple sum of vectors and scalars.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lipsitz et al_2017_One-Step Generalized Estimating Equations With Large Cluster Sizes.pdf}
}

@article{lipsitzPerformanceGeneralizedEstimating1994,
  title = {Performance of {{Generalized Estimating Equations}} in {{Practical Situations}}},
  author = {Lipsitz, Stuart R. and Fitzmaurice, Garrett M. and Orav, Endel J. and Laird, Nan M.},
  year = {1994},
  month = mar,
  journal = {Biometrics},
  volume = {50},
  number = {1},
  eprint = {2533218},
  eprinttype = {jstor},
  pages = {270},
  issn = {0006341X},
  doi = {10/dtj5rq},
  urldate = {2019-08-14},
  abstract = {Moment methods for analyzing repeated binary responses have been proposed by Liang and Zeger (1986, Biometrika 73, 13-22), and extended by Prentice (1988, Biometrics 44, 1033-1048). In their generalized estimating equations (GEE), both Liang and Zeger (1986) and Prentice (1988) estimate the parameters associated with the expected value of an individual's vector of binary responses as well as the correlations between pairs of binary responses. In this paper, we discuss one-step estimators, i.e., estimators obtained from one step of the generalized estimating equations, and compare their performance to that of the fully iterated estimators in small samples. In simulations, we find the performance of the one-step estimator to be qualitatively similar to that of the fully iterated estimator. When the sample size is small and the association between binary responses is high, we recommend using the one-step estimator to circumvent convergence problems associated with the fully iterated GEE algorithm. Furthermore, we find the GEE methods to be more efficient than ordinary logistic regression with variance correction for estimating the effect of a time-varying covariate.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lipsitz et al_1994_Performance of Generalized Estimating Equations in Practical Situations.pdf}
}

@article{liSampleSizeFormulae2011,
  title = {Sample Size Formulae for Two-Stage Randomized Trials with Survival Outcomes},
  author = {Li, Zhiguo and Murphy, Susan A.},
  year = {2011},
  journal = {Biometrika},
  volume = {98},
  number = {3},
  pages = {503--518},
  issn = {00063444},
  doi = {10.1093/biomet/asr019},
  abstract = {Two-stage randomized trials are growing in importance in developing adaptive treatment strategies, i.e. treatment policies or dynamic treatment regimes. Usually, the first stage involves randomization to one of the several initial treatments. The second stage of treatment begins when an early nonresponse criterion or response criterion is met. In the second-stage, nonresponding subjects are re-randomized among second-stage treatments. Sample size calculations for planning these two-stage randomized trials with failure time outcomes are challenging because the variances of common test statistics depend in a complex manner on the joint distribution of time to the early nonresponse criterion or response criterion and the primary failure time outcome. We produce simple, albeit conservative, sample size formulae by using upper bounds on the variances. The resulting formulae only require the working assumptions needed to size a standard single-stage randomized trial and, in common settings, are only mildly conservative. These sample size formulae are based on either a weighted Kaplan-Meier estimator of survival probabilities at a fixed time-point or a weighted version of the log-rank test.},
  pmid = {22363091},
  keywords = {Dynamic treatment regime,Sample size calculation,Sequential multiple assignment randomized trial,Weighted Kaplan-Meier estimator,Weighted log-rank test},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Li_Murphy_2011_Sample size formulae for two-stage randomized trials with survival outcomes.pdf}
}

@article{littleEstimandsEstimatorsEstimates2021,
  title = {Estimands, {{Estimators}}, and {{Estimates}}},
  author = {Little, Roderick J. and Lewis, Roger J.},
  year = {2021},
  month = sep,
  journal = {JAMA},
  volume = {326},
  number = {10},
  pages = {967--968},
  issn = {0098-7484},
  doi = {10.1001/jama.2021.2886},
  urldate = {2021-11-19},
  abstract = {The primary goal of most randomized clinical trials (RCTs) is to draw conclusions about the effect of a treatment in a specific population of patients. The true effect of the intervention, termed the estimand, is estimated with the data acquired in the trial, subject to limitations associated with variations in adherence to treatment, patients being lost to follow-up, and data quality.The choice of estimand and associated target population should reflect the goals of the trial, and can vary according to who designed or sponsored the study, who will use the results of the study, and the motivating scientific question. In the PIONEER 3 trial, investigators compared 3 doses of oral semaglutide with sitagliptin, added to background therapy, in adults with type 2 diabetes. The primary end point was the change in glycated hemoglobin (HbA1c). The trial design considered 2 estimands for summarizing treatment effect, termed the treatment policy estimand and the trial product estimand.},
  file = {/Users/nseewald/Zotero/storage/MLI9JKR9/2783611.html}
}

@article{littleTestingEqualityTwo1989,
  title = {Testing the {{Equality}} of {{Two Independent Binomial Proportions}}},
  author = {Little, Roderick J. A.},
  year = {1989},
  month = nov,
  journal = {The American Statistician},
  volume = {43},
  number = {4},
  eprint = {2685390},
  eprinttype = {jstor},
  pages = {283},
  issn = {00031305},
  doi = {10.2307/2685390},
  urldate = {2018-10-12},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Little_1989_Testing the Equality of Two Independent Binomial Proportions.pdf}
}

@article{liuBayesianOptimalInterval2015,
  title = {Bayesian Optimal Interval Designs for Phase {{I}} Clinical Trials},
  author = {Liu, Suyu and Yuan, Ying},
  year = {2015},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {64},
  number = {3},
  eprint = {24772999},
  eprinttype = {jstor},
  pages = {507--523},
  publisher = {[Wiley, Royal Statistical Society]},
  issn = {0035-9254},
  urldate = {2024-02-13},
  abstract = {In phase I trials, effectively treating patients and minimizing the chance of exposing them to subtherapeutic and overly toxic doses are clinicians' top priority. Motived by this practical consideration, we propose Bayesian optimal interval (BOIN) designs to find the maximum tolerated dose and to minimize the probability of inappropriate dose assignments for patients. We show, both theoretically and numerically, that the BOIN design not only has superior finite and large sample properties but also can be easily implemented in a simple way similar to the traditional '3+3' design. Compared with the well-known continual reassessment method, the BOIN design yields comparable average performance to select the maximum tolerated dose but has a substantially lower risk of assigning patients to subtherapeutic and overly toxic doses. We apply the BOIN design to two cancer clinical trials.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liu_Yuan_2015_Bayesian_optimal_interval_designs_for_phase_I_clinical_trials.pdf}
}

@article{liuControllingCumulativeAdverse,
  title = {Controlling {{Cumulative Adverse Risk}} in {{Learning Optimal Dynamic Treatment Regimens}}},
  author = {Liu, Mochuan and Wang, Yuanjia and Fu, Haoda and Zeng, Donglin},
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--12},
  publisher = {ASA Website},
  issn = {0162-1459},
  doi = {10.1080/01621459.2023.2270637},
  urldate = {2024-10-18},
  abstract = {Dynamic treatment regimen (DTR) is one of the most important tools to tailor treatment in personalized medicine. For many diseases such as cancer and type 2 diabetes mellitus (T2D), more aggressive treatments can lead to a higher efficacy but may also increase risk. However, few methods for estimating DTRs can take into account both cumulative benefit and risk. In this work, we propose a general statistical learning framework to learn optimal DTRs that maximize the reward outcome while controlling the cumulative adverse risk to be below a pre-specified threshold. We convert this constrained optimization problem into an unconstrained optimization using a Lagrange function. We then solve the latter using either backward learning algorithms or simultaneously over all stages based on constructing a novel multistage ramp loss. Theoretically, we establish Fisher consistency of the proposed method and further obtain non-asymptotic convergence rates for both reward and risk outcomes under the estimated DTRs. The finite sample performance of the proposed method is demonstrated via simulation studies and through an application to a two-stage clinical trial for T2D patients. Supplementary materials for this article are available online.},
  keywords = {Benefit-risk tradeoff,Lagrange function,Multistage decision-making,Outcome weighted learning,Q-learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liu_et_al_Controlling_Cumulative_Adverse_Risk_in_Learning_Optimal_Dynamic_Treatment.pdf}
}

@article{liuMixedEffectsRegressionModel2006,
  title = {A {{Mixed-Effects Regression Model}} for {{Longitudinal Multivariate Ordinal Data}}},
  author = {Liu, Li C. and Hedeker, Donald},
  year = {2006},
  month = mar,
  journal = {Biometrics},
  volume = {62},
  number = {1},
  pages = {261--268},
  issn = {0006341X},
  doi = {10/bjbw3r},
  urldate = {2021-09-07},
  abstract = {A mixed-effects item response theory model that allows for three-level multivariate ordinal outcomes and accommodates multiple random subject effects is proposed for analysis of multivariate ordinal outcomes in longitudinal studies. This model allows for the estimation of different item factor loadings (item discrimination parameters) for the multiple outcomes. The covariates in the model do not have to follow the proportional odds assumption and can be at any level. Assuming either a probit or logistic response function, maximum marginal likelihood estimation is proposed utilizing multidimensional Gauss--Hermite quadrature for integration of the random effects. An iterative Fisher scoring solution, which provides standard errors for all model parameters, is used. An analysis of a longitudinal substance use data set, where four items of substance use behavior (cigarette use, alcohol use, marijuana use, and getting drunk or high) are repeatedly measured over time, is used to illustrate application of the proposed model.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liu_Hedeker_2006_A Mixed-Effects Regression Model for Longitudinal Multivariate Ordinal Data.pdf}
}

@article{liuMultivariateCommonLanguage2015,
  title = {Multivariate {{Common Language Effect Size}}},
  author = {Liu, Xiaofeng Steven},
  year = {2015},
  month = jan,
  journal = {Therapeutic Innovation \& Regulatory Science},
  volume = {49},
  number = {1},
  pages = {126--131},
  issn = {2168-4790, 2168-4804},
  doi = {10.1177/2168479014542603},
  urldate = {2018-10-12},
  abstract = {To find a common language effect size of multivariate outcomes, we convert the standardized multivariate effect size (Mahalanobis distance) to a probability of a randomly selected subject from one population having a larger discriminant function than a randomly selected subject from another population. This probability is simple to calculate and comprehensible to laypeople. It can serve as the multivariate common language effect size to compare not only two groups but also more than two groups.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liu_2015_Multivariate Common Language Effect Size.pdf}
}

@article{liuOptimalDesignLongitudinal2017,
  title = {Optimal Design of Longitudinal Data Analysis Using Generalized Estimating Equation Models},
  author = {Liu, Jingxia and Colditz, Graham A.},
  year = {2017},
  journal = {Biometrical Journal},
  volume = {59},
  number = {2},
  pages = {315--330},
  issn = {1521-4036},
  doi = {10/f3s8zk},
  urldate = {2019-09-05},
  abstract = {Longitudinal studies are often applied in biomedical research and clinical trials to evaluate the treatment effect. The association pattern within the subject must be considered in both sample size calculation and the analysis. One of the most important approaches to analyze such a study is the generalized estimating equation (GEE) proposed by Liang and Zeger, in which ``working correlation structure'' is introduced and the association pattern within the subject depends on a vector of association parameters denoted by {$\rho$}. The explicit sample size formulas for two-group comparison in linear and logistic regression models are obtained based on the GEE method by Liu and Liang. For cluster randomized trials (CRTs), researchers proposed the optimal sample sizes at both the cluster and individual level as a function of sampling costs and the intracluster correlation coefficient (ICC). In these approaches, the optimal sample sizes depend strongly on the ICC. However, the ICC is usually unknown for CRTs and multicenter trials. To overcome this shortcoming, Van Breukelen et al. consider a range of possible ICC values identified from literature reviews and present Maximin designs (MMDs) based on relative efficiency (RE) and efficiency under budget and cost constraints. In this paper, the optimal sample size and number of repeated measurements using GEE models with an exchangeable working correlation matrix is proposed under the considerations of fixed budget, where ``optimal'' refers to maximum power for a given sampling budget. The equations of sample size and number of repeated measurements for a known parameter value {$\rho$} are derived and a straightforward algorithm for unknown {$\rho$} is developed. Applications in practice are discussed. We also discuss the existence of the optimal design when an AR(1) working correlation matrix is assumed. Our proposed method can be extended under the scenarios when the true and working correlation matrix are different.},
  copyright = {{\copyright} 2016 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liu_Colditz_2017_Optimal design of longitudinal data analysis using generalized estimating.pdf;/Users/nseewald/Zotero/storage/RQ79V7MK/bimj.html}
}

@article{liuSampleSizeCalculations1997,
  title = {Sample {{Size Calculations}} for {{Studies}} with {{Correlated Observations}}},
  author = {Liu, Guanghan and Liang, Kung-Yee},
  year = {1997},
  month = sep,
  journal = {Biometrics},
  volume = {53},
  number = {3},
  eprint = {2533554},
  eprinttype = {jstor},
  pages = {937},
  issn = {0006341X},
  doi = {10.2307/2533554},
  urldate = {2018-10-12},
  abstract = {Correlated data occur frequently in biomedical research. Examples include longitudinal studies, family studies, and ophthalmologic studies. In this paper, we present a method to compute sample sizes and statistical powers for studies involving correlated observations. This is a multivariate extension of the work by Self and Mauritsen (1988, Biometrics 44, 79-86), who derived a sample size and power formula for generalized linear models based on the score statistic. For correlated data, we appeal to a statistic based on the generalized estimating equation method (Liang and Zeger, 1986, Biometrika 73, 13-22). We highlight the additional assumptions needed to deal with correlated data. Some special cases that are commonly seen in practice are discussed, followed by simulation studies.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liu_Liang_1997_Sample Size Calculations for Studies with Correlated Observations.pdf}
}

@article{liuSampleSizePower2002,
  title = {Sample Size and Power Determination for Clustered Repeated Measurements},
  author = {Liu, Aiyi and Shih, W. J. and Gehan, E.},
  year = {2002},
  journal = {Statistics in Medicine},
  volume = {21},
  number = {12},
  pages = {1787--1801},
  issn = {1097-0258},
  doi = {10.1002/sim.1154},
  urldate = {2021-10-15},
  abstract = {It is common in epidemiological and clinical studies that each subject has repeated measurements on a single common variable, while the subjects are also `clustered'. To compute sample size or power of a test, we have to consider two types of correlation: correlation among repeated measurements within the same subject, and correlation among subjects in the same cluster. We develop, based on generalized estimating equations, procedures for computing sample size and power with clustered repeated measurements. Explicit formulae are derived for comparing two means, two slopes and two proportions, under several simple correlation structures. Copyright {\copyright} 2002 John Wiley \& Sons, Ltd. abstract},
  langid = {english},
  keywords = {correlation structure,generalized estimating equation,longitudinal data,within- and between-subject correlation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liu et al_2002_Sample size and power determination for clustered repeated measurements.pdf;/Users/nseewald/Zotero/storage/VIQV6FHI/sim.html}
}

@article{liuShouldBaselineBe2009,
  title = {Should Baseline Be a Covariate or Dependent Variable in Analyses of Change from Baseline in Clinical Trials?},
  author = {Liu, Guanghan F. and Lu, Kaifeng and Mogg, Robin and Mallick, Madhuja and Mehrotra, Devan V.},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {20},
  pages = {2509--2530},
  issn = {1097-0258},
  doi = {10/fv7hbh},
  urldate = {2020-01-23},
  abstract = {In randomized clinical trials, a pre-treatment measurement is often taken at baseline, and post-treatment effects are measured at several time points post-baseline, say t=1, {\dots}, T. At the end of the trial, it is of interest to assess the treatment effect based on the mean change from baseline at the last time point T. We consider statistical methods for (i) a point estimate and 95 per cent confidence interval for the mean change from baseline at time T for each treatment group, and (ii) a p-value and 95 per cent confidence interval for the between-group difference in the mean change from baseline. The manner in which the baseline responses are used in the analysis influences both the accuracy and the efficiency of items (i) and (ii). In this paper, we will consider the ANCOVA approach with change from baseline as a dependent variable and compare that with a constrained longitudinal data analysis (cLDA) model proposed by Liang and Zeger (Sankhya: Indian J. Stat. (Ser B) 2000; 62:134--148), in which the baseline is modeled as a dependent variable in conjunction with the constraint of a common baseline mean across the treatment groups. Some drawbacks of the ANCOVA model and potential advantages of the cLDA approach are discussed and illustrated using numerical simulations. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Liu et al_2009_Should baseline be a covariate or dependent variable in analyses of change from.pdf;/Users/nseewald/Zotero/storage/NV43DENQ/sim.html}
}

@article{llombart-cussacFulvestrantPalbociclibVsLetrozolePalbociclib2021,
  title = {Fulvestrant-{{Palbociclib}} vs {{Letrozole-Palbociclib}} as {{Initial Therapy}} for {{Endocrine-Sensitive}}, {{Hormone Receptor}}--{{Positive}}, {{ERBB2-Negative Advanced Breast Cancer}}: {{A Randomized Clinical Trial}}},
  shorttitle = {Fulvestrant-{{Palbociclib}} vs {{Letrozole-Palbociclib}} as {{Initial Therapy}} for {{Endocrine-Sensitive}}, {{Hormone Receptor}}--{{Positive}}, {{ERBB2-Negative Advanced Breast Cancer}}},
  author = {{Llombart-Cussac}, Antonio and {P{\'e}rez-Garc{\'i}a}, Jos{\'e} Manuel and Bellet, Meritxell and Dalenc, Florence and {Gil-Gil}, Miguel and {Ru{\'i}z-Borrego}, Manuel and Gavil{\'a}, Joaqu{\'i}n and {Sampayo-Cordero}, Miguel and Aguirre, Elena and Schmid, Peter and Marm{\'e}, Frederik and Di Cosimo, Serena and Gligorov, Joseph and Schneeweiss, Andreas and Albanell, Joan and Zamora, Pilar and Wheatley, Duncan and {Mart{\'i}nez-de Due{\~n}as}, Eduardo and Amillano, Kepa and Malfettone, Andrea and Cort{\'e}s, Javier and {PARSIFAL Steering Committee and Trial Investigators}},
  year = {2021},
  month = dec,
  journal = {JAMA Oncology},
  volume = {7},
  number = {12},
  pages = {1791--1799},
  issn = {2374-2437},
  doi = {10.1001/jamaoncol.2021.4301},
  urldate = {2024-02-06},
  abstract = {The cyclin-dependent kinase 4 and 6 inhibitor palbociclib in combination with letrozole has become a standard first-line treatment for patients with endocrine-sensitive, hormone receptor--positive, ERBB2-negative advanced breast cancer. Meanwhile, the antiestrogen fulvestrant was shown to be superior to anastrozole in the absence of cyclin-dependent kinase 4 and 6 inhibition for this patient population.To assess whether fulvestrant is superior to letrozole when combined with palbociclib in the first-line scenario.In this international, randomized, open-label, phase 2 clinical study conducted from July 30, 2015, to January 8, 2018, patients with hormone receptor--positive, ERBB2-negative advanced breast cancer with no prior therapy in the metastatic setting and endocrine-sensitive criteria were recruited from 47 centers in 7 countries. Data were analyzed from February 11 to May 15, 2020.Patients were randomly assigned (1:1 ratio) to receive palbociclib with either fulvestrant or letrozole. Stratification factors were type of disease presentation (de novo vs recurrent) and the presence of visceral involvement (yes vs no).The primary end point was investigator-assessed progression-free survival determined by Response Evaluation Criteria in Solid Tumors, version 1.1.A total of 486 women (median age, 63 years [range, 25-90 years]; 3 Asian women [0.6\%]; 4 Black women [0.8\%]; 461 White women [94.9\%]; 18 women of unknown race [3.7\%]) were randomized (243 to fulvestrant-palbociclib and 243 to letrozole-palbociclib). Median investigator-assessed progression-free survival was 27.9 months (95\% CI, 24.2-33.1 months) in the fulvestrant-palbociclib group vs 32.8 months (95\% CI, 25.8-35.9 months) in the letrozole-palbociclib group (hazard ratio, 1.13; 95\% CI, 0.89-1.45; P\,=\,.32). This result was consistent across the stratification factors. No significant differences were observed in objective response rate (46.5\% vs 50.2\%) and 3-year overall survival rate (79.4\% vs 77.1\%) for fulvestrant-palbociclib and letrozole-palbociclib, respectively. Grade 3-4 adverse events were comparable among treatment groups, and no new safety signals were identified. No treatment-related deaths were reported.Although fulvestrant-palbociclib demonstrated significant antitumor activity, this randomized clinical trial failed to identify an improvement in progression-free survival with this regimen over letrozole-palbociclib in patients with endocrine-sensitive, hormone receptor--positive, ERBB2-negative advanced breast cancer.ClinicalTrials.gov Identifier: NCT02491983},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Llombart-Cussac et al_2021_Fulvestrant-Palbociclib vs Letrozole-Palbociclib as Initial Therapy for.pdf;/Users/nseewald/Zotero/storage/SJR95E4U/2784608.html}
}

@article{loDemystifyingIntegratedTail2019,
  title = {Demystifying the {{Integrated Tail Probability Expectation Formula}}},
  author = {Lo, Ambrose},
  year = {2019},
  month = oct,
  journal = {The American Statistician},
  volume = {73},
  number = {4},
  pages = {367--374},
  issn = {Undefined},
  doi = {10/ggdf25},
  urldate = {2019-11-23},
  annotation = {Saved from BrowZine: http://thirdiron.com/download},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lo_2019_Demystifying the Integrated Tail Probability Expectation Formula.pdf}
}

@article{lohRegressionTreeApproach2015,
  title = {A Regression Tree Approach to Identifying Subgroups with Differential Treatment Effects},
  author = {Loh, Wei-Yin and He, Xu and Man, Michael},
  year = {2015},
  journal = {Statistics in Medicine},
  volume = {34},
  number = {11},
  pages = {1818--1833},
  issn = {1097-0258},
  doi = {10.1002/sim.6454},
  urldate = {2024-06-13},
  abstract = {In the fight against hard-to-treat diseases such as cancer, it is often difficult to discover new treatments that benefit all subjects. For regulatory agency approval, it is more practical to identify subgroups of subjects for whom the treatment has an enhanced effect. Regression trees are natural for this task because they partition the data space. We briefly review existing regression tree algorithms. Then, we introduce three new ones that are practically free of selection bias and are applicable to data from randomized trials with two or more treatments, censored response variables, and missing values in the predictor variables. The algorithms extend the generalized unbiased interaction detection and estimation (GUIDE) approach by using three key ideas: (i) treatment as a linear predictor, (ii) chi-squared tests to detect residual patterns and lack of fit, and (iii) proportional hazards modeling via Poisson regression. Importance scores with thresholds for identifying influential variables are obtained as by-products. A bootstrap technique is used to construct confidence intervals for the treatment effects in each node. The methods are compared using real and simulated data. Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {_tablet,bootstrap,missing values,proportional hazards,selection bias},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Loh et al_2015_A regression tree approach to identifying subgroups with differential treatment.pdf;/Users/nseewald/Zotero/storage/VU723Q83/sim.html}
}

@article{longabaughOriginsIssuesOptions2005,
  ids = {longabaughOriginsIssuesOptions2005b},
  title = {Origins, Issues and Options in the Development of the Combined Behavioral Intervention.},
  author = {Longabaugh, Richard and Zweben, Allen and Locastro, Joseph S and Miller, William R},
  year = {2005},
  month = jul,
  journal = {Journal of Studies on Alcohol, Supplement},
  number = {s15},
  pages = {179--187},
  publisher = {Alcohol Research Documentation, Inc.},
  issn = {0363-468X},
  doi = {10/ghpb9f},
  urldate = {2020-12-28},
  abstract = {Objective: The aim of the investigators was to develop a moderate intensity comprehensive behavioral treatment based on the principles of motivational interviewing and Cognitive Behavioral Therapy that, within the confines of a standardized abstinence-oriented treatment, would provide a broad spectrum of modules to assist those seeking treatment to achieve reduction of problematic drinking. Methods: The core issue of how to deliver a flexible therapy tailored to the needs of individual clients while at the same time providing a standardized treatment protocol for a randomized clinical trial provided the dilemma out of which this unique standardized protocol arose. By using a single decision tree, client choice, combined with limited options, we were able to reconcile these conflicting demands. Results: Key decisions that were made in developing the treatment protocol and the thinking leading to these decisions are described. Conclusions: Understanding these key issues and the factors that led to the decisions made will assist would-be users in their own clinical and/or clinical research needs.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Longabaugh et al_2005_Origins, issues and options in the development of the combined behavioral.pdf;/Users/nseewald/Zotero/storage/3RRAU3BG/jsas.2005.s15.html}
}

@article{longabaughOriginsIssuesOptions2015,
  title = {Origins, Issues and Options in the Development of the Combined Behavioral Intervention.},
  author = {Longabaugh, Richard and Zweben, Allen and Locastro, Joseph S. and Miller, William R.},
  year = {2015},
  month = jan,
  journal = {Journal of Studies on Alcohol, Supplement},
  publisher = {Rutgers University Piscataway, NJ},
  doi = {10/ghpb9f},
  urldate = {2020-12-28},
  abstract = {Objective: The aim of the investigators was to develop a moderate intensity comprehensive behavioral treatment based on the principles of motivational interviewing and Cognitive Behavioral Therapy ...},
  copyright = {Copyright {\copyright} 2005 by Alcohol Research Documentation, Inc.},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/SW8HVV3V/jsas.2005.s15.html}
}

@article{longfordSelectionBiasTreatment1999,
  title = {Selection Bias and Treatment Heterogeneity in Clinical Trials},
  author = {Longford, Nicholas T.},
  year = {1999},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {18},
  number = {12},
  pages = {1467--1474},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/(SICI)1097-0258(19990630)18:12<1467::AID-SIM149>3.0.CO;2-H},
  urldate = {2018-10-12},
  abstract = {A common perception about many commercially available medical treatments is that they are e!ective for every patient having the relevant indication and that developers have provided the regulatory authorities with evidence of such a property. We show that the standard of evidence is much lower and that the standard is appropriate only when the treatment e!ects are almost constant. We discuss the implications on the design and analysis of clinical trials if the standards were made to correspond with the common perception. We conclude that the evidence of positive mean treatment e!ect should be accompanied by evidence of limited dispersion of the e!ects and by a sensitivity analysis that explores the impact of the selection bias in recruitment. Copyright 1999 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Longford_1999_Selection bias and treatment heterogeneity in clinical trials.pdf}
}

@article{lopezbernalUseControlsInterrupted2018,
  title = {The Use of Controls in Interrupted Time Series Studies of Public Health Interventions},
  author = {Lopez Bernal, James and Cummins, Steven and Gasparrini, Antonio},
  year = {2018},
  month = dec,
  journal = {International Journal of Epidemiology},
  volume = {47},
  number = {6},
  pages = {2082--2093},
  issn = {0300-5771, 1464-3685},
  doi = {10/gfppvn},
  urldate = {2020-12-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lopez Bernal et al_2018_The use of controls in interrupted time series studies of public health.pdf}
}

@article{lordParadoxInterpretationGroup1967,
  title = {A Paradox in the Interpretation of Group Comparisons.},
  author = {Lord, Frederic M.},
  year = {1967},
  journal = {Psychological Bulletin},
  volume = {68},
  number = {5},
  pages = {304--305},
  issn = {1939-1455, 0033-2909},
  doi = {10/bt4nfb},
  urldate = {2019-08-29},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lord_1967_A paradox in the interpretation of group comparisons.pdf}
}

@article{loucaEmancipationInteractionHow2009,
  ids = {loucaEmancipationInteractionHow2009b},
  title = {Emancipation {{Through Interaction}} -- {{How Eugenics}} and {{Statistics Converged}} and {{Diverged}}},
  author = {Lou{\c c}{\~a}, Francisco},
  year = {2009},
  month = nov,
  journal = {Journal of the History of Biology},
  volume = {42},
  number = {4},
  pages = {649--684},
  issn = {0022-5010, 1573-0387},
  doi = {10.1007/s10739-008-9167-7},
  urldate = {2020-02-04},
  abstract = {The paper discusses the scope and influence of eugenics in defining the scientific programme of statistics and the impact of the evolution of biology on social scientists. It argues that eugenics was instrumental in providing a bridge between sciences, and therefore created both the impulse and the institutions necessary for the birth of modern statistics in its applications first to biology and then to the social sciences. Looking at the question from the point of view of the history of statistics and the social sciences, and mostly concentrating on evidence from the British debates, the paper discusses how these disciplines became emancipated from eugenics precisely because of the inspiration of biology. It also relates how social scientists were fascinated and perplexed by the innovations taking place in statistical theory and practice.},
  langid = {english},
  keywords = {nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Louçã_2009_Emancipation Through Interaction – How Eugenics and Statistics Converged and.pdf}
}

@article{louSampleSizeCalculations2017,
  title = {Sample Size Calculations for Time-Averaged Difference of Longitudinal Binary Outcomes},
  author = {Lou, Ying and Cao, Jing and Zhang, Song and Ahn, Chul},
  year = {2017},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {46},
  number = {1},
  pages = {344--353},
  issn = {0361-0926},
  doi = {10/gf5bd4},
  urldate = {2019-07-22},
  abstract = {In clinical trials with repeated measurements, the responses from each subject are measured multiple times during the study period. Two approaches have been widely used to assess the treatment effect, one that compares the rate of change between two groups and the other that tests the time-averaged difference (TAD). While sample size calculations based on comparing the rate of change between two groups have been reported by many investigators, the literature has paid relatively little attention to the sample size estimation for time-averaged difference (TAD) in the presence of heterogeneous correlation structure and missing data in repeated measurement studies. In this study, we investigate sample size calculation for the comparison of time-averaged responses between treatment groups in clinical trials with longitudinally observed binary outcomes. The generalized estimating equation (GEE) approach is used to derive a closed-form sample size formula, which is flexible enough to account for arbitrary missing patterns and correlation structures. In particular, we demonstrate that the proposed sample size can accommodate a mixture of missing patterns, which is frequently encountered by practitioners in clinical trials. To our knowledge, this is the first study that considers the mixture of missing patterns in sample size calculation. Our simulation shows that the nominal power and type I error are well preserved over a wide range of design parameters. Sample size calculation is illustrated through an example.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lou et al_2017_Sample size calculations for time-averaged difference of longitudinal binary.pdf}
}

@article{lucasRationaleCannabisbasedInterventions2017,
  title = {Rationale for Cannabis-Based Interventions in the Opioid Overdose Crisis},
  author = {Lucas, Philippe},
  year = {2017},
  month = aug,
  journal = {Harm Reduction Journal},
  volume = {14},
  number = {1},
  pages = {58},
  issn = {1477-7517},
  doi = {10.1186/s12954-017-0183-9},
  urldate = {2024-05-01},
  abstract = {North America is currently in the grips of a crisis rooted in the use of licit and illicit opioid-based analgesics. Drug overdose is the leading cause of accidental death in Canada and the US, and the growing toll of opioid-related morbidity and mortality requires a diversity of novel therapeutic and harm reduction-based interventions. Research suggests that increasing adult access to both medical and recreational cannabis has significant positive impacts on public health and safety as a result of substitution effect. Observational and epidemiological studies have found that medical cannabis programs are associated with a reduction in the use of opioids and associated morbidity and mortality.},
  langid = {english},
  keywords = {_tablet,Addiction,Cannabis,Harm reduction,Marijuana,Opioids,Substitution},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lucas_2017_Rationale_for_cannabis-based_interventions_in_the_opioid_overdose_crisis.pdf}
}

@article{luComparingDynamicTreatment2016,
  title = {Comparing Dynamic Treatment Regimes Using Repeated-Measures Outcomes: Modeling Considerations in {{SMART}} Studies},
  shorttitle = {Comparing Dynamic Treatment Regimes Using Repeated-Measures Outcomes},
  author = {Lu, Xi and Nahum-Shani, Inbal and Kasari, Connie and Lynch, Kevin G. and Oslin, David W. and Pelham, William E. and Fabiano, Gregory and Almirall, Daniel},
  year = {2016},
  journal = {Statistics in Medicine},
  volume = {35},
  number = {10},
  pages = {1595--1615},
  issn = {1097-0258},
  doi = {10/gg2gxc},
  urldate = {2021-02-08},
  abstract = {A dynamic treatment regime (DTR) is a sequence of decision rules, each of which recommends a treatment based on a patient's past and current health status. Sequential, multiple assignment, randomized trials (SMARTs) are multi-stage trial designs that yield data specifically for building effective DTRs. Modeling the marginal mean trajectories of a repeated-measures outcome arising from a SMART presents challenges, because traditional longitudinal models used for randomized clinical trials do not take into account the unique design features of SMART. We discuss modeling considerations for various forms of SMART designs, emphasizing the importance of considering the timing of repeated measures in relation to the treatment stages in a SMART. For illustration, we use data from three SMART case studies with increasing level of complexity, in autism, child attention deficit hyperactivity disorder, and adult alcoholism. In all three SMARTs, we illustrate how to accommodate the design features along with the timing of the repeated measures when comparing DTRs based on mean trajectories of the repeated-measures outcome. Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {adaptive intervention,longitudinal analysis,marginal structural model,sequential multiple assignment randomized trial},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lu et al_2016_Comparing dynamic treatment regimes using repeated-measures outcomes.pdf;/Users/nseewald/Zotero/storage/8L8MKBWR/sim.html}
}

@article{luComparingTreatmentPolicies2016,
  title = {Comparing Treatment Policies with Assistance from the Structural Nested Mean Model},
  author = {Lu, Xi and Lynch, Kevin G. and Oslin, David W. and Murphy, Susan A.},
  year = {2016},
  journal = {Biometrics},
  volume = {72},
  number = {1},
  pages = {10--19},
  issn = {1541-0420},
  doi = {10/f8w9fm},
  urldate = {2019-09-05},
  abstract = {Treatment policies, also known as dynamic treatment regimes, are sequences of decision rules that link the observed patient history with treatment recommendations. Multiple, plausible, treatment policies are frequently constructed by researchers using expert opinion, theories, and reviews of the literature. Often these different policies represent competing approaches to managing an illness. Here, we develop an ``assisted estimator'' that can be used to compare the mean outcome of competing treatment policies. The term ``assisted'' refers to the fact estimators from the Structural Nested Mean Model, a parametric model for the causal effect of treatment at each time point, are used in the process of estimating the mean outcome. This work is motivated by our work on comparing the mean outcome of two competing treatment policies using data from the ExTENd study in alcohol dependence.},
  copyright = {{\copyright} 2015, The International Biometric Society},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/VIBK4UWS/biom.html}
}

@article{ludwigDoesHeadStart2007,
  title = {Does {{Head Start Improve Children}}'s {{Life Chances}}? {{Evidence}} from a {{Regression Discontinuity Design}}*},
  shorttitle = {Does {{Head Start Improve Children}}'s {{Life Chances}}?},
  author = {Ludwig, Jens and Miller, Douglas L.},
  year = {2007},
  month = feb,
  journal = {The Quarterly Journal of Economics},
  volume = {122},
  number = {1},
  pages = {159--208},
  issn = {0033-5533},
  doi = {10.1162/qjec.122.1.159},
  urldate = {2024-06-10},
  abstract = {This paper exploits a new source of variation in Head Start funding to identify the program's effects on health and schooling. In 1965 the Office of Economic Opportunity (OEO) provided technical assistance to the 300 poorest counties to develop Head Start proposals. The result was a large and lasting discontinuity in Head Start funding rates at the OEO cutoff for grant-writing assistance. We find evidence of a large drop at the OEO cutoff in mortality rates for children from causes that could be affected by Head Start, as well as suggestive evidence for a positive effect on educational attainment.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ludwig_Miller_2007_Does_Head_Start_Improve_Children's_Life_Chances.pdf;/Users/nseewald/Zotero/storage/GJSM3RCX/1924719.html}
}

@article{luersLinearMixedModels,
  title = {Linear {{Mixed Models}} for {{Comparing Dynamic Treatment Regimens}} on a {{Longitudinal Outcome}} in {{Sequentially Randomized Trials}}},
  author = {Luers, Brook and Qian, Min and {Nahum-Shani}, Inbal and Kasari, Connie and Almirall, Daniel},
  year = {2019},
  journal = {arXiv Methodology},
  doi = {arXiv:1910.10078v1},
  abstract = {A dynamic treatment regimen (DTR) is a pre-specified sequence of decision rules which maps baseline or time-varying measurements on an individual to a recommended intervention or set of interventions. Sequential multiple assignment randomized trials (SMARTs) represent an important data collection tool for informing the construction of effective DTRs. A common primary aim in a SMART is the marginal mean comparison between two or more of the DTRs embedded in the trial. This manuscript develops a mixed effects modeling and estimation approach for these primary aim comparisons based on a continuous, longitudinal outcome. The method is illustrated using data from a SMART in autism research.},
  keywords = {Invalid DOI,nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Luers et al_2019_Linear Mixed Models for Comparing Dynamic Treatment Regimens on a Longitudinal.pdf}
}

@misc{luersLinearMixedModels2019,
  title = {Linear {{Mixed Models}} for {{Comparing Dynamic Treatment Regimens}} on a {{Longitudinal Outcome}} in {{Sequentially Randomized Trials}}},
  author = {Luers, Brook and Qian, Min and {Nahum-Shani}, Inbal and Kasari, Connie and Almirall, Daniel},
  year = {2019},
  month = oct,
  number = {arXiv:1910.10078},
  eprint = {1910.10078},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.10078},
  urldate = {2024-08-20},
  abstract = {A dynamic treatment regimen (DTR) is a pre-specified sequence of decision rules which maps baseline or time-varying measurements on an individual to a recommended intervention or set of interventions. Sequential multiple assignment randomized trials (SMARTs) represent an important data collection tool for informing the construction of effective DTRs. A common primary aim in a SMART is the marginal mean comparison between two or more of the DTRs embedded in the trial. This manuscript develops a mixed effects modeling and estimation approach for these primary aim comparisons based on a continuous, longitudinal outcome. The method is illustrated using data from a SMART in autism research.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/nseewald/Zotero/storage/CSGR87FP/Luers et al. - 2019 - Linear Mixed Models for Comparing Dynamic Treatmen.pdf;/Users/nseewald/Zotero/storage/DHW3MN28/1910.html}
}

@article{luersStandardizedEffectSizes2019,
  ids = {luersStandardizedEffectSizes2018},
  title = {Standardized {{Effect Sizes}} for {{Preventive Mobile Health Interventions}} in {{Micro-randomized Trials}}},
  author = {Luers, Brook and Klasnja, Predrag and Murphy, Susan A.},
  year = {2019},
  month = jan,
  journal = {Prevention Science},
  volume = {20},
  number = {1},
  pages = {100--109},
  issn = {1573-6695},
  doi = {10/ghvbd7},
  urldate = {2021-01-23},
  abstract = {Mobile Health (mHealth) interventions are behavioral interventions that are accessible to individuals in their daily lives via a mobile device. Most mHealth interventions consist of multiple intervention components. Some of the components are ``pull'' components, which require individuals to access the component on their mobile device at moments when they decide they need help. Other intervention components are ``push'' components, which are initiated by the intervention, not the individual, and are delivered via notifications or text messages. Micro-randomized trials (MRTs) have been developed to provide data to assess the effects of push intervention components on subsequent emotions and behavior. In this paper, we review the micro-randomized trial design and provide an approach to computing a standardized effect size for these intervention components. This effect size can be used to compare different push intervention components that may be included in an mHealth intervention. In addition, a standardized effect size can be used to inform sample size calculations for future MRTs. Here, the standardized effect size is a function of time because the push notifications can occur repeatedly over time. We illustrate this methodology using data from an MRT involving HeartSteps, an mHealth intervention for physical activity as part of the secondary prevention of heart disease.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Luers et al_2019_Standardized Effect Sizes for Preventive Mobile Health Interventions in.pdf;G\:\\My Drive\\Papers\\Luers_et_al_2019_Standardized_Effect_Sizes_for_Preventive_Mobile_Health_Interventions_in2.pdf}
}

@article{luncefordEstimationSurvivalDistributions2002,
  title = {Estimation of {{Survival Distributions}} of {{Treatment Policies}} in {{Two-Stage Randomization Designs}} in {{Clinical Trials}}},
  author = {Lunceford, Jared K. and Davidian, Marie and Tsiatis, Anastasios A.},
  year = {2002},
  month = mar,
  journal = {Biometrics},
  volume = {58},
  number = {1},
  pages = {48--57},
  issn = {0006341X},
  doi = {10/bk2dj9},
  urldate = {2020-12-13},
  langid = {english},
  keywords = {nosource}
}

@article{luSampleSizeDetermination2009,
  title = {Sample Size Determination for Constrained Longitudinal Data Analysis},
  author = {Lu, Kaifeng and Mehrotra, Devan V. and Liu, Guanghan},
  year = {2009},
  month = feb,
  journal = {Statistics in Medicine},
  volume = {28},
  number = {4},
  pages = {679--699},
  issn = {1097-0258},
  doi = {10/dxhknb},
  urldate = {2018-12-08},
  abstract = {The longitudinal data analysis model proposed by Liang and Zeger (Sankhy{\=a}: Indian J. Stat. Ser. B 2000; 62:134--148) uses the baseline as well as postbaseline values as the dependent variables, and the baseline mean responses are constrained to be the same across treatment groups due to randomization. Compared with the conventional longitudinal analysis of covariance, this approach can correctly estimate the variance of within-group mean changes and achieve the specified coverage probabilities. General results on the sample size and power calculations for this model in the presence of missing data are obtained. The sample size relationship between the constrained and unconstrained longitudinal data analysis is established. Simple expressions for sample size calculation are obtained for the compound symmetry and first-order autoregressive correlation structures. The sensitivity of the sample size requirement to the configuration of correlation structure and retention pattern is assessed. The performance of several ad hoc approximations for longitudinal data analysis sample size calculation is evaluated. Simulation studies are conducted to assess the validity of the proposed sample size formulas with deviation from normality. The sample size formulas are also illustrated in detail using real clinical trial data. Copyright {\copyright} 2008 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lu et al_2009_Sample size determination for constrained longitudinal data analysis.pdf;/Users/nseewald/Zotero/storage/YNBQ6DFI/sim.html}
}

@article{luSampleSizeEstimation2008,
  title = {Sample {{Size Estimation}} for {{Repeated Measures Analysis}} in {{Randomized Clinical Trials}} with {{Missing Data}}},
  author = {Lu, Kaifeng and Luo, Xiaohui and Chen, Pei-Yun},
  year = {2008},
  month = jan,
  journal = {The International Journal of Biostatistics},
  volume = {4},
  number = {1},
  issn = {1557-4679},
  doi = {10/bjcdjc},
  urldate = {2019-07-02},
  abstract = {In designing longitudinal studies, researchers must determine the number of subjects to randomize based on the power to detect a clinically meaningful treatment difference and a proposed analysis plan. In this paper, we present formulas for sample size estimation and an assessment of statistical power for a two-treatment repeated measures design allowing for subject attrition. These formulas can be used for comparing two treatment groups across time in terms of linear contrasts. Subjects are assumed to drop out of the study at random so that the missing data do not alter the parameters of interest.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Lu et al_2008_Sample Size Estimation for Repeated Measures Analysis in Randomized Clinical.pdf}
}

@article{mackinnonHeteroskedasticityconsistentCovarianceMatrix1985,
  title = {Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties},
  author = {MacKinnon, James G and White, Halbert},
  year = {1985},
  month = sep,
  journal = {Journal of Econometrics},
  volume = {29},
  number = {3},
  pages = {305--325},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(85)90158-7},
  urldate = {2022-03-17},
  abstract = {We examine several modified versions of the heteroskedasticity-consistent covariance matrix estimator of Hinkley (1977) and White (1980). On the basis of sampling experiments which compare the performance of quasi t-statistics, we find that one estimator, based on the jackknife, performs better in small samples than the rest. We also examine the finite-sample properties of using modified critical values based on Edgeworth approximations, as proposed by Rothenberg (1984). In addition, we compare the power of several tests for heteroskedasticity, and find that it may be wise to employ the jackknife heteroskedasticity-consistent covariance matrix even in the absence of detected heteroskedasticity.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/MacKinnon_White_1985_Some heteroskedasticity-consistent covariance matrix estimators with improved.pdf;/Users/nseewald/Zotero/storage/F4QUDZXZ/0304407685901587.html}
}

@article{madenciStrengtheningHealthServices2021,
  title = {Strengthening {{Health Services Research Using Target Trial Emulation}}: {{An Application}} to {{Volume-Outcomes Studies}}},
  shorttitle = {Strengthening {{Health Services Research Using Target Trial Emulation}}},
  author = {Madenci, Arin L and Wanis, Kerollos Nashat and Cooper, Zara and Haneuse, Sebastien and Subramanian, S V and Hofman, Albert and Hern{\'a}n, Miguel A},
  year = {2021},
  month = nov,
  journal = {American Journal of Epidemiology},
  volume = {190},
  number = {11},
  pages = {2453--2460},
  issn = {0002-9262},
  doi = {10.1093/aje/kwab170},
  urldate = {2023-03-22},
  abstract = {The number of operations that surgeons have previously performed is associated with their patients' outcomes. However, this association may not be causal, because previous studies have often been cross-sectional and their analyses have not considered time-varying confounding or positivity violations. In this paper, using the example of surgeons who perform coronary artery bypass grafting, we describe (hypothetical) target trials for estimation of the causal effect of the surgeons' operative volumes on patient mortality. We then demonstrate how to emulate these target trials using data from US Medicare claims and provide effect estimates. Our target trial emulations suggest that interventions on physicians' volume of coronary artery bypass grafting operations have little effect on patient mortality. The target trial framework highlights key assumptions and draws attention to areas of bias in previous observational analyses that deviated from their implicit target trials. The principles of the presented methodology may be adapted to other scenarios of substantive interest in health services research.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Madenci et al_2021_Strengthening Health Services Research Using Target Trial Emulation.pdf;/Users/nseewald/Zotero/storage/92CEGU3P/6292356.html}
}

@article{madrasFairnessCausalAwareness2018,
  title = {Fairness {{Through Causal Awareness}}: {{Learning Latent-Variable Models}} for {{Biased Data}}},
  shorttitle = {Fairness {{Through Causal Awareness}}},
  author = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.02519 [cs, stat]},
  eprint = {1809.02519},
  primaryclass = {cs, stat},
  urldate = {2018-10-12},
  abstract = {How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data and reframe fair classification as an intervention problem. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help us to learn policies which are both more accurate and fair, when presented with a historically biased dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Madras et al_2018_Fairness Through Causal Awareness.pdf}
}

@article{maharScopingReviewStudies2021,
  title = {A Scoping Review of Studies Using Observational Data to Optimise Dynamic Treatment Regimens},
  author = {Mahar, Robert K. and McGuinness, Myra B. and Chakraborty, Bibhas and Carlin, John B. and IJzerman, Maarten J. and Simpson, Julie A.},
  year = {2021},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {21},
  number = {1},
  pages = {21--39},
  publisher = {BioMed Central},
  issn = {1471-2288},
  doi = {10.1186/s12874-021-01211-2},
  urldate = {2022-10-12},
  abstract = {Dynamic treatment regimens (DTRs) formalise the multi-stage and dynamic decision problems that clinicians often face when treating chronic or progressive medical conditions. Compared to randomised controlled trials, using observational data to optimise DTRs may allow a wider range of treatments to be evaluated at a lower cost. This review aimed to provide an overview of how DTRs are optimised with observational data in practice. Using the PubMed database, a scoping review of studies in which DTRs were optimised using observational data was performed in October 2020. Data extracted from eligible articles included target medical condition, source and type of data, statistical methods, and translational relevance of the included studies. From 209 PubMed abstracts, 37 full-text articles were identified, and a further 26 were screened from the reference lists, totalling 63 articles for inclusion in a narrative data synthesis. Observational DTR models are a recent development and their application has been concentrated in a few medical areas, primarily HIV/AIDS (27, 43\%), followed by cancer (8, 13\%), and diabetes (6, 10\%). There was substantial variation in the scope, intent, complexity, and quality between the included studies. Statistical methods that were used included inverse-probability weighting (26, 41\%), the parametric G-formula (16, 25\%), Q-learning (10, 16\%), G-estimation (4, 6\%), targeted maximum likelihood/minimum loss-based estimation (4, 6\%), regret regression (3, 5\%), and other less common approaches (10, 16\%). Notably, studies that were primarily intended to address real-world clinical questions (18, 29\%) tended to use inverse-probability weighting and the parametric G-formula, relatively well-established methods, along with a large amount of data. Studies focused on methodological developments (45, 71\%) tended to be more complicated and included a demonstrative real-world application only. As chronic and progressive conditions become more common, the need will grow for personalised treatments and methods to estimate the effects of DTRs. Observational DTR studies will be necessary, but so far their use to inform clinical practice has been limited. Focusing on simple DTRs, collecting large and rich clinical datasets, and fostering tight partnerships between content experts and data analysts may result in more clinically relevant observational DTR studies.},
  copyright = {2021 The Author(s)},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mahar et al_2021_A scoping review of studies using observational data to optimise dynamic.pdf;/Users/nseewald/Zotero/storage/CYLF8UMI/s12874-021-01211-2.html}
}

@article{malikMultivariateLogisticDistributions1973,
  title = {Multivariate {{Logistic Distributions}}},
  author = {Malik, Henrick J. and Abraham, Bovas},
  year = {1973},
  month = may,
  journal = {The Annals of Statistics},
  volume = {1},
  number = {3},
  pages = {588--590},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176342430},
  urldate = {2021-12-10},
  abstract = {In this paper a multivariate analogue of the logistic distribution is considered. We suggest two families of multivariate logistic distributions with the property that marginal distributions are of univariate form and discuss some distributional properties of the multivariate distributions.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Malik_Abraham_1973_Multivariate Logistic Distributions.pdf;/Users/nseewald/Zotero/storage/BC533ZPT/1176342430.html}
}

@inproceedings{mammoserPrimaryCNSLymphoma2015,
  title = {Primary {{CNS}} Lymphoma; a Review of the {{University}} of {{Michigan}} Experience 2004-2013.},
  booktitle = {Journal of {{Clinical Oncology}}},
  author = {Mammoser, Aaron Gerald and Weathers, Shiao-Pei S. and Seewald, Nicholas J. and Taylor, Jeremy MG and Junck, Larry},
  year = {2015},
  month = may,
  volume = {33},
  pages = {e13012-e13012},
  doi = {10.1200/jco.2015.33.15_suppl.e13012},
  urldate = {2024-10-22},
  langid = {english}
}

@article{manclCovarianceEstimatorGEE2001,
  title = {A {{Covariance Estimator}} for {{GEE}} with {{Improved Small-Sample Properties}}},
  author = {Mancl, Lloyd A. and DeRouen, Timothy A.},
  year = {2001},
  month = mar,
  journal = {Biometrics},
  volume = {57},
  number = {1},
  pages = {126--134},
  issn = {0006341X},
  doi = {10.1111/j.0006-341X.2001.00126.x},
  urldate = {2018-10-12},
  abstract = {In this paper, we propose an alternative covariance estimator to the robust covariance estimator of generalized estimating equations (GEE). Hypothesis tests using the robust covariance estimator can have inflated size when the number of independent clusters is small. Resampling methods, such as the jackknife and bootstrap, have been suggested for covariance estimation when the number of clusters is small. A drawback of the resampling methods when the response is binary is that the methods can break down when the number of subjects is small due to zero or near-zero cell counts caused by resampling. We propose a bias-corrected covariance estimator that avoids this problem. In a small simulation study, we compare the bias-corrected covariance estimator to the robust and jackknife covariance estimators for binary responses for situations involving 10-40 subjects with equal and unequal cluster sizes of 16-64 observations. The biascorrected covariance estimator gave tests with sizes close to the nominal level even when the number of subjects was 10 and cluster sizes were unequal, whereas the robust and jackknife covariance estimators gave tests with sizes that could be 2-3 times the nominal level. The methods are illustrated using data from a randomized clinical trial on treatment for bone loss in subjects with periodontal disease.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mancl_DeRouen_2001_A Covariance Estimator for GEE with Improved Small-Sample Properties.pdf}
}

@article{manderTwostageDesignsOptimal2010,
  title = {Two-Stage Designs Optimal under the Alternative Hypothesis for Phase {{II}} Cancer Clinical Trials},
  author = {Mander, A.P. and Thompson, S.G.},
  year = {2010},
  month = nov,
  journal = {Contemporary Clinical Trials},
  volume = {31},
  number = {6},
  pages = {572--578},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2010.07.008},
  urldate = {2024-01-11},
  abstract = {The Simon two-stage optimal design is often used for phase II cancer clinical trials. A study proceeds to the second stage unless the null hypothesis, that the true tumour response rate is below some specified value, is already accepted at the end of stage one. The conventional optimal design, for given type 1 and type 2 error rates, is the one which minimises the expected sample size under the null hypothesis. However, at least some new agents are active, and designs that explicitly address this possibility should be considered. We therefore investigate novel designs which are optimal under the alternative hypothesis, that the tumour response rate is higher than the null hypothesis value, and also designs which allow early stopping for efficacy. We make available, software for identifying the corresponding optimal and minimax designs. Considerable savings in expected sample sizes can be achieved if the alternative hypothesis is in fact true, without sample sizes suffering too much if the null hypothesis is true. We present an example discussing the merits of different designs in a practical context. We conclude that it is relevant to consider optimal designs under a range of hypotheses about the true response rate, and that allowing early stopping for efficacy is always advantageous in terms of expected sample size.},
  pmcid = {PMC3049867},
  pmid = {20678585},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mander_Thompson_2010_Two-stage designs optimal under the alternative hypothesis for phase II cancer.pdf}
}

@article{mansourniaHandlingTimeVarying2017,
  title = {Handling Time Varying Confounding in Observational Research},
  author = {Mansournia, Mohammad Ali and Etminan, Mahyar and Danaei, Goodarz and Kaufman, Jay S. and Collins, Gary},
  year = {2017},
  month = oct,
  journal = {BMJ},
  volume = {359},
  pages = {j4587},
  publisher = {British Medical Journal Publishing Group},
  issn = {0959-8138, 1756-1833},
  doi = {10.1136/bmj.j4587},
  urldate = {2023-12-11},
  abstract = {{$<$}p{$>$}Many exposures of epidemiological interest are time varying, and the values of potential confounders may change over time leading to time varying confounding. The aim of many longitudinal studies is to estimate the causal effect of a time varying exposure on an outcome that requires adjusting for time varying confounding. Time varying confounding affected by previous exposure often occurs in practice, but it is usually adjusted for by using conventional analytical methods such as time dependent Cox regression, random effects models, or generalised estimating equations, which are known to provide biased effect estimates in this setting. This article explains time varying confounding affected by previous exposure and outlines three causal methods proposed to appropriately adjust for this potential bias: inverse-probability-of-treatment weighting, the parametric G formula, and G estimation\emph{.}{$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
  langid = {english},
  pmid = {29038130},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mansournia_et_al_2017_Handling_time_varying_confounding_in_observational_research.pdf}
}

@article{markowitzWhatWhenPsychotherapy2015,
  title = {What to Do When a Psychotherapy Fails},
  author = {Markowitz, John C and Milrod, Barbara L},
  year = {2015},
  month = feb,
  journal = {The Lancet Psychiatry},
  volume = {2},
  number = {2},
  pages = {186--190},
  issn = {22150366},
  doi = {10/gfj39f},
  urldate = {2018-11-17},
  abstract = {Psychotherapies, like other medical interventions, should be chosen carefully and timed and tailored to individual patients' presenting circumstances and needs. Psychotherapies should be assessed regularly during delivery, and reassessed and changed if ineffective. Change of psychotherapy should not occur as rarely as it does now, albeit at present we do not have sufficient data to guide clinicians on this crucial topic. Increased availability of research data and diagnosis-specific psychotherapy algorithms would allow clinicians to make such changes with greater confidence than they can at present.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Markowitz_Milrod_2015_What to do when a psychotherapy fails.pdf}
}

@article{marshall-rothNonclassicalOxygenAtom2012,
  title = {Nonclassical Oxygen Atom Transfer Reactions of Oxomolybdenum({{VI}}) Bis(Catecholate)},
  author = {{Marshall-Roth}, Travis and Liebscher, Sean C and Rickert, Karl and Seewald, Nicholas J and Oliver, Allen G and Brown, Seth N},
  year = {2012},
  journal = {Chemical Communications},
  volume = {48},
  number = {63},
  eprint = {22785616},
  eprinttype = {pubmed},
  pages = {7826--7828},
  issn = {1359-7345},
  doi = {10.1039/c2cc33523a},
  abstract = {Mechanistic studies indicate that the oxomolybdenum(VI) bis(3,5-di-tert-butylcatecholate) fragment deoxygenates pyridine-N-oxides in a reaction where the oxygen is delivered to molyb-denum but the electrons for substrate reduction are drawn from the bound catecholate ligands, forming 3,5-di-tert-butyl-1,2-benzoquinone. Inner-sphere redox reactions involve both changes in oxida-tion state and changes in bonding. Classically, in reactions such as the oxygen atom transfer (OAT) 1 reaction depicted in eqn (1a), the changes in oxidation state and those in bonding are co-localized: molybdenum is both oxidized and forms a new bond to oxygen, while nitrogen is reduced and the N--O bond is broken. Co-localization is not, however, obligatory. For example, in complexes with redox-active ligands, bonding changes may take place at a redox-inert metal center while the corresponding changes in oxidation state take place at the coordinated ligand (eqn (1b)). The most thoroughly studied example of such a ''non-classical'' inner-sphere redox reaction is proton-coupled electron transfer (PCET), where the motion of the hydrogen nucleus may be quite separated from the motion of the electron. 2 In contrast to PCET, nonclassical OAT (eqn (1b)) would be a two-electron redox process. {\dh}1a{\TH}},
  copyright = {All rights reserved},
  pmid = {22785616},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Marshall-Roth et al_2012_Nonclassical oxygen atom transfer reactions of oxomolybdenum(VI).pdf}
}

@article{marshallCombiningEstimatesInterest2009,
  title = {Combining Estimates of Interest in Prognostic Modelling Studies after Multiple Imputation: Current Practice and Guidelines},
  shorttitle = {Combining Estimates of Interest in Prognostic Modelling Studies after Multiple Imputation},
  author = {Marshall, Andrea and Altman, Douglas G. and Holder, Roger L. and Royston, Patrick},
  year = {2009},
  month = jul,
  journal = {BMC Medical Research Methodology},
  volume = {9},
  number = {1},
  pages = {57},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-9-57},
  urldate = {2024-01-09},
  abstract = {Multiple imputation (MI) provides an effective approach to handle missing covariate data within prognostic modelling studies, as it can properly account for the missing data uncertainty. The multiply imputed datasets are each analysed using standard prognostic modelling techniques to obtain the estimates of interest. The estimates from each imputed dataset are then combined into one overall estimate and variance, incorporating both the within and between imputation variability. Rubin's rules for combining these multiply imputed estimates are based on asymptotic theory. The resulting combined estimates may be more accurate if the posterior distribution of the population parameter of interest is better approximated by the normal distribution. However, the normality assumption may not be appropriate for all the parameters of interest when analysing prognostic modelling studies, such as predicted survival probabilities and model performance measures.},
  keywords = {_tablet,Imputation Model,Impute Dataset,Likelihood Ratio Statistic,Multiple Imputation,Prognostic Modelling},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Marshall et al_2009_Combining estimates of interest in prognostic modelling studies after multiple.pdf;/Users/nseewald/Zotero/storage/FKJ2GZNK/1471-2288-9-57.html}
}

@article{martinez-perezPrivacySecurityMobile2015,
  title = {Privacy and {{Security}} in {{Mobile Health Apps}}: {{A Review}} and {{Recommendations}}},
  shorttitle = {Privacy and {{Security}} in {{Mobile Health Apps}}},
  author = {{Mart{\'i}nez-P{\'e}rez}, Borja and {de la Torre-D{\'i}ez}, Isabel and {L{\'o}pez-Coronado}, Miguel},
  year = {2015},
  month = jan,
  journal = {Journal of Medical Systems},
  volume = {39},
  number = {1},
  issn = {0148-5598, 1573-689X},
  doi = {10.1007/s10916-014-0181-3},
  urldate = {2018-10-12},
  abstract = {In a world where the industry of mobile applications is continuously expanding and new health care apps and devices are created every day, it is important to take special care of the collection and treatment of users' personal health information. However, the appropriate methods to do this are not usually taken into account by apps designers and insecure applications are released. This paper presents a study of security and privacy in mHealth, focusing on three parts: a study of the existing laws regulating these aspects in the European Union and the United States, a review of the academic literature related to this topic, and a proposal of some recommendations for designers in order to create mobile health applications that satisfy the current security and privacy legislation. This paper will complement other standards and certifications about security and privacy and will suppose a quick guide for apps designers, developers and researchers.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Martínez-Pérez et al_2015_Privacy and Security in Mobile Health Apps.pdf}
}

@article{martinHowMuchClinical2017,
  title = {How Much Do Clinical Trials Cost?},
  author = {Martin, Linda and Hutchens, Melissa and Hawkins, Conrad and Radnov, Alaina},
  year = {2017},
  month = jun,
  journal = {Nature Reviews Drug Discovery},
  volume = {16},
  number = {6},
  pages = {381--382},
  issn = {1474-1776, 1474-1784},
  doi = {10/gjhm5x},
  urldate = {2021-03-21},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Martin et al_2017_How much do clinical trials cost.pdf}
}

@misc{MatrixComputationsHopkins,
  title = {Matrix {{Computations}} {\textbar} {{Hopkins Press}}},
  urldate = {2022-06-22},
  howpublished = {https://www.press.jhu.edu/books/title/10678/matrix-computations},
  file = {/Users/nseewald/Zotero/storage/KR7KLDQ5/matrix-computations.html}
}

@article{matsuyamaEstimationMarginalSurvival2008,
  title = {Estimation of the Marginal Survival Time in the Presence of Dependent Competing Risks Using Inverse Probability of Censoring Weighted ({{IPCW}}) Methods},
  author = {Matsuyama, Yutaka and Yamaguchi, Takuhiro},
  year = {2008},
  journal = {Pharmaceutical Statistics},
  volume = {7},
  number = {3},
  pages = {202--214},
  issn = {1539-1612},
  doi = {10.1002/pst.290},
  urldate = {2024-07-18},
  abstract = {In medical studies, there is interest in inferring the marginal distribution of a survival time subject to competing risks. The Kyushu Lipid Intervention Study (KLIS) was a clinical study for hypercholesterolemia, where pravastatin treatment was compared with conventional treatment. The primary endpoint was time to events of coronary heart disease (CHD). In this study, however, some subjects died from causes other than CHD or were censored due to loss to follow-up. Because the treatments were targeted to reduce CHD events, the investigators were interested in the effect of the treatment on CHD events in the absence of causes of death or events other than CHD. In this paper, we present a method for estimating treatment group-specific marginal survival curves of time-to-event data in the presence of dependent competing risks. The proposed method is a straightforward extension of the Inverse Probability of Censoring Weighted (IPCW) method to settings with more than one reason for censoring. The results of our analysis showed that the IPCW marginal incidence for CHD was almost the same as the lower bound for which subjects with competing events were assumed to be censored at the end of all follow-up. This result provided reassurance that the results in KLIS were robust to competing risks. Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cause of failure,competing risks,Cox proportional hazards model,inverse weighting methods,Kaplan-Meier estimator,survival analysis},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Matsuyama_Yamaguchi_2008_Estimation of the marginal survival time in the presence of dependent competing.pdf;/Users/nseewald/Zotero/storage/VKAV7IDF/pst.html}
}

@article{matthayRevolutionWillBe2021,
  title = {The {{Revolution Will Be Hard}} to {{Evaluate}}: {{How Co-Occurring Policy Changes Affect Research}} on the {{Health Effects}} of {{Social Policies}}},
  shorttitle = {The {{Revolution Will Be Hard}} to {{Evaluate}}},
  author = {Matthay, Ellicott C and Hagan, Erin and Joshi, Spruha and Tan, May Lynn and Vlahov, David and Adler, Nancy and Glymour, M Maria},
  year = {2021},
  month = dec,
  journal = {Epidemiologic Reviews},
  volume = {43},
  number = {1},
  pages = {19--32},
  issn = {1478-6729},
  doi = {10.1093/epirev/mxab009},
  urldate = {2023-12-05},
  abstract = {Extensive empirical health research leverages variation in the timing and location of policy changes as quasi-experiments. Multiple social policies may be adopted simultaneously in the same locations, creating co-occurrence that must be addressed analytically for valid inferences. The pervasiveness and consequences of co-occurring policies have received limited attention. We analyzed a systematic sample of 13 social policy databases covering diverse domains including poverty, paid family leave, and tobacco use. We quantified policy co-occurrence in each database as the fraction of variation in each policy measure across different jurisdictions and times that could be explained by covariation with other policies. We used simulations to estimate the ratio of the variance of effect estimates under the observed policy co-occurrence to variance if policies were independent. Policy co-occurrence ranged from very high for state-level cannabis policies to low for country-level sexual minority-rights policies. For 65\% of policies, greater than 90\% of the place-time variation was explained by other policies. Policy co-occurrence increased the variance of effect estimates by a median of 57-fold. Co-occurring policies are common and pose a major methodological challenge to rigorously evaluating health effects of individual social policies. When uncontrolled, co-occurring policies confound one another, and when controlled, resulting positivity violations may substantially inflate the variance of estimated effects. Tools to enhance validity and precision for evaluating co-occurring policies are needed.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Matthay et al_2021_The Revolution Will Be Hard to Evaluate.pdf;/Users/nseewald/Zotero/storage/K4X8QC94/6383631.html}
}

@article{matthewsAnalysisSerialMeasurements1990,
  title = {Analysis of Serial Measurements in Medical Research.},
  author = {Matthews, J. N. and Altman, D. G. and Campbell, M. J. and Royston, P.},
  year = {1990},
  month = jan,
  journal = {British Medical Journal},
  volume = {300},
  number = {6719},
  pages = {230--235},
  issn = {0959-8138, 1468-5833},
  doi = {10/dw7kqf},
  urldate = {2019-07-03},
  abstract = {In medical research data are often collected serially on subjects. The statistical analysis of such data is often inadequate in two ways: it may fail to settle clinically relevant questions and it may be statistically invalid. A commonly used method which compares groups at a series of time points, possibly with t tests, is flawed on both counts. There may, however, be a remedy, which takes the form of a two stage method that uses summary measures. In the first stage a suitable summary of the response in an individual, such as a rate of change or an area under a curve, is identified and calculated for each subject. In the second stage these summary measures are analysed by simple statistical techniques as though they were raw data. The method is statistically valid and likely to be more relevant to the study questions. If this method is borne in mind when the experiment is being planned it should promote studies with enough subjects and sufficient observations at critical times to enable useful conclusions to be drawn. Use of summary measures to analyse serial measurements, though not new, is potentially a useful and simple tool in medical research.},
  langid = {english},
  pmid = {2106931},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Matthews et al_1990_Analysis of serial measurements in medical research.pdf;/Users/nseewald/Zotero/storage/XZ5XBELQ/230.html}
}

@article{matthewsTargetTrialEmulation2022,
  title = {Target Trial Emulation: Applying Principles of Randomised Trials to Observational Studies},
  shorttitle = {Target Trial Emulation},
  author = {Matthews, Anthony A. and Danaei, Goodarz and Islam, Nazrul and Kurth, Tobias},
  year = {2022},
  month = aug,
  journal = {BMJ},
  volume = {378},
  pages = {e071108},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj-2022-071108},
  urldate = {2023-08-02},
  abstract = {{$<$}p{$>$}The randomised trial is the preferred study design for evaluating the effectiveness and safety of interventions. Yet such trials can be prohibitively expensive, unethical, or take too long. When it is not possible to carry out a randomised trial, observational data can be used to answer similar questions. Here, we describe the process of using observational data to emulate a target trial, which applies the study design principles of randomised trials to observational studies that aim to estimate the causal effect of an intervention. The target trial provides a formal framework to help avoid self-inflicted biases common to observational studies.{$<$}/p{$>$}},
  chapter = {Research},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
  langid = {english},
  pmid = {36041749},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Matthews et al_2022_Target trial emulation.pdf}
}

@article{maUseInterruptedTimeSeries2013,
  title = {Use of {{Interrupted Time-Series Method}} to {{Evaluate}} the {{Impact}} of {{Cigarette Excise Tax Increases}} in {{Pennsylvania}}, 2000--2009},
  author = {Ma, Zhen-qiang},
  year = {2013},
  journal = {Preventing Chronic Disease},
  volume = {10},
  issn = {1545-1151},
  doi = {10.5888/pcd10.120268},
  urldate = {2024-05-23},
  abstract = {Scientific evidence shows that cigarette price increases can significantly reduce smoking prevalence and smoking initiation among adolescents and young adults. However, data are lacking regarding the effectiveness of increasing Pennsylvania's cigarette tax to reduce smoking and/or adverse health effects of smoking.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ma_2013_Use of Interrupted Time-Series Method to Evaluate the Impact of Cigarette.pdf;/Users/nseewald/Zotero/storage/97A2CC9F/12_0268.html}
}

@article{maxwellLongitudinalDesignsRandomized1998,
  title = {Longitudinal Designs in Randomized Group Comparisons: {{When}} Will Intermediate Observations Increase Statistical Power?},
  shorttitle = {Longitudinal Designs in Randomized Group Comparisons},
  author = {Maxwell, Scott E.},
  year = {1998},
  month = sep,
  journal = {Psychological Methods},
  volume = {3},
  number = {3},
  pages = {275--290},
  issn = {1082-989X},
  doi = {10/dw3ktb},
  urldate = {2018-12-06},
  abstract = {A variety of experimental designs have been developed to estimate the effect of an intervention. This article compared analyses in 2 standard randomized designs, the posttest-only and pretest-posttest designs, with an analysis in the newly suggested intensive design in terms of the statistical power and precision of estimated intervention effect afforded by each design. These comparisons are especially interesting because the suggested analysis of data from the intensive design is equivalent to a possible use of hierarchical linear modeling when there are no missing data. Results show that an analysis of variance (ANOVA) of slopes from the intensive design is almost always more powerful than an ANOVA in the posttest-only design, and can also be more powerful than an analysis of covariance in the pretest-posttest design, but typically only when the number of measurement waves is 5 or more. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Maxwell_1998_Longitudinal designs in randomized group comparisons.pdf}
}

@article{maxwellSampleSizePlanning2008,
  title = {Sample {{Size Planning}} for {{Statistical Power}} and {{Accuracy}} in {{Parameter Estimation}}},
  author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
  year = {2008},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {59},
  number = {1},
  pages = {537--563},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.59.103006.093735},
  urldate = {2018-10-12},
  abstract = {This review examines recent advances in sample size planning, not only from the perspective of an individual researcher, but also with regard to the goal of developing cumulative knowledge. Psychologists have traditionally thought of sample size planning in terms of power analysis. Although we review recent advances in power analysis, our main focus is the desirability of achieving accurate parameter estimates, either instead of or in addition to obtaining sufficient power. Accuracy in parameter estimation (AIPE) has taken on increasing importance in light of recent emphasis on effect size estimation and formation of confidence intervals. The review provides an overview of the logic behind sample size planning for AIPE and summarizes recent advances in implementing this approach in designs commonly used in psychological research.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Maxwell et al_2008_Sample Size Planning for Statistical Power and Accuracy in Parameter Estimation.pdf}
}

@article{mayerRealworldTimeTreatment2025,
  title = {Real-World Time on Treatment ({{rwTOT}}) with First-Line ({{1L}}) Enfortumab Vedotin and Pembrolizumab ({{EV}}+{{P}}) after {{U}}.{{S}}. {{Food}} and {{Drug Administration}} Approval for Advanced Urothelial Cancer ({{aUC}}).},
  author = {Mayer, Melanie and Seewald, Nicholas and Perez, Ernesto Ulloa and Homet Moreno, Blanca and Ramamurthy, Chethan and Babcock, Aram and Li, Haojie and Mamtani, Ronac},
  year = {2025},
  month = feb,
  journal = {Journal of Clinical Oncology},
  volume = {43},
  number = {5\_suppl},
  pages = {731--731},
  publisher = {Wolters Kluwer},
  issn = {0732-183X},
  doi = {10.1200/JCO.2025.43.5_suppl.731},
  urldate = {2025-03-17},
  abstract = {731 Background: EV+P received accelerated approval (AA) for cisplatin (cis)-ineligible aUC patients in April 2023 (EV-103) and full approval (FA) for all previously untreated patients in December 2023 (EV-302). In EV-302 trial, median durations of treatment with EV+P was 9.4 months (7.0 and 8.5 months for EV and P, respectively). We previously demonstrated high uptake of EV+P post AA in the real-world. Here, we examine rwToT with 1L EV+P with nearly 1 year of follow-up post-AA. Methods: This descriptive, post-marketing, retrospective cohort study used the Flatiron Health longitudinal database derived from EHR records of US patients with aUC initiating 1L EV+P after April 5, 2023 (AA) but before December 15, 2023 (FA). rwToT for EV+P was defined as length from first administration date of EV+P regimen to 1L therapy discontinuation, defined as last administration date of either component (i.e., EV or P) if patient initiated a next line of therapy, died during therapy, or had a gap of {$>$}60 days between last recorded dose and last contact date. rwToT was also estimated for each EV+P component. If no discontinuation criteria were met, the patient was censored at data cut-off (March 31, 2024). The Kaplan-Meier method was used for analysis of rwToT, including median rwToT (months) and 30-, 90-, 180-day on-treatment rates (\%). Results: We identified 111 patients with aUC who initiated 1L EV+P after AA but before FA (mean age: 73.9 y, 75.7\% male, 77.0\% white, 23.7\% ECOG performance status {$\geq$}2, 75.2\% cis-ineligible, and 84.7\% from community practices). As of March 31, 2024, approximately 41.4\% (n=46) discontinued both EV and P; 9.9\% [n=11] began subsequent therapy, 27.0\% [n=30] died, and remaining patients were censored at end of follow-up (58.6\%, n=65). Median rwToT (95\% confidence interval [CI]) for EV+P, EV, and P were 8.2 months (6.5-not reached [NR]), 7.2 months (5.2-NR), and NR (6.3-NR), respectively (on-treatment rates reported in Table). Among 1L EV+P treated patients receiving subsequent therapies, 72.7\% (9/11) received gemcitabine and carboplatin as the first subsequent therapy. Conclusions: In this large and predominantly cis-ineligible cohort of advanced urothelial cancer patients treated with EV+P in contemporary practice, rwTOT approximated duration of treatment in clinical trials. Most 1L EV+P users receiving subsequent anticancer therapy received platinum-based chemotherapy. rwToT with 1L EV+P for patients with advanced urothelial cancer (N=111). ~	EV+P	EV	P Discontinued, n (\%)	46 (41.4)	52 (46.8)	47 (42.3) Censored, n (\%)	65 (58.6)	59 (53.2)	64 (57.7) Median rwToT, months (95\% CI)	8.1 (6.4-NR)	7.1 (5.1-NR)	NR (6.2-NR) On-treatment rate, \% (95\%CI)	~	~	~ 30-d	84.7 (78.2, 91.7)	82.0 (75.1, 89.5)	81.1 (74.1, 88.7) 90-d	72.1 (64.2, 80.9)	65.8 (57.5, 75.2)	72.1 (64.2, 80.9) 180-d	60.4 (51.4, 70.9)	54.0 (45.0, 64.9)	60.0 (51.1, 70.4) EXPAND TABLE OPEN IN VIEWER}
}

@article{mayoBirnbaumArgumentStrong2014,
  title = {On the {{Birnbaum Argument}} for the {{Strong Likelihood Principle}}},
  author = {Mayo, Deborah G.},
  year = {2014},
  month = may,
  journal = {Statistical Science},
  volume = {29},
  number = {2},
  pages = {227--239},
  issn = {0883-4237},
  doi = {10.1214/13-STS457},
  urldate = {2018-10-30},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mayo_2014_On the Birnbaum Argument for the Strong Likelihood Principle.pdf}
}

@article{maziarzLongitudinalPredictionTimetoEvent2017,
  title = {On {{Longitudinal Prediction}} with {{Time-to-Event Outcome}}: {{Comparison}} of {{Modeling Options}}},
  shorttitle = {On {{Longitudinal Prediction}} with {{Time-to-Event Outcome}}},
  author = {Maziarz, Marlena and Heagerty, Patrick and Cai, Tianxi and Zheng, Yingye},
  year = {2017},
  month = mar,
  journal = {Biometrics},
  volume = {73},
  number = {1},
  pages = {83--93},
  issn = {0006-341X},
  doi = {10.1111/biom.12562},
  urldate = {2024-02-16},
  abstract = {Long-term follow-up is common in many medical investigations where the interest lies in predicting patients' risks for a future adverse outcome using repeatedly measured predictors over time. A key quantity is the likelihood of developing an adverse outcome among individuals who survived up to time s given their covariate information up to time s. Simple, yet reliable, methodology for updating the predicted risk of disease progression using longitudinal markers remains elusive. Two main approaches have been considered in the literature. One approach, based on joint modeling (JM) of failure time and longitudinal covariate process (Tsiatis and Davidian, 2004), derives such longitudinal predictive probability from the joint probability of a longitudinal marker and an event at a given time. A second approach, the partly conditional (PC) modeling (Zheng and Heagerty, 2005), directly models the predictive probability conditional on survival up to a landmark time and information accrued by that time. In this article, we propose new PC models for longitudinal prediction that are more flexible than joint modeling and improve the prediction accuracy over existing PC models. We provide procedures for making inference regarding future risk for an individual with longitudinal measures up to a given time. In addition, we conduct simulations to evaluate both JM and PC approaches in order to provide practical guidance on modeling choices. We use standard measures of predictive accuracy adapted to our setting to explore the predictiveness of the two approaches. We illustrate the performance of the two approaches on a dataset from the End Stage Renal Disease Study (ESRDS).},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Maziarz_et_al_2017_On_Longitudinal_Prediction_with_Time-to-Event_Outcome.pdf;/Users/nseewald/Zotero/storage/LIB7AA6H/7537690.html}
}

@article{mccaffreyPropensityScoreEstimation2004,
  title = {Propensity {{Score Estimation With Boosted Regression}} for {{Evaluating Causal Effects}} in {{Observational Studies}}},
  author = {McCaffrey, Daniel F. and Ridgeway, Greg and Morral, Andrew R.},
  year = {2004},
  month = dec,
  journal = {Psychological Methods},
  volume = {9},
  number = {4},
  pages = {403--425},
  publisher = {Psychological Methods},
  issn = {1082-989X},
  urldate = {2022-04-27},
  abstract = {Causal effect modeling with naturalistic rather than experimental data is challenging. In observational studies participants in different treatment conditions may also differ on pretreatment characteristics that influence outcomes. Propensity score methods can theoretically eliminate these confounds for all observed covariates, but accurate estimation of propensity scores is impeded by large numbers of covariates, uncertain functional forms for their associations with treatment selection, and other problems. This article demonstrates that boosting, a modern statistical technique, can overcome many of these obstacles. The authors illustrate this approach with a study of adolescent probationers in substance abuse treatment programs. Propensity score weights estimated using boosting eliminate most pretreatment group differences and substantially alter the apparent relative effects of adolescent substance abuse treatment.},
  keywords = {Adolescents,Causal Analysis,Causal Models,Drug Abuse,Evaluation Methods,Evaluation Research,Observation,Regression (Statistics),Scores,Statistical Analysis,Statistical Estimation,Statistical Inference,Statistical Measurement,Substance Abuse,Substance Use Treatment,Treatment Outcomes},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McCaffrey et al_2004_Propensity Score Estimation With Boosted Regression for Evaluating Causal.pdf;/Users/nseewald/Zotero/storage/HTD8EQ8Z/2004-21445-001.html}
}

@article{mccaffreyTutorialPropensityScore2013,
  title = {A Tutorial on Propensity Score Estimation for Multiple Treatments Using Generalized Boosted Models},
  author = {McCaffrey, Daniel F. and Griffin, Beth Ann and Almirall, Daniel and Slaughter, Mary Ellen and Ramchand, Rajeev and Burgette, Lane F.},
  year = {2013},
  journal = {Statistics in Medicine},
  volume = {32},
  number = {19},
  pages = {3388--3414},
  issn = {1097-0258},
  doi = {10.1002/sim.5753},
  urldate = {2024-11-27},
  abstract = {The use of propensity scores to control for pretreatment imbalances on observed variables in non-randomized or observational studies examining the causal effects of treatments or interventions has become widespread over the past decade. For settings with two conditions of interest such as a treatment and a control, inverse probability of treatment weighted estimation with propensity scores estimated via boosted models has been shown in simulation studies to yield causal effect estimates with desirable properties. There are tools (e.g., the twang package in R) and guidance for implementing this method with two treatments. However, there is not such guidance for analyses of three or more treatments. The goals of this paper are twofold: (1) to provide step-by-step guidance for researchers who want to implement propensity score weighting for multiple treatments and (2) to propose the use of generalized boosted models (GBM) for estimation of the necessary propensity score weights. We define the causal quantities that may be of interest to studies of multiple treatments and derive weighted estimators of those quantities. We present a detailed plan for using GBM to estimate propensity scores and using those scores to estimate weights and causal effects. We also provide tools for assessing balance and overlap of pretreatment variables among treatment groups in the context of multiple treatments. A case study examining the effects of three treatment programs for adolescent substance abuse demonstrates the methods. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {causal effects,causal modeling,GBM,inverse probability of treatment weighting,twang},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McCaffrey et al. - 2013 - A tutorial on propensity score estimation for multiple treatments using generalized boosted models.pdf;/Users/nseewald/Zotero/storage/MHHYHVHM/sim.html}
}

@article{mcdanielFastPureImplementation2013,
  title = {Fast {{Pure R Implementation}} of {{GEE}}: {{Application}} of the {{Matrix Package}}},
  shorttitle = {Fast {{Pure R Implementation}} of {{GEE}}},
  author = {McDaniel, S., Lee and Henderson, C., Nicholas and Rathouz, J., Paul},
  year = {2013},
  journal = {The R Journal},
  volume = {5},
  number = {1},
  pages = {181},
  issn = {2073-4859},
  doi = {10/gf2375},
  urldate = {2019-05-25},
  abstract = {Generalized estimating equation solvers in R only allow for a few pre-determined options for the link and variance functions. We provide a package, geeM, which is implemented entirely in R and allows for user specified link and variance functions. The sparse matrix representations provided in the Matrix package enable a fast implementation. To gain speed, we make use of analytic inverses of the working correlation when possible and a trick to find quick numeric inverses when an analytic inverse is not available. Through three examples, we demonstrate the speed of geeM, which is not much worse than C implementations like geepack and gee on small data sets and faster on large data sets.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McDaniel et al_2013_Fast Pure R Implementation of GEE.pdf}
}

@article{mcgeeDevaluedBlackLatino2016,
  title = {Devalued {{Black}} and {{Latino Racial Identities}}: {{A By-Product}} of {{STEM College Culture}}?},
  shorttitle = {Devalued {{Black}} and {{Latino Racial Identities}}},
  author = {McGee, Ebony O.},
  year = {2016},
  month = dec,
  journal = {American Educational Research Journal},
  volume = {53},
  number = {6},
  pages = {1626--1662},
  publisher = {American Educational Research Association},
  issn = {0002-8312},
  doi = {10/ghfsp9},
  urldate = {2020-10-16},
  abstract = {At some point most Black and Latino/a college students---even long-term high achievers---question their own abilities because of multiple forms of racial bias. The 38 high-achieving Black and Latino/a STEM study participants, who attended institutions with racially hostile academic spaces, deployed an arsenal of strategies (e.g., stereotype management) to deflect stereotyping and other racial assaults (e.g., racial microaggressions), which are particularly prevalent in STEM fields. These students rely heavily on coping strategies that alter their authentic racial identities but create internal turmoil. Institutions of higher education, including minority-serving schools, need to examine institutional racism and other structural barriers that damage the racial identities of Black and Latino/a students in STEM and cause lasting psychological strain.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McGee_2016_Devalued Black and Latino Racial Identities.pdf}
}

@article{mcgintyEffectsStateMedical2023,
  title = {Effects of {{U}}.{{S}}. {{State Medical Cannabis Laws}} on {{Treatment}} of {{Chronic Noncancer Pain}}},
  author = {McGinty, Emma E. and Tormohlen, Kayla N. and Seewald, Nicholas J. and Bicket, Mark C. and McCourt, Alexander D. and Rutkow, Lainie and White, Sarah A. and Stuart, Elizabeth A.},
  year = {2023},
  month = jul,
  journal = {Annals of Internal Medicine},
  volume = {176},
  number = {7},
  pages = {904--912},
  publisher = {American College of Physicians},
  issn = {0003-4819},
  doi = {10.7326/M23-0053},
  urldate = {2023-07-06},
  copyright = {All rights reserved},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McGinty et al_2023_Effects of U.pdf}
}

@article{mcgintyEffectsStateOpioid2022,
  title = {Effects of {{State Opioid Prescribing Laws}} on {{Use}} of {{Opioid}} and {{Other Pain Treatments Among Commercially Insured U}}.{{S}}. {{Adults}}},
  author = {McGinty, Emma E. and Bicket, Mark C. and Seewald, Nicholas J. and Stuart, Elizabeth A. and Alexander, G. Caleb and Barry, Colleen L. and McCourt, Alexander D. and Rutkow, Lainie},
  year = {2022},
  month = may,
  journal = {Annals of Internal Medicine},
  volume = {175},
  number = {5},
  pages = {617--627},
  publisher = {American College of Physicians},
  issn = {0003-4819},
  doi = {10.7326/M21-4363},
  urldate = {2022-03-15},
  copyright = {All rights reserved},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McGinty et al_2022_Effects of State Opioid Prescribing Laws on Use of Opioid and Other Pain.pdf}
}

@article{mcgintyIntegratingMentalHealth2020,
  title = {Integrating {{Mental Health}} and {{Addiction Treatment Into General Medical Care}}: {{The Role}} of {{Policy}}},
  shorttitle = {Integrating {{Mental Health}} and {{Addiction Treatment Into General Medical Care}}},
  author = {McGinty, Emma E. and Daumit, Gail L.},
  year = {2020},
  month = nov,
  journal = {Psychiatric Services},
  volume = {71},
  number = {11},
  pages = {1163--1169},
  issn = {1075-2730, 1557-9700},
  doi = {10.1176/appi.ps.202000183},
  urldate = {2020-12-08},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McGinty_Daumit_2020_Integrating Mental Health and Addiction Treatment Into General Medical Care.pdf}
}

@article{mcgintyProtocolMixedmethodsStudy2018,
  title = {Protocol: Mixed-Methods Study to Evaluate Implementation, Enforcement, and Outcomes of {{U}}.{{S}}. State Laws Intended to Curb High-Risk Opioid Prescribing},
  shorttitle = {Protocol},
  author = {McGinty, Emma E. and Stuart, Elizabeth A. and Alexander, G. Caleb and Barry, Colleen L. and Bicket, Mark C. and Rutkow, Lainie},
  year = {2018},
  month = dec,
  journal = {Implementation Science},
  volume = {13},
  number = {1},
  pages = {1--10},
  publisher = {BioMed Central},
  issn = {1748-5908},
  doi = {10/gk55km},
  urldate = {2021-07-01},
  abstract = {The U.S. opioid epidemic has been driven by the high volume of opioids prescribed by healthcare providers. U.S. states have recently enacted four types of laws designed to curb high-risk prescribing practices, such as high-dose and long-term opioid prescribing, associated with opioid-related mortality: (1) mandatory Prescription Drug Monitoring Program (PDMP) enrollment laws, which require prescribers to enroll in their state's PDMP, an electronic database of patients' controlled substance prescriptions, (2) mandatory PDMP query laws, which require prescribers to query the PDMP prior to prescribing an opioid, (3) opioid prescribing cap laws, which limit the dose and/or duration of opioid prescriptions, and (4) pill mill laws, which strictly regulate pain clinics to prevent nonmedical opioid prescribing. Some pain experts have expressed concern that these laws could negatively affect pain management among patients with chronic non-cancer pain. This paper describes the protocol for a mixed-methods study analyzing the independent effects of these four types of laws on opioid prescribing patterns and chronic non-cancer pain treatment, accounting for variation in implementation and enforcement of laws across states. Many states have enacted multiple opioid prescribing laws at or around the same time. To overcome this issue, our study focuses on 18 treatment states that each enacted a single law of interest, and no other potentially confounding laws, over a 4-year period (2 years pre-/post-law). Qualitative interviews with key leaders in each of the 18 treatment states will characterize the timing, scope, and strength of each state law's implementation and enforcement. This information will inform the design and interpretation of synthetic control models analyzing the effects of each of the two types of laws on two sets of outcomes: measures of (1) high-risk opioid prescribing and (2) non-opioid treatments for chronic non-cancer pain. Study of mandatory PDMP enrollment, mandatory PDMP query, opioid prescribing cap, and pill mill laws is timely given a dynamic policy environment in which numerous states pass, revise, implement, and enforce varied laws to address opioid prescribing each year. Findings will inform enactment, implementation, and enforcement of these laws in additional states.},
  copyright = {2018 The Author(s).},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McGinty et al_2018_Protocol.pdf;/Users/nseewald/Zotero/storage/PRL3Z5E2/s13012-018-0719-8.html}
}

@article{mcgintyProtocolMixedmethodsStudy2021,
  title = {Protocol: Mixed-Methods Study of How Implementation of {{US}} State Medical Cannabis Laws Affects Treatment of Chronic Non-Cancer Pain and Adverse Opioid Outcomes},
  shorttitle = {Protocol},
  author = {McGinty, Emma E. and Tormohlen, Kayla N. and Barry, Colleen L. and Bicket, Mark C. and Rutkow, Lainie and Stuart, Elizabeth A.},
  year = {2021},
  month = dec,
  journal = {Implementation Science},
  volume = {16},
  number = {1},
  pages = {2},
  issn = {1748-5908},
  doi = {10/gk55kp},
  urldate = {2021-07-01},
  abstract = {Background: Thirty-three US states and Washington, D.C., have enacted medical cannabis laws allowing patients with chronic non-cancer pain to use cannabis, when recommended by a physician, to manage their condition. However, clinical guidelines do not recommend cannabis for treatment of chronic non-cancer pain due to limited and mixed evidence of effectiveness. How state medical cannabis laws affect delivery of evidence-based treatment for chronic non-cancer pain is unclear. These laws could lead to substitution of cannabis in place of clinical guideline-discordant opioid prescribing, reducing risk of opioid use disorder and overdose. Conversely, state medical cannabis laws could lead to substitution of cannabis in place of guideline-concordant treatments such as topical analgesics or physical therapy. This protocol describes a mixed-methods study examining the implementation and effects of state medical cannabis laws on treatment of chronic non-cancer pain. A key contribution of the study is the examination of how variation in state medical cannabis laws' policy implementation rules affects receipt of chronic non-cancer pain treatments. Methods: The study uses a concurrent-embedded design. The primary quantitative component of the study employs a difference-in-differences design using a policy trial emulation approach. Quantitative analyses will evaluate state medical cannabis laws' effects on treatment for chronic non-cancer pain as well as on receipt of treatment for opioid use disorder, opioid overdose, cannabis use disorder, and cannabis poisoning among people with chronic non-cancer pain. Secondary qualitative and survey methods will be used to characterize implementation of state medical cannabis laws through interviews with state leaders and representative surveys of physicians who treat, and patients who experience, chronic non-cancer pain in states with medical cannabis laws. Discussion: This study will examine the effects of medical cannabis laws on patients' receipt of guidelineconcordant non-opioid, non-cannabis treatments for chronic non-cancer pain and generate new evidence on the effects of state medical cannabis laws on adverse opioid outcomes. Results will inform the dynamic policy environment in which numerous states consider, enact, and/or amend medical cannabis laws each year.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McGinty et al_2021_Protocol.pdf}
}

@article{mcgintyScalingInterventionsManage2022,
  title = {Scaling {{Interventions}} to {{Manage Chronic Disease}}: {{Innovative Methods}} at the {{Intersection}} of {{Health Policy Research}} and {{Implementation Science}}},
  shorttitle = {Scaling {{Interventions}} to {{Manage Chronic Disease}}},
  author = {McGinty, Emma E. and Seewald, Nicholas J. and Bandara, Sachini and Cerd{\'a}, Magdalena and Daumit, Gail L. and Eisenberg, Matthew D. and Griffin, Beth Ann and Igusa, Tak and Jackson, John W. and {Kennedy-Hendricks}, Alene and Marsteller, Jill and Miech, Edward J. and Purtle, Jonathan and Schmid, Ian and Schuler, Megan S. and Yuan, Christina T. and Stuart, Elizabeth A.},
  year = {2022},
  month = sep,
  journal = {Prevention Science},
  issn = {1573-6695},
  doi = {10.1007/s11121-022-01427-8},
  urldate = {2022-09-06},
  abstract = {Policy implementation is a key component of scaling effective chronic disease prevention and management interventions. Policy can support scale-up by mandating or incentivizing intervention adoption, but enacting a policy is only the first step. Fully implementing a policy designed to facilitate implementation of health interventions often requires a range of accompanying implementation structures, like health IT systems, and implementation strategies, like training. Decision makers need to know what policies can support intervention adoption and how to implement those policies, but to date research on policy implementation is limited and innovative methodological approaches are needed. In December 2021, the Johns Hopkins ALACRITY Center for Health and Longevity in Mental Illness and the Johns Hopkins Center for Mental Health and Addiction Policy convened a forum of research experts to discuss approaches for studying policy implementation. In this report, we summarize the ideas that came out of the forum. First, we describe a motivating example focused on an Affordable Care Act Medicaid health home waiver policy used by some US states to support scale-up of an evidence-based integrated care model shown in clinical trials to improve cardiovascular care for people with serious mental illness. Second, we define key policy implementation components including structures, strategies, and outcomes. Third, we provide an overview of descriptive, predictive and associational, and causal approaches that can be used to study policy implementation. We conclude with discussion of priorities for methodological innovations in policy implementation research, with three key areas identified by forum experts: effect modification methods for making causal inferences about how policies' effects on outcomes vary based on implementation structures/strategies; causal mediation approaches for studying policy implementation mechanisms; and characterizing uncertainty in systems science models. We conclude with discussion of overarching methods considerations for studying policy implementation, including measurement of policy implementation, strategies for studying the role of context in policy implementation, and the importance of considering when establishing causality is the goal of policy implementation research.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {Implementation,Policy,Scale-up},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McGinty et al_2022_Scaling Interventions to Manage Chronic Disease.pdf}
}

@article{mckayEffectivenessTelephoneBasedContinuing2005,
  title = {The {{Effectiveness}} of {{Telephone-Based Continuing Care}} for {{Alcohol}} and {{Cocaine Dependence}}: 24-{{Month Outcomes}}},
  shorttitle = {The {{Effectiveness}} of {{Telephone-Based Continuing Care}} for {{Alcohol}} and {{Cocaine Dependence}}},
  author = {McKay, James R. and Lynch, Kevin G. and Shepard, Donald S. and Pettinati, Helen M.},
  year = {2005},
  month = feb,
  journal = {Archives of General Psychiatry},
  volume = {62},
  number = {2},
  pages = {199--207},
  issn = {0003-990X},
  doi = {10/bz4q28},
  urldate = {2020-12-13},
  abstract = {Telephone-based disease management protocols have shown promise in improving outcomes in a number of medical and psychiatric disorders, but this approach to continuing care has received little study in alcohol- and drug-dependent individuals.To compare telephone-based continuing care with 2 more intensive face-to-face continuing care interventions.A randomized 3-group clinical trial with a 2-year follow-up.Two outpatient substance abuse treatment programs, one community-based and the other at a Veterans Affairs medical center facility.Alcohol- and/or cocaine-dependent patients (N~=~359) who had completed 4-week intensive outpatient programs.Three 12-week continuing care treatments: weekly telephone-based monitoring and brief counseling contacts combined with weekly supportive group sessions in the first 4 weeks (TEL), twice-weekly cognitive-behavioral relapse prevention (RP), and twice-weekly standard group counseling (STND).Percentage of days abstinent from alcohol and cocaine, total abstinence from alcohol and cocaine, negative consequences of substance use, cocaine urine toxicological results, and {$\gamma$}-glutamyltransferase.Participants in TEL had higher rates of total abstinence over the follow-up than those in STND (P\&lt;.05). In alcohol-dependent participants, 24-month {$\gamma$}-glutamyltransferase levels were lower in TEL than in RP (P~=~.005). In cocaine-dependent participants, there was a significant group~{\texttimes}~time interaction (P~=~.03) in which the rate of cocaine-positive urine samples increased more rapidly in RP as compared with TEL. On percentage of days abstinent or negative consequences of substance use, TEL did not differ from RP or STND. Participants with high scores on a composite risk indicator, based on co-occurring alcohol and cocaine dependence and poor progress toward achieving intensive outpatient program goals, had better total abstinence outcomes up to 21 months if they received STND rather than TEL, whereas those with lower scores had higher abstinence rates in TEL than in STND (P~=~.04).Telephone-based continuing care appears to be an effective form of step-down treatment for most patients with alcohol and cocaine dependence who complete an initial stabilization treatment, compared with more intensive face-to-face interventions. However, high-risk patients may have better outcomes if they first receive group counseling continuing care after completing intensive outpatient programs.Arch Gen Psychiatry. 2005;62:199-207--{$>$}},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McKay et al_2005_The Effectiveness of Telephone-Based Continuing Care for Alcohol and Cocaine.pdf;/Users/nseewald/Zotero/storage/F7MEK2IH/208326.html}
}

@article{mckayEffectPatientChoice2015,
  title = {Effect of Patient Choice in an Adaptive Sequential Randomization Trial of Treatment for Alcohol and Cocaine Dependence.},
  author = {McKay, James R. and Drapkin, Michelle L. and Van Horn, Deborah H. A. and Lynch, Kevin G. and Oslin, David W. and DePhilippis, Dominick and Ivey, Megan and Cacciola, John S.},
  year = {2015},
  month = dec,
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {83},
  number = {6},
  pages = {1021--1032},
  issn = {1939-2117, 0022-006X},
  doi = {10.1037/a0039534},
  urldate = {2018-10-12},
  abstract = {The results of this study indicated that individuals who failed to engage in an intensive outpatient program (IOP) for substance-use disorders had better drinking outcomes if outreach attempts focused on engagement in the IOP, rather than on offering several treatment options in addition to IOP. Therefore, providing treatment choice to nonengaged patients did not lead to better outcomes.},
  langid = {english},
  keywords = {alcohol dependence,belief that patient preferences,cocaine dependence,interventions,should be,smart study design,taken into consideration in,telephone,the selection of treatment,there is a growing,treatment choice},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McKay et al_2015_Effect of patient choice in an adaptive sequential randomization trial of.pdf}
}

@article{mckayThereCaseExtended2005,
  title = {Is There a Case for Extended Interventions for Alcohol and Drug Use Disorders?},
  author = {McKay, James R.},
  year = {2005},
  month = nov,
  journal = {Addiction},
  volume = {100},
  number = {11},
  pages = {1594--1610},
  issn = {09652140, 13600443},
  doi = {10/btpvtr},
  urldate = {2018-11-17},
  langid = {english},
  keywords = {Adaptive protocols,addiction management,continuing care,extended interventions,substance use disorders},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McKay_2005_Is there a case for extended interventions for alcohol and drug use disorders.pdf;/Users/nseewald/Zotero/storage/UYGBYK6E/j.1360-0443.2005.01208.html}
}

@article{mcneishUnnecessaryUbiquityHierarchical2017,
  title = {On the Unnecessary Ubiquity of Hierarchical Linear Modeling},
  author = {McNeish, Daniel and Stapleton, Laura M. and Silverman, Rebecca D.},
  year = {2017},
  month = mar,
  journal = {Psychological Methods},
  volume = {22},
  number = {1},
  pages = {114--140},
  issn = {1082-989X},
  doi = {10/f9zh55},
  urldate = {2019-12-03},
  abstract = {In psychology and the behavioral sciences generally, the use of the hierarchical linear model (HLM) and its extensions for discrete outcomes are popular methods for modeling clustered data. HLM and its discrete outcome extensions, however, are certainly not the only methods available to model clustered data. Although other methods exist and are widely implemented in other disciplines, it seems that psychologists have yet to consider these methods in substantive studies. This article compares and contrasts HLM with alternative methods including generalized estimating equations and cluster-robust standard errors. These alternative methods do not model random effects and thus make a smaller number of assumptions and are interpreted identically to single-level methods with the benefit that estimates are adjusted to reflect clustering of observations. Situations where these alternative methods may be advantageous are discussed including research questions where random effects are and are not required, when random effects can change the interpretation of regression coefficients, challenges of modeling with random effects with discrete outcomes, and examples of published psychology articles that use HLM that may have benefitted from using alternative methods. Illustrative examples are provided and discussed to demonstrate the advantages of the alternative methods and also when HLM would be the preferred method. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/McNeish et al_2017_On the unnecessary ubiquity of hierarchical linear modeling.pdf}
}

@misc{MedicalMarijuanaLaws,
  title = {Medical {{Marijuana Laws May Be Associated With A Decline In The Number Of Prescriptions For Medicaid Enrollees}} {\textbar} {{Health Affairs}}},
  urldate = {2022-06-17},
  howpublished = {https://www.healthaffairs.org/doi/full/10.1377/hlthaff.2016.1135},
  file = {/Users/nseewald/Zotero/storage/CQ97XSC4/hlthaff.2016.html}
}

@article{merloGeneralSpecificContextual2018,
  title = {General and Specific Contextual Effects in Multilevel Regression Analyses and Their Paradoxical Relationship: {{A}} Conceptual Tutorial},
  shorttitle = {General and Specific Contextual Effects in Multilevel Regression Analyses and Their Paradoxical Relationship},
  author = {Merlo, Juan and Wagner, Philippe and Austin, Peter C. and Subramanian, {\relax SV} and Leckie, George},
  year = {2018},
  month = may,
  journal = {SSM - Population Health},
  volume = {5},
  pages = {33--37},
  issn = {2352-8273},
  doi = {10.1016/j.ssmph.2018.05.006},
  urldate = {2024-05-30},
  abstract = {To be relevant for public health, a context (e.g., neighborhood, school, hospital) should influence or affect the health status of the individuals included in it. The greater the influence of the shared context, the higher the correlation of subject outcomes within that context is likely to be. This intra-context or intra-class correlation is of substantive interest and has been used to quantify the magnitude of the general contextual effect (GCE). Furthermore, ignoring the intra-class correlation in a regression analysis results in spuriously narrow 95\% confidence intervals around the estimated regression coefficients of the specific contextual variables entered as covariates and, thereby, overestimates the precision of the estimated specific contextual effects (SCEs)., Multilevel regression analysis is an appropriate methodology for investigating both GCEs and SCEs. However, frequently researchers only report SCEs and disregard the study of the GCE, unaware that small GCEs lead to more precise estimates of SCEs so, paradoxically, the less relevant the context is, the easier it is to detect (and publish) small but ``statistically significant'' SCEs. We describe this paradoxical situation and encourage researchers performing multilevel regression analysis to consider simultaneously both the GCE and SCEs when interpreting contextual influences on individual health.,                                        {$\bullet$}               The intra-context correlation is a measure of the general contextual effect (GCE).                                         {$\bullet$}               Contextual measures of association inform on specific contextual effects (SCEs).                                         {$\bullet$}               Many multilevel regression analyses only report SCEs.                                         {$\bullet$}               Paradoxically, the lower the GCE the easier it is to detect ``statistically significant'' SCEs.                                         {$\bullet$}               Multilevel regression analysis need to consider both GCEs and SCEs.},
  pmcid = {PMC5993177},
  pmid = {29892693},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Merlo et al_2018_General and specific contextual effects in multilevel regression analyses and.pdf}
}

@article{merolaEffectivenessResearchOncology2023,
  title = {Effectiveness Research in Oncology with Electronic Health Record Data: {{A}} Retrospective Cohort Study Emulating the {{PALOMA-2}} Trial},
  shorttitle = {Effectiveness Research in Oncology with Electronic Health Record Data},
  author = {Merola, David and Young, Jessica and Schrag, Deborah and Lin, Kueiyu Joshua and Alwardt, Sarah and Schneeweiss, Sebastian},
  year = {2023},
  journal = {Pharmacoepidemiology and Drug Safety},
  volume = {32},
  number = {4},
  pages = {426--434},
  issn = {1099-1557},
  doi = {10.1002/pds.5565},
  urldate = {2024-01-30},
  abstract = {Purpose Oncology electronic health record (EHR) databases have increased in quality and availability over the past decade, yet it remains unclear whether these clinical practice data can be used to conduct reliable comparative effectiveness studies. We sought to emulate a clinical trial with EHR data in the advanced breast cancer population and compare our results against the trial. Methods This cohort study used EHR data from US oncology practices. All elements of the study were defined to mimic the PALOMA-2 trial as closely as possible. Patients with hormone-positive, HER-2 negative metastatic breast cancer with no prior treatment for metastatic disease were included. Patients initiating palbociclib and letrozole on the same day following the earliest record of metastasis were compared to those initiating letrozole only. The primary associational measure was the conditional hazard ratio for time-to-next treatment (TTNT). TTNT is well-measured in our data source and amenable for calibration against the randomized study results of the PALOMA-2 trial. We used multiple imputation for several patient characteristics with missing values. Results There were 3836 study-eligible women with advanced breast cancer. The hazard ratio for TTNT in the observational study (HR: 0.62; 95\% CI: 0.56--0.68) was closely aligned with that of the randomized trial (HR: 0.64; 95\% CI: 0.52--0.78). Conclusions Under our assumptions on missing data and comparability of the two study populations, results from our non-randomized study closely matched that of the randomized trial. Further studies are needed to determine whether EHR data can yield reliable conclusions on treatment effects in oncology.},
  copyright = {{\copyright} 2022 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {_tablet,comparative effectiveness,electronic health records,healthcare databases,metastatic breast cancer,oncology,real-world evidence},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Merola et al_2023_Effectiveness research in oncology with electronic health record data.pdf;/Users/nseewald/Zotero/storage/MLJATCNP/pds.html}
}

@article{merolaOncologyDrugEffectiveness2022,
  title = {Oncology {{Drug Effectiveness}} from {{Electronic Health Record Data Calibrated Against RCT Evidence}}: {{The PARSIFAL Trial Emulation}}},
  shorttitle = {Oncology {{Drug Effectiveness}} from {{Electronic Health Record Data Calibrated Against RCT Evidence}}},
  author = {Merola, David and Young, Jessica and Schrag, Deborah and Lin, Kueiyu Joshua and Robert, Nicholas and Schneeweiss, Sebastian},
  year = {2022},
  month = oct,
  journal = {Clinical Epidemiology},
  volume = {14},
  pages = {1135--1144},
  publisher = {Dove Press},
  doi = {10.2147/CLEP.S373291},
  urldate = {2024-01-30},
  abstract = {Evaluating to compare the results of a randomized controlled trial investigating the efficacy of palbociclib with fulvestrant vs. letrozole in breast cancer},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Merola_et_al_2022_Oncology_Drug_Effectiveness_from_Electronic_Health_Record_Data_Calibrated_annotated.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Merola_et_al_2022_Oncology_Drug_Effectiveness_from_Electronic_Health_Record_Data_Calibrated.pdf}
}

@article{meurerAdaptiveClinicalTrials2012,
  title = {Adaptive {{Clinical Trials}}: {{A Partial Remedy}} for the {{Therapeutic Misconception}}?},
  shorttitle = {Adaptive {{Clinical Trials}}},
  author = {Meurer, William J. and Lewis, Roger J. and Berry, Donald A.},
  year = {2012},
  month = jun,
  journal = {JAMA},
  volume = {307},
  number = {22},
  pages = {2377--2378},
  issn = {0098-7484},
  doi = {10/gf3pmm},
  urldate = {2020-12-13},
  abstract = {There is a common ``therapeutic misconception'' among patients considering participation in clinical trials. Some trial participants and family members believe that the goal of a clinical trial is to improve their outcomes---a misperception often reinforced by media advertising of clinical research. Clinical trials have primarily scientific aims and rarely attempt to collectively improve the outcomes of their participants. The overarching goal of most clinical trials is to evaluate the effect of a treatment on disease outcomes. Comparisons are usually made with placebo for conditions having no established treatments and with standard care for conditions having effective treatments. Any benefit to an individual trial participant is a chance effect of randomization and the true, but unknown, relative effects of the treatments. Available evidence is conflicting regarding whether patients receive some benefit from simply participating in a clinical trial. Thus, even though serving as a research participant is essentially an altruistic activity, many clinical trial volunteers do not participate in research out of altruism. An adaptive clinical trial design can be used to increase the likelihood that study participants will benefit by being in a clinical trial.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Meurer et al_2012_Adaptive Clinical Trials.pdf;/Users/nseewald/Zotero/storage/DV5ZJRL3/1182867.html}
}

@article{meurerSequentialMultipleAssignment2017,
  title = {Sequential {{Multiple Assignment Randomized Trials}}: {{An Opportunity}} for {{Improved Design}} of {{Stroke Reperfusion Trials}}},
  shorttitle = {Sequential {{Multiple Assignment Randomized Trials}}},
  author = {Meurer, William J. and Seewald, Nicholas J. and Kidwell, Kelley},
  year = {2017},
  month = apr,
  journal = {Journal of Stroke and Cerebrovascular Diseases},
  volume = {26},
  number = {4},
  pages = {717--724},
  issn = {10523057},
  doi = {10.1016/j.jstrokecerebrovasdis.2016.09.010},
  urldate = {2018-10-12},
  copyright = {All rights reserved},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Meurer et al_2017_Sequential Multiple Assignment Randomized Trials.pdf}
}

@article{migliorettiMarginalModelingNonnested2007,
  title = {Marginal {{Modeling}} of {{Nonnested Multilevel Data}} Using {{Standard Software}}},
  author = {Miglioretti, Diana L. and Heagerty, Patrick J.},
  year = {2007},
  month = feb,
  journal = {American Journal of Epidemiology},
  volume = {165},
  number = {4},
  pages = {453--463},
  issn = {0002-9262},
  doi = {10.1093/aje/kwk020},
  urldate = {2024-10-01},
  abstract = {Epidemiologic data are often clustered within multiple levels that may not be nested within each other. Generalized estimating equations are commonly used to adjust for correlation among observations within clusters when fitting regression models; however, standard software does not currently accommodate nonnested clusters. This paper introduces a simple generalized estimating equation strategy that uses available commercial or public software for the regression analysis of nonnested multilevel data. The authors describe how to obtain empirical standard error estimates for constructing valid confidence intervals and conducting statistical hypothesis tests. The method is evaluated using simulations and illustrated with an analysis of data from the Breast Cancer Surveillance Consortium that estimates the influence of woman, radiologist, and facility characteristics on the positive predictive value of screening mammography. Performance with a small number of clusters is discussed. Both the simulations and the example demonstrate the importance of accounting for the correlation within all levels of clustering for proper inference.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Miglioretti_Heagerty_2007_Marginal Modeling of Nonnested Multilevel Data using Standard Software.pdf;/Users/nseewald/Zotero/storage/WAGXB2VU/109343.html}
}

@article{migliorettiMarginalModelingNonnested2007a,
  title = {Marginal {{Modeling}} of {{Nonnested Multilevel Data}} Using {{Standard Software}}},
  author = {Miglioretti, Diana L. and Heagerty, Patrick J.},
  year = {2007},
  month = feb,
  journal = {American Journal of Epidemiology},
  volume = {165},
  number = {4},
  pages = {453--463},
  issn = {0002-9262},
  doi = {10.1093/aje/kwk020},
  urldate = {2025-01-28},
  abstract = {Epidemiologic data are often clustered within multiple levels that may not be nested within each other. Generalized estimating equations are commonly used to adjust for correlation among observations within clusters when fitting regression models; however, standard software does not currently accommodate nonnested clusters. This paper introduces a simple generalized estimating equation strategy that uses available commercial or public software for the regression analysis of nonnested multilevel data. The authors describe how to obtain empirical standard error estimates for constructing valid confidence intervals and conducting statistical hypothesis tests. The method is evaluated using simulations and illustrated with an analysis of data from the Breast Cancer Surveillance Consortium that estimates the influence of woman, radiologist, and facility characteristics on the positive predictive value of screening mammography. Performance with a small number of clusters is discussed. Both the simulations and the example demonstrate the importance of accounting for the correlation within all levels of clustering for proper inference.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Miglioretti and Heagerty - 2007 - Marginal Modeling of Nonnested Multilevel Data using Standard Software.pdf;/Users/nseewald/Zotero/storage/K2XHJEGC/109343.html}
}

@misc{mikerspencerColouringPlotUsing2015,
  title = {Colouring a Plot Using a Continuous Variable in {{R}} {\textbar} {{R-bloggers}}},
  author = {{mikerspencer}},
  year = {2015},
  month = sep,
  urldate = {2024-11-13},
  abstract = {I often find myself coming back to this answer I gave on Stack Overflow in 2014. It shows how to colour a plot based on an independent continuous variable using the base graphics{\dots} Continue reading {$\rightarrow$}},
  langid = {american}
}

@misc{millerMissingDataGroup2019,
  title = {Missing {{Data}} in {{Group Design Studies}}: {{Revisions}} in {{WWC Standards Version}} 4.0},
  author = {Miller, David and Spybrook, Jessaca and Caverly, Sarah},
  year = {2019},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/TRBMF294/Miller et al. - Missing Data in Group Design Studies Revisions in.pdf}
}

@article{millerUSPoisedEase2024,
  title = {{{US}} Poised to Ease Restrictions on Marijuana in Historic Shift, but It'll Remain Controlled Substance},
  author = {Miller, Zeke and Goodman, Joshua and Mustian, Jim and Whitehurst, Lindsay},
  year = {2024},
  month = apr,
  journal = {Associated Press},
  urldate = {2024-05-04}
}

@article{milliSocialCostStrategic2018,
  title = {The {{Social Cost}} of {{Strategic Classification}}},
  author = {Milli, Smitha and Miller, John and Dragan, Anca D. and Hardt, Moritz},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.08460 [cs, stat]},
  eprint = {1808.08460},
  primaryclass = {cs, stat},
  urldate = {2018-10-22},
  abstract = {Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Milli et al_2018_The Social Cost of Strategic Classification.pdf}
}

@misc{MinoritiesOutliersCase,
  title = {On Minorities and Outliers: {{The}} Case for Making {{Big Data}} Small - {{Brooke Foucault Welles}}, 2014},
  urldate = {2021-07-02},
  howpublished = {https://journals-sagepub-com.proxy1.library.jhu.edu/doi/10.1177/2053951714540613},
  file = {/Users/nseewald/Zotero/storage/YVPAWDRR/2053951714540613.html}
}

@article{mitchellAlgorithmicFairnessChoices2021,
  title = {Algorithmic {{Fairness}}: {{Choices}}, {{Assumptions}}, and {{Definitions}}},
  shorttitle = {Algorithmic {{Fairness}}},
  author = {Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour, Alexander and Lum, Kristian},
  year = {2021},
  journal = {Annual Review of Statistics and Its Application},
  volume = {8},
  number = {1},
  pages = {141--163},
  doi = {10.1146/annurev-statistics-042720-125902},
  urldate = {2022-10-16},
  abstract = {A recent wave of research has attempted to define fairness quantitatively. In particular, this work has explored what fairness might mean in the context of decisions based on the predictions of statistical and machine learning models. The rapid growth of this new field has led to wildly inconsistent motivations, terminology, and notation, presenting a serious challenge for cataloging and comparing definitions. This article attempts to bring much-needed order. First, we explicate the various choices and assumptions made---often implicitly---to justify the use of prediction-based decision-making. Next, we show how such choices and assumptions can raise fairness concerns and we present a notationally consistent catalog of fairness definitions from the literature. In doing so, we offer a concise reference for thinking through the choices, assumptions, and fairness considerations of prediction-based decision-making.},
  keywords = {algorithmic fairness,decision theory,machine learning,predictive modeling,statistical learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mitchell et al_2021_Algorithmic Fairness.pdf}
}

@article{modaveMobileDeviceAccuracy2017,
  title = {Mobile {{Device Accuracy}} for {{Step Counting Across Age Groups}}},
  author = {Modave, Fran{\c c}ois and Guo, Yi and Bian, Jiang and Gurka, Matthew J and Parish, Alice and Smith, Megan D and Lee, Alexandra M and Buford, Thomas W},
  year = {2017},
  month = jun,
  journal = {JMIR mHealth and uHealth},
  volume = {5},
  number = {6},
  pages = {e88},
  issn = {2291-5222},
  doi = {10.2196/mhealth.7870},
  urldate = {2018-10-12},
  abstract = {Background: Only one in five American meets the physical activity recommendations of the Department of Health and Human Services. The proliferation of wearable devices and smartphones for physical activity tracking has led to an increasing number of interventions designed to facilitate regular physical activity, in particular to address the obesity epidemic, but also for cardiovascular disease patients, cancer survivors, and older adults. However, the inconsistent findings pertaining to the accuracy of wearable devices for step counting needs to be addressed, as well as factors known to affect gait (and thus potentially impact accuracy) such as age, body mass index (BMI), or leading arm. Objective: We aim to assess the accuracy of recent mobile devices for counting steps, across three different age groups. Methods: We recruited 60 participants in three age groups: 18-39 years, 40-64 years, and 65-84 years, who completed two separate 1000 step walks on a treadmill at a self-selected speed between 2 and 3 miles per hour. We tested two smartphones attached on each side of the waist, and five wrist-based devices worn on both wrists (2 devices on one wrist and 3 devices on the other), as well as the Actigraph wGT3X-BT, and swapped sides between each walk. All devices were swapped dominant-to-nondominant side and vice-versa between the two 1000 step walks. The number of steps was recorded with a tally counter. Age, sex, height, weight, and dominant hand were self-reported by each participant. Results: Among the 60 participants, 36 were female (60\%) and 54 were right-handed (90\%). Median age was 53 years (min=19, max=83), median BMI was 24.1 (min=18.4, max=39.6). There was no significant difference in left- and right-hand step counts by device. Our analyses show that the Fitbit Surge significantly undercounted steps across all age groups. Samsung Gear S2 significantly undercounted steps only for participants among the 40-64 year age group. Finally, the Nexus 6P significantly undercounted steps for the group ranging from 65-84 years. Conclusions: Our analysis shows that apart from the Fitbit Surge, most of the recent mobile devices we tested do not overcount or undercount steps in the 18-39-year-old age group, however some devices undercount steps in older age groups. This finding suggests that accuracy in step counting may be an issue with some popular wearable devices, and that age may be a factor in undercounting. These results are particularly important for clinical interventions using such devices and other activity trackers, in particular to balance energy requirements with energy expenditure in the context of a weight loss intervention program.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Modave et al_2017_Mobile Device Accuracy for Step Counting Across Age Groups.pdf}
}

@misc{ModelingTumorGrowth,
  title = {Modeling {{Tumor Growth Using Partly Conditional Survival Models}}: {{A Case Study}} in {{Colorectal Cancer}} {\textbar} {{JCO Clinical Cancer Informatics}}},
  urldate = {2024-03-10},
  howpublished = {https://ascopubs.org/doi/10.1200/CCI.22.00203}
}

@article{moerbeekPowerfulCostEfficientDesigns2008,
  title = {Powerful and {{Cost-Efficient Designs}} for {{Longitudinal Intervention Studies With Two Treatment Groups}}},
  author = {Moerbeek, Mirjam},
  year = {2008},
  month = mar,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {33},
  number = {1},
  pages = {41--61},
  issn = {1076-9986, 1935-1054},
  doi = {10/d5h7b7},
  urldate = {2020-01-15},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Moerbeek_2008_Powerful and Cost-Efficient Designs for Longitudinal Intervention Studies With.pdf}
}

@article{moodieCaseStudySMART2016,
  title = {A Case Study of {{SMART}} Attributes: A Qualitative Assessment of Generalizability, Retention Rate, and Trial Quality},
  shorttitle = {A Case Study of {{SMART}} Attributes},
  author = {Moodie, Erica E. M. and Karran, James C. and Shortreed, Susan M.},
  year = {2016},
  month = may,
  journal = {Trials},
  volume = {17},
  number = {1},
  pages = {242},
  issn = {1745-6215},
  doi = {10/f8mgrz},
  urldate = {2019-02-11},
  abstract = {Personalizing medical care is becoming increasingly popular, particularly mental health care. There is growing interest in formalizing medical decision making based on evolving patient symptoms in an evidence-based manner. To determine optimal sequencing of treatments, the sequences themselves must be studied; this may be accomplished by using a sequential multiple assignment randomized trial (SMART). It has been hypothesized that SMART studies may improve participant retention and generalizability.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Moodie et al_2016_A case study of SMART attributes.pdf;/Users/nseewald/Zotero/storage/4YXCAT8N/s13063-016-1368-3.html}
}

@article{moodieDemystifyingOptimalDynamic2007,
  title = {Demystifying {{Optimal Dynamic Treatment Regimes}}},
  author = {Moodie, Erica E. M. and Richardson, Thomas S. and Stephens, David A.},
  year = {2007},
  journal = {Biometrics},
  volume = {63},
  number = {2},
  pages = {447--455},
  issn = {1541-0420},
  doi = {10/ffcq8r},
  urldate = {2020-12-13},
  abstract = {A dynamic regime is a function that takes treatment and covariate history and baseline covariates as inputs and returns a decision to be made. Murphy (2003, Journal of the Royal Statistical Society, Series B65, 331--366) and Robins (2004, Proceedings of the Second Seattle Symposium on Biostatistics, 189--326) have proposed models and developed semiparametric methods for making inference about the optimal regime in a multi-interval trial that provide clear advantages over traditional parametric approaches. We show that Murphy's model is a special case of Robins's and that the methods are closely related but not equivalent. Interesting features of the methods are highlighted using the Multicenter AIDS Cohort Study and through simulation.},
  copyright = {{\copyright}2007, The International Biometric Society},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Moodie et al_2007_Demystifying Optimal Dynamic Treatment Regimes.pdf;/Users/nseewald/Zotero/storage/9UGCF3V2/j.1541-0420.2006.00686.html}
}

@article{moodieMarginalStructuralModels2011,
  title = {Marginal {{Structural Models}}: Unbiased Estimation for Longitudinal Studies},
  shorttitle = {Marginal {{Structural Models}}},
  author = {Moodie, Erica E. M. and Stephens, D. A.},
  year = {2011},
  month = feb,
  journal = {International Journal of Public Health},
  volume = {56},
  number = {1},
  pages = {117--119},
  issn = {1661-8564},
  doi = {10/fgxzz7},
  urldate = {2021-08-17},
  abstract = {In this article, we introduce Marginal Structural Models, which yield unbiased estimates of causal effects of exposures in the presence of time-varying confounding variables that also act as mediators.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Moodie_Stephens_2011_Marginal Structural Models.pdf}
}

@article{moodiePrecisionMedicineStatistical2020,
  title = {Precision Medicine: {{Statistical}} Methods for Estimating Adaptive Treatment Strategies},
  shorttitle = {Precision Medicine},
  author = {Moodie, Erica E. M. and Krakow, Elizabeth F.},
  year = {2020},
  month = oct,
  journal = {Bone Marrow Transplantation},
  volume = {55},
  number = {10},
  pages = {1890--1896},
  publisher = {Nature Publishing Group},
  issn = {1476-5365},
  doi = {10.1038/s41409-020-0871-z},
  urldate = {2022-10-11},
  abstract = {The beauty of science is that all the important things are unpredictable.Freeman Dyson},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Acute myeloid leukaemia,Biostatistics,Disease-free survival},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Moodie_Krakow_2020_Precision medicine.pdf;/Users/nseewald/Zotero/storage/TCHL9L6L/s41409-020-0871-z.html}
}

@article{moodieQlearningEstimatingOptimal2012,
  title = {Q-Learning for Estimating Optimal Dynamic Treatment Rules from Observational Data},
  author = {Moodie, Erica E. M. and Chakraborty, Bibhas and Kramer, Michael S.},
  year = {2012},
  journal = {Canadian Journal of Statistics},
  volume = {40},
  number = {4},
  pages = {629--645},
  issn = {1708-945X},
  doi = {10.1002/cjs.11162},
  urldate = {2022-04-07},
  abstract = {The area of dynamic treatment regimes (DTR) aims to make inference about adaptive, multistage decision-making in clinical practice. A DTR is a set of decision rules, one per interval of treatment, where each decision is a function of treatment and covariate history that returns a recommended treatment. Q-learning is a popular method from the reinforcement learning literature that has recently been applied to estimate DTRs. While, in principle, Q-learning can be used for both randomized and observational data, the focus in the literature thus far has been exclusively on the randomized treatment setting. We extend the method to incorporate measured confounding covariates, using direct adjustment and a variety of propensity score approaches. The methods are examined under various settings including non-regular scenarios. We illustrate the methods in examining the effect of breastfeeding on vocabulary testing, based on data from the Promotion of Breastfeeding Intervention Trial. The Canadian Journal of Statistics 40: 629--645; 2012 {\copyright} 2012 Statistical Society of Canada},
  langid = {english},
  keywords = {Bias,confounding,dynamic treatment regime,inverse probability of treatment weighting,MSC 2010: Primary 62L12,non-regularity,propensity scores,secondary 92B15},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Moodie et al_2012_Q-learning for estimating optimal dynamic treatment rules from observational.pdf;/Users/nseewald/Zotero/storage/TULC52BV/cjs.html}
}

@book{mooreAppliedSurvivalAnalysis2016,
  title = {Applied {{Survival Analysis Using R}}},
  author = {Moore, Dirk F.},
  year = {2016},
  series = {Use {{R}}!},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-31245-3},
  urldate = {2024-01-11},
  isbn = {978-3-319-31243-9 978-3-319-31245-3},
  langid = {english},
  keywords = {censoring,competing risks analyses,confidence intervals,log-rank test,Nelson-Aalen nonparametric estimator,partial likelihood,Prentice-modification of Gehan test,product-limit Kaplan-Meier estimator,proportional hazards,survival curves,survival data,truncation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Moore_2016_Applied Survival Analysis Using R.pdf}
}

@article{moorePrioritizedSweepingReinforcement1993,
  title = {Prioritized Sweeping: {{Reinforcement}} Learning with Less Data and Less Time},
  shorttitle = {Prioritized Sweeping},
  author = {Moore, Andrew W. and Atkeson, Christopher G.},
  year = {1993},
  month = oct,
  journal = {Machine Learning},
  volume = {13},
  number = {1},
  pages = {103--130},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00993104},
  urldate = {2018-10-12},
  abstract = {We present a new algorithm,prioritized sweeping, for efficientprediction and control of stochastic Markovsystems.Incrementallearningmethodssuchas temporaldifferencingand Q-learninghavereal-timeperformance. Classicalmethodsare slower,but moreaccurate,becausetheymakefull use of the observations.Prioritized sweepingaims for the best of both worlds. It uses all previous experiencesboth to prioritize important dynamicprogrammingsweepsand to guidethe explorationof state-space. Wecompareprioritizedsweepingwith other reinforcementlearningschemesfor a numberofdifferentstochasticoptimalcontrolproblems.It successfully solves large state-space real-time problems with which other methods have difficulty.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Moore_Atkeson_1993_Prioritized sweeping.pdf}
}

@article{moraAlternativeDiffindiffsEstimators2019,
  title = {Alternative Diff-in-Diffs Estimators with Several Pretreatment Periods},
  author = {Mora, Ricardo and Reggio, Iliana},
  year = {2019},
  month = may,
  journal = {Econometric Reviews},
  volume = {38},
  number = {5},
  pages = {465--486},
  publisher = {Taylor \& Francis},
  issn = {0747-4938},
  doi = {10.1080/07474938.2017.1348683},
  urldate = {2023-06-28},
  abstract = {This paper deals with the identification of treatment effects using difference-in-differences estimators when several pretreatment periods are available. We define a family of identifying nonnested assumptions that lead to alternative difference-in-differences estimators. We show that the most usual difference-in-differences estimators imply equivalence conditions for the identifying nonnested assumptions. We further propose a model that can be used to test multiple equivalence conditions without imposing any of them. We conduct a Monte Carlo analysis and apply our approach to several recent papers to show its practical relevance.},
  keywords = {C21,C52,Common trends,difference-in-differences,identification,treatment effect},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mora_Reggio_2019_Alternative diff-in-diffs estimators with several pretreatment periods.pdf}
}

@techreport{moraTreatmentEffectIdentification2012,
  type = {{{workingPaper}}},
  title = {Treatment Effect Identification Using Alternative Parallel Assumptions},
  author = {Mora, Ricardo and Reggio, Iliana},
  year = {2012},
  month = dec,
  issn = {2340-5031},
  urldate = {2023-06-28},
  abstract = {The core assumption to identify the treatment effect in difference-in-differences estimators is the  so-called Parallel Paths assumption, namely that the average change in outcome for the treated in  the absence of treatment equals the average change in outcome for the non-treated. We define a  family of alternative Parallel assumptions and show for a number of frequently used empirical  specifications which parameters of the model identify the treatment effect under the alternative  Parallel assumptions. We further propose a fully flexible model which has two desirable features  not present in the usual econometric specifications implemented in applied research. First, it  allows for flexible dynamics and for testing restrictions on these dynamics. Second, it does not  impose equivalence between alternative Parallel assumptions. We illustrate the usefulness of our  approach by revising the results of several recent papers in which the difference-in-differences  technique has been applied.The core assumption to identify the treatment effect in difference-in-differences estimators is the  so-called Parallel Paths assumption, namely that the average change in outcome for the treated in  the absence of treatment equals the average change in outcome for the non-treated. We define a  family of alternative Parallel assumptions and show for a number of frequently used empirical  specifications which parameters of the model identify the treatment effect under the alternative  Parallel assumptions. We further propose a fully flexible model which has two desirable features  not present in the usual econometric specifications implemented in applied research. First, it  allows for flexible dynamics and for testing restrictions on these dynamics. Second, it does not  impose equivalence between alternative Parallel assumptions. We illustrate the usefulness of our  approach by revising the results of several recent papers in which the difference-in-differences  technique has been applied},
  copyright = {Atribuci{\'o}n-NoComercial-SinDerivadas 3.0 Espa{\~n}a},
  langid = {english},
  annotation = {Accepted: 2013-04-04T14:24:23Z},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mora_Reggio_2012_Treatment effect identification using alternative parallel assumptions.pdf}
}

@article{morcianoOptimalAllocationTreatments2021,
  title = {Optimal Allocation to Treatments in a Sequential Multiple Assignment Randomized Trial},
  author = {Morciano, Andrea and Moerbeek, Mirjam},
  year = {2021},
  month = nov,
  journal = {Statistical Methods in Medical Research},
  volume = {30},
  number = {11},
  pages = {2471--2484},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/09622802211037066},
  urldate = {2022-08-26},
  abstract = {One of the main questions in the design of a trial is how many subjects should be assigned to each treatment condition. Previous research has shown that equal randomization is not necessarily the best choice. We study the optimal allocation for a novel trial design, the sequential multiple assignment randomized trial, where subjects receive a sequence of treatments across various stages. A subject's randomization probabilities to treatments in the next stage depend on whether he or she responded to treatment in the current stage. We consider a prototypical sequential multiple assignment randomized trial design with two stages. Within such a design, many pairwise comparisons of treatment sequences can be made, and a multiple-objective optimal design strategy is proposed to consider all such comparisons simultaneously. The optimal design is sought under either a fixed total sample size or a fixed budget. A Shiny App is made available to find the optimal allocations and to evaluate the efficiency of competing designs. As the optimal design depends on the response rates to first-stage treatments, maximin optimal design methodology is used to find robust optimal designs. The proposed methodology is illustrated using a sequential multiple assignment randomized trial example on weight loss management.},
  langid = {english},
  keywords = {cost constraint,efficiency,maximin designs,optimal allocation,response rates,sequential multiple assignment randomized trial trials},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Morciano_Moerbeek_2021_Optimal allocation to treatments in a sequential multiple assignment randomized.pdf}
}

@article{morrisonOptimalSpacingRepeated1970,
  title = {The {{Optimal Spacing}} of {{Repeated Measurements}}},
  author = {Morrison, Donald F.},
  year = {1970},
  month = jun,
  journal = {Biometrics},
  volume = {26},
  number = {2},
  eprint = {2529075},
  eprinttype = {jstor},
  pages = {281},
  issn = {0006341X},
  doi = {10/ccvr3b},
  urldate = {2019-09-06},
  abstract = {The treatments in a repeated measurements design can frequently be ordered along some natural continuum (e.g. time), and as a consequence the expected values and covariance structure of the responses can be represented as functions of their positions on the scale. We shall consider rules for spacing the treatments so as to maximize the power of the T2 test for equal treatment effects, as well as general two-sample T2 tests for the equality of mean vectors. These results will be obtained under the assumptions of covariance structures of the kinds associated with Wiener and Markov stochastic processes, and alternative hypothesis mean vectors whose elements are linear or quadratic functions of the scale variable.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Morrison_1970_The Optimal Spacing of Repeated Measurements.pdf}
}

@article{moshagenSampleSizeRequirements2014,
  title = {Sample {{Size Requirements}} of the {{Robust Weighted Least Squares Estimator}}},
  author = {Moshagen, Morten and Musch, Jochen},
  year = {2014},
  month = jan,
  journal = {Methodology},
  volume = {10},
  number = {2},
  pages = {60--70},
  issn = {1614-1881},
  doi = {10/f54tqn},
  urldate = {2020-01-20},
  abstract = {The present study investigated sample size requirements of maximum likelihood (ML) and robust weighted least squares (robust WLS) estimation for ordinal data with confirmatory factor analysis (CFA) models with 3--10 indicators per factor, primary loadings between .4 and .9, and four different levels of categorization (2, 3, 5, and 7). Additionally, the utility of the H-measure of construct reliability (an index combining the number of indicators and the magnitude of loadings) in predicting sample size requirements was examined. Results indicated that a higher number of indicators per factors and higher factor loadings increased the rates of proper convergence and solution propriety. However, the H-measure could only partly account for the results. Moreover, it was demonstrated that robust WLS was mostly superior to ML, suggesting that there is little reason to prefer ML over robust WLS when the data are ordinal. Sample size recommendations for the robust WLS estimator are provided.},
  file = {/Users/nseewald/Zotero/storage/3CJ8ZIZK/a000068.html}
}

@article{moultonIllustrationPitfallEstimating1990,
  title = {An {{Illustration}} of a {{Pitfall}} in {{Estimating}} the {{Effects}} of {{Aggregate Variables}} on {{Micro Units}}},
  author = {Moulton, Brent R.},
  year = {1990},
  journal = {The Review of Economics and Statistics},
  volume = {72},
  number = {2},
  eprint = {2109724},
  eprinttype = {jstor},
  pages = {334--338},
  publisher = {The MIT Press},
  issn = {0034-6535},
  doi = {10.2307/2109724},
  urldate = {2025-01-16},
  abstract = {Many economic researchers have attempted to measure the effect of aggregate market or public policy variables on micro units by merging aggregate data with micro observations by industry, occupation, or geographical location, then using multiple regression or similar statistical models to measure the effect of the aggregate variable on the micro units. The methods are usually based upon the assumption of independent disturbances, which is typically not appropriate for data from populations with grouped structure. Incorrectly using ordinary least squares can lead to standard errors that are seriously biased downward. This note illustrates the danger of spurious regression from this kind of misspecification, using as an example a wage regression estimated on data for individual workers that includes in the specification aggregate regressors for characteristics of geographical states.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Moulton - 1990 - An Illustration of a Pitfall in Estimating the Effects of Aggregate Variables on Micro Units.pdf}
}

@article{mouzannarFairDecisionMaking2018,
  title = {From {{Fair Decision Making}} to {{Social Equality}}},
  author = {Mouzannar, Hussein and Ohannessian, Mesrob I. and Srebro, Nathan},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.02952 [cs, stat]},
  eprint = {1812.02952},
  primaryclass = {cs, stat},
  urldate = {2019-01-16},
  abstract = {The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action. We then give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In more realistic scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. This gives quantifiable insight into both sides of the mismatch hypothesis. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mouzannar et al_2018_From Fair Decision Making to Social Equality.pdf}
}

@misc{MovingBestPractice,
  title = {Moving towards Best Practice When Using Inverse Probability of Treatment Weighting ({{IPTW}}) Using the Propensity Score to Estimate Causal Treatment Effects in Observational Studies - {{Austin}} - 2015 - {{Statistics}} in {{Medicine}} - {{Wiley Online Library}}},
  urldate = {2024-05-29},
  howpublished = {https://onlinelibrary.wiley.com/doi/10.1002/sim.6607},
  file = {/Users/nseewald/Zotero/storage/7WZYH9W3/sim.html}
}

@article{muganAutonomousLearningHighLevel2012,
  title = {Autonomous {{Learning}} of {{High-Level States}} and {{Actions}} in {{Continuous Environments}}},
  author = {Mugan, Jonathan and Kuipers, Benjamin},
  year = {2012},
  month = mar,
  journal = {IEEE Transactions on Autonomous Mental Development},
  volume = {4},
  number = {1},
  pages = {70--86},
  issn = {1943-0604, 1943-0612},
  doi = {10.1109/TAMD.2011.2160943},
  urldate = {2018-10-12},
  abstract = {How can an agent bootstrap up from a low-level representation to autonomously learn high-level states and actions using only domain-general knowledge? In this paper, we assume that the learning agent has a set of continuous variables describing the environment. There exist methods for learning models of the environment, and there also exist methods for planning. However, for autonomous learning, these methods have been used almost exclusively in discrete environments. We propose attacking the problem of learning high-level states and actions in continuous environments by using a qualitative representation to bridge the gap between continuous and discrete variable representations. In this approach, the agent begins with a broad discretization and initially can only tell if the value of each variable is increasing, decreasing, or remaining steady. The agent then simultaneously learns a qualitative representation (discretization) and a set of predictive models of the environment. These models are converted into plans to perform actions. The agent then uses those learned actions to explore the environment. The method is evaluated using a simulated robot with realistic physics. The robot is sitting at a table that contains a block and other distractor objects that are out of reach. The agent autonomously explores the environment without being given a task. After learning, the agent is given various tasks to determine if it learned the necessary states and actions to complete them. The results show that the agent was able to use this method to autonomously learn to perform the tasks.},
  langid = {english},
  keywords = {Active learning,intrinsic motivation,qualitative reasoning,Reinforcement learning,unsupervised learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Mugan_Kuipers_2012_Autonomous Learning of High-Level States and Actions in Continuous Environments.pdf}
}

@article{mullerPowerCalculationsGeneral1992,
  title = {Power {{Calculations}} for {{General Linear Multivariate Models Including Repeated Measures Applications}}},
  author = {Muller, Keith E. and Lavange, Lisa M. and Ramey, Sharon Landesman and Ramey, Craig T.},
  year = {1992},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {87},
  number = {420},
  pages = {1209--1226},
  issn = {0162-1459, 1537-274X},
  doi = {10/gghxt3},
  urldate = {2020-01-16},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Muller et al_1992_Power Calculations for General Linear Multivariate Models Including Repeated.pdf}
}

@misc{MultipleImputationPrimer,
  title = {Multiple Imputation: A Primer},
  shorttitle = {Multiple Imputation},
  doi = {10.1177/096228029900800102},
  urldate = {2024-01-05},
  howpublished = {https://journals.sagepub.com/doi/epdf/10.1177/096228029900800102},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/HRRBQ3WH/096228029900800102.html}
}

@misc{MultipleImputationStrategy,
  title = {A Multiple Imputation Strategy for Sequential Multiple Assignment Randomized Trials - {{Shortreed}} - 2014 - {{Statistics}} in {{Medicine}} - {{Wiley Online Library}}},
  urldate = {2022-09-29},
  howpublished = {https://onlinelibrary.wiley.com/doi/10.1002/sim.6223},
  file = {/Users/nseewald/Zotero/storage/2UMR2APZ/sim.html}
}

@incollection{munozmaldonadoMixedModelsPosterior2009,
  title = {Mixed {{Models}}, {{Posterior Means}} and {{Penalized Least-Squares}}},
  booktitle = {Institute of {{Mathematical Statistics Lecture Notes}} - {{Monograph Series}}},
  author = {Mu{\~n}oz Maldonado, Yolanda},
  year = {2009},
  pages = {216--236},
  publisher = {Institute of Mathematical Statistics},
  address = {Beachwood, Ohio, USA},
  doi = {10.1214/09-LNMS5713},
  urldate = {2018-10-12},
  abstract = {This paper reviews the connections between estimators that derive from three different modeling methodologies: Mixed-effects models, Bayesian models and Penalized Least-squares. Extension of classical results on the equivalence for smoothing spline estimators and best linear unbiased prediction and/or posterior analysis of certain Gaussian signal-plus-noise models is examined in a more general setting. These connections allow for the application of an efficient, linear time algorithm, to estimate parameters, compute random effects predictions and evaluate likelihoods in a large class of model scenarios. We also show that the methods of generalized cross-validation, restricted maximum likelihood and unbiased risk prediction can be used to estimate the variance components or adaptively select the smoothing parameters in any of the three settings.},
  isbn = {978-0-940600-77-5},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Muñoz Maldonado_2009_Mixed Models, Posterior Means and Penalized Least-Squares.pdf}
}

@article{murphyDevelopingAdaptiveTreatment2007,
  title = {Developing Adaptive Treatment Strategies in Substance Abuse Research},
  author = {Murphy, Susan A. and Lynch, Kevin G. and Oslin, David and McKay, James R. and TenHave, Tom},
  year = {2007},
  month = may,
  journal = {Drug and Alcohol Dependence},
  volume = {88},
  pages = {S24-S30},
  issn = {03768716},
  doi = {10.1016/j.drugalcdep.2006.09.008},
  urldate = {2018-10-12},
  abstract = {For many individuals, substance abuse possesses characteristics of chronic disorders in that individuals experience repeated cycles of cessation and relapse; hence viewing drug dependence as a chronic, relapsing disorder is increasingly accepted. The development of a treatment for a chronic disorder requires consideration of the ordering of treatments, the timing of changes in treatment, and the use of measures of response, burden and adherence collected during treatment to make further treatment decisions. Adaptive treatment strategies provide a vehicle through which these issues can be addressed and thus provide a means toward improving and informing the clinical management of chronic substance abuse disorders. The sequential multiple assignment randomized trial (SMART) is particularly useful in developing adaptive treatment strategies. Simple analyses that can be used with the SMART design are described. Furthermore, the SMART design is compared with standard experimental designs.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Murphy et al_2007_Developing adaptive treatment strategies in substance abuse research.pdf}
}

@incollection{murphyDynamicTreatmentRegimens2009,
  title = {Dynamic {{Treatment Regimens}}},
  booktitle = {Encyclopedia of {{Medical Decision Making}}},
  author = {Murphy, Susan A. and Almirall, Daniel},
  year = {2009},
  volume = {1},
  pages = {419--422},
  publisher = {SAGE Publications},
  address = {Thousand Oaks, CA},
  isbn = {978-1-4129-5372-6},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Murphy_Almirall_2009_Dynamic Treatment Regimens.pdf}
}

@article{murphyExperimentalDesignDevelopment2005,
  title = {An Experimental Design for the Development of Adaptive Treatment Strategies},
  author = {Murphy, Susan A.},
  year = {2005},
  journal = {Statistics in Medicine},
  volume = {24},
  number = {10},
  pages = {1455--1481},
  issn = {0277-6715},
  doi = {10.1002/sim.2022},
  abstract = {In adaptive treatment strategies, the treatment level and type is repeatedly adjusted according to ongoing individual response. Since past treatment may have delayed effects, the development of these treatment strategies is challenging. This paper advocates the use of sequential multiple assignment randomized trials in the development of adaptive treatment strategies. Both a simple ad hoc method for ascertaining sample sizes and simple analysis methods are provided.},
  keywords = {dynamic treatment regimes,Multi-stage decision problems,Sequential multiple assignment randomized trial,Therapeutic strategies,Treatment algorithms},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Murphy_2005_An experimental design for the development of adaptive treatment strategies.pdf}
}

@article{murphyHowReadStatistical1997,
  title = {How to {{Read}} the {{Statistical Methods Literature}}: {{A Guide}} for {{Students}}},
  shorttitle = {How to {{Read}} the {{Statistical Methods Literature}}},
  author = {Murphy, James R.},
  year = {1997},
  journal = {The American Statistician},
  volume = {51},
  number = {2},
  eprint = {2685409},
  eprinttype = {jstor},
  pages = {155--157},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0003-1305},
  doi = {10.2307/2685409},
  urldate = {2022-07-29},
  abstract = {Statistical methods papers are densely written. The writers assume that the readers already have sophisticated knowledge of the topic. In addition, a standard statistical notation has not been developed. Students who learn a technique in one notation may be confused when reading articles written with a different notation. This paper contains suggestions for making the student's task easier and more productive.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Murphy_1997_How to Read the Statistical Methods Literature.pdf}
}

@article{murphyMarginalMeanModels2001,
  title = {Marginal {{Mean Models}} for {{Dynamic Regimes}}},
  author = {Murphy, S A and {van der Laan}, M.J. J and Robins, J M and {Conduct Problems Prevention Research Group} and {Group}},
  year = {2001},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {456},
  pages = {1410--1423},
  issn = {0162-1459},
  doi = {10.1198/016214501753382327},
  abstract = {A dynamic treatment regime is a list of rules for how the level of treatment will be tailored through time to an individual's changing severity. In general, individuals who receive the highest level of treatment are the individuals with the greatest severity and need for treatment. Thus there is planned selection of the treatment dose. In addition to the planned selection mandated by the treatment rules, the use of staff judgment results in unplanned selection of the treatment level. Given observational longitudinal data or data in which there is unplanned selection, of the treatment level, the methodology proposed here allows the estimation of a mean response to a dynamic treatment regime under the assumption of sequential randomization.},
  pmid = {20019887},
  keywords = {Causal inference,Confounding,Dynamic treatment regime,dynamic treatment regimes,Nondynamic treatment regime,nondynamic treatment regimes},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Murphy et al_2001_Marginal Mean Models for Dynamic Regimes.pdf}
}

@article{murphyOptimalDynamicTreatment2003,
  title = {Optimal Dynamic Treatment Regimes},
  author = {Murphy, Susan A.},
  year = {2003},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {65},
  number = {2},
  pages = {331--355},
  issn = {1467-9868},
  doi = {10/dmmr89},
  urldate = {2020-12-13},
  abstract = {Summary. A dynamic treatment regime is a list of decision rules, one per time interval, for how the level of treatment will be tailored through time to an individual's changing status. The goal of this paper is to use experimental or observational data to estimate decision regimes that result in a maximal mean response. To explicate our objective and to state the assumptions, we use the potential outcomes model. The method proposed makes smooth parametric assumptions only on quantities that are directly relevant to the goal of estimating the optimal rules. We illustrate the methodology proposed via a small simulation.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Murphy_2003_Optimal dynamic treatment regimes.pdf;/Users/nseewald/Zotero/storage/UV3FGIRM/1467-9868.html}
}

@article{murphyScreeningExperimentsDeveloping2009,
  title = {Screening {{Experiments}} for {{Developing Dynamic Treatment Regimes}}},
  author = {Murphy, Susan A. and Bingham, D.},
  year = {2009},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {104},
  number = {485},
  pages = {391--408},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10/dk2gpv},
  urldate = {2020-12-13},
  abstract = {Dynamic treatment regimes are time-varying treatments that individualize sequences of treatments to the patient. The construction of dynamic treatment regimes is challenging because a patient will be eligible for some treatment components only if he has not responded (or has responded) to other treatment components. In addition, there are usually a number of potentially useful treatment components and combinations thereof. In this article, we propose new methodology for identifying promising components and screening out negligible ones. First, we define causal factorial effects for treatment components that may be applied sequentially to a patient. Second, we propose experimental designs that can be used to study the treatment components. Surprisingly, modifications can be made to (fractional) factorial designs---more commonly found in the engineering statistics literature---for screening in this setting. Furthermore, we provide an analysis model that can be used to screen the factorial effects. We demonstrate the proposed methodology using examples motivated in the literature and also via a simulation study.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Murphy_Bingham_2009_Screening Experiments for Developing Dynamic Treatment Regimes.pdf;/Users/nseewald/Zotero/storage/8JG9I96N/jasa.2009.html}
}

@article{musciEnsuringCausalNot2019,
  title = {Ensuring {{Causal}}, {{Not Casual}}, {{Inference}}},
  author = {Musci, Rashelle J. and Stuart, Elizabeth},
  year = {2019},
  month = apr,
  journal = {Prevention Science},
  volume = {20},
  number = {3},
  pages = {452--456},
  issn = {1573-6695},
  doi = {10.1007/s11121-018-0971-9},
  urldate = {2022-04-28},
  abstract = {With innovation in causal inference methods and a rise in non-experimental data availability, a growing number of prevention researchers and advocates are thinking about causal inference. In this commentary, we discuss the current state of science as it relates to causal inference in prevention research, and reflect on key assumptions of these methods. We review challenges associated with the use of causal inference methodology, as well as considerations for hoping to integrate causal inference methods into their research. In short, this commentary addresses the key concepts of causal inference and suggests a greater emphasis on thoughtfully designed studies (to avoid the need for strong and potentially untestable assumptions) combined with analyses of sensitivity to those assumptions.},
  langid = {english},
  keywords = {Assumptions,Causal inference,Mediation,Randomized controlled trials},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Musci_Stuart_2019_Ensuring Causal, Not Casual, Inference.pdf}
}

@incollection{myersGeneralizedEstimatingEquations2012,
  title = {Generalized {{Estimating Equations}}},
  booktitle = {Generalized {{Linear Models}}},
  author = {Myers, Raymond H. and Montgomery, Douglas C. and Vining, G. Geoffrey and Robinson, Timothy J.},
  year = {2012},
  month = jan,
  pages = {272--318},
  publisher = {John Wiley \& Sons, Inc.},
  address = {Hoboken, NJ, USA},
  doi = {10.1002/9780470556986.ch6},
  urldate = {2018-10-12},
  isbn = {978-0-470-55698-6 978-0-470-45463-3},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Myers et al_2012_Generalized Estimating Equations.pdf}
}

@article{myersRecruitmentEffortCosts2019,
  title = {Recruitment Effort and Costs from a Multi-Center Randomized Controlled Trial for Treating Depression in Type 2 Diabetes},
  author = {Myers, Barbara A. and Pillay, Yegan and Guyton Hornsby, W. and Shubrook, Jay and Saha, Chandan and Mather, Kieren J. and Fitzpatrick, Karen and {de Groot}, Mary},
  year = {2019},
  month = nov,
  journal = {Trials},
  volume = {20},
  number = {1},
  pages = {621},
  issn = {1745-6215},
  doi = {10/gg5d49},
  urldate = {2021-04-09},
  abstract = {Participant recruitment for clinical trials is a significant challenge for the scientific research community. Federal funding agencies have made continuation of funding of clinical trials contingent on meeting recruitment targets. It is incumbent on investigators to carefully set study recruitment timelines and resource needs to meet those goals as required under current funding mechanisms. This paper highlights the cost, labor, and barriers to recruitment for Program ACTVE II, a successful multisite randomized controlled trial of behavioral treatments for depression in adults with type 2 diabetes, conducted in rural and urban settings in three states.},
  keywords = {Clinical trial,Depression,Diabetes,Study recruitment},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Myers et al_2019_Recruitment effort and costs from a multi-center randomized controlled trial.pdf;/Users/nseewald/Zotero/storage/9K82G8KB/s13063-019-3712-x.html}
}

@article{mylesUsingBlandAltman2007,
  title = {Using the {{Bland}}--{{Altman}} Method to Measure Agreement with Repeated Measures},
  author = {Myles, P.S. and Cui, J.},
  year = {2007},
  month = sep,
  journal = {British Journal of Anaesthesia},
  volume = {99},
  number = {3},
  pages = {309--311},
  issn = {00070912},
  doi = {10/bwh398},
  urldate = {2021-09-10},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Myles_Cui_2007_Using the Bland–Altman method to measure agreement with repeated measures.pdf}
}

@article{naar-kingSequentialMultipleAssignment2016,
  ids = {naar-kingSequentialMultipleAssignment2016b},
  title = {Sequential {{Multiple Assignment Randomized Trial}} ({{SMART}}) to {{Construct Weight Loss Interventions}} for {{African American Adolescents}}},
  author = {{Naar-King}, Sylvie and Ellis, Deborah A. and Idalski Carcone, April and Templin, Thomas and {Jacques-Tiura}, Angela J. and Brogan Hartlieb, Kathryn and Cunningham, Phillippe and Jen, Kai-Lin Catherine},
  year = {2016},
  month = jul,
  journal = {Journal of Clinical Child \& Adolescent Psychology},
  volume = {45},
  number = {4},
  pages = {428--441},
  issn = {1537-4416, 1537-4424},
  doi = {10/gf4ks4},
  urldate = {2020-07-14},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Naar-King et al_2016_Sequential Multiple Assignment Randomized Trial (SMART) to Construct Weight.pdf}
}

@article{nahum-shaniDesignExperimentsSequential2024,
  title = {Design of Experiments with Sequential Randomizations on Multiple Timescales: The Hybrid Experimental Design},
  shorttitle = {Design of Experiments with Sequential Randomizations on Multiple Timescales},
  author = {{Nahum-Shani}, Inbal and Dziak, John J. and Venera, Hanna and Pfammatter, Angela F. and Spring, Bonnie and Dempsey, Walter},
  year = {2024},
  month = mar,
  journal = {Behavior Research Methods},
  volume = {56},
  number = {3},
  pages = {1770--1792},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02119-z},
  urldate = {2025-01-16},
  abstract = {Psychological interventions, especially those leveraging mobile and wireless technologies, often include multiple components that are delivered and adapted on multiple timescales (e.g., coaching sessions adapted monthly based on clinical progress, combined with motivational messages from a mobile device adapted daily based on the person's daily emotional state). The hybrid experimental design (HED) is a new experimental approach that enables researchers to answer scientific questions about the construction of psychological interventions in which components are delivered and adapted on different timescales. These designs involve sequential randomizations of study participants to intervention components, each at an appropriate timescale (e.g., monthly randomization to different intensities of coaching sessions and daily randomization to different forms of motivational messages). The goal of the current manuscript is twofold. The first is to highlight the flexibility of the HED by conceptualizing this experimental approach as a special form of a factorial design in which different factors are introduced at multiple timescales. We also discuss how the structure of the HED can vary depending on the scientific question(s) motivating the study. The second goal is to explain how data from various types of HEDs can be analyzed to answer a variety of scientific questions about the development of multicomponent psychological interventions. For illustration, we use a completed HED to inform the development of a technology-based weight loss intervention that integrates components that are delivered and adapted on multiple timescales.},
  langid = {english},
  keywords = {Digital interventions,factorial experiments,Hybrid experimental designs (HED),Micro-randomized trial (MRT),Multimodal adaptive intervention (MADI),Sequential multiple assignment randomized trial (SMART)},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nahum-Shani et al. - 2024 - Design of experiments with sequential randomizations on multiple timescales the hybrid experimental.pdf}
}

@article{nahum-shaniExperimentalDesignPrimary2012,
  title = {Experimental Design and Primary Data Analysis Methods for Comparing Adaptive Interventions.},
  author = {{Nahum-Shani}, Inbal and Qian, Min and Almirall, Daniel and Pelham, William E. and Gnagy, Beth and Fabiano, Gregory A. and Waxmonsky, James G. and Yu, Jihnhee and Murphy, Susan A.},
  year = {2012},
  month = dec,
  journal = {Psychological Methods},
  volume = {17},
  number = {4},
  pages = {457--477},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0029372},
  urldate = {2018-10-12},
  abstract = {In recent years, research in the area of intervention development has been shifting from the traditional fixed-intervention approach to adaptive interventions, which allow greater individualization and adaptation of intervention options (i.e., intervention type and/or dosage) over time. Adaptive interventions are operationalized via a sequence of decision rules that specify how intervention options should be adapted to an individual's characteristics and changing needs, with the general aim to optimize the long-term effectiveness of the intervention. Here, we review adaptive interventions, discussing the potential contribution of this concept to research in the behavioral and social sciences. We then propose the sequential multiple assignment randomized trial (SMART), an experimental design useful for addressing research questions that inform the construction of high-quality adaptive interventions. To clarify the SMART approach and its advantages, we compare SMART with other experimental approaches. We also provide methods for analyzing data from SMART to address primary research questions that inform the construction of a high-quality adaptive intervention.},
  langid = {english},
  keywords = {Behavioral Sciences,Behavioral Sciences: methods,Data Interpretation,Humans,Randomized Controlled Trials as Topic,Randomized Controlled Trials as Topic: methods,Research Design,Social Sciences,Social Sciences: methods,Statistical},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nahum-Shani et al_2012_Experimental design and primary data analysis methods for comparing adaptive.pdf}
}

@article{nahum-shaniJustinTimeAdaptiveInterventions2016,
  title = {Just-in-{{Time Adaptive Interventions}} ({{JITAIs}}) in {{Mobile Health}}: {{Key Components}} and {{Design Principles}} for {{Ongoing Health Behavior Support}}},
  shorttitle = {Just-in-{{Time Adaptive Interventions}} ({{JITAIs}}) in {{Mobile Health}}},
  author = {{Nahum-Shani}, Inbal and Smith, Shawna N. and Spring, Bonnie J. and Collins, Linda M. and Witkiewitz, Katie and Tewari, Ambuj and Murphy, Susan A.},
  year = {2016},
  month = sep,
  journal = {Annals of Behavioral Medicine},
  issn = {0883-6612, 1532-4796},
  doi = {10.1007/s12160-016-9830-8},
  urldate = {2018-10-12},
  abstract = {Background The just-in-time adaptive intervention (JITAI) is an intervention design aiming to provide the right type/amount of support, at the right time, by adapting to an individual's changing internal and contextual state. The availability of increasingly powerful mobile and sensing technologies underpins the use of JITAIs to support health behavior, as in such a setting an individual's state can change rapidly, unexpectedly, and in his/her natural environment.},
  langid = {english},
  keywords = {health behavior,Just-in-time adaptive intervention,mhealth,Mobile,mobile health,Support},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nahum-Shani et al_2016_Just-in-Time Adaptive Interventions (JITAIs) in Mobile Health.pdf}
}

@article{nahum-shaniMCMTCPragmaticFramework2022,
  title = {{{MCMTC}}: {{A Pragmatic Framework}} for {{Selecting}} an {{Experimental Design}} to {{Inform}} the {{Development}} of {{Digital Interventions}}},
  shorttitle = {{{MCMTC}}},
  author = {{Nahum-Shani}, Inbal and Dziak, John J. and Wetter, David W.},
  year = {2022},
  journal = {Frontiers in Digital Health},
  volume = {4},
  issn = {2673-253X},
  urldate = {2022-10-12},
  abstract = {Advances in digital technologies have created unprecedented opportunities to deliver effective and scalable behavior change interventions. Many digital interventions include multiple components, namely several aspects of the intervention that can be differentiated for systematic investigation. Various types of experimental approaches have been developed in recent years to enable researchers to obtain the empirical evidence necessary for the development of effective multiple-component interventions. These include factorial designs, Sequential Multiple Assignment Randomized Trials (SMARTs), and Micro-Randomized Trials (MRTs). An important challenge facing researchers concerns selecting the right type of design to match their scientific questions. Here, we propose MCMTC -- a pragmatic framework that can be used to guide investigators interested in developing digital interventions in deciding which experimental approach to select. This framework includes five questions that investigators are encouraged to answer in the process of selecting the most suitable design: (1) Multiple-component intervention: Is the goal to develop an intervention that includes multiple components; (2) Component selection: Are there open scientific questions about the selection of specific components for inclusion in the intervention; (3) More than a single component: Are there open scientific questions about the inclusion of more than a single component in the intervention; (4) Timing: Are there open scientific questions about the timing of component delivery, that is when to deliver specific components; and (5) Change: Are the components in question designed to address conditions that change relatively slowly (e.g., over months or weeks) or rapidly (e.g., every day, hours, minutes). Throughout we use examples of tobacco cessation digital interventions to illustrate the process of selecting a design by answering these questions. For simplicity we focus exclusively on four experimental approaches---standard two- or multi-arm randomized trials, classic factorial designs, SMARTs, and MRTs---acknowledging that the array of possible experimental approaches for developing digital interventions is not limited to these designs.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nahum-Shani et al_2022_MCMTC.pdf}
}

@article{nahum-shaniQlearningDataAnalysis2012,
  title = {Q-Learning: {{A}} Data Analysis Method for Constructing Adaptive Interventions.},
  author = {{Nahum-Shani}, Inbal and Qian, Min and Almirall, Daniel and Pelham, William E and Gnagy, Beth and Fabiano, Gregory A. and Waxmonsky, James G. and Yu, Jihnhee and Murphy, Susan A.},
  year = {2012},
  journal = {Psychological Methods},
  volume = {17},
  number = {4},
  pages = {478--494},
  issn = {1082-989X},
  doi = {10.1037/a0029373},
  abstract = {Increasing interest in individualizing and adapting intervention services over time has led to the development of adaptive interventions. Adaptive interventions operationalize the individualization of a sequence of intervention options over time via the use of decision rules that input participant information and output intervention recommendations. We introduce Q-learning, which is a generalization of regression analysis to settings in which a sequence of decisions regarding intervention options or services is made. The use of Q is to indicate that this method is used to assess the relative quality of the intervention options. In particular, we use Q-learning with linear regression to estimate the optimal (i.e., most effective) sequence of decision rules. We illustrate how Q-learning can be used with data from sequential multiple assignment randomized trials (SMARTs; Murphy, 2005) to inform the construction of a more deeply tailored sequence of decision rules than those embedded in the SMART design. We also discuss the advantages of Q-learning compared to other data analysis approaches. Finally, we use the Adaptive Interventions for Children With ADHD SMART study (Center for Children and Families, University at Buffalo, State University of New York, William E. Pelham as principal investigator) for illustration.},
  pmid = {23025434},
  keywords = {adaptive interventions,and social sciences,are,decision rules,edged in the behavioral,in adaptive interven-,intensity of the intervention,interventions are widely acknowl-,Q-learning,regression,the advantages of adaptive,the composition and the,tions},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nahum-Shani et al_2012_Q-learning.pdf}
}

@article{nahum-shaniSMARTDataAnalysis2017,
  ids = {nahum-shaniSMARTDataAnalysis2017a},
  title = {A {{SMART}} Data Analysis Method for Constructing Adaptive Treatment Strategies for Substance Use Disorders},
  author = {Nahum-Shani, Inbal and Ertefaie, Ashkan and Lu, Xi (Lucy) and Lynch, Kevin G. and McKay, James R. and Oslin, David W. and Almirall, Daniel},
  year = {2017},
  journal = {Addiction},
  volume = {112},
  number = {5},
  pages = {901--909},
  issn = {1360-0443},
  doi = {10/ghpb9n},
  urldate = {2021-03-23},
  abstract = {Aims To demonstrate how Q-learning, a novel data analysis method, can be used with data from a sequential, multiple assignment, randomized trial (SMART) to construct empirically an adaptive treatment strategy (ATS) that is more tailored than the ATSs already embedded in a SMART. Method We use Q-learning with data from the Extending Treatment Effectiveness of Naltrexone (ExTENd) SMART (N = 250) to construct empirically an ATS employing naltrexone, behavioral intervention, and telephone disease management to reduce alcohol consumption over 24 weeks in alcohol dependent individuals. Results Q-learning helped to identify a subset of individuals who, despite showing early signs of response to naltrexone, require additional treatment to maintain progress. Conclusions Q-learning can inform the development of more cost-effective, adaptive treatment strategies for treating substance use disorders.},
  copyright = {{\copyright} 2016 Society for the Study of Addiction},
  langid = {english},
  keywords = {Adaptive interventions,adaptive treatment strategies,alcohol dependence,Q-learning,Sequential Multiple Assignment Randomized Trial (SMART),stepped-care},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nahum‐Shani et al_2017_A SMART data analysis method for constructing adaptive treatment strategies for.pdf;/Users/nseewald/Zotero/storage/DT7ACMIZ/add.html;/Users/nseewald/Zotero/storage/FE2BV4VU/add.html;/Users/nseewald/Zotero/storage/YAXKU3SP/add.html}
}

@article{nahum-shaniSMARTLongitudinalAnalysis,
  title = {{{SMART Longitudinal Analysis}}: {{A Tutorial}} for {{Using Repeated Outcome Measures From SMART Studies}} to {{Compare Adaptive Interventions}}},
  author = {{Nahum-Shani}, Inbal and Almirall, Daniel and Yap, Jamie R T and McKay, James R and Lynch, Kevin G},
  abstract = {In recent years, there has been increased interest in the development of adaptive interventions across various domains of health and psychological research. An adaptive intervention is a protocolized sequence of individualized treatments that seeks to address the unique and changing needs of individuals as they progress through an intervention program. The sequential, multiple assignment, randomized trial (SMART) is an experimental study design that can be used to build the empirical basis for the construction of effective adaptive interventions. A SMART involves multiple stages of randomizations; each stage of randomization is designed to address scientific questions concerning the best intervention option to employ at that point in the intervention. Several adaptive interventions are embedded in a SMART by design; many SMARTs are motivated by scientific questions that concern the comparison of these embedded adaptive interventions. Until recently, analysis methods available for the comparison of adaptive interventions were limited to end-of-study outcomes. The current article provides an accessible and comprehensive tutorial to a new methodology for using repeated outcome data from SMART studies to compare adaptive interventions. We discuss how existing methods for comparing adaptive interventions in terms of end-of-study outcome data from a SMART can be extended for use with longitudinal outcome data. We also highlight the scientific utility of using longitudinal data from a SMART to compare adaptive interventions. A SMART study aiming to develop an adaptive intervention to engage alcohol- and cocaine-dependent individuals in treatment is used to demonstrate the application of this new methodology.},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/9GJKCIW6/2019-39467-001.pdf}
}

@article{nahum-shaniSMARTLongitudinalAnalysis2020,
  title = {{{SMART}} Longitudinal Analysis: {{A}} Tutorial for Using Repeated Outcome Measures from {{SMART}} Studies to Compare Adaptive Interventions},
  shorttitle = {{{SMART}} Longitudinal Analysis},
  author = {{Nahum-Shani}, Inbal and Almirall, Daniel and Yap, Jamie R. T. and McKay, James R. and Lynch, Kevin G. and Freiheit, Elizabeth A. and Dziak, John J.},
  year = {2020},
  month = feb,
  journal = {Psychological Methods},
  volume = {25},
  number = {1},
  pages = {1--29},
  publisher = {American Psychological Association},
  issn = {1082-989X},
  doi = {10/ggttht},
  urldate = {2021-03-22},
  abstract = {In recent years, there has been increased interest in the development of adaptive interventions across various domains of health and psychological research. An adaptive intervention is a protocolized sequence of individualized treatments that seeks to address the unique and changing needs of individuals as they progress through an intervention program. The sequential, multiple assignment, randomized trial (SMART) is an experimental study design that can be used to build the empirical basis for the construction of effective adaptive interventions. A SMART involves multiple stages of randomizations; each stage of randomization is designed to address scientific questions concerning the best intervention option to employ at that point in the intervention. Several adaptive interventions are embedded in a SMART by design; many SMARTs are motivated by scientific questions that concern the comparison of these embedded adaptive interventions. Until recently, analysis methods available for the comparison of adaptive interventions were limited to end-of-study outcomes. The current article provides an accessible and comprehensive tutorial to a new methodology for using repeated outcome data from SMART studies to compare adaptive interventions. We discuss how existing methods for comparing adaptive interventions in terms of end-of-study outcome data from a SMART can be extended for use with longitudinal outcome data. We also highlight the scientific utility of using longitudinal data from a SMART to compare adaptive interventions. A SMART study aiming to develop an adaptive intervention to engage alcohol- and cocaine-dependent individuals in treatment is used to demonstrate the application of this new methodology. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {adaptive interventions,Alcoholism,Cocaine,dynamic treatment regimens,engagement,Intervention,longitudinal data,Longitudinal Studies,Methodology,multiple assignment,Randomized Clinical Trials,randomized trial (SMART),sequential,Treatment Effectiveness Evaluation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nahum-Shani et al_2020_SMART longitudinal analysis.pdf;/Users/nseewald/Zotero/storage/6D7C6HBX/2019-39467-001.html}
}

@article{nahum-shaniSMARTLongitudinalAnalysis2020a,
  title = {{{SMART}} Longitudinal Analysis: {{A}} Tutorial for Using Repeated Outcome Measures from {{SMART}} Studies to Compare Adaptive Interventions},
  shorttitle = {{{SMART}} Longitudinal Analysis},
  author = {{Nahum-Shani}, Inbal and Almirall, Daniel and Yap, Jamie R. T. and McKay, James R. and Lynch, Kevin G. and Freiheit, Elizabeth A. and Dziak, John J.},
  year = {2020},
  month = feb,
  journal = {Psychological Methods},
  volume = {25},
  number = {1},
  pages = {1--29},
  issn = {1939-1463},
  doi = {10.1037/met0000219},
  abstract = {In recent years, there has been increased interest in the development of adaptive interventions across various domains of health and psychological research. An adaptive intervention is a protocolized sequence of individualized treatments that seeks to address the unique and changing needs of individuals as they progress through an intervention program. The sequential, multiple assignment, randomized trial (SMART) is an experimental study design that can be used to build the empirical basis for the construction of effective adaptive interventions. A SMART involves multiple stages of randomizations; each stage of randomization is designed to address scientific questions concerning the best intervention option to employ at that point in the intervention. Several adaptive interventions are embedded in a SMART by design; many SMARTs are motivated by scientific questions that concern the comparison of these embedded adaptive interventions. Until recently, analysis methods available for the comparison of adaptive interventions were limited to end-of-study outcomes. The current article provides an accessible and comprehensive tutorial to a new methodology for using repeated outcome data from SMART studies to compare adaptive interventions. We discuss how existing methods for comparing adaptive interventions in terms of end-of-study outcome data from a SMART can be extended for use with longitudinal outcome data. We also highlight the scientific utility of using longitudinal data from a SMART to compare adaptive interventions. A SMART study aiming to develop an adaptive intervention to engage alcohol- and cocaine-dependent individuals in treatment is used to demonstrate the application of this new methodology. (PsycINFO Database Record (c) 2020 APA, all rights reserved).},
  langid = {english},
  pmcid = {PMC7480232},
  pmid = {31318231},
  keywords = {Data Interpretation Statistical,Humans,Longitudinal Studies,Outcome Assessment Health Care,Psychology,Randomized Controlled Trials as Topic,Research Design,Substance-Related Disorders},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nahum-Shani_et_al_2020_SMART_longitudinal_analysis.pdf}
}

@article{naimiConstructingInverseProbability2014,
  title = {Constructing {{Inverse Probability Weights}} for {{Continuous Exposures}}: {{A Comparison}} of {{Methods}}},
  shorttitle = {Constructing {{Inverse Probability Weights}} for {{Continuous Exposures}}},
  author = {Naimi, Ashley I. and Moodie, Erica E. M. and Auger, Nathalie and Kaufman, Jay S.},
  year = {2014},
  month = mar,
  journal = {Epidemiology},
  volume = {25},
  number = {2},
  pages = {292--299},
  issn = {1044-3983},
  doi = {10/f5t939},
  urldate = {2021-10-08},
  abstract = {Inverse probability--weighted marginal structural models with binary exposures are common in epidemiology. Constructing inverse probability weights for a continuous exposure can be complicated by the presence of outliers, and the need to identify a parametric form for the exposure and account for nonconstant exposure variance. We explored the performance of various methods to construct inverse probability weights for continuous exposures using Monte Carlo simulation. We generated two continuous exposures and binary outcomes using data sampled from a large empirical cohort. The first exposure followed a normal distribution with homoscedastic variance. The second exposure followed a contaminated Poisson distribution, with heteroscedastic variance equal to the conditional mean. We assessed six methods to construct inverse probability weights using: a normal distribution, a normal distribution with heteroscedastic variance, a truncated normal distribution with heteroscedastic variance, a gamma distribution, a t distribution (1, 3, and 5 degrees of freedom), and a quantile binning approach (based on 10, 15, and 20 exposure categories). We estimated the marginal odds ratio for a single-unit increase in each simulated exposure in a regression model weighted by the inverse probability weights constructed using each approach, and then computed the bias and mean squared error for each method. For the homoscedastic exposure, the standard normal, gamma, and quantile binning approaches performed best. For the heteroscedastic exposure, the quantile binning, gamma, and heteroscedastic normal approaches performed best. Our results suggest that the quantile binning approach is a simple and versatile way to construct inverse probability weights for continuous exposures.},
  langid = {american},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Naimi et al_2014_Constructing Inverse Probability Weights for Continuous Exposures.pdf;/Users/nseewald/Zotero/storage/A79ZP4XI/Constructing_Inverse_Probability_Weights_for.21.html;/Users/nseewald/Zotero/storage/UN9DIFH7/Constructing_Inverse_Probability_Weights_for.21.html}
}

@article{naimiIntroductionMethods2017,
  title = {An Introduction to g Methods},
  author = {Naimi, Ashley I and Cole, Stephen R and Kennedy, Edward H},
  year = {2017},
  month = apr,
  journal = {International Journal of Epidemiology},
  volume = {46},
  number = {2},
  pages = {756--762},
  issn = {0300-5771},
  doi = {10/gdnfn3},
  urldate = {2021-08-03},
  abstract = {Robins' generalized methods (g methods) provide consistent estimates of contrasts (e.g. differences, ratios) of potential outcomes under a less restrictive set of identification conditions than do standard regression methods (e.g. linear, logistic, Cox regression). Uptake of g methods by epidemiologists has been hampered by limitations in understanding both conceptual and technical details. We present a simple worked example that illustrates basic concepts, while minimizing technical complications.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Naimi et al_2017_An introduction to g methods.pdf;/Users/nseewald/Zotero/storage/IXL3273D/2760169.html}
}

@article{nakagawaCoefficientDeterminationR22017,
  title = {The Coefficient of Determination {{R2}} and Intra-Class Correlation Coefficient from Generalized Linear Mixed-Effects Models Revisited and Expanded},
  author = {Nakagawa, Shinichi and Johnson, Paul C. D. and Schielzeth, Holger},
  year = {2017},
  month = sep,
  journal = {Journal of The Royal Society Interface},
  volume = {14},
  number = {134},
  pages = {20170213},
  publisher = {Royal Society},
  doi = {10.1098/rsif.2017.0213},
  urldate = {2021-11-09},
  abstract = {The coefficient of determination R2 quantifies the proportion of variance explained by a statistical model and is an important summary statistic of biological interest. However, estimating R2 for generalized linear mixed models (GLMMs) remains challenging. We have previously introduced a version of R2 that we called  for Poisson and binomial GLMMs, but not for other distributional families. Similarly, we earlier discussed how to estimate intra-class correlation coefficients (ICCs) using Poisson and binomial GLMMs. In this paper, we generalize our methods to all other non-Gaussian distributions, in particular to negative binomial and gamma distributions that are commonly used for modelling biological data. While expanding our approach, we highlight two useful concepts for biologists, Jensen's inequality and the delta method, both of which help us in understanding the properties of GLMMs. Jensen's inequality has important implications for biologically meaningful interpretation of GLMMs, whereas the delta method allows a general derivation of variance associated with non-Gaussian distributions. We also discuss some special considerations for binomial GLMMs with binary or proportion data. We illustrate the implementation of our extension by worked examples from the field of ecology and evolution in the R environment. However, our method can be used across disciplines and regardless of statistical environments.},
  keywords = {goodness of fit,heritability,model fit,reliability analysis,repeatability,variance decomposition},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nakagawa et al_2017_The coefficient of determination R2 and intra-class correlation coefficient.pdf}
}

@article{nakagawaRepeatabilityGaussianNonGaussian2010,
  title = {Repeatability for {{Gaussian}} and Non-{{Gaussian}} Data: A Practical Guide for Biologists},
  shorttitle = {Repeatability for {{Gaussian}} and Non-{{Gaussian}} Data},
  author = {Nakagawa, Shinichi and Schielzeth, Holger},
  year = {2010},
  journal = {Biological Reviews},
  volume = {85},
  number = {4},
  pages = {935--956},
  issn = {1469-185X},
  doi = {10.1111/j.1469-185X.2010.00141.x},
  urldate = {2021-11-09},
  abstract = {Repeatability (more precisely the common measure of repeatability, the intra-class correlation coefficient, ICC) is an important index for quantifying the accuracy of measurements and the constancy of phenotypes. It is the proportion of phenotypic variation that can be attributed to between-subject (or between-group) variation. As a consequence, the non-repeatable fraction of phenotypic variation is the sum of measurement error and phenotypic flexibility. There are several ways to estimate repeatability for Gaussian data, but there are no formal agreements on how repeatability should be calculated for non-Gaussian data (e.g. binary, proportion and count data). In addition to point estimates, appropriate uncertainty estimates (standard errors and confidence intervals) and statistical significance for repeatability estimates are required regardless of the types of data. We review the methods for calculating repeatability and the associated statistics for Gaussian and non-Gaussian data. For Gaussian data, we present three common approaches for estimating repeatability: correlation-based, analysis of variance (ANOVA)-based and linear mixed-effects model (LMM)-based methods, while for non-Gaussian data, we focus on generalised linear mixed-effects models (GLMM) that allow the estimation of repeatability on the original and on the underlying latent scale. We also address a number of methods for calculating standard errors, confidence intervals and statistical significance; the most accurate and recommended methods are parametric bootstrapping, randomisation tests and Bayesian approaches. We advocate the use of LMM- and GLMM-based approaches mainly because of the ease with which confounding variables can be controlled for. Furthermore, we compare two types of repeatability (ordinary repeatability and extrapolated repeatability) in relation to narrow-sense heritability. This review serves as a collection of guidelines and recommendations for biologists to calculate repeatability and heritability from both Gaussian and non-Gaussian data.},
  langid = {english},
  keywords = {analysis of variance (ANOVA),confidence intervals,credibility intervals,generalised linear mixed-effects model (GLMM),heritability,intra-class correlation coefficient (ICC),Markov chain Monte Carlo (MCMC),restricted maximum likelihood (REML),statistical significance},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nakagawa_Schielzeth_2010_Repeatability for Gaussian and non-Gaussian data.pdf;/Users/nseewald/Zotero/storage/IGRAGAZI/j.1469-185X.2010.00141.html}
}

@article{namEffectStatinsAssociation2019,
  title = {Effect of Statins on the Association between High Temperature and All-Cause Mortality in a Socioeconomically Disadvantaged Population: A Cohort Study},
  shorttitle = {Effect of Statins on the Association between High Temperature and All-Cause Mortality in a Socioeconomically Disadvantaged Population},
  author = {Nam, Young Hee and Bilker, Warren B. and Leonard, Charles E. and Bell, Michelle L. and Alexander, Lacy M. and Hennessy, Sean},
  year = {2019},
  month = mar,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {4685},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-41109-0},
  urldate = {2024-01-05},
  abstract = {Abstract             High temperature increases all-cause mortality. Thermoregulatory ability is impaired in persons with elevated serum cholesterol, but can be improved by the administration of statins, even in the short-term. We investigated whether the impact of high temperature ({$\geq$}24\,{$^\circ$}C) on all-cause mortality among socioeconomically disadvantaged adults with a current or past indication for a statin is attenuated by current use of a statin with temperature dependence, by using claims data from five US Medicaid programs supplemented with Medicare claims for dual-enrollees and meteorological data from 1999--2010. We identified 3,508,948 persons (3,181,752 person-years) in a 1:1 propensity score-matched cohort. The incidence rate of all-cause mortality (deaths per 1,000 person-years) was 21.9 (95\% confidence interval [CI]: 21.6 to 22.3) in current statin users and 30.1 (95\% CI: 30.2 to 30.6) in former users. The adjusted odds ratios of mortality for current vs. former statin use were statistically significantly lower than 1.0, suggesting a protective effect of current statin use, on days with high temperature, with either daily average temperature or daily maximum temperature, and declined as daily average temperature increased from 29\,{$^\circ$}C and daily maximum temperature increased from 34\,{$^\circ$}C. These results were robust to the adjustment for daily relative humidity.},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/JT8QU2TR/Nam et al. - 2019 - Effect of statins on the association between high .pdf}
}

@article{namOutdoorTemperatureSurvival2019,
  title = {Outdoor Temperature and Survival Benefit of Empiric Potassium in Users of Furosemide in {{US Medicaid}} Enrollees: A Cohort Study},
  shorttitle = {Outdoor Temperature and Survival Benefit of Empiric Potassium in Users of Furosemide in {{US Medicaid}} Enrollees},
  author = {Nam, Young Hee and Bilker, Warren B and Leonard, Charles E and Bell, Michelle L and Hennessy, Sean},
  year = {2019},
  month = feb,
  journal = {BMJ Open},
  volume = {9},
  number = {2},
  pages = {e023809},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2018-023809},
  urldate = {2024-01-05},
  abstract = {Objective{\enspace} Heat is associated with elevated all-cause mortality, and furosemide-induced potassium depletion might be worsened by heat-induced sweating. Because empiric potassium is associated with a marked survival benefit in users of furosemide at a dose of {$\geq$}40\,mg/day, we hypothesised that this empiric potassium's survival benefit would increase with higher temperature ({$\geq$}24{$^\circ$}C). Design{\enspace} Cohort study. Setting{\enspace} Outpatient setting, captured by Medicaid claims, supplemented with Medicare claims for dual enrollees, from 5 US states from 1999 to 2010, linked to meteorological data. Population/Participants{\enspace} Furosemide ({$\geq$}40\,mg/day) initiators among adults continuously enrolled in Medicaid for at least 1\,year prior to cohort entry (defined as the day following the dispensing day of each individual's first observed furosemide prescription). Exposure{\enspace} Interaction between: (1) empiric potassium, dispensed the day of or the day following the dispensing of the initial furosemide prescription, and (2) daily average temperature and daily maximum temperature, examined separately. Outcome{\enspace} All-cause mortality. Results{\enspace} In 1:1 propensity score matched cohorts (total n=211\,878) that included 89\,335 person-years and 9007 deaths, all-cause mortality rates per 1000 person-years were 96.0 (95\% CI 93.2 to 98.9) and 105.8 (95\% CI 102.8 to 108.9) for potassium users and non-users, respectively. The adjusted OR of all-cause mortality for potassium use declined (ie, its apparent protective effect increased) as temperature increased, from a daily average temperature of about 28{$^\circ$}C and a daily maximum temperature of about 31{$^\circ$}C. This relationship was not statistically significant with daily average temperature, but was statistically significant with daily maximum temperature (p values for the interaction of potassium with daily maximum temperature and daily maximum temperature squared were 0.031 and 0.028, respectively). Conclusions{\enspace} The results suggest that empiric potassium's survival benefit among furosemide ({$\geq$}40\,mg/day) initiators may increase as daily maximum temperature increases. If this relationship is real, use of empiric potassium in Medicaid enrollees initiating furosemide might be particularly important on hot days.},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/C8FKCGX8/Nam et al. - 2019 - Outdoor temperature and survival benefit of empiri.pdf}
}

@misc{ncslStateCannabisPolicy,
  title = {State {{Cannabis Policy Enactment Database}}},
  author = {{National Conference of State Legislatures}},
  year = {2023},
  month = may,
  urldate = {2023-06-09},
  howpublished = {https://www.ncsl.org/health/state-cannabis-policy-enactment-database},
  file = {/Users/nseewald/Zotero/storage/DJULBNDX/state-cannabis-policy-enactment-database.html}
}

@article{necampComparingClusterlevelDynamic2017,
  title = {Comparing Cluster-Level Dynamic Treatment Regimens Using Sequential, Multiple Assignment, Randomized Trials: {{Regression}} Estimation and Sample Size Considerations},
  shorttitle = {Comparing Cluster-Level Dynamic Treatment Regimens Using Sequential, Multiple Assignment, Randomized Trials},
  author = {NeCamp, Timothy and Kilbourne, Amy and Almirall, Daniel},
  year = {2017},
  month = aug,
  journal = {Statistical Methods in Medical Research},
  volume = {26},
  number = {4},
  pages = {1572--1589},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280217708654},
  urldate = {2018-10-12},
  abstract = {Cluster-level dynamic treatment regimens can be used to guide sequential treatment decision-making at the cluster level in order to improve outcomes at the individual or patient-level. In a cluster-level dynamic treatment regimen, the treatment is potentially adapted and re-adapted over time based on changes in the cluster that could be impacted by prior intervention, including aggregate measures of the individuals or patients that compose it. Cluster-randomized sequential multiple assignment randomized trials can be used to answer multiple open questions preventing scientists from developing high-quality cluster-level dynamic treatment regimens. In a cluster-randomized sequential multiple assignment randomized trial, sequential randomizations occur at the cluster level and outcomes are observed at the individual level. This manuscript makes two contributions to the design and analysis of cluster-randomized sequential multiple assignment randomized trials. First, a weighted least squares regression approach is proposed for comparing the mean of a patient-level outcome between the cluster-level dynamic treatment regimens embedded in a sequential multiple assignment randomized trial. The regression approach facilitates the use of baseline covariates which is often critical in the analysis of cluster-level trials. Second, sample size calculators are derived for two common clusterrandomized sequential multiple assignment randomized trial designs for use when the primary aim is a betweendynamic treatment regimen comparison of the mean of a continuous patient-level outcome. The methods are motivated by the Adaptive Implementation of Effective Programs Trial which is, to our knowledge, the first-ever cluster-randomized sequential multiple assignment randomized trial in psychiatry.},
  langid = {english},
  keywords = {adaptive interventions,Adaptive treatment strategies,adept,cluster-randomized,dynamic treatment regimens,group-randomized,Statistics - Methodology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/NeCamp et al_2017_Comparing cluster-level dynamic treatment regimens using sequential, multiple.pdf}
}

@article{neelonModelingZeromodifiedCount2016,
  title = {Modeling Zero-Modified Count and Semicontinuous Data in Health Services Research Part 2: Case Studies},
  shorttitle = {Modeling Zero-Modified Count and Semicontinuous Data in Health Services Research Part 2},
  author = {Neelon, Brian and O'Malley, A. James and Smith, Valerie A.},
  year = {2016},
  journal = {Statistics in Medicine},
  volume = {35},
  number = {27},
  pages = {5094--5112},
  issn = {1097-0258},
  doi = {10/ggqhnm},
  urldate = {2020-12-11},
  abstract = {This article is the second installment of a two-part tutorial on the analysis of zero-modified count and semicontinuous data. Part 1, which appears as a companion piece in this issue of Statistics in Medicine, provides a general background and overview of the topic, with particular emphasis on applications to health services research. Here, we present three case studies highlighting various approaches for the analysis of zero-modified data. The first case study describes methods for analyzing zero-inflated longitudinal count data. Case study 2 considers the use of hurdle models for the analysis of spatiotemporal count data. The third case study discusses an application of marginalized two-part models to the analysis of semicontinuous health expenditure data. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Neelon et al_2016_Modeling zero-modified count and semicontinuous data in health services2.pdf;/Users/nseewald/Zotero/storage/83MVLVQR/sim.html}
}

@article{neelonModelingZeromodifiedCount2016a,
  title = {Modeling Zero-Modified Count and Semicontinuous Data in Health Services Research {{Part}} 1: Background and Overview},
  shorttitle = {Modeling Zero-Modified Count and Semicontinuous Data in Health Services Research {{Part}} 1},
  author = {Neelon, Brian and O'Malley, A. James and Smith, Valerie A.},
  year = {2016},
  journal = {Statistics in Medicine},
  volume = {35},
  number = {27},
  pages = {5070--5093},
  issn = {1097-0258},
  doi = {10/ggqhnk},
  urldate = {2020-12-11},
  abstract = {Health services data often contain a high proportion of zeros. In studies examining patient hospitalization rates, for instance, many patients will have no hospitalizations, resulting in a count of zero. When the number of zeros is greater or less than expected under a standard count model, the data are said to be zero modified relative to the standard model. A similar phenomenon arises with semicontinuous data, which are characterized by a spike at zero followed by a continuous distribution with positive support. When analyzing zero-modified count and semicontinuous data, flexible mixture distributions are often needed to accommodate both the excess zeros and the typically skewed distribution of nonzero values. Various models have been introduced over the past three decades to accommodate such data, including hurdle models, zero-inflated models, and two-part semicontinuous models. This tutorial describes recent modeling strategies for zero-modified count and semicontinuous data and highlights their role in health services research studies. Part 1 of the tutorial, presented here, provides a general overview of the topic. Part 2, appearing as a companion piece in this issue of Statistics in Medicine, discusses three case studies illustrating applications of the methods to health services research. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Neelon et al_2016_Modeling zero-modified count and semicontinuous data in health services.pdf;/Users/nseewald/Zotero/storage/R9ZD85LJ/sim.html}
}

@article{nelsonAddressingOpioidEpidemic2015,
  title = {Addressing the {{Opioid Epidemic}}},
  author = {Nelson, Lewis S. and Juurlink, David N. and Perrone, Jeanmarie},
  year = {2015},
  month = oct,
  journal = {JAMA},
  volume = {314},
  number = {14},
  pages = {1453--1454},
  issn = {0098-7484},
  doi = {10.1001/jama.2015.12397},
  urldate = {2024-05-01},
  abstract = {After alcohol intoxication, opioids are the most common cause of poisoning in patients presenting to North American emergency departments. Most opioids misused by patients originate from prescription medication. Most patients who overdose on prescription opioids are taking their medications differently than prescribed or are using opioids prescribed to someone else. These 2 main types of nonmedical opioid use represent a major cause of morbidity and mortality. Some individuals who misuse opioids are seeking euphoric effects, but others have developed dependence through chronic opioid use and are simply trying to avoid opioid withdrawal. Opioid-related harm has now reached epidemic levels: emergency department visits for nonmedical use of prescription opioids more than doubled from 2004 to 2011, accounting for an estimated 488\,000 visits in 2011. Deaths have more than tripled since 1999, with an estimated 16\,235 deaths attributable to prescription opioids in 2013.},
  file = {/Users/nseewald/Zotero/storage/6RBYMZ5M/2456149.html}
}

@article{nelsonStagedTreatmentEarly2018,
  title = {Staged {{Treatment}} in {{Early Psychosis}}: {{A}} Sequential Multiple Assignment Randomised Trial of Interventions for Ultra High Risk of Psychosis Patients},
  shorttitle = {Staged {{Treatment}} in {{Early Psychosis}}},
  author = {Nelson, Barnaby and Amminger, G. Paul and Yuen, Hok Pan and Wallis, Nicky and Kerr, Melissa J. and Dixon, Lisa and Carter, Cameron and Loewy, Rachel and Niendam, Tara A. and Shumway, Martha and Morris, Sarah and Blasioli, Julie and McGorry, Patrick D.},
  year = {2018},
  journal = {Early Intervention in Psychiatry},
  volume = {12},
  number = {3},
  pages = {292--306},
  issn = {1751-7893},
  doi = {10/gbnb6t},
  urldate = {2019-06-19},
  abstract = {Aim Previous research indicates that preventive intervention is likely to benefit patients ``at risk'' of psychosis, in terms of functional improvement, symptom reduction and delay or prevention of onset of threshold psychotic disorder. The primary aim of the current study is to test outcomes of ultra high risk (UHR) patients, primarily functional outcome, in response to a sequential intervention strategy consisting of support and problem solving (SPS), cognitive-behavioural case management and antidepressant medication. A secondary aim is to test biological and psychological variables that moderate and mediate response to this sequential treatment strategy. Methods This is a sequential multiple assignment randomised trial (SMART) consisting of three steps: Step 1: SPS (1.5 months); Step 2: SPS vs Cognitive Behavioural Case Management (4.5 months); Step 3: Cognitive Behavioural Case Management + Antidepressant Medication vs Cognitive Behavioural Case Management + Placebo (6 months). The intervention is of 12 months duration in total and participants will be followed up at 18 months and 24 months post baseline. Conclusion This paper reports on the rationale and protocol of the Staged Treatment in Early Psychosis (STEP) study. With a large sample of 500 UHR participants this study will investigate the most effective type and sequence of treatments for improving functioning and reducing the risk of developing psychotic disorder in this clinical population.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nelson et al_2018_Staged Treatment in Early Psychosis.pdf;/Users/nseewald/Zotero/storage/7BBZ9Y5Y/eip.html}
}

@article{netheryIntegratedCausalpredictiveMachine2021,
  title = {Integrated Causal-Predictive Machine Learning Models for Tropical Cyclone Epidemiology},
  author = {Nethery, Rachel C and {Katz-Christy}, Nina and Kioumourtzoglou, Marianthi-Anna and Parks, Robbie M and Schumacher, Andrea and Anderson, G Brooke},
  year = {2021},
  month = dec,
  journal = {Biostatistics},
  pages = {kxab047},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxab047},
  urldate = {2023-04-04},
  abstract = {Strategic preparedness reduces the adverse health impacts of hurricanes and tropical storms, referred to collectively as tropical cyclones (TCs), but its protective impact could be enhanced by a more comprehensive and rigorous characterization of TC epidemiology. To generate the insights and tools necessary for high-precision TC preparedness, we introduce a machine learning approach that standardizes estimation of historic TC health impacts, discovers common patterns and sources of heterogeneity in those health impacts, and enables identification of communities at highest health risk for future TCs. The model integrates (i) a causal inference component to quantify the immediate health impacts of recent historic TCs at high spatial resolution and (ii) a predictive component that captures how TC meteorological features and socioeconomic/demographic characteristics of impacted communities are associated with health impacts. We apply it to a rich data platform containing detailed historic TC exposure information and records of all-cause mortality and cardiovascular- and respiratory-related hospitalization among Medicare recipients. We report a high degree of heterogeneity in the acute health impacts of historic TCs, both within and across TCs, and, on average, substantial TC-attributable increases in respiratory hospitalizations. TC-sustained windspeeds are found to be the primary driver of mortality and respiratory risks.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nethery et al_2021_Integrated causal-predictive machine learning models for tropical cyclone.pdf;/Users/nseewald/Zotero/storage/VKLP9FXS/6485226.html}
}

@article{neufeldDiscussionBreimanTwo2021,
  title = {Discussion of {{Breiman}}'s "{{Two Cultures}}": {{From Two Cultures}} to {{One}}},
  shorttitle = {Discussion of {{Breiman}}'s "{{Two Cultures}}"},
  author = {Neufeld, Anna and Witten, Daniela},
  year = {2021},
  journal = {Observational Studies},
  volume = {7},
  number = {1},
  pages = {171--174},
  publisher = {University of Pennsylvania Press},
  issn = {2767-3324},
  doi = {10.1353/obs.2021.0004},
  urldate = {2022-08-19},
  abstract = {We argue that algorithmic models, though powerful and appropriate in some circumstances, rely on just as many tenuous assumptions as parametric probabilistic models; these assumptions, their violations, and the ethical consequences of these violations are simply obscured within a black box. We advocate for a future in which statisticians play a central role in bridging the gap between Breiman's two cultures.},
  keywords = {Algorithmic ethics,assumptions,black box models,prediction,two cultures},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Neufeld_Witten_2021_Discussion of Breiman's Two Cultures.pdf}
}

@misc{NewStateIce1932,
  title = {New {{State Ice Co}}. v. {{Liebmann}}, 285 {{U}}.{{S}}. 262 (1932)},
  shorttitle = {New {{State Ice}}},
  year = {1932},
  month = mar,
  pages = {262},
  urldate = {2023-06-09},
  abstract = {New State Ice Co. v. Liebmann},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/WD5ZEYI2/262.html}
}

@article{neymanNoteArticleSir1956,
  title = {Note on an {{Article}} by {{Sir Ronald Fisher}}},
  author = {Neyman, Jerzy},
  year = {1956},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {18},
  number = {2},
  eprint = {2983716},
  eprinttype = {jstor},
  pages = {288--294},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Neyman_1956_Note on an Article by Sir Ronald Fisher.pdf}
}

@article{ngAnnualResearchReview2016,
  title = {Annual {{Research Review}}: {{Building}} a Science of Personalized Intervention for Youth Mental Health},
  shorttitle = {Annual {{Research Review}}},
  author = {Ng, Mei Yi and Weisz, John R.},
  year = {2016},
  month = mar,
  journal = {Journal of Child Psychology and Psychiatry},
  volume = {57},
  number = {3},
  pages = {216--236},
  issn = {00219630},
  doi = {10.1111/jcpp.12470},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ng_Weisz_2016_Annual Research Review.pdf}
}

@article{nguyenClarifyingCausalMediation2021,
  title = {Clarifying Causal Mediation Analysis for the Applied Researcher: {{Defining}} Effects Based on What We Want to Learn},
  shorttitle = {Clarifying Causal Mediation Analysis for the Applied Researcher},
  author = {Nguyen, Trang Quynh and Schmid, Ian and Stuart, Elizabeth A.},
  year = {2021},
  month = apr,
  journal = {Psychological Methods},
  volume = {26},
  number = {2},
  pages = {255--271},
  publisher = {American Psychological Association},
  issn = {1082-989X},
  doi = {10.1037/met0000299},
  urldate = {2022-07-13},
  abstract = {The incorporation of causal inference in mediation analysis has led to theoretical and methodological advancements---effect definitions with causal interpretation, clarification of assumptions required for effect identification, and an expanding array of options for effect estimation. However, the literature on these results is fast-growing and complex, which may be confusing to researchers unfamiliar with causal inference or unfamiliar with mediation. The goal of this article is to help ease the understanding and adoption of causal mediation analysis. It starts by highlighting a key difference between the causal inference and traditional approaches to mediation analysis and making a case for the need for explicit causal thinking and the causal inference approach in mediation analysis. It then explains in as-plain-as-possible language existing effect types, paying special attention to motivating these effects with different types of research questions, and using concrete examples for illustration. This presentation differentiates 2 perspectives (or purposes of analysis): the explanatory perspective (aiming to explain the total effect) and the interventional perspective (asking questions about hypothetical interventions on the exposure and mediator, or hypothetically modified exposures). For the latter perspective, the article proposes tapping into a general class of interventional effects that contains as special cases most of the usual effect types---interventional direct and indirect effects, controlled direct effects and also a generalized interventional direct effect type, as well as the total effect and overall effect. This general class allows flexible effect definitions which better match many research questions than the standard interventional direct and indirect effects. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Causal Analysis,causal mediation analysis,effect definitions,estimands,Estimation,Exposure,Inference,Intervention,interventional effects,natural effects},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nguyen et al_2021_Clarifying causal mediation analysis for the applied researcher.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nguyen et al_2021_Clarifying causal mediation analysis for the applied researcher2.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nguyen et al_2021_Clarifying causal mediation analysis for the applied researcher3.pdf;/Users/nseewald/Zotero/storage/G5W27A6C/2020-52228-001.html}
}

@article{nguyenEstimatingIndividualizedTreatment2019,
  title = {Estimating {{Individualized Treatment Regimes}} from {{Crossover Designs}}},
  author = {Nguyen, Crystal T. and Luckett, Daniel J. and Kahkoska, Anna R. and Shearrer, Grace E. and {Spruijt-Metz}, Donna and Davis, Jaimie N. and Kosorok, Michael R.},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.05499 [cs, stat]},
  eprint = {1902.05499},
  primaryclass = {cs, stat},
  urldate = {2019-02-19},
  abstract = {The field of precision medicine aims to tailor treatment based on patientspecific factors in a reproducible way. To this end, estimating an optimal individualized treatment regime (ITR) that recommends treatment decisions based on patient characteristics to maximize the mean of a pre-specified outcome is of particular interest. Several methods have been proposed for estimating an optimal ITR from clinical trial data in the parallel group setting where each subject is randomized to a single intervention. However, little work has been done in the area of estimating the optimal ITR from crossover study designs. Such designs naturally lend themselves to precision medicine, because they allow for observing the response to multiple treatments for each patient. In this paper, we introduce a method for estimating the optimal ITR using data from a 2 {\texttimes} 2 crossover study with or without carryover effects. The proposed method is similar to policy search methods such as outcome weighted learning (OWL); however, we take advantage of the crossover design by using the difference in responses under each treatment as the observed reward. We establish Fisher and global consistency, present numerical experiments, and analyze data from a feeding trial to demonstrate the improved performance of the proposed method compared to standard methods for a parallel study design.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Nguyen et al_2019_Estimating Individualized Treatment Regimes from Crossover Designs.pdf}
}

@article{noarTailoredHealthCommunication2011,
  title = {Tailored {{Health Communication}} to {{Change Lifestyle Behaviors}}},
  author = {Noar, Seth M. and Grant Harrington, Nancy and Van Stee, Stephanie K. and Shemanski Aldrich, Rosalie},
  year = {2011},
  month = mar,
  journal = {American Journal of Lifestyle Medicine},
  volume = {5},
  number = {2},
  pages = {112--122},
  issn = {1559-8276, 1559-8284},
  doi = {10.1177/1559827610387255},
  urldate = {2018-10-12},
  abstract = {Tailored health communication research represents a very promising line of inquiry that has the potential to produce major impacts on lifestyle behaviors. This study defines tailoring and discusses how tailored interventions operate, including comparing/ contrasting different tailoring channels. Next, the authors review the literature on tailored interventions to change lifestyle behaviors, with a focus on smoking cessation, dietary change, and physical activity, as well as interventions that address multiple lifestyle behaviors. Finally, future directions for tailoring research are discussed. To date, a large literature has amassed showing the promise of tailored programs delivered via print, Internet, local computer/kiosk, telephone, and interpersonal channels. Numerous studies demonstrate that these programs are capable of significant impacts on smoking cessation, dietary change, physical activity, and multiple behavior change. It is concluded that the potential of tailoring will be more fully realized as (a) the field builds a more cumulative science of tailoring and (b) greater dissemination of efficacious tailored programs takes place.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Noar et al_2011_Tailored Health Communication to Change Lifestyle Behaviors.pdf}
}

@misc{ObservationalStudiesAnalyzed,
  title = {Observational {{Studies Analyzed Like Randomized Experiments}}: {{An Application}} to {{Postmenopausal Hormone Therapy}} and {{Coronary Heart Disease}} on {{JSTOR}}},
  urldate = {2024-03-02},
  howpublished = {https://www-jstor-org.proxy.library.upenn.edu/stable/25662633},
  file = {/Users/nseewald/Zotero/storage/CFPTYWNH/25662633.html}
}

@article{oconnellMethodsAnalysisPrePost2017,
  title = {Methods for {{Analysis}} of {{Pre-Post Data}} in {{Clinical Research}}: {{A Comparison}} of {{Five Common Methods}}},
  shorttitle = {Methods for {{Analysis}} of {{Pre-Post Data}} in {{Clinical Research}}},
  author = {O'Connell, Nathaniel S. and Dai, Lin and Jiang, Yunyun and Speiser, Jaime L. and Ward, Ralph and Wei, Wei and Carroll, Rachel and Gebregziabher, Mulugeta},
  year = {2017},
  month = feb,
  journal = {Journal of biometrics \& biostatistics},
  volume = {8},
  number = {1},
  pages = {1--8},
  issn = {2155-6180},
  doi = {10/ggh2w9},
  urldate = {2020-01-17},
  abstract = {Often repeated measures data are summarized into pre-post-treatment measurements. Various methods exist in the literature for estimating and testing treatment effect, including ANOVA, analysis of covariance (ANCOVA), and linear mixed modeling (LMM). Under the first two methods, outcomes can either be modeled as the post treatment measurement (ANOVA-POST or ANCOVA-POST), or a change score between pre and post measurements (ANOVA-CHANGE, ANCOVA-CHANGE). In LMM, the outcome is modeled as a vector of responses with or without Kenward-Rogers adjustment. We consider five methods common in the literature, and discuss them in terms of supporting simulations and theoretical derivations of variance. Consistent with existing literature, our results demonstrate that each method leads to unbiased treatment effect estimates, and based on precision of estimates, 95\% coverage probability, and power, ANCOVA modeling of either change scores or post-treatment score as the outcome, prove to be the most effective. We further demonstrate each method in terms of a real data example to exemplify comparisons in real clinical context.},
  pmcid = {PMC6290914},
  pmid = {30555734},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/O'Connell et al_2017_Methods for Analysis of Pre-Post Data in Clinical Research.pdf}
}

@incollection{oettingStatisticalMethodologySMART2011,
  title = {Statistical {{Methodology}} for a {{SMART Design}} in the {{Development}} of {{Adaptive Treatment Strategies}}},
  booktitle = {Causality and {{Psychopathology}}: {{Finding}} the {{Determinants}} of {{Disorders}} and {{Their Cures}}},
  author = {Oetting, Alena I. and Levy, Janet A. and Weiss, Roger D. and Murphy, Susan A.},
  editor = {Shrout, Patrick E. and Keyes, Katherine M. and Ornstein, Katherine},
  year = {2011},
  pages = {179--205},
  publisher = {Oxford University Press},
  address = {New York},
  isbn = {978-0-19-975464-9},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Oetting et al_2011_Statistical Methodology for a SMART Design in the Development of Adaptive.pdf}
}

@article{ogbagaberDesignSequentiallyRandomized2016,
  title = {Design of Sequentially Randomized Trials for Testing Adaptive Treatment Strategies},
  author = {Ogbagaber, Semhar B. and Karp, Jordan and Wahed, Abdus S.},
  year = {2016},
  journal = {Statistics in Medicine},
  volume = {35},
  number = {6},
  pages = {840--858},
  doi = {10.1002/sim.6747},
  abstract = {An adaptive treatment strategy (ATS) is an outcome-guided algorithm that allows personalized treatment of complex diseases based on patients' disease status and treatment history. Conditions such as AIDS, depression, and cancer usually require several stages of treatment because of the chronic, multifactorial nature of illness progression and management. Sequential multiple assignment randomized (SMAR) designs permit simultaneous inference about multiple ATSs, where patients are sequentially randomized to treatments at different stages depending upon response status. The purpose of the article is to develop a sample size formula to ensure adequate power for comparing two or more ATSs. Based on a Wald-type statistic for comparing multiple ATSs with a continuous endpoint, we develop a sample size formula and test it through simulation studies. We show via simulation that the proposed sample size formula maintains the nominal power. The proposed sample size formula is not applicable to designs with time-to-event endpoints but the formula will be useful for practitioners while designing SMAR trials to compare adaptive treatment strategies.},
  keywords = {Adaptive treatment strategy (ATS),Power,Sample size,Sequential multiple assignment randomized trial (S},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ogbagaber et al_2016_Design of sequentially randomized trials for testing adaptive treatment.pdf}
}

@article{ogburnCausalModellingTwo2021,
  title = {Causal {{Modelling}}: {{The Two Cultures}}},
  shorttitle = {Causal {{Modelling}}},
  author = {Ogburn, Elizabeth L. and Shpitser, Ilya},
  year = {2021},
  journal = {Observational Studies},
  volume = {7},
  number = {1},
  pages = {179--183},
  publisher = {University of Pennsylvania Press},
  issn = {2767-3324},
  doi = {10.1353/obs.2021.0006},
  urldate = {2022-08-19},
  abstract = {We offer descriptive and normative standards for the principled pursuit of causal inference. These standards address critiques of both the algorithmic and the data modeling cultures identified in (Breiman, 2001), and provide a fruitful synthesis of both cultures. We contrast the resulting "cautious causal inference" with overly optimistic methods inspired by algorithmic data analysis methods prevalent in machine learning, as well as older approaches to causal modeling that employ overly restrictive parametric models.},
  keywords = {Causal Inference,Machine Learning,Nonparametric Identification},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ogburn_Shpitser_2021_Causal Modelling.pdf}
}

@article{ogburnVaccinesContagionSocial2017,
  ids = {ogburnVACCINESCONTAGIONSOCIAL2017},
  title = {Vaccines, Contagion, and Social Networks},
  author = {Ogburn, Elizabeth L. and VanderWeele, Tyler J.},
  year = {2017},
  month = jun,
  journal = {The Annals of Applied Statistics},
  volume = {11},
  number = {2},
  pages = {919--948},
  publisher = {Institute of Mathematical Statistics},
  issn = {1932-6157, 1941-7330},
  doi = {10/gk6jsg},
  urldate = {2021-07-13},
  abstract = {Consider the causal effect that one individual's treatment may have on another individual's outcome when the outcome is contagious, with specific application to the effect of vaccination on an infectious disease outcome. The effect of one individual's vaccination on another's outcome can be decomposed into two different causal effects, called the ``infectiousness'' and ``contagion'' effects. We present identifying assumptions and estimation or testing procedures for infectiousness and contagion effects in two different settings: (1) using data sampled from independent groups of observations, and (2) using data collected from a single interdependent social network. The methods that we propose for social network data require fitting generalized linear models (GLMs). GLMs and other statistical models that require independence across subjects have been used widely to estimate causal effects in social network data, but because the subjects in networks are presumably not independent, the use of such models is generally invalid, resulting in inference that is expected to be anticonservative. We describe a subsampling scheme that ensures that GLM errors are uncorrelated across subjects despite the fact that outcomes are nonindependent. This simultaneously demonstrates the possibility of using GLMs and related statistical models for network data and highlights their limitations.},
  keywords = {Causal inference,contagion,peer effects,social networks},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ogburn_VanderWeele_2017_Vaccines, contagion, and social networks.pdf;/Users/nseewald/Zotero/storage/FHI7AV26/17-AOAS1023.html}
}

@book{oluoYouWantTalk2019,
  title = {So You Want to Talk about Race},
  author = {Oluo, Ijeoma},
  year = {2019},
  isbn = {978-1-58005-882-7},
  langid = {english},
  keywords = {nosource},
  annotation = {OCLC: 1162544697}
}

@incollection{optumlabs2022,
  title = {Optum {{Labs}} and {{Optum Labs Data Warehouse}} ({{OLDW}}) {{Descriptions}} and {{Citation}}},
  author = {{Optum Labs}},
  year = {2022},
  month = mar,
  publisher = {n.p},
  address = {Minnetonka, MN},
  copyright = {Reproduced with permission from Optum Labs}
}

@article{orellanaDynamicRegimeMarginal2010,
  title = {Dynamic {{Regime Marginal Structural Mean Models}} for {{Estimation}} of {{Optimal Dynamic Treatment Regimes}}, {{Part I}}: {{Main Content}}},
  author = {Orellana, Liliana and Robins, James M and Rotnitzky, Andrea},
  year = {2010},
  journal = {The International Journal of Biostatistics},
  volume = {6},
  number = {2},
  doi = {10.2202/1557-4679.1200},
  abstract = {Dynamic treatment regimes are set rules for sequential decision making based on patient covariate history. Observational studies are well suited for the investigation of the effects of dynamic treatment regimes because of the variability in treatment decisions found in them. This variability exists because different physicians make different decisions in the face of similar patient histories. In this article we describe an approach to estimate the optimal dynamic treatment regime among a set of enforceable regimes. This set is comprised by regimes defined by simple rules based on a subset of past information. The regimes in the set are indexed by a Euclidean vector. The optimal regime is the one that maximizes the expected counterfactual utility over all regimes in the set. We discuss assumptions under which it is possible to identify the optimal regime from observational longitudinal data. Murphy et al. (2001) developed efficient augmented inverse probability weighted estimators of the expected utility of one fixed regime. Our methods are based on an extension of the marginal structural mean model of Robins (1998, 1999) which incorporate the estimation ideas of Murphy et al. (2001). Our models, which we call dynamic regime marginal structural mean models, are specially suitable for estimating the optimal treatment regime in a moderately small class of enforceable regimes of interest. We consider both parametric and semiparametric dynamic regime marginal structural models. We discuss locally efficient, double-robust estimation of the model parameters and of the index of the optimal treatment regime in the set. In a companion paper in this issue of the journal we provide proofs of the main results.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Orellana et al_2010_Dynamic Regime Marginal Structural Mean Models for Estimation of Optimal.pdf}
}

@article{orellanaDynamicRegimeMarginal2010a,
  title = {Dynamic {{Regime Marginal Structural Mean Models}} for {{Estimation}} of {{Optimal Dynamic Treatment Regimes}}, {{Part II}}: {{Proofs}} of {{Results}}},
  author = {Orellana, Liliana and Robins, James M and Rotnitzky, Andrea},
  year = {2010},
  journal = {The International Journal of Biostatistics},
  volume = {6},
  number = {2},
  doi = {10.2202/1557-4679.1242},
  abstract = {In this companion article to "Dynamic Regime Marginal Structural Mean Models for Estimation of Optimal Dynamic Treatment Regimes, Part I: Main Content" [Orellana, Rotnitzky and Robins (2010), IJB, Vol. 6, Iss. 2, Art. 7] we present (i) proofs of the claims in that paper, (ii) a proposal for the computation of a confidence set for the optimal index when this lies in a finite set, and (iii) an example to aid the interpretation of the positivity assumption.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Orellana et al_2010_Dynamic Regime Marginal Structural Mean Models for Estimation of Optimal2.pdf}
}

@misc{oslinManagingAlcoholismPeople2005,
  title = {Managing {{Alcoholism}} in {{People Who Do Not Respond}} to {{Naltrexone}}},
  author = {Oslin, David},
  year = {2005},
  journal = {ClinicalTrials.gov},
  urldate = {2018-12-08},
  abstract = {ClinicalTrials.gov Identifier NCT00115037},
  howpublished = {https://clinicaltrials.gov/ct2/show/NCT00115037},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/3RBKERRS/NCT00115037.html}
}

@article{oslinTargetingTreatmentsAlcohol2006,
  title = {Targeting Treatments for Alcohol Dependence: The Pharmacogenetics of Naltrexone},
  shorttitle = {{{REVIEW}}},
  author = {Oslin, David W. and Berrettini, Wade H. and O'Brien, Charles P.},
  year = {2006},
  journal = {Addiction Biology},
  volume = {11},
  number = {3-4},
  pages = {397--403},
  issn = {1369-1600},
  doi = {10/fgcfbk},
  urldate = {2020-12-13},
  abstract = {Alcohol dependence is one of the leading causes of morbidity worldwide, yet only a minority of those afflicted engages in treatment. While increasing access to treatment is an important public health approach, increasing the success of treatment is also likely to lead to greater engagement. However, alcohol dependence is a complex disorder likely to consist of several biological subtypes. Recent evidence from a number of different studies has suggested that genetic variation in the mu-opioid receptor has a significant influence on clinical presentation of alcohol problems and response to treatment with an opioid antagonist. Most notably, the A118G polymorphism of the mu-receptor gene has been demonstrated to predict clinical response to naltrexone in alcohol-dependent individuals. This article reviews the biological correlates and outlines a scientific agenda for better understanding the role of opioid neurotransmission in the etiology, maintenance and treatment of alcohol dependence.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Oslin et al_2006_Targeting treatments for alcohol dependence.pdf;/Users/nseewald/Zotero/storage/MZKLDKBF/j.1369-1600.2006.00036.html}
}

@article{ouyangAccountingComplexIntracluster2023,
  title = {Accounting for Complex Intracluster Correlations in Longitudinal Cluster Randomized Trials: A Case Study in Malaria Vector Control},
  shorttitle = {Accounting for Complex Intracluster Correlations in Longitudinal Cluster Randomized Trials},
  author = {Ouyang, Yongdong and Kulkarni, Manisha A. and Protopopoff, Natacha and Li, Fan and Taljaard, Monica},
  year = {2023},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {23},
  number = {1},
  pages = {1--10},
  publisher = {BioMed Central},
  issn = {1471-2288},
  doi = {10.1186/s12874-023-01871-2},
  urldate = {2023-07-11},
  abstract = {The effectiveness of malaria vector control interventions is often evaluated using cluster randomized trials (CRT) with outcomes assessed using repeated cross-sectional surveys. A key requirement for appropriate design and analysis of longitudinal CRTs is accounting for the intra-cluster correlation coefficient (ICC). In addition to exchangeable correlation (constant ICC over time), correlation structures proposed for longitudinal CRT are block exchangeable (allows a different within- and between-period ICC) and exponential decay (allows between-period ICC to decay exponentially). More flexible correlation structures are available in statistical software packages and, although not formally proposed for longitudinal CRTs, may offer some advantages. Our objectives were to empirically explore the impact of these correlation structures on treatment effect inferences, identify gaps in the methodological literature, and make practical recommendations. We obtained data from a parallel-arm CRT conducted in Tanzania to compare four different types of insecticide-treated bed-nets. Malaria prevalence was assessed in cross-sectional surveys of 45 households in each of 84 villages at baseline, 12-, 18- and 24-months post-randomization. We re-analyzed the data using mixed-effects logistic regression according to a prespecified analysis plan but under five different correlation structures as well as a robust variance estimator under exchangeable correlation and compared the estimated correlations and treatment effects. A proof-of-concept simulation was conducted to explore general conclusions. The estimated correlation structures varied substantially across different models. The unstructured model was the best-fitting model based on information criteria. Although point estimates and confidence intervals for the treatment effect were similar, allowing for more flexible correlation structures led to different conclusions based on statistical significance. Use of robust variance estimators generally led to wider confidence intervals. Simulation results showed that under-specification can lead to coverage probabilities much lower than nominal levels, but over-specification is more likely to maintain nominal coverage. More flexible correlation structures should not be ruled out in longitudinal CRTs. This may be particularly important in malaria trials where outcomes may fluctuate over time. In the absence of robust methods for selecting the best-fitting correlation structure, researchers should examine sensitivity of results to different assumptions about the ICC and consider robust variance estimators.},
  copyright = {2023 The Author(s)},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ouyang et al_2023_Accounting for complex intracluster correlations in longitudinal cluster.pdf}
}

@article{ouyangEstimatingIntraclusterCorrelation2023,
  title = {Estimating Intra-Cluster Correlation Coefficients for Planning Longitudinal Cluster Randomized Trials: A Tutorial},
  shorttitle = {Estimating Intra-Cluster Correlation Coefficients for Planning Longitudinal Cluster Randomized Trials},
  author = {Ouyang, Yongdong and Hemming, Karla and Li, Fan and Taljaard, Monica},
  year = {2023},
  month = may,
  journal = {International Journal of Epidemiology},
  pages = {dyad062},
  issn = {0300-5771},
  doi = {10.1093/ije/dyad062},
  urldate = {2023-07-10},
  abstract = {It is well-known that designing a cluster randomized trial (CRT) requires an advance estimate of the intra-cluster correlation coefficient (ICC). In the case of longitudinal CRTs, where outcomes are assessed repeatedly in each cluster over time, estimates for more complex correlation structures are required. Three common types of correlation structures for longitudinal CRTs are exchangeable, nested/block exchangeable and exponential decay correlations---the latter two allow the strength of the correlation to weaken over time. Determining sample sizes under these latter two structures requires advance specification of the within-period ICC and cluster autocorrelation coefficient as well as the intra-individual autocorrelation coefficient in the case of a cohort design. How to estimate these coefficients is a common challenge for investigators. When appropriate estimates from previously published longitudinal CRTs are not available, one possibility is to re-analyse data from an available trial dataset or to access observational data to estimate these parameters in advance of a trial. In this tutorial, we demonstrate how to estimate correlation parameters under these correlation structures for continuous and binary outcomes. We first introduce the correlation structures and their underlying model assumptions under a mixed-effects regression framework. With practical advice for implementation, we then demonstrate how the correlation parameters can be estimated using examples and we provide programming code in R, SAS, and Stata. An Rshiny app is available that allows investigators to upload an existing dataset and obtain the estimated correlation parameters. We conclude by identifying some gaps in the literature.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ouyang et al_2023_Estimating intra-cluster correlation coefficients for planning longitudinal.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ouyang et al_2023_Estimating intra-cluster correlation coefficients for planning longitudinal2.pdf;/Users/nseewald/Zotero/storage/JCLDL8BZ/7169442.html}
}

@article{overallEstimatingSampleSizes1994,
  title = {Estimating Sample Sizes for Repeated Measurement Designs},
  author = {Overall, John E. and Doyle, Suzanne R.},
  year = {1994},
  month = apr,
  journal = {Controlled Clinical Trials},
  volume = {15},
  number = {2},
  pages = {100--123},
  issn = {01972456},
  doi = {10.1016/0197-2456(94)90015-9},
  urldate = {2020-01-16},
  abstract = {Formulas for estimating sample sizes that are required to provide specified power for analysis of variance (ANOVA) tests of significance in a two-group repeated measurements design are presented and evaluated. Power and sample size requirements depend on the pattern of treatment effects and the pattern of correlations among the repeated measurements, as well as on parameters common to sample size estimation for cross-sectional comparisons of treatment effects in simple randomized designs. Simplifying assumptions permit generation of these numerous parameter estimates from predictions of the magnitude of the standardized "effect size" at end of trial and the single correlation between the baseline and endpoint measurements. Monte Carlo methods are used to verify the actual power of different tests of significance for treatment effects in repeated measurement designs using sample sizes estimated by the formulas. The sample size implications of different patterns of treatment effects, levels of correlation, and numbers of repeated measurements are evaluated.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Overall_Doyle_1994_Estimating sample sizes for repeated measurement designs.pdf;/Users/nseewald/Zotero/storage/RSGVASEC/0197245694900159.html}
}

@article{pageComparativeCostAnalysis2016,
  title = {Comparative {{Cost Analysis}} of {{Sequential}}, {{Adaptive}}, {{Behavioral}}, {{Pharmacological}}, and {{Combined Treatments}} for {{Childhood ADHD}}},
  author = {Page, Timothy F. and Pelham, William E. and Fabiano, Gregory A. and Greiner, Andrew R. and Gnagy, Elizabeth M. and Hart, Katie C. and Coxe, Stefany and Waxmonsky, James G. and Foster, E. Michael and Pelham, William E.},
  year = {2016},
  month = jul,
  journal = {Journal of Clinical Child \& Adolescent Psychology},
  volume = {45},
  number = {4},
  pages = {416--427},
  publisher = {Routledge},
  issn = {1537-4416},
  doi = {10.1080/15374416.2015.1055859},
  urldate = {2022-08-26},
  abstract = {We conducted a cost analysis of the behavioral, pharmacological, and combined interventions employed in a sequential, multiple assignment, randomized, and adaptive trial investigating the sequencing and enhancement of treatment for children with attention deficit hyperactivity disorder (ADHD; Pelham et al., 201X; N = 146, 76\% male, 80\% Caucasian). The quantity of resources expended on each child's treatment was determined from records that listed the type, date, location, persons present, and duration of all services provided. The inputs considered were the amount of physician time, clinician time, paraprofessional time, teacher time, parent time, medication, and gasoline. Quantities of these inputs were converted into costs in 2013 USD using national wage estimates from the Bureau of Labor Statistics, the prices of 30-day supplies of prescription drugs from the national Express Scripts service, and mean fuel prices from the Energy Information Administration. Beginning treatment with a low-dose/intensity regimen of behavior modification (large-group parent training) was less costly for a school year of treatment (\$961) than beginning treatment with a low dose of stimulant medication (\$1,669), regardless of whether the initial treatment was intensified with a higher ``dose'' or if the other modality was added. Outcome data from the parent study (Pelham et al., 201X) found equivalent or superior outcomes for treatments beginning with low-intensity behavior modification compared to intervention beginning with medication. Combined with the present analyses, these findings suggest that initiating treatment with behavior modification rather than medication is the more cost-effective option for children with ADHD.},
  pmid = {26808137},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Page et al_2016_Comparative Cost Analysis of Sequential, Adaptive, Behavioral, Pharmacological,.pdf}
}

@article{panNoteMarginalLinear2000,
  title = {A {{Note}} on {{Marginal Linear Regression}} with {{Correlated Response Data}}},
  author = {Pan, Wei and Louis, Thomas A. and Connett, John E.},
  year = {2000},
  month = aug,
  journal = {The American Statistician},
  volume = {54},
  number = {3},
  pages = {191--195},
  issn = {0003-1305},
  doi = {10/ggfrvz},
  urldate = {2019-12-21},
  abstract = {Correlated response data often arise in longitudinal and familial studies. The marginal regression model and its associated generalized estimating equation (GEE) method are becoming more and more popular in handling such data. Pepe and Anderson pointed out that there is an important yet implicit assumption behind the marginal model and GEE. If the assumption is violated and a nondiagonal working correlation matrix is used in GEE, biased estimates of regression coefficients may result. On the other hand, if a diagonal correlation matrix is used, irrespective of whether the assumption is violated, the resulting estimates are (nearly) unbiased. A straightforward interpretation of this phenomenon is lacking, in part due to the unavailability of a closed form for the resulting GEE estimates. In this note, we show how the bias may arise in the context of linear regression, where the GEE estimates of regression coefficients are the ordinary or generalized least squares (LS) estimates. Also we explain why the generalized LS estimator may be biased, in contrast to the well-known result that it is usually unbiased. In addition, we discuss the bias properties of the sandwich variance estimator of the ordinary LS estimate.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Pan et al_2000_A Note on Marginal Linear Regression with Correlated Response Data.pdf}
}

@article{pantelopoulosSurveyWearableSensorBased2010,
  title = {A {{Survey}} on {{Wearable Sensor-Based Systems}} for {{Health Monitoring}} and {{Prognosis}}},
  author = {Pantelopoulos, A. and Bourbakis, N.G.},
  year = {2010},
  month = jan,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {40},
  number = {1},
  pages = {1--12},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2009.2032660},
  urldate = {2018-10-12},
  abstract = {The design and development of wearable biosensor systems for health monitoring has garnered lots of attention in the scientific community and the industry during the last years. Mainly motivated by increasing healthcare costs and propelled by recent technological advances in miniature biosensing devices, smart textiles, microelectronics, and wireless communications, the continuous advance of wearable sensor-based systems will potentially transform the future of healthcare by enabling proactive personal health management and ubiquitous monitoring of a patient's health condition. These systems can comprise various types of small physiological sensors, transmission modules and processing capabilities, and can thus facilitate low-cost wearable unobtrusive solutions for continuous all-day and any-place health, mental and activity status monitoring. This paper attempts to comprehensively review the current research and development on wearable biosensor systems for health monitoring. A variety of system implementations are compared in an approach to identify the technological shortcomings of the current state-of-the-art in wearable biosensor solutions. An emphasis is given to multiparameter physiological sensing system designs, providing reliable vital signs measurements and incorporating real-time decision support for early detection of symptoms or context awareness. In order to evaluate the maturity level of the top current achievements in wearable health-monitoring systems, a set of significant features, that best describe the functionality and the characteristics of the systems, has been selected to derive a thorough study. The aim of this survey is not to criticize, but to serve as a reference for researchers and developers in this scientific area and to provide direction for future research improvements.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Pantelopoulos_Bourbakis_2010_A Survey on Wearable Sensor-Based Systems for Health Monitoring and Prognosis.pdf}
}

@article{parzenGeneralizedLinearMixed2011,
  title = {A Generalized Linear Mixed Model for Longitudinal Binary Data with a Marginal Logit Link Function},
  author = {Parzen, Michael and Ghosh, Souparno and Lipsitz, Stuart and Sinha, Debajyoti and Fitzmaurice, Garrett M. and Mallick, Bani K. and Ibrahim, Joseph G.},
  year = {2011},
  journal = {The annals of applied statistics},
  volume = {5},
  number = {1},
  pages = {449--467},
  issn = {1932-6157},
  doi = {10.1214/10-AOAS390},
  urldate = {2021-12-20},
  abstract = {Longitudinal studies of a binary outcome are common in the health, social, and behavioral sciences. In general, a feature of random effects logistic regression models for longitudinal binary data is that the marginal functional form, when integrated over the distribution of the random effects, is no longer of logistic form. Recently,  proposed a random intercept model in the clustered binary data setting where the marginal model has a logistic form. An acknowledged limitation of their model is that it allows only a single random effect that varies from cluster to cluster. In this paper, we propose a modification of their model to handle longitudinal data, allowing separate, but correlated, random intercepts at each measurement occasion. The proposed model allows for a flexible correlation structure among the random intercepts, where the correlations can be interpreted in terms of Kendall's {$\tau$}. For example, the marginal correlations among the repeated binary outcomes can decline with increasing time separation, while the model retains the property of having matching conditional and marginal logit link functions. Finally, the proposed method is used to analyze data from a longitudinal study designed to monitor cardiac abnormalities in children born to HIV-infected women.},
  pmcid = {PMC3082943},
  pmid = {21532998},
  keywords = {mixed-effects models},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Parzen et al_2011_A generalized linear mixed model for longitudinal binary data with a marginal.pdf}
}

@article{paulusPredictablyUnequalUnderstanding2020,
  title = {Predictably Unequal: Understanding and Addressing Concerns That Algorithmic Clinical Prediction May Increase Health Disparities},
  shorttitle = {Predictably Unequal},
  author = {Paulus, Jessica K. and Kent, David M.},
  year = {2020},
  month = jul,
  journal = {npj Digital Medicine},
  volume = {3},
  number = {1},
  pages = {1--8},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-020-0304-9},
  urldate = {2022-08-17},
  abstract = {The machine learning community has become alert to the ways that predictive algorithms can inadvertently introduce unfairness in decision-making. Herein, we discuss how concepts of algorithmic fairness might apply in healthcare, where predictive algorithms are being increasingly used to support decision-making. Central to our discussion is the distinction between algorithmic fairness and algorithmic bias. Fairness concerns apply specifically when algorithms are used to support polar decisions (i.e., where one pole of prediction leads to decisions that are generally more desired than the other), such as when predictions are used to allocate scarce health care resources to a group of patients that could all benefit. We review different fairness criteria and demonstrate their mutual incompatibility. Even when models are used to balance benefits-harms to make optimal decisions for individuals (i.e., for non-polar decisions)--and fairness concerns are not germane--model, data or sampling issues can lead to biased predictions that support decisions that are differentially harmful/beneficial across groups. We review these potential sources of bias, and also discuss ways to diagnose and remedy algorithmic bias. We note that remedies for algorithmic fairness may be more problematic, since we lack agreed upon definitions of fairness. Finally, we propose a provisional framework for the evaluation of clinical prediction models offered for further elaboration and refinement. Given the proliferation of prediction models used to guide clinical decisions, developing consensus for how these concerns can be addressed should be prioritized.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Health care,Medical research},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Paulus_Kent_2020_Predictably unequal.pdf;/Users/nseewald/Zotero/storage/X9A2IUSJ/s41746-020-0304-9.html}
}

@article{pelhamTreatmentSequencingChildhood2016,
  title = {Treatment {{Sequencing}} for {{Childhood ADHD}}: {{A Multiple-Randomization Study}} of {{Adaptive Medication}} and {{Behavioral Interventions}}},
  shorttitle = {Treatment {{Sequencing}} for {{Childhood ADHD}}},
  author = {Pelham, Jr., William E. and Fabiano, Gregory A. and Waxmonsky, James G. and Greiner, Andrew R. and Gnagy, Elizabeth M. and III, William E. Pelham and Coxe, Stefany and Verley, Jessica and Bhatia, Ira and Hart, Katie and Karch, Kathryn and Konijnendijk, Evelien and Tresco, Katy and {Nahum-Shani}, Inbal and Murphy, Susan A.},
  year = {2016},
  month = jul,
  journal = {Journal of Clinical Child \& Adolescent Psychology},
  volume = {45},
  number = {4},
  pages = {396--415},
  issn = {1537-4416},
  doi = {10/gfn9xr},
  urldate = {2018-12-08},
  abstract = {Behavioral and pharmacological treatments for children with attention deficit/hyperactivity disorder (ADHD) were evaluated to address whether endpoint outcomes are better depending on which treatment is initiated first and, in case of insufficient response to initial treatment, whether increasing dose of initial treatment or adding the other treatment modality is superior. Children with ADHD (ages 5--12, N = 146, 76\% male) were treated for 1 school year. Children were randomized to initiate treatment with low doses of either (a) behavioral parent training (8 group sessions) and brief teacher consultation to establish a Daily Report Card or (b) extended-release methylphenidate (equivalent to .15 mg/kg/dose bid). After 8 weeks or at later monthly intervals as necessary, insufficient responders were rerandomized to secondary interventions that either increased the dose/intensity of the initial treatment or added the other treatment modality, with adaptive adjustments monthly as needed to these secondary treatments. The group beginning with behavioral treatment displayed significantly lower rates of observed classroom rule violations (the primary outcome) at study endpoint and tended to have fewer out-of-class disciplinary events. Further, adding medication secondary to initial behavior modification resulted in better outcomes on the primary outcomes and parent/teacher ratings of oppositional behavior than adding behavior modification to initial medication. Normalization rates on teacher and parent ratings were generally high. Parents who began treatment with behavioral parent training had substantially better attendance than those assigned to receive training following medication. Beginning treatment with behavioral intervention produced better outcomes overall than beginning treatment with medication.},
  pmid = {26882332},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Pelham et al_2016_Treatment Sequencing for Childhood ADHD.pdf;/Users/nseewald/Zotero/storage/QXNF59EU/15374416.2015.html}
}

@article{pepeCautionaryNoteInference1994,
  title = {A Cautionary Note on Inference for Marginal Regression Models with Longitudinal Data and General Correlated Response Data},
  author = {Pepe, Margaret Sullivan and Anderson, Garnet L.},
  year = {1994},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {23},
  number = {4},
  pages = {939--951},
  issn = {0361-0918},
  doi = {10/fc284x},
  urldate = {2019-08-14},
  abstract = {Inference for cross-sectional models using longitudinal data, can be accomplished with generalized estimating equations (Zeger and Liang, 1992). We show that either a diagonal working covariance matrix should be used or a key assumption should be verified. The assumption is non-trivial when covariates vary over time. The validity of this assumption is explored for some broad classes of correlation structures. Similar considerations are shown to be relevant for the more general problem of correlated response data and marginal regression analysis with individual level covariates.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Pepe_Anderson_1994_A cautionary note on inference for marginal regression models with longitudinal.pdf}
}

@article{peraltaAssociationAfricanAncestry2010,
  title = {The {{Association}} of {{African Ancestry}} and {{Elevated Creatinine}} in the {{Coronary Artery Risk Development}} in {{Young Adults}} ({{CARDIA}}) {{Study}}},
  author = {Peralta, Carmen A. and Risch, Neil and Lin, Feng and Shlipak, Michael G. and Reiner, Alex and Ziv, Elad and Tang, Hua and Siscovick, David and {Bibbins-Domingo}, Kirsten},
  year = {2010},
  journal = {American Journal of Nephrology},
  volume = {31},
  number = {3},
  pages = {202--208},
  publisher = {Karger Publishers},
  issn = {0250-8095, 1421-9670},
  doi = {10.1159/000268955},
  urldate = {2022-08-19},
  abstract = {Whether genetic factors account for differences in early kidney disease among blacks in a young healthy population is not well known. We evaluated the association of self-reported race and genetic African ancestry with elevated creatinine ({$\geqq$}1.3 mg/dl for men, {$\geqq$}1.1 mg/dl for women) among 3,113 black and white participants in the Coronary Artery Risk Development in Young Adults (CARDIA) study, ages 38--50 years. We estimated individual African ancestry using 42 ancestry informative markers. Blacks were more likely to have elevated creatinine than whites, and this effect was more pronounced in men: adjusted odds ratio (AOR) for black versus white men = 7.03, 4.15--11.91; AOR for women = 2.40, 1.15--5.02. Higher African ancestry was independently associated with elevated creatinine among black men (AOR = 1.53,1.08--2.16 per SD increase in African ancestry), but not women. A graded increase in odds of elevated creatinine by African Ancestry was observed among black men compared with white men: AOR = 4.27 (2.26--10.06) for black men with 40--70\% African ancestry; AOR = 8.09 (4.19--15.61) for black men with 70--80\% African ancestry; AOR = 9.05 (4.81--17.02) for black men with {$>$}80\% African ancestry. Genetic factors common to African ancestry may be associated with increased risk of early kidney dysfunction in a young, healthy population, particularly among black men.},
  langid = {english},
  pmid = {20029176},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Peralta et al_2010_The Association of African Ancestry and Elevated Creatinine in the Coronary.pdf;/Users/nseewald/Zotero/storage/8SC5RM4E/268955.html}
}

@article{peskunTwoTailedValuesCoherent2020,
  ids = {peskunTwoTailedValuesCoherent2020a},
  title = {Two-{{Tailed}} p -{{Values}} and {{Coherent Measures}} of {{Evidence}}},
  author = {Peskun, Peter H.},
  year = {2020},
  month = jan,
  journal = {The American Statistician},
  volume = {74},
  number = {1},
  pages = {80--86},
  issn = {Undefined},
  doi = {10/ggk5sn},
  urldate = {2020-02-18},
  keywords = {nosource},
  annotation = {Saved from BrowZine: http://thirdiron.com/download},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Peskun_2020_Two-Tailed p -Values and Coherent Measures of Evidence.pdf}
}

@article{pfammatterSMARTStudyProtocol2019,
  title = {{{SMART}}: {{Study}} Protocol for a Sequential Multiple Assignment Randomized Controlled Trial to Optimize Weight Loss Management},
  shorttitle = {{{SMART}}},
  author = {Pfammatter, Angela Fidler and {Nahum-Shani}, Inbal and DeZelar, Margaret and Scanlan, Laura and McFadden, H. Gene and Siddique, Juned and Hedeker, Donald and Spring, Bonnie},
  year = {2019},
  month = jul,
  journal = {Contemporary Clinical Trials},
  volume = {82},
  pages = {36--45},
  issn = {15517144},
  doi = {10/gjhm5n},
  urldate = {2021-03-18},
  abstract = {Background: Stepped care is a rational resource allocation approach to reduce population obesity. Evidence is lacking to guide decisions on use of low cost treatment components such as mobile health (mHealth) tools without compromising weight loss of those needing more expensive traditional treatment components (e.g., coaching, meal replacement). A sequential multiple assignment randomization trial (SMART) will be conducted to inform the development of an empirically based stepped care intervention that incorporates mHealth and traditional treatment components.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Pfammatter et al_2019_SMART.pdf}
}

@article{pirondiniCovariateAdjustmentCardiovascular2022,
  title = {Covariate {{Adjustment}} in {{Cardiovascular Randomized Controlled Trials}}},
  author = {Pirondini, Leah and Gregson, John and Owen, Ruth and Collier, Tim and Pocock, Stuart},
  year = {2022},
  month = may,
  journal = {JACC: Heart Failure},
  volume = {10},
  number = {5},
  pages = {297--305},
  publisher = {American College of Cardiology Foundation},
  doi = {10.1016/j.jchf.2022.02.007},
  urldate = {2024-10-18},
  keywords = {covariate adjustment,randomized controlled trials,statistics},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Pirondini et al. - 2022 - Covariate Adjustment in Cardiovascular Randomized Controlled Trials.pdf}
}

@misc{PoissonFrequencyModels,
  title = {Poisson Frequency Models in Glmnet {\textbar} Data Science Topics for Actuaries},
  urldate = {2024-09-08},
  howpublished = {https://actuary.rbind.io/post/glmnet-frequency/},
  file = {/Users/nseewald/Zotero/storage/RDCYTDK8/glmnet-frequency.html}
}

@article{powellMedicalMarijuanaLaws2018,
  title = {Do Medical Marijuana Laws Reduce Addictions and Deaths Related to Pain Killers?},
  author = {Powell, David and Pacula, Rosalie Liccardo and Jacobson, Mireille},
  year = {2018},
  month = mar,
  journal = {Journal of Health Economics},
  volume = {58},
  pages = {29--42},
  issn = {0167-6296},
  doi = {10.1016/j.jhealeco.2017.12.007},
  urldate = {2022-06-17},
  abstract = {Recent work finds that medical marijuana laws reduce the daily doses filled for opioid analgesics among Medicare Part-D and Medicaid enrollees, as well as population-wide opioid overdose deaths. We replicate the result for opioid overdose deaths and explore the potential mechanism. The key feature of a medical marijuana law that facilitates a reduction in overdose death rates is a relatively liberal allowance for dispensaries. As states have become more stringent in their regulation of dispensaries, the protective value generally has fallen. These findings suggest that broader access to medical marijuana facilitates substitution of marijuana for powerful and addictive opioids.},
  langid = {english},
  keywords = {Dispensaries,Medical marijuana,Mortality,Opioids,Pain killers,Substance abuse},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Powell et al_2018_Do medical marijuana laws reduce addictions and deaths related to pain killers.pdf;/Users/nseewald/Zotero/storage/NVUKE6C8/S0167629617311852.html}
}

@article{prenticeCorrelatedBinaryRegression1988,
  title = {Correlated {{Binary Regression}} with {{Covariates Specific}} to {{Each Binary Observation}}},
  author = {Prentice, Ross L.},
  year = {1988},
  journal = {Biometrics},
  volume = {44},
  number = {4},
  eprint = {2531733},
  eprinttype = {jstor},
  pages = {1033--1048},
  issn = {0006-341X},
  doi = {10/fwpgpv},
  urldate = {2019-08-14},
  abstract = {Regression methods are considered for the analysis of correlated binary data when each binary observation may have its own covariates. It is argued that binary response models that condition on some or all binary responses in a given "block" are useful for studying certain types of dependencies, but not for the estimation of marginal response probabilities or pairwise correlations. Fully parametric approaches to these latter problems appear to be unduly complicated except in such special cases as the analysis of paired binary data. Hence, a generalized estimating equation approach is advocated for inference on response probabilities and correlations. Illustrations involving both small and large block sizes are provided.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Prentice_1988_Correlated Binary Regression with Covariates Specific to Each Binary Observation.pdf}
}

@misc{PrevalenceChronicPain,
  title = {Prevalence of Chronic Pain among Adults in the {{United States}} : {{PAIN}}},
  urldate = {2023-06-09},
  howpublished = {https://journals.lww.com/pain/Abstract/2022/02000/Prevalence\_of\_chronic\_pain\_among\_adults\_in\_the.31.aspx},
  file = {/Users/nseewald/Zotero/storage/CX7PSACZ/Prevalence_of_chronic_pain_among_adults_in_the.31.html}
}

@article{priceMHealthMechanismDeliver2014,
  title = {{{mHealth}}: {{A Mechanism}} to {{Deliver More Accessible}}, {{More Effective Mental Health Care}}: {{mHealth Opportunities}}},
  shorttitle = {{{mHealth}}},
  author = {Price, Matthew and Yuen, Erica K. and Goetter, Elizabeth M. and Herbert, James D. and Forman, Evan M. and Acierno, Ron and Ruggiero, Kenneth J.},
  year = {2014},
  month = sep,
  journal = {Clinical Psychology \& Psychotherapy},
  volume = {21},
  number = {5},
  pages = {427--436},
  issn = {10633995},
  doi = {10.1002/cpp.1855},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Price et al_2014_mHealth.pdf}
}

@misc{PromisingSubgroupIdentification,
  title = {A Promising Subgroup Identification Method Based on a Genetic Algorithm for Censored Survival Data: {{Journal}} of {{Biopharmaceutical Statistics}}: {{Vol}} 34 , {{No}} 1 - {{Get Access}}},
  urldate = {2024-06-18},
  howpublished = {https://www.tandfonline.com/doi/full/10.1080/10543406.2023.2170397},
  file = {/Users/nseewald/Zotero/storage/7KUK9N4K/10543406.2023.html}
}

@misc{PropensityScoreWeighting,
  title = {Propensity Score Weighting under Limited Overlap and Model Misspecification - {{Yunji Zhou}}, {{Roland A Matsouaka}}, {{Laine Thomas}}, 2020},
  urldate = {2024-09-16},
  howpublished = {https://journals-sagepub-com.proxy.library.upenn.edu/doi/full/10.1177/0962280220940334?casa\_token=B4cdpNxpGTEAAAAA\%3A9bKK1c7cImS1aJqejtfvldjeuGYj9BKT3TCXJcChjQMh3Qswb6H6CYWbGa7lK-7ESbUFNKLS0Z11TQ},
  file = {/Users/nseewald/Zotero/storage/TVVPSNKG/0962280220940334.html}
}

@book{proschanStatisticalThinkingClinical2022,
  title = {Statistical Thinking in Clinical Trials},
  author = {Proschan, Michael A. and {Barreiro-Gomez}, Julian},
  year = {2022},
  series = {Chapman \& {{Hall}}/{{CRC}} Biostatistics Series},
  edition = {First edition},
  publisher = {CRC Press},
  address = {Boca Raton London New York},
  doi = {10.1201/9781315164090},
  abstract = {1. Evidence and Inference Terminology and Paradigm of Inference Classical Inference Hypothesis Tests and P-Values Confidence Intervals Criticisms of Classical Methods The Bayesian Approach Large Sample Inference Robust Methods Are Preferred in Clinical Trials Summary 2. 2 {\copyright}{$_7$} 2 Tables Measures of Treatment Effect Exact Tests and Confidence Intervals Fisher{\'undefined}{$_9$}s Exact Test Exact Confidence Interval for Odds Ratio Oddities of Fisher{\'undefined}{$_9$}s Exact Test and Confidence Interval Unconditional Tests As Alternatives to Fisher{\'undefined}{$_9$}s Exact Test Appendix: P(X = x {\textbar} S = s) in Table Summary 3. Introduction to Clinical Trials Summary 4. Design of Clinical Trials Different Phases of Trials Blinding Baseline Variables Controls Regression to the Mean Appropriate Control Choice of Primary Endpoint Reducing Variability Replication and Averaging Differencing Stratification Regression Different Types of Trials Superiority Versus Noninferiority Parallel Arm Trials Crossover Trials Cluster-Randomized Trials Multi-Arm Trials Appendix: The Geometry of Stratification Summary 5. Randomization/Allocation Sanctity and Placement of Randomization Simple Randomization Permuted Block Randomization Biased Coin Randomization Stratified Randomization Minimization and Covariate-Adaptive Randomization Response-Adaptive Randomization Adaptive Randomization And Temporal TrendsSummary 6. Randomization-Based Inference Introduction Paired Data An Example Control of Conditional Type Error Rate Asymptotic Equivalence to a T-test The Null Hypothesis and Generalizing Does A Re-randomization Test Assume Independence? Unpaired Data: Traditional Randomization Introduction Control of Conditional Type Error Rate The Null Hypothesis and Generalizing Does a Re-randomization Test Require Independence? Asymptotic Equivalence to a t-Test Protection Against Temporal Trends Fisher{\'undefined}{$_9$}s Exact Test As a Re-Randomization Test Unpaired Data: Covariate-Adaptive Randomization Introduction Control of Conditional Type Error Rate Protection Against Temporal Trends A More Rigorous Null Hypothesis Unpaired Data: Response-Adaptive Randomization Introduction Re-randomization Tests \& Strength of Randomized Evidence Confidence Intervals A Philosophical Criticism of Re-randomization Tests Appendix: The Permutation Variance of {\textasciimacron} YC {\'undefined}{$_2$} {\textasciimacron} YT Summary 7. Survival Analysis Introduction to Survival Methods Comparing Survival Across Arms Comparing Survival At A Specific Time The Logrank Test The Hazard Rate and Cox Model Competing Risk Analysis Parametric Approaches Conditional Binomial Procedure Appendix: Partial Likelihood Summary 8. Sample Size/Power Introduction The EZ Principle Illustrated through the -Sample t-Test Important Takeaways from the EZ Principle EZ Principle Applied More Generally -Sample t-test Test of Proportions Logrank Test Cluster-Randomized Trials In a Nutshell Nonzero Nulls Practical Aspects of Sample Size Calculations Test of Means Test of Proportions Specification of Treatment Effect Exact Power t-Tests Exact Power for Fisher{\'undefined}{$_9$}s Exact Test Adjusting for Noncompliance and Other Factors Appendix: Other Sample Size Formulas for Two Proportions Summary 9. Monitoring Introduction Efficacy Monitoring A Brief History of Efficacy Boundaries Z-scores, B-Values, and Information Revisiting O{\'undefined}{$_9$}Brien-Fleming Alpha Spending Functions The Effect of Monitoring on Power Small Sample Sizes Futility Monitoring What is Futility? Conditional Power Beta Spending Functions Practical Aspects of Monitoring Inference after A Monitored Trial Statistical Contrast between Unmonitored and Monitored Trials Defining A P-Value after a Monitored Trial Defining A Confidence Interval after A Monitored Trial Correcting Bias after A Monitored Trial Bayesian Monitoring Summary 10. M\&Ms: Multiplicity \& Missing Data Introduction Multiple Comparisons The Debate Control of Familywise Error Rate (FWER) Showing Strong Control by Enumeration Intuition Behind Multiple Comparison Procedures Independent Comparisons Closure Principle The Dunnett Procedure And A Conditioning Technique Missing Data Definitions And An Example Methods for Data That Are MAR Sensitivity Analyses Summary 11. Adaptive Methods Introduction Adaptive Sample Size Based on Nuisance Parameters Continuous Outcomes Binary Outcomes Adaptive Sample Size Based on Treatment Effect Introduction and Notation Non-adaptive Two-Stage Setting Adaptation Principle Bauer-K{\textcircledP}{$\cdot$}ohne () Proschan and Hunsberger, () Criticisms of Adaptive Methods Based on The Treatment Effect Unplanned Changes before Breaking the Blind Summary Index},
  isbn = {978-1-138-05856-9 978-1-315-16409-0},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Proschan and Barreiro-Gomez - 2022 - Statistical thinking in clinical trials.epub}
}

@article{prosperiCausalInferenceCounterfactual2020,
  title = {Causal Inference and Counterfactual Prediction in Machine Learning for Actionable Healthcare},
  author = {Prosperi, Mattia and Guo, Yi and Sperrin, Matt and Koopman, James S. and Min, Jae S. and He, Xing and Rich, Shannan and Wang, Mo and Buchan, Iain E. and Bian, Jiang},
  year = {2020},
  month = jul,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {7},
  pages = {369--375},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0197-y},
  urldate = {2022-08-18},
  abstract = {Big data, high-performance computing, and (deep) machine learning are increasingly becoming key to precision medicine---from identifying disease risks and taking preventive measures, to making diagnoses and personalizing treatment for individuals. Precision medicine, however, is not only about predicting risks and outcomes, but also about weighing interventions. Interventional clinical predictive models require the correct specification of cause and effect, and the calculation of so-called counterfactuals, that is, alternative scenarios. In biomedical research, observational studies are commonly affected by confounding and selection bias. Without robust assumptions, often requiring a priori domain knowledge, causal inference is not feasible. Data-driven prediction models are often mistakenly used to draw causal effects, but neither their parameters nor their predictions necessarily have a causal interpretation. Therefore, the premise that data-driven prediction models lead to trustable decisions/interventions for precision medicine is questionable. When pursuing intervention modelling, the bio-health informatics community needs to employ causal approaches and learn causal structures. Here we discuss how target trials (algorithmic emulation of randomized studies), transportability (the licence to transfer causal effects from one population to another) and prediction invariance (where a true causal model is contained in the set of all prediction models whose accuracy does not vary across different settings) are linchpins to developing and testing intervention models.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Education,Machine learning,Outcomes research,Research management},
  file = {/Users/nseewald/Zotero/storage/B4KCADJI/s42256-020-0197-y.html}
}

@article{pullenayegumLongitudinalDataSubject2016,
  title = {Longitudinal Data Subject to Irregular Observation: {{A}} Review of Methods with a Focus on Visit Processes, Assumptions, and Study Design},
  shorttitle = {Longitudinal Data Subject to Irregular Observation},
  author = {Pullenayegum, Eleanor M and Lim, Lily SH},
  year = {2016},
  month = dec,
  journal = {Statistical Methods in Medical Research},
  volume = {25},
  number = {6},
  pages = {2992--3014},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280214536537},
  urldate = {2018-10-12},
  abstract = {When data are collected longitudinally, measurement times often vary among patients. This is of particular concern in clinic-based studies, for example retrospective chart reviews. Here, typically no two patients will share the same set of measurement times and moreover, it is likely that the timing of the measurements is associated with disease course; for example, patients may visit more often when unwell. While there are statistical methods that can help overcome the resulting bias, these make assumptions about the nature of the dependence between visit times and outcome processes, and the assumptions differ across methods. The purpose of this paper is to review the methods available with a particular focus on how the assumptions made line up with visit processes encountered in practice. Through this we show that no one method can handle all plausible visit scenarios and suggest that careful analysis of the visit process should inform the choice of analytic method for the outcomes. Moreover, there are some commonly encountered visit scenarios that are not handled well by any method, and we make recommendations with regard to study design that would minimize the chances of these problematic visit scenarios arising.},
  langid = {english},
  keywords = {correlated,informative observation,inverse-,Longitudinal data,observational study,random effects},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Pullenayegum_Lim_2016_Longitudinal data subject to irregular observation.pdf}
}

@article{qaqishFamilyMultivariateBinary2003,
  title = {A Family of Multivariate Binary Distributions for Simulating Correlated Binary Variables with Specified Marginal Means and Correlations},
  author = {Qaqish, Bahjat F.},
  year = {2003},
  journal = {Biometrika},
  volume = {90},
  number = {2},
  pages = {455--463},
  issn = {00063444},
  doi = {10.1093/biomet/90.2.455},
  abstract = {... The latter step generally requires a Choleski decomposition and generation of n standard normal variates ... i in (3) is maximised over the 2i-1 possible y 1 ,...,y i-1 sequences ... This is not a big limitation since the difficulty in simulating correlated binary observations lies mainly with ... \${\textbackslash}backslash\$n},
  keywords = {Autoregressive correlation structure,Clustered data,Correlated Bernoulli variates,Exchangeable correlation structure,Simulation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Qaqish_2003_A family of multivariate binary distributions for simulating correlated binary.pdf}
}

@incollection{qianDynamicTreatmentRegimes2013,
  title = {Dynamic {{Treatment Regimes}}},
  booktitle = {Modern {{Clinical Trial Analysis}}},
  author = {Qian, Min and {Nahum-Shani}, Inbal and Murphy, Susan A.},
  editor = {Tang, Wan and Tu, Xin},
  year = {2013},
  series = {Applied {{Bioinformatics}} and {{Biostatistics}} in {{Cancer Research}}},
  pages = {127--148},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4614-4322-3_5},
  urldate = {2021-03-27},
  abstract = {Recent research (see Lavori and Dawson 2000, 2004) stresses the need to take into account patients' heterogeneity in need for treatment when developing intervention programs. In order to improve patient care the type of treatment and the dosage should vary by patients. Additionally, in many cases, the need for treatment may change over time, yielding repeated opportunities to adapt the intervention.},
  isbn = {978-1-4614-4322-3},
  langid = {english},
  keywords = {Approximation Space,Decision Point,Primary Research Question,Randomization Probability,Sample Size Formula},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Qian et al_2013_Dynamic Treatment Regimes.pdf}
}

@article{qianLinearMixedModels2020,
  title = {Linear {{Mixed Models}} with {{Endogenous Covariates}}: {{Modeling Sequential Treatment Effects}} with {{Application}} to a {{Mobile Health Study}}},
  shorttitle = {Linear {{Mixed Models}} with {{Endogenous Covariates}}},
  author = {Qian, Tianchen and Klasnja, Predrag and Murphy, Susan A.},
  year = {2020},
  month = aug,
  journal = {Statistical Science},
  volume = {35},
  number = {3},
  pages = {375--390},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/19-STS720},
  urldate = {2023-02-23},
  abstract = {Mobile health is a rapidly developing field in which behavioral treatments are delivered to individuals via wearables or smartphones to facilitate health-related behavior change. Micro-randomized trials (MRT) are an experimental design for developing mobile health interventions. In an MRT, the treatments are randomized numerous times for each individual over course of the trial. Along with assessing treatment effects, behavioral scientists aim to understand between-person heterogeneity in the treatment effect. A natural approach is the familiar linear mixed model. However, directly applying linear mixed models is problematic because potential moderators of the treatment effect are frequently endogenous---that is, may depend on prior treatment. We discuss model interpretation and biases that arise in the absence of additional assumptions when endogenous covariates are included in a linear mixed model. In particular, when there are endogenous covariates, the coefficients no longer have the customary marginal interpretation. However, these coefficients still have a conditional-on-the-random-effect interpretation. We provide an additional assumption that, if true, allows scientists to use standard software to fit linear mixed model with endogenous covariates, and person-specific predictions of effects can be provided. As an illustration, we assess the effect of activity suggestion in the HeartSteps MRT and analyze the between-person treatment effect heterogeneity.},
  keywords = {Causal inference,endogenous covariates,linear mixed model,micro-randomized trial},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Qian et al_2020_Linear Mixed Models with Endogenous Covariates.pdf}
}

@article{qianPerformanceGuaranteesIndividualized2011,
  title = {Performance Guarantees for Individualized Treatment Rules},
  author = {Qian, Min and Murphy, Susan A.},
  year = {2011},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {39},
  number = {2},
  pages = {1180--1210},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/10-AOS864},
  urldate = {2024-10-18},
  abstract = {Because many illnesses show heterogeneous response to treatment, there is increasing interest in individualizing treatment to patients [Arch. Gen. Psychiatry 66 (2009) 128--133]. An individualized treatment rule is a decision rule that recommends treatment according to patient characteristics. We consider the use of clinical trial data in the construction of an individualized treatment rule leading to highest mean response. This is a difficult computational problem because the objective function is the expectation of a weighted indicator function that is nonconcave in the parameters. Furthermore, there are frequently many pretreatment variables that may or may not be useful in constructing an optimal individualized treatment rule, yet cost and interpretability considerations imply that only a few variables should be used by the individualized treatment rule. To address these challenges, we consider estimation based on l1-penalized least squares. This approach is justified via a finite sample upper bound on the difference between the mean response due to the estimated individualized treatment rule and the mean response due to the optimal individualized treatment rule.},
  keywords = {62H99,62J07,62P10,decision making,l1-penalized least squares,value},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Qian_Murphy_2011_Performance_guarantees_for_individualized_treatment_rules.pdf;/Users/nseewald/Zotero/storage/8QDZNVMQ/Qian and Murphy - 2011 - Performance guarantees for individualized treatmen.pdf}
}

@article{quanbeckBalancedOpioidInitiative2020,
  title = {The {{Balanced Opioid Initiative}}: Protocol for a Clustered, Sequential, Multiple-Assignment Randomized Trial to Construct an Adaptive Implementation Strategy to Improve Guideline-Concordant Opioid Prescribing in Primary Care},
  shorttitle = {The {{Balanced Opioid Initiative}}},
  author = {Quanbeck, Andrew and Almirall, Daniel and Jacobson, Nora and Brown, Randall T. and Landeck, Jillian K. and Madden, Lynn and Cohen, Andrew and Deyo, Brienna M. F. and Robinson, James and Johnson, Roberta A. and Schumacher, Nicholas},
  year = {2020},
  month = dec,
  journal = {Implementation Science},
  volume = {15},
  number = {1},
  pages = {26},
  issn = {1748-5908},
  doi = {10/gjh5tx},
  urldate = {2021-03-22},
  abstract = {Methods/design: The study is a hybrid type 3 clustered, sequential, multiple-assignment randomized trial (SMART) that randomizes clinics from two health systems at two points, months 3 and 9, of a 21-month intervention. Clinics are provided one of four sequences of implementation strategies: a condition consisting of educational/ engagement meetings and audit and feedback alone (EM/AF), EM/AF plus practice facilitation (PF), EM/AF + prescriber peer consulting (PPC), and EM/AF + PF + PPC. The study's primary outcome is morphine-milligram equivalent (MME) dose by prescribing clinicians within clinics. The study's primary aim is the comparison of EM/AF + PF + PPC versus EM/AF alone on change in MME from month 3 to month 21. The secondary aim is to derive cost estimates for each of the four sequences and compare them. The exploratory aim is to examine four tailoring variables that can be used to construct an adaptive implementation strategy to meet the needs of different primary care clinics. Discussion: Systems consultation is a practical blend of implementation strategies used in this case to improve opioid prescribing practices in primary care. The blend offers a range of strategies in sequences from minimally to substantially intensive. The results of this study promise to help us understand how to cost effectively improve the implementation of evidence-based practices. Trial registration: NCT 04044521 (ClinicalTrials.gov). Registered 05 August 2019.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Quanbeck et al_2020_The Balanced Opioid Initiative.pdf}
}

@article{quImprovingGeneralisedEstimating2000,
  title = {Improving Generalised Estimating Equations Using Quadratic Inference Functions},
  author = {Qu, A. and Lindsay, Bruce G. and Li, Bing},
  year = {2000},
  month = dec,
  journal = {Biometrika},
  volume = {87},
  number = {4},
  pages = {823--836},
  issn = {0006-3444, 1464-3510},
  doi = {10/fb2tfq},
  urldate = {2019-08-15},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Qu et al_2000_Improving generalised estimating equations using quadratic inference functions.pdf}
}

@article{rabbiReVibeContextassistedEvening2019,
  title = {{{ReVibe}}: {{A Context-assisted Evening Recall Approach}} to {{Improve Self-report Adherence}}},
  shorttitle = {{{ReVibe}}},
  author = {Rabbi, Mashfiqui and Li, Katherine and Yan, H. Yanna and Hall, Kelly and Klasnja, Predrag and Murphy, Susan},
  year = {2019},
  month = dec,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {3},
  number = {4},
  pages = {1--27},
  issn = {2474-9567},
  doi = {10.1145/3369806},
  urldate = {2022-04-28},
  abstract = {Besides passive sensing, ecological momentary assessments (EMAs) are one of the primary methods to collect in-the-moment data in ubiquitous computing and mobile health. While EMAs have the advantage of low recall bias, a disadvantage is that they frequently interrupt the user and thus long-term adherence is generally poor. In this paper, we propose a less-disruptive self-reporting method, "assisted recall," in which in the evening individuals are asked to answer questions concerning a moment from earlier in the day assisted by contextual information such as location, physical activity, and ambient sounds collected around the moment to be recalled. Such contextual information is automatically collected from phone sensor data, so that self-reporting does not require devices other than a smartphone. We hypothesized that providing assistance based on such automatically collected contextual information would increase recall accuracy (i.e., if recall responses for a moment match the EMA responses at the same moment) as compared to no assistance, and we hypothesized that the overall completion rate of evening recalls (assisted or not) would be higher than for in-the-moment EMAs. We conducted a two-week study (N=54) where participants completed recalls and EMAs each day. We found that providing assistance via contextual information increased recall accuracy by 5.6\% (p = 0.032) and the overall recall completion rate was on average 27.8\% (p {$<$} 0.001) higher than that of EMAs.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rabbi et al_2019_ReVibe.pdf}
}

@misc{RaciallybiasedAlgorithmDelaying,
  title = {Is a Racially-Biased Algorithm Delaying Health Care for One Million {{Black}} People?},
  urldate = {2022-08-19},
  howpublished = {https://www.nature.com/articles/d41586-020-03419-6},
  file = {/Users/nseewald/Zotero/storage/2CJ6ZE4G/d41586-020-03419-6.html}
}

@article{rahmanStoppingEfficacySinglearm2022,
  title = {Stopping for Efficacy in Single-Arm Phase {{II}} Clinical Trials},
  author = {Rahman, Rezoanoor and Iftakhar Alam, M.},
  year = {2022},
  month = jul,
  journal = {Journal of Applied Statistics},
  volume = {49},
  number = {10},
  pages = {2447--2466},
  publisher = {Taylor \& Francis},
  issn = {0266-4763},
  doi = {10.1080/02664763.2021.1904846},
  urldate = {2024-01-11},
  abstract = {Phase II clinical trials investigate whether a new drug or treatment has sufficient evidence of effectiveness against the disease under study. Two-stage designs are popular for phase II since they can stop in the first stage if the drug is ineffective. Investigators often face difficulties in determining the target response rates, and adaptive designs can help to set the target response rate tested in the second stage based on the number of responses observed in the first stage. Popular adaptive designs consider two alternate response rates, and they generally minimise the expected sample size at the maximum uninterested response rate. Moreover, these designs consider only futility as the reason for early stopping and have high expected sample sizes if the provided drug is effective. Motivated by this problem, we propose an adaptive design that enables us to terminate the single-arm trial at the first stage for efficacy and conclude which alternate response rate to choose. Comparing the proposed design with a popular adaptive design from literature reveals that the expected sample size decreases notably if any of the two target response rates are correct. In contrast, the expected sample size remains almost the same under the null hypothesis.},
  keywords = {_tablet_modified,optimal design,Phase II trial,sample size,single-arm trial,two-stage design},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rahman_Iftakhar Alam_2022_Stopping for efficacy in single-arm phase II clinical trials.pdf}
}

@inproceedings{raijPrivacyRisksEmerging2011,
  title = {Privacy Risks Emerging from the Adoption of Innocuous Wearable Sensors in the Mobile Environment},
  booktitle = {Proceedings of the 2011 Annual Conference on {{Human}} Factors in Computing Systems - {{CHI}} '11},
  author = {Raij, Andrew and Ghosh, Animikh and Kumar, Santosh and Srivastava, Mani},
  year = {2011},
  pages = {11},
  publisher = {ACM Press},
  address = {Vancouver, BC, Canada},
  doi = {10.1145/1978942.1978945},
  urldate = {2018-10-12},
  abstract = {Wearable sensors are revolutionizing healthcare and science by enabling capture of physiological, psychological, and behavioral measurements in natural environments. However, these seemingly innocuous measurements can be used to infer potentially private behaviors such as stress, conversation, smoking, drinking, illicit drug usage, and others. We conducted a study to assess how concerned people are about disclosure of a variety of behaviors and contexts that are embedded in wearable sensor data. Our results show participants are most concerned about disclosures of conversation episodes and stress --- inferences that are not yet widely publicized. These concerns are mediated by temporal and physical context associated with the data and the participant's personal stake in the data. Our results provide key guidance on the extent to which people understand the potential for harm and data characteristics researchers should focus on to reduce the perceived harm from such datasets.},
  isbn = {978-1-4503-0228-9},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Raij et al_2011_Privacy risks emerging from the adoption of innocuous wearable sensors in the.pdf}
}

@article{rajiAssociationCannabisLaws2019,
  title = {Association between Cannabis Laws and Opioid Prescriptions among Privately Insured Adults in the {{US}}},
  author = {Raji, Mukaila A. and Abara, N. Ogechi and Salameh, Habeeb and Westra, Jordan R. and Kuo, Yong-Fang},
  year = {2019},
  month = aug,
  journal = {Preventive Medicine},
  volume = {125},
  pages = {62--68},
  issn = {0091-7435},
  doi = {10.1016/j.ypmed.2019.05.012},
  urldate = {2022-06-17},
  abstract = {We examine the association between opioid prescription patterns in privately insured adults and changes in state cannabis laws among five age groups (18--25, 26--35 36--45, 46--55 and 56--64\,years). Using the 2016 Clinformatics Data Mart, a nationwide commercial health insurance database, we performed a cross-sectional analysis of two types of opioid prescribing ({$>$}30-day and {$>$}90-day prescriptions) among all adults aged 18--64 based on the stringency of cannabis laws. We found a significant interaction between age and cannabis law on opioid prescriptions. Age-stratified multilevel multivariable analyses showed lower opioid prescription rates in the four younger age groups only in states with medical cannabis laws, when considering both {$>$}30\,day and {$>$}90\,day opioid use [{$>$}30\,day adjusted odds ratio (aOR)\,=\,0.56, in 18--25, aOR\,=\,0.67 in 26--35, aOR\,=\,0.67 in 36--45, and aOR\,=\,0.76 in 46--54\,years; {$>$}90\,day aOR\,=\,0.56, in 18--25, aOR\,=\,0.68 in 26--35, aOR\,=\,0.69 in 36--45, and aOR\,=\,0.77 in 46--54\,years, P\,{$<$}\,0.0001 for all]. This association was not significant in the oldest age group of 55--64\,years. There was no significant association between opioid prescriptions and other categories of cannabis laws (recreational use and decriminalization) in any of the age groups studied.},
  langid = {english},
  keywords = {Cannabis,Commercial insurance,Cross-sectional,Marijuana,Medical cannabis,Opioid,Prescription},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Raji et al_2019_Association between cannabis laws and opioid prescriptions among privately.pdf;/Users/nseewald/Zotero/storage/YZMFXDDH/S0091743519301860.html}
}

@article{ramanathanLegalEpidemiologyScience2017,
  title = {Legal {{Epidemiology}}: {{The Science}} of {{Law}}},
  shorttitle = {Legal {{Epidemiology}}},
  author = {Ramanathan, Tara and Hulkower, Rachel and Holbrook, Joseph and Penn, Matthew},
  year = {2017},
  month = mar,
  journal = {The Journal of Law, Medicine \& Ethics},
  volume = {45},
  number = {1\_suppl},
  pages = {69--72},
  publisher = {SAGE Publications Inc},
  issn = {1073-1105},
  doi = {10.1177/1073110517703329},
  urldate = {2023-08-16},
  abstract = {The importance of legal epidemiology in public health law research has undoubtedly grown over the last five years. Scholars and practitioners together have developed guidance on best practices for the field, including: placing emphasis on transdisciplinary collaborations; creating valid, reliable, and repeatable research; and publishing timely products for use in decision-making and change. Despite the energy and expertise researchers have brought to this important work, they name significant challenges in marshalling the diverse skill sets, quality controls, and funding to implement legal epidemiology activities. The Centers for Disease Control and Prevention (CDC) has worked to develop cross-cutting research and translation on issues of national priority in legal epidemiology, and has explored ways to overcome some of these challenges. As such, this article describes a case study of the use of law to characterize states' prior authorization policies regarding medication used to treat attention-deficit/hyperactivity disorder (ADHD), a central component of a broader effort to improve behavior therapy options for young children with ADHD. This article highlights the types of legal epidemiology work we have undertaken, the application of this work to an emerging public health problem, and the lessons learned in creating impactful research for the field.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ramanathan et al_2017_Legal Epidemiology.pdf}
}

@article{randolphTris5ditertbutylcatecholatoMolybdenum2013,
  title = {Tris(3,5-Di-Tert-Butylcatecholato)Molybdenum({{VI}}): {{Lewis}} Acidity and Nonclassical Oxygen Atom Transfer Reactions.},
  author = {Randolph, Amanda H and Seewald, Nicholas J and Rickert, Karl and Brown, Seth N},
  year = {2013},
  journal = {Inorganic chemistry},
  volume = {52},
  number = {21},
  eprint = {24147870},
  eprinttype = {pubmed},
  pages = {12587--98},
  issn = {1520-510X},
  doi = {10.1021/ic401736f},
  abstract = {In the solid state, tris(3,5-di-tert-butylcatecholato)molybdenum(VI) forms a dimer with seven-coordinate molybdenum and bridging catecholates. NMR spectroscopy indicates that the dimeric structure is retained in solution. The molybdenum center has a high affinity for Lewis bases such as pyridine or pyridine-N-oxide, forming seven-coordinate monomers with a capped octahedral geometry, as illustrated by the solid-state structure of (3,5-(t)Bu2Cat)3Mo(py). Structural data indicate that the complexes are best considered as Mo(VI) with substantial {$\pi$} donation from the nonbridging catecholates to molybdenum. Both the dimeric and the monomeric tris(catecholates) react rapidly with water to form free catechol and oxomolybdenum bis(catecholate) complexes. Monooxomolybdenum complexes are also obtained, more slowly, on reaction with dioxygen, with organic products consisting mostly of 3,5-di-tert-butyl-1,2-benzoquinone with minor amounts of the extradiol oxidation product 4,6-di-tert-butyl-1-oxacyclohepta-4,6-diene-2,3-dione. The pyridine-N-oxide complex reacts on heating (with excess pyO) to form initially (3,5-(t)Bu2Cat)2MoO(Opy) and ultimately MoO3(Opy), with quinone and free pyridine as the only organic products. The decay of (3,5-(t)Bu2Cat)3Mo(Opy) shows an accelerated, autocatalytic profile because the oxidation of its product, (3,5-(t)Bu2Cat)2MoO(Opy), produces an oxo-rich, catecholate-poor intermediate which rapidly conproportionates with (3,5-(t)Bu2Cat)3Mo(Opy), providing an additional pathway for its conversion to the mono-oxo product. The tris(catecholate) fragment Mo(3,5-(t)Bu2Cat)3 deoxygenates Opy in this nonclassical oxygen atom transfer reaction slightly less rapidly than does its oxidized product, MoO(3,5-(t)Bu2Cat)2.},
  copyright = {All rights reserved},
  pmid = {24147870},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Randolph et al_2013_Tris(3,5-di-tert-butylcatecholato)molybdenum(VI).pdf}
}

@article{raoInterplaySampleSurvey2005,
  title = {Interplay {{Between Sample Survey Theory}} and {{Practice}}: An {{Appraisal}}},
  author = {Rao, J N K},
  year = {2005},
  journal = {Survey Methodology},
  volume = {31},
  number = {2},
  pages = {117--138},
  abstract = {A large part of sample survey theory has been directly motivated by practical problems encountered in the design and analysis of sample surveys. On the other hand, sample survey theory has influenced practice, often leading to significant improvements. This paper will examine this interplay over the past 60 years or so. Examples where new theory is needed or where theory exists but is not used will also be presented.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rao_2005_Interplay Between Sample Survey Theory and Practice.pdf}
}

@article{rassenInstrumentalVariablesInstrumental2009,
  title = {Instrumental Variables {{I}}: Instrumental Variables Exploit Natural Variation in Nonexperimental Data to Estimate Causal Relationships},
  shorttitle = {Instrumental Variables {{I}}},
  author = {Rassen, Jeremy A. and Brookhart, M. Alan and Glynn, Robert J. and Mittleman, Murray A. and Schneeweiss, Sebastian},
  year = {2009},
  month = dec,
  journal = {Journal of Clinical Epidemiology},
  volume = {62},
  number = {12},
  pages = {1226--1232},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2008.12.005},
  urldate = {2024-07-10},
  abstract = {The gold standard of study design for treatment evaluation is widely acknowledged to be the randomized controlled trial (RCT). Trials allow for the estimation of causal effect by randomly assigning participants either to an intervention or comparison group; through the assumption of ``exchangeability'' between groups, comparing the outcomes will yield an estimate of causal effect. In the many cases where RCTs are impractical or unethical, instrumental variable (IV) analysis offers a nonexperimental alternative based on many of the same principles. IV analysis relies on finding a naturally varying phenomenon, related to treatment but not to outcome except through the effect of treatment itself, and then using this phenomenon as a proxy for the confounded treatment variable. This article demonstrates how IV analysis arises from an analogous but potentially impossible RCT design, and outlines the assumptions necessary for valid estimation. It gives examples of instruments used in clinical epidemiology and concludes with an outline on estimation of effects.},
  keywords = {Bias (epidemiology),Confounding factor (epidemiology),Instrumental variable,Pharmacoepidemiology,Physician prescribing preference,Unmeasuredconfounding},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rassen et al_2009_Instrumental variables I.pdf;/Users/nseewald/Zotero/storage/UYP54AYU/S0895435609000146.html}
}

@article{raudenbushEffectsStudyDuration2001,
  title = {Effects of Study Duration, Frequency of Observation, and Sample Size on Power in Studies of Group Differences in Polynomial Change.},
  author = {Raudenbush, Stephen W. and {Xiao-Feng}, Liu},
  year = {2001},
  journal = {Psychological Methods},
  volume = {6},
  number = {4},
  pages = {387--401},
  issn = {1082-989X},
  doi = {10.1037//1082-989X.6.4.387},
  urldate = {2018-11-17},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Raudenbush_Xiao-Feng_2001_Effects of study duration, frequency of observation, and sample size on power.pdf}
}

@article{raudenbushStatisticalAnalysisOptimal1997,
  title = {Statistical Analysis and Optimal Design for Cluster Randomized Trials},
  author = {Raudenbush, Stephen W.},
  year = {1997},
  month = jun,
  journal = {Psychological Methods},
  volume = {2},
  number = {2},
  pages = {173--185},
  publisher = {American Psychological Association},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.2.2.173},
  urldate = {2023-07-27},
  abstract = {In many intervention studies, therapy outcome evaluations, and educational field trials, random treatment assignment of clusters rather than persons is desirable for political feasibility, logistics, or ecological validity. However, cluster randomized designs are widely regarded as lacking statistical precision. This article considers when and to what extent using a pretreatment covariate can increase experimental precision. To answer this question, the author first optimizes allocation of resources within and between clusters for the no-covariate case. Optimal sample sizes at each level depend on variation within and between clusters and on the cost of sampling at each level. Next, the author considers optimal allocation when a covariate is added. In this case, the explanatory power of the covariate at each level becomes highly relevant for choosing optimal sample sizes. A key conclusion is that statistical analysis that fully uses information about the covariate-outcome relationship can substantially increase the efficiency of the cluster randomized trial, especially when the cost of sampling clusters is high and the covariate accounts for substantial variation between clusters. Recent multilevel studies indicate that these conditions are common. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Analysis of Covariance,Cluster Analysis,covariate-outcome relationship information use in cluster randomized designs,precision of randomized trial analyses,Statistical Probability,Statistical Sample Parameters,Statistical Samples},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Raudenbush_1997_Statistical analysis and optimal design for cluster randomized trials.pdf}
}

@article{ravivAdolescentsHelpseekingBehaviour2000,
  title = {Adolescents' Help-Seeking Behaviour: The Difference between Self- and Other-Referral},
  shorttitle = {Adolescents' Help-Seeking Behaviour},
  author = {Raviv, Amiram and Sills, Rachel and Raviv, Alona and Wilansky, Pamela},
  year = {2000},
  month = dec,
  journal = {Journal of Adolescence},
  volume = {23},
  number = {6},
  pages = {721--740},
  issn = {01401971},
  doi = {10.1006/jado.2000.0355},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Raviv et al_2000_Adolescents' help-seeking behaviour.pdf}
}

@article{rawatKingdonStreamsModel2016,
  title = {{Kingdon's ``Streams'' Model at Thirty: Still Relevant in the 21st Century?}},
  shorttitle = {{Kingdon's ``Streams'' Model at Thirty}},
  author = {Rawat, Pragati and Morris, John Charles},
  year = {2016},
  journal = {Politics \& Policy},
  volume = {44},
  number = {4},
  pages = {608--638},
  issn = {1747-1346},
  doi = {10.1111/polp.12168},
  urldate = {2023-07-25},
  abstract = {John Kingdon's multiple-streams model, developed to explain the policy formulation process, is often cited in the policy literature, and remains a staple of policy courses. In spite of critiques suggesting Kingdon's work is theoretically shaky and difficult to observe empirically, the work seems to retain a prominent place in the policy literature, garnering hundreds of citations since its publication. This article provides a review of the literature employing Kingdon's work, from 1984 to the present. The results show that the model has been used across all inhabited continents but suggest a decline in the use of the model in the U.S. scholarly literature. Other theories are liberally used in conjunction with Kingdon's model. The study provides an update on recent trends in the model's use and offers guidelines for interested researchers about topics that are considered as a productive endeavor, going forward. Related Articles Knutsen, Wenjue Lu. 2012. ``An Institutional Account of China's HIV/AIDS Policy Process from 1985 to 2010.'' Politics \& Policy 40 (1): 161-192. http://onlinelibrary.wiley.com/doi/10.1111/j.1747-1346.2011.00339.x/abstract Weiner, Terry. 2007. ``Touching the Third Rail: Explaining the Failure of Bush's Social Security Initiative.'' Politics \& Policy 35 (4): 872-897. http://onlinelibrary.wiley.com/doi/10.1111/j.1747-1346.2007.00087.x/abstract David, Charles-Philippe. 2015. ``Policy Entrepreneurs and the Reorientation of National Security Policy under the G. W. Bush Administration (2001-04).'' Politics \& Policy 43 (1): 163-195. http://onlinelibrary.wiley.com/doi/10.1111/polp.12106/abstract Related Media Kingdon, John W. https://en.wikiquote.org/wiki/John\_W.\_Kingdon Dow, Katheryn. 2013. ``Kingdon's 3 Streams.'' https://www.youtube.com/watch?v=s-wIyS-hFNI Urban Policy Lab Konstanz. 2015. ``Multiple Streams Approach: An Introduction.'' https://www.youtube.com/watch?v=JUlvyBVoJiI},
  copyright = {{\copyright} 2016 Policy Studies Organization},
  langid = {spanish},
  keywords = {" Kingdon in the Literature,"Agendas Alternatives and Public Policy,Criticisms,Garbage Can Model,John Kingdon,Literature Review Article,Multiple Streams,Policy Entrepreneurs,Policy Formation,Policy Process,Policy Theory,Streams Model},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rawat_Morris_2016_Kingdon's “Streams” Model at Thirty.pdf;/Users/nseewald/Zotero/storage/VZUJDGX4/polp.html}
}

@misc{RegressionAnalysisMultivariate,
  title = {Regression {{Analysis}} of {{Multivariate Incomplete Failure Time Data}} by {{Modeling Marginal Distributions}}},
  issn = {0162-1459},
  urldate = {2023-10-27},
  howpublished = {https://www-tandfonline-com.proxy.library.upenn.edu/doi/epdf/10.1080/01621459.1989.10478873?needAccess=true},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/Q9EUKFY6/01621459.1989.html}
}

@article{ReTwiskProper2005,
  ids = {boshuizenReTwiskProper2005,boshuizenReTwiskProper2005a},
  title = {Re: {{Twisk}} and {{Proper}}: Evaluation of the Results of a Randomized Controlled Trial: How to Define Changes between Baseline and Follow-Up},
  shorttitle = {Re},
  author = {Boshuizen, Hendriek C.},
  year = {2005},
  month = feb,
  journal = {Journal of Clinical Epidemiology},
  volume = {58},
  number = {2},
  pages = {209--210},
  issn = {0895-4356},
  doi = {10/bc2wx6},
  urldate = {2019-12-10},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Boshuizen_2005_Re.pdf;/Users/nseewald/Zotero/storage/C84P8RZV/S0895435604002367.html}
}

@article{richardHowMedicalAre2021,
  title = {How Medical Are States' Medical Cannabis Policies?: {{Proposing}} a Standardized Scale},
  shorttitle = {How Medical Are States' Medical Cannabis Policies?},
  author = {Richard, Emma L. and Althouse, Andrew D. and Arnsten, Julia H. and Bulls, Hailey W. and Kansagara, Devan and Kerbag, Megan N. and Lichius, Claire and Lipsey, Daniel and Morasco, Benjamin J. and Nugent, Shannon M. and Merlin, Jessica S. and Starrels, Joanna L.},
  year = {2021},
  month = aug,
  journal = {International Journal of Drug Policy},
  volume = {94},
  pages = {103202},
  issn = {0955-3959},
  doi = {10.1016/j.drugpo.2021.103202},
  urldate = {2024-05-01},
  abstract = {Background There are important differences in medical cannabis laws across the U.S.. However, prior studies investigating the effect of medical cannabis laws on outcomes disregard this heterogeneity. Findings from the body of literature using a simple dichotomous assessment of whether a particular state has enacted a medical cannabis law are equivocal or conflicting. To advance the science, a national advisory group of experts in medical cannabis developed and utilized a systematic methodology, the ``medicalization of cannabis laws standardized scale'' (MCLaSS), to characterize and quantify state laws' degree of medicalization, the extent to which medical cannabis is treated similarly to pharmaceutical medications. Methods We conducted a systematic review of state-level medical cannabis laws in the U.S. Using the novel MCLaSS, we calculated seven domain scores (patient-clinician relationship, manufacturing and testing, product labeling, types of products, supply and dose limit, prescription drug monitoring program, and dispensing practices) and a summary score for each state which had enacted medical cannabis laws as of July 2019. Results There is substantial heterogeneity in the degree of medicalization of states' medical cannabis laws, as demonstrated by the MCLaSS summary score, which ranged from 23 (least medicalized) to 86 (most medicalized). Conclusion This methodology will advance the evidence base about the impact of medical cannabis laws on patient and public health outcomes, which is urgently needed to ensure the development of policies that minimize the risks and maximize the benefits of medical cannabis.},
  keywords = {Drug policy,Medical cannabis},
  file = {/Users/nseewald/Zotero/storage/WFSIFHX6/S0955395921001006.html}
}

@article{richardsonGeneralizedDifferenceinDifferences2023,
  title = {Generalized {{Difference-in-Differences}}},
  author = {Richardson, David B. and Ye, Ting and Tchetgen Tchetgen, Eric J.},
  year = {2023},
  month = mar,
  journal = {Epidemiology},
  volume = {34},
  number = {2},
  pages = {167},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000001568},
  urldate = {2024-03-01},
  abstract = {Difference-in-differences (DID) analyses are used in a variety of research areas as a strategy for estimating the causal effect of a policy, program, intervention, or environmental hazard (hereafter, treatment). The approach offers a strategy for estimating the causal effect of a treatment using observational (i.e., nonrandomized) data in which outcomes on each study unit have been measured both before and after treatment. To identify a causal effect, a DID analysis relies on an assumption that confounding of the treatment effect in the pretreatment period is equivalent to confounding of the treatment effect in the post treatment period. We propose an alternative approach that can yield identification of causal effects under different identifying conditions than those usually required for DID. The proposed approach, which we refer to as generalized DID, has the potential to be used in routine policy evaluation across many disciplines, as it essentially combines two popular quasiexperimental designs, leveraging their strengths while relaxing their usual assumptions. We provide a formal description of the conditions for identification of causal effects, illustrate the method using simulations, and provide an empirical example based on Card and Krueger's landmark study of the impact of an increase in minimum wage in New Jersey on employment.},
  langid = {american},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Richardson et al_2023_Generalized Difference-in-Differences.pdf;/Users/nseewald/Zotero/storage/9K99UB7V/generalized_difference_in_differences.2.html}
}

@article{richardsonGeneralizedDifferenceinDifferences2023a,
  title = {Generalized {{Difference-in-Differences}}},
  author = {Richardson, David B. and Ye, Ting and Tchetgen Tchetgen, Eric J.},
  year = {2023},
  month = mar,
  journal = {Epidemiology},
  volume = {34},
  number = {2},
  pages = {167--174},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000001568},
  urldate = {2024-05-07},
  abstract = {Difference-in-differences (DID) analyses are used in a variety of research areas as a strategy for estimating the causal effect of a policy, program, intervention, or environmental hazard (hereafter, treatment). The approach offers a strategy for estimating the causal effect of a treatment using observational (i.e., nonrandomized) data in which outcomes on each study unit have been measured both before and after treatment. To identify a causal effect, a DID analysis relies on an assumption that confounding of the treatment effect in the pretreatment period is equivalent to confounding of the treatment effect in the post treatment period. We propose an alternative approach that can yield identification of causal effects under different identifying conditions than those usually required for DID. The proposed approach, which we refer to as generalized DID, has the potential to be used in routine policy evaluation across many disciplines, as it essentially combines two popular quasiexperimental designs, leveraging their strengths while relaxing their usual assumptions. We provide a formal description of the conditions for identification of causal effects, illustrate the method using simulations, and provide an empirical example based on Card and Krueger's landmark study of the impact of an increase in minimum wage in New Jersey on employment.},
  langid = {english}
}

@article{ridoutTestingRandomDropouts1991,
  title = {Testing for {{Random Dropouts}} in {{Repeated Measurement Data}}},
  author = {Ridout, Martin S. and Diggle, Peter J.},
  year = {1991},
  journal = {Biometrics},
  volume = {47},
  number = {4},
  eprint = {2532413},
  eprinttype = {jstor},
  pages = {1617--1621},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10/fg7pzd},
  urldate = {2021-05-28},
  abstract = {Diggle (1989, Biometrics 45, 1255-1258) proposes a test for random dropouts in repeated measurement data when the experiment has a completely randomized design. It is argued here that logistic regression is a comparable but more flexible technique for studying the occurrence of dropouts.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ridout_Diggle_1991_Testing for Random Dropouts in Repeated Measurement Data.pdf}
}

@misc{riveraInternalModelControl2002,
  title = {Internal Model Control: {{PID}} Controller Design},
  shorttitle = {Internal Model Control},
  author = {Rivera, Daniel E. and Morari, Manfred and Skogestad, Sigurd},
  year = {2002},
  month = may,
  publisher = {American Chemical Society},
  doi = {10.1021/i200032a041},
  urldate = {2021-07-01},
  howpublished = {https://pubs.acs.org/doi/pdf/10.1021/i200032a041},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/T2BGPGTR/i200032a041.html}
}

@misc{rizopoulosIntroductionJointModeling2021,
  title = {Introduction to the {{Joint Modeling}} of {{Longitudinal}} and {{Survival Data}}, with {{Applications}} in {{R}}},
  author = {Rizopoulos, Dimitris},
  year = {2021},
  urldate = {2024-01-22},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rizopoulos_2021_Introduction to the Joint Modeling of Longitudinal and Survival Data, with.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rizopoulos_2021_Introduction to the Joint Modeling of Longitudinal and Survival Data, with2.pdf}
}

@article{robertsBaselineImbalanceRandomised1999,
  title = {Baseline Imbalance in Randomised Controlled Trials},
  author = {Roberts, Chris and Torgerson, David J.},
  year = {1999},
  month = jul,
  journal = {BMJ},
  volume = {319},
  number = {7203},
  pages = {185},
  publisher = {British Medical Journal Publishing Group},
  issn = {0959-8138, 1468-5833},
  doi = {10/cfcw2h},
  urldate = {2021-06-22},
  abstract = {In a controlled trial randomisation ensures that allocation of patients to treatments is left purely to chance. The characteristics of patients that may influence outcome are distributed between treatment groups so that any difference in outcome can be assumed to be due to the intervention. However, imbalance between groups in baseline variables that may influence outcome (such as age or disease severity) can bias statistical tests, a property sometimes referred to as chance bias Observed differences in outcome between groups in a particular trial could by chance be due to characteristics of the patients, not treatments. Some protection against chance bias is given by stratified randomisation or minimisation and by adjusting in the statistical analysis for baseline variables. In reporting clinical trials it is recommended that prognostic variables should be described for each treatment group.1 This may {\dots}},
  chapter = {Education and debate},
  copyright = {{\copyright} 1999 BMJ Publishing Group Ltd.},
  langid = {english},
  pmid = {10406763},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Roberts_Torgerson_1999_Baseline imbalance in randomised controlled trials.pdf;/Users/nseewald/Zotero/storage/BTAQTWXA/185.html}
}

@incollection{robinsCausalInferenceComplex1997,
  title = {Causal {{Inference}} from {{Complex Longitudinal Data}}},
  booktitle = {Latent {{Variable Modeling}} and {{Applications}} to {{Causality}}},
  author = {Robins, James M.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Berkane, Maia},
  year = {1997},
  volume = {120},
  pages = {69--117},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-1842-5_4},
  urldate = {2018-10-18},
  isbn = {978-0-387-94917-8},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Robins_1997_Causal Inference from Complex Longitudinal Data.pdf}
}

@article{robinsCorrectingNoncomplianceRandomized1994,
  title = {Correcting for Non-Compliance in Randomized Trials Using Structural Nested Mean Models},
  author = {Robins, James M.},
  year = {1994},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {23},
  number = {8},
  pages = {2379--2412},
  issn = {0361-0926, 1532-415X},
  doi = {10.1080/03610929408831393},
  urldate = {2018-10-12},
  abstract = {In a randomized trial designed to study the effect of a treatment of interest on the evolution of the mean of a timedependent outcome variable. subjects an assigned to a treatment regime. or. equivalently, a treaanent promcol. Unfortunately, subjects often kdl to comply with their assigned regime. From a public health point of view, the causal pawmeter of interest will often be a function of the treatment di5ennces that would have been observed had, \&, all subjects remained on protocol. This paper considers the identification and estimation of these treatment differences based on a new class of structurPl models, the multivariate strucnual nested mean models, when reliable estiimates of each subject's actual treatment are available. Estimates of " a c w l treatment" might, for example, be obrained by measuring rhe amount of "active drug" in the subject's blood or urine at each follow-up visit or by pill counting techniques. In addition, we discuss a nanusll extension of our methods to observational studies.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Robins_1994_Correcting for non-compliance in randomized trials using structural nested mean.pdf}
}

@incollection{robinsEstimationCausalEffects2008,
  title = {Estimation of the Causal Effects of Time-Varying Exposures},
  booktitle = {Longitudinal {{Data Analysis}}},
  author = {Robins, James M. and Hernan, Miguel A.},
  editor = {Fitzmaurice, Garrett and Davidian, Marie and Verbeke, Geert and Molenberghs, Geert},
  year = {2008},
  month = aug,
  series = {Handbooks of {{Modern Statistical Methods}}},
  pages = {553--599},
  publisher = {CRC Press},
  address = {Boca Raton},
  doi = {10.1201/9781420011579.ch23},
  urldate = {2023-09-06},
  abstract = {With contributions from some of the most prominent researchers in the field, this carefully edited collection provides a clear, comprehensive, and unified overview of recent developments in statistical methods for the analysis of longitudinal data. Focusing on both theory and applications, it addresses challenges that arise in analyzing longitudinal data, emphasizes statistical models and methods likely to endure in the future, highlights connections between various research threads in the statistical literature, and contains numerous examples and case studies drawn from a range of disciplines. Data sets, software programs, and other material are available on the editors' website.},
  isbn = {978-1-58488-658-7 978-1-4200-1157-9},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/EMGLZESE/9781420011579.html}
}

@article{robinsMarginalStructuralModels2000,
  title = {Marginal {{Structural Models}} and {{Causal Inference}} in {{Epidemiology}}},
  shorttitle = {Marginal {{Structural Models}} and {{Causal Inference}} in {{Epidemiology}}},
  author = {Robins, James M. and Hern{\'a}n, Miguel {\'A}ngel and Brumback, Babette},
  year = {2000},
  month = sep,
  journal = {Epidemiology},
  volume = {11},
  number = {5},
  pages = {550--560},
  issn = {1044-3983},
  doi = {10.1097/00001648-200009000-00011},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Robins et al_2000_Marginal Structural Models and Causal Inference in Epidemiology.pdf}
}

@incollection{robinsMarginalStructuralModels2000a,
  title = {Marginal {{Structural Models}} versus {{Structural}} Nested {{Models}} as {{Tools}} for {{Causal}} Inference},
  booktitle = {Statistical {{Models}} in {{Epidemiology}}, the {{Environment}}, and {{Clinical Trials}}},
  author = {Robins, James M.},
  editor = {Miller, Willard and Halloran, M. Elizabeth and Berry, Donald},
  year = {2000},
  volume = {116},
  pages = {95--133},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-1284-3_2},
  urldate = {2018-10-12},
  abstract = {Robins (1993, 1994, 1997, 1998ab) has developed a set of causal or counterfactual models, the structural nested models (SNMs). This paper describes an alternative new class of causal models - the (non-nested) marginal structural models (MSMs). We will then describe a class of semiparametric estimators for the parameters of these new models under a sequential randomization (i.e., ignorability) assumption. We then compare the strengths and weaknesses of MSMs versus SNMs for causal inference from complex longitudinal data with time-dependent treatments and confounders. Our results provide an extension to continuous treatments of propensity score estimators of an average treatment effect.},
  isbn = {978-1-4612-7078-2 978-1-4612-1284-3},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Robins_2000_Marginal Structural Models versus Structural nested Models as Tools for Causal.pdf}
}

@article{robinsonTeachingYourselfStructural,
  title = {Teaching Yourself about Structural Racism Will Improve Your Machine Learning},
  author = {Robinson, Whitney R and Renson, Audrey and Naimi, Ashley I},
  journal = {Biostatistics},
  doi = {10/ggmfcp},
  abstract = {In this commentary, we put forth the following argument: Anyone conducting machine learning in a health-related domain should educate themselves about structural racism. We argue that structural racism is a critical body of knowledge needed for generalizability in almost all domains of health research.},
  keywords = {Researcher App},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Robinson et al_Teaching yourself about structural racism will improve your machine learning.pdf}
}

@incollection{robinsOptimalStructuralNested2004,
  title = {Optimal {{Structural Nested Models}} for {{Optimal Sequential Decisions}}},
  booktitle = {Proceedings of the {{Second Seattle Symposium}} in {{Biostatistics}}},
  author = {Robins, James M.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Gather, U. and Olkin, I. and Zeger, S. and Lin, D. Y. and Heagerty, P. J.},
  year = {2004},
  volume = {179},
  pages = {189--326},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4419-9076-1_11},
  urldate = {2018-10-12},
  abstract = {I describe two new methods for estimating the optimal treatment regime (equivalently, protocol, plan or strategy) from very high dimesional observational and experimental data: (i) g-estimation of an optimal double-regime structural nested mean model (drSNMM) and (ii) g-estimation of a standard single regime SNMM combined with sequential dynamicprogramming (DP) regression. These methods are compared to certain regression methods found in the sequential decision and reinforcement learning literatures and to the regret modelling methods of Murphy (2003). I consider both Bayesian and frequentist inference. In particular, I propose a novel "Bayes-frequentist compromise" that combines honest subjective non- or semiparametric Bayesian inference with good frequentist behavior, even in cases where the model is so large and the likelihood function so complex that standard (uncompromised) Bayes procedures have poor frequentist performance.},
  isbn = {978-0-387-20862-6 978-1-4419-9076-1},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Robins_2004_Optimal Structural Nested Models for Optimal Sequential Decisions.pdf}
}

@article{rocheMeasurementSubjectiveApprehension2020,
  title = {On the {{Measurement}} of {{Subjective Apprehension Risk}}},
  author = {Roche, Sean Patrick and Pickett, Justin T. and Intravia, Jonathan and Thompson, Andrew J.},
  year = {2020},
  month = dec,
  journal = {Criminal Justice Review},
  pages = {073401682097882},
  issn = {0734-0168, 1556-3839},
  doi = {10/ghpv97},
  urldate = {2020-12-16},
  abstract = {Do people think about offending risk in verbal or numerical terms? Does the elicitation method affect reported subjective probabilities? Rational choice models require potential outcomes (e.g., benefits/costs) to be weighted by their probability of occurrence. Indeed, the subjective likelihood of being apprehended is the central construct in criminological deterrence theory---the so-called certainty principle. Yet, extant literature has measured the construct inconsistently and with little attention to potential consequences. Using a series of randomized experiments conducted with nationwide samples of American adults (aged 18 and over), this study examines the degree of correspondence between verbal and numeric measures of apprehension risk, assesses the durability of numeric estimates specifically, and attempts to elicit how respondents naturally think about apprehension risk. The findings suggest that laypeople are somewhat inconsistent in their use of both verbal and numeric descriptors of probability, their numeric estimates of probability are unlikely to be precise or durable, and many seem to prefer thinking of risk in verbal terms (compared to numeric terms). Researchers should consider including both verbal and numeric measures of probability and explore alternative measurement strategies, including anchoring vignettes, which have been valuable in standardizing verbal responses in other disciplines.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Roche et al_2020_On the Measurement of Subjective Apprehension Risk.pdf}
}

@article{rochonApplicationGEEProcedures1998,
  title = {Application of {{GEE}} Procedures for Sample Size Calculations in Repeated Measures Experiments},
  author = {Rochon, James},
  year = {1998},
  month = jul,
  journal = {Statistics in Medicine},
  volume = {17},
  number = {14},
  pages = {1643--1658},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/(SICI)1097-0258(19980730)17:14<1643::AID-SIM869>3.0.CO;2-3},
  urldate = {2018-10-12},
  abstract = {Derivation of the minimum sample size is an important consideration in an applied research effort. When the outcome is measured at a single time point, sample size procedures are well known and widely applied. The corresponding situation for longitudinal designs, however, is less well developed. In this paper, we adapt the generalized estimating equation (GEE) approach of Liang and Zeger to sample size calculations for discrete and continuous outcome variables. The non-central version of the Wald   test is considered. We use the damped exponential family of correlation structures described in Mun  oz et al. for the `working' correlation matrix among the repeated measures. We present a table of minimum sample sizes for binary outcomes, and discuss extensions that account for unequal allocation, staggered entry and loss to follow-up. 1998 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rochon_1998_Application of GEE procedures for sample size calculations in repeated measures.pdf}
}

@article{rochonSampleSizeCalculations1991,
  title = {Sample {{Size Calculations}} for {{Two-Group Repeated-Measures Experiments}}},
  author = {Rochon, James},
  year = {1991},
  journal = {Biometrics},
  volume = {47},
  number = {4},
  eprint = {2532393},
  eprinttype = {jstor},
  pages = {1383--1398},
  issn = {0006-341X},
  doi = {10/fs5z4r},
  urldate = {2019-11-08},
  abstract = {Vonesh and Schork (1986, Biometrics 42, 601-610) presented a statistical methodology for computing the minimum sample size required for the within-subjects repeated-measures design. They applied Hotelling's T2 analysis and demonstrated the utility of these techniques under general covariance structures. In this paper, we extend these procedures to the between-subjects repeated-measures design when there are two treatment groups under consideration. The multivariate analysis of variance approach to analyzing repeated measurements is considered and this model also resolves to Hotelling's T2 analysis. Two models are put forward to contend with those situations where the correlation structure among the repeated measures is unknown. These include the autoregressive model and the assumption of compound symmetry, and they are compared and contrasted throughout. Tables of the minimum sample sizes required for several hypotheses arising from repeated-measures designs are presented.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rochon_1991_Sample Size Calculations for Two-Group Repeated-Measures Experiments.pdf}
}

@article{rokickiInferenceDifferenceinDifferencesSmall2018,
  title = {Inference {{With Difference-in-Differences With}} a {{Small Number}} of {{Groups}}: {{A Review}}, {{Simulation Study}}, and {{Empirical Application Using SHARE Data}}},
  shorttitle = {Inference {{With Difference-in-Differences With}} a {{Small Number}} of {{Groups}}},
  author = {Rokicki, Slawa and Cohen, Jessica and Fink, G{\"u}nther and Salomon, Joshua A. and Landrum, Mary Beth},
  year = {2018},
  month = jan,
  journal = {Medical Care},
  volume = {56},
  number = {1},
  pages = {97--105},
  issn = {0025-7079},
  doi = {10.1097/MLR.0000000000000830},
  urldate = {2022-04-20},
  abstract = {Background:~         Difference-in-differences (DID) estimation has become increasingly popular as an approach to evaluate the effect of a group-level policy on individual-level outcomes. Several statistical methodologies have been proposed to correct for the within-group correlation of model errors resulting from the clustering of data. Little is known about how well these corrections perform with the often small number of groups observed in health research using longitudinal data.         Methods:~         First, we review the most commonly used modeling solutions in DID estimation for panel data, including generalized estimating equations (GEE), permutation tests, clustered standard errors (CSE), wild cluster bootstrapping, and aggregation. Second, we compare the empirical coverage rates and power of these methods using a Monte Carlo simulation study in scenarios in which we vary the degree of error correlation, the group size balance, and the proportion of treated groups. Third, we provide an empirical example using the Survey of Health, Ageing, and Retirement in Europe.         Results:~         When the number of groups is small, CSE are systematically biased downwards in scenarios when data are unbalanced or when there is a low proportion of treated groups. This can result in over-rejection of the null even when data are composed of up to 50 groups. Aggregation, permutation tests, bias-adjusted GEE, and wild cluster bootstrap produce coverage rates close to the nominal rate for almost all scenarios, though GEE may suffer from low power.         Conclusions:~         In DID estimation with a small number of groups, analysis using aggregation, permutation tests, wild cluster bootstrap, or bias-adjusted GEE is recommended.},
  langid = {american},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rokicki et al_2018_Inference With Difference-in-Differences With a Small Number of Groups.docx;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rokicki et al_2018_Inference With Difference-in-Differences With a Small Number of Groups.pdf;/Users/nseewald/Zotero/storage/RCEHRQPZ/Inference_With_Difference_in_Differences_With_a.15.html}
}

@article{romero-sandovalCannabisChronicPain2018,
  title = {Cannabis for {{Chronic Pain}}: {{Challenges}} and {{Considerations}}},
  shorttitle = {Cannabis for {{Chronic Pain}}},
  author = {{Romero-Sandoval}, E. Alfonso and Fincham, Jack E. and Kolano, Ashley L. and Sharpe, Brandi N. and {Alvarado-V{\'a}zquez}, P. Abigail},
  year = {2018},
  journal = {Pharmacotherapy: The Journal of Human Pharmacology and Drug Therapy},
  volume = {38},
  number = {6},
  pages = {651--662},
  issn = {1875-9114},
  doi = {10.1002/phar.2115},
  urldate = {2024-05-03},
  abstract = {The National Academies of Sciences, Engineering, and Medicine has found substantial evidence that cannabis (plant) is effective for the treatment of chronic pain in adults, and moderate evidence that oromucosal cannabinoids (extracts, especially nabiximols) improve short-term sleep disturbances in chronic pain. The paradoxical superiority of the cannabis plant over cannabinoid molecules represents a challenge for the medical community and the established processes that define modern pharmacy. The expanding and variable legalization of cannabis in multiple states nationwide represents an additional challenge for patients and the medical community because recreational and medicinal cannabis are irresponsibly overlapped. Cannabis designed for recreational use (containing high levels of active ingredients) is increasingly available to patients with chronic pain who do not find relief with current pharmacologic entities, which exposes patients to potential harm. This article analyzes the available scientific evidence to address controversial questions that the current state of cannabis poses for health care professionals and chronic pain patients and sets the basis for a more open discussion about the role of cannabis in modern medicine for pain management. A critical discussion on these points, the legal status of cannabis, and considerations for health care providers is presented.},
  copyright = {{\copyright} 2018 Pharmacotherapy Publications, Inc.},
  langid = {english},
  keywords = {marijuana,medical cannabis,medical marijuana,medicinal cannabis,medicinal marijuana,recreational cannabis,recreational marijuana},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Romero-Sandoval_et_al_2018_Cannabis_for_Chronic_Pain.pdf;/Users/nseewald/Zotero/storage/UIMXHRS5/phar.html}
}

@article{rosenbaumCentralRolePropensity,
  title = {The Central Role of the Propensity Score in Observational Studies for Causal Effects},
  author = {Rosenbaum, Paul R and Rubin, Donald B},
  year = {1983},
  month = apr,
  journal = {Biometrika},
  volume = {70},
  number = {1},
  eprint = {2335942},
  eprinttype = {jstor},
  pages = {41--55},
  doi = {10.2307/2335942},
  abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a twodimensional plot.},
  langid = {english},
  keywords = {Multiple DOI},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rosenbaum_Rubin_1983_The central role of the propensity score in observational studies for causal.pdf}
}

@article{rosenbaumConsquencesAdjustmentConcomitant1984,
  title = {The {{Consquences}} of {{Adjustment}} for a {{Concomitant Variable That Has Been Affected}} by the {{Treatment}}},
  author = {Rosenbaum, Paul R.},
  year = {1984},
  journal = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {147},
  number = {5},
  eprint = {10.2307/2981697},
  eprinttype = {jstor},
  pages = {656},
  issn = {00359238},
  doi = {10.2307/2981697},
  urldate = {2018-10-12},
  abstract = {Adjustments for bias in observational studies are not always confined to variables that were measured prior to treatment. Estimators that adjust for a concomitant variable that has been affected by the treatment are generally biased. The bias may be written as the sum of two easily interpreted components: one component is present only in observational studies; the other is common to both observational studies and randomized experiments. The first component of bias will be zero when the affected posttreatment concomitant variable is, in a certain sense, a surrogate for an unobserved pretreatment variable. The second component of bias can often be addressed by an appropriate sensitivity analysis.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rosenbaum_1984_The Consquences of Adjustment for a Concomitant Variable That Has Been Affected.pdf}
}

@article{roseSampleSizeCalculations2019,
  title = {Sample {{Size Calculations}} for {{SMARTs}}},
  author = {Rose, Eric J. and Laber, Eric B. and Davidian, Marie and Tsiatis, Anastasios A. and Zhao, Ying-Qi and Kosorok, Michael R.},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.06646 [stat]},
  eprint = {1906.06646},
  primaryclass = {stat},
  urldate = {2019-07-09},
  abstract = {Sequential Multiple Assignment Randomized Trials (SMARTs) are considered the gold standard for estimation and evaluation of treatment regimes. SMARTs are typically sized to ensure sufficient power for a simple comparison, e.g., the comparison of two fixed treatment sequences. Estimation of an optimal treatment regime is conducted as part of a secondary and hypothesis-generating analysis with formal evaluation of the estimated optimal regime deferred to a follow-up trial. However, running a follow-up trial to evaluate an estimated optimal treatment regime is costly and time-consuming; furthermore, the estimated optimal regime that is to be evaluated in such a follow-up trial may be far from optimal if the original trial was underpowered for estimation of an optimal regime. We derive sample size procedures for a SMART that ensure: (i) sufficient power for comparing the optimal treatment regime with standard of care; and (ii) the estimated optimal regime is within a given tolerance of the true optimal regime with high-probability. We establish asymptotic validity of the proposed procedures and demonstrate their finite sample performance in a series of simulation experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {No DOI found,Q-learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rose et al_2019_Sample Size Calculations for SMARTs.pdf}
}

@article{roseUsingPilotData2022,
  title = {Using {{Pilot Data}} to {{Size Observational Studies}} for the {{Estimation}} of {{Dynamic Treatment Regimes}}},
  author = {Rose, Eric J. and Moodie, Erica E. M. and Shortreed, Susan},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.09451 [stat]},
  eprint = {2202.09451},
  primaryclass = {stat},
  urldate = {2022-02-26},
  abstract = {There has been significant attention given to developing data-driven methods for tailoring patient care based on individual patient characteristics. Dynamic treatment regimes formalize this through a sequence of decision rules that map patient information to a suggested treatment. The data for estimating and evaluating treatment regimes are ideally gathered through the use of Sequential Multiple Assignment Randomized Trials (SMARTs) though longitudinal observational studies are commonly used due to the potentially prohibitive costs of conducting a SMART. These studies are typically sized for simple comparisons of fixed treatment sequences or, in the case of observational studies, a priori sample size calculations are often not performed. We develop sample size procedures for the estimation of dynamic treatment regimes from observational studies. Our approach uses pilot data to ensure a study will have sufficient power for comparing the value of the optimal regime, i.e. the expected outcome if all patients in the population were treated by following the optimal regime, with a known comparison mean. Our approach also ensures the value of the estimated optimal treatment regime is within an a priori set range of the value of the true optimal regime with a high probability. We examine the performance of the proposed procedure with a simulation study and use it to size a study for reducing depressive symptoms using data from electronic health records.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rose et al_2022_Using Pilot Data to Size Observational Studies for the Estimation of Dynamic.pdf;/Users/nseewald/Zotero/storage/XCUPVZ48/2202.html}
}

@article{rotaOptimalCutpointDefinition2015,
  title = {Optimal Cut-Point Definition in Biomarkers: The Case of Censored Failure Time Outcome},
  shorttitle = {Optimal Cut-Point Definition in Biomarkers},
  author = {Rota, Matteo and Antolini, Laura and Valsecchi, Maria Grazia},
  year = {2015},
  month = mar,
  journal = {BMC Medical Research Methodology},
  volume = {15},
  number = {1},
  pages = {24},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0009-y},
  urldate = {2024-06-20},
  abstract = {Cut-point finding is a crucial step for clinical decision making when dealing with diagnostic (or prognostic) biomarkers. The extension of ROC-based cut-point finding methods to the case of censored failure time outcome is of interest when we are in the presence of a biomarker, measured at baseline, used to identify whether there will be the development, or not, of some disease condition within a given time point {$\tau$} of clinical interest.},
  langid = {english},
  keywords = {Censored failure time outcome,Concordance probability,Optimal cut-point,Point closest-to-(01) corner in the ROC plane,Youden index},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rota et al_2015_Optimal cut-point definition in biomarkers.pdf}
}

@article{rothbardTutorialApplyingDifferenceinDifferences2024,
  title = {A {{Tutorial}} on {{Applying}} the {{Difference-in-Differences Method}} to {{Health Data}}},
  author = {Rothbard, Sarah and Etheridge, James C. and Murray, Eleanor J.},
  year = {2024},
  month = jun,
  journal = {Current Epidemiology Reports},
  volume = {11},
  number = {2},
  pages = {85--95},
  issn = {2196-2995},
  doi = {10.1007/s40471-023-00327-x},
  urldate = {2024-07-10},
  abstract = {Difference-in-differences analyses are a useful tool for estimating group-level decisions, such as policy changes, training programs, or other non-randomized interventions, on outcomes which occur within the intervention group. However, there is little practical advice on how to apply difference-in-differences to epidemiologic and health data. Here, we provide a tutorial on applying the difference-in-differences method to health services data, targeted at epidemiologists and other biomedical researchers.},
  langid = {english},
  keywords = {Causal effect estimation,Causal inference,Difference-in-differences,Statistical methods},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rothbard et al_2024_A Tutorial on Applying the Difference-in-Differences Method to Health Data.pdf}
}

@article{rothbardTutorialApplyingDifferenceinDifferences2024a,
  title = {A {{Tutorial}} on {{Applying}} the {{Difference-in-Differences Method}} to {{Health Data}}},
  author = {Rothbard, Sarah and Etheridge, James C. and Murray, Eleanor J.},
  year = {2024},
  month = jun,
  journal = {Current Epidemiology Reports},
  volume = {11},
  number = {2},
  pages = {85--95},
  issn = {2196-2995},
  doi = {10.1007/s40471-023-00327-x},
  urldate = {2024-07-11},
  abstract = {Difference-in-differences analyses are a useful tool for estimating group-level decisions, such as policy changes, training programs, or other non-randomized interventions, on outcomes which occur within the intervention group. However, there is little practical advice on how to apply difference-in-differences to epidemiologic and health data. Here, we provide a tutorial on applying the difference-in-differences method to health services data, targeted at epidemiologists and other biomedical researchers.},
  langid = {english},
  keywords = {Causal effect estimation,Causal inference,Difference-in-differences,Statistical methods},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rothbard et al_2024_A Tutorial on Applying the Difference-in-Differences Method to Health Data2.pdf}
}

@article{rothmanEpidemiologicMethodsClinical1977,
  title = {Epidemiologic Methods in Clinical Trials},
  author = {Rothman, Kenneth J.},
  year = {1977},
  journal = {Cancer},
  volume = {39},
  number = {S4},
  pages = {1771--1775},
  issn = {1097-0142},
  doi = {10/czh58r},
  urldate = {2020-01-05},
  abstract = {Epidemiologic methods developed to control confounding in non-experimental studies are equally applicable for experiments. In experiments, most confounding is usually controlled by random allocation of subjects to treatment groups, but randomization does not preclude confounding except for extremely large studies, the degree of confounding expected being inversely related to the size of the treatment groups. In experiments, as in non-experimental studies, the extent of confounding for each risk indicator should be assessed, and if sufficiently large, controlled. Confounding is properly assessed by comparing the unconfounded effect estimate to the crude effect estimate; a common error is to assess confounding by statistical tests of significance. Assessment of confounding involves its control as a prerequisite. Control is most readily and cogently achieved by stratification of the data, though with many factors to control simultaneously, multivariate analysis or a combination of multivariate analysis and stratification might be necessary.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rothman_1977_Epidemiologic methods in clinical trials.pdf;/Users/nseewald/Zotero/storage/HIBMZLNW/1097-0142(197704)394+1771AID-CNCR28203908033.0.html}
}

@article{rothmanNoAdjustmentsAre1990,
  title = {No {{Adjustments}} Are {{Needed}} for {{Multiple Comparisons}}},
  author = {Rothman, Kenneth J.},
  year = {1990},
  journal = {Epidemiology},
  volume = {1},
  number = {1},
  pages = {43--6},
  doi = {10.1097/00001648-199001000-00010},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rothman_1990_No Adjustments are Needed for Multiple Comparisons.pdf}
}

@article{rothWhatTrendingDifferenceindifferences2023,
  title = {What's Trending in Difference-in-Differences? {{A}} Synthesis of the Recent Econometrics Literature},
  shorttitle = {What's Trending in Difference-in-Differences?},
  author = {Roth, Jonathan and Sant'Anna, Pedro H. C. and Bilinski, Alyssa and Poe, John},
  year = {2023},
  month = aug,
  journal = {Journal of Econometrics},
  volume = {235},
  number = {2},
  pages = {2218--2244},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2023.03.008},
  urldate = {2023-12-06},
  abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of ``canonical'' assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on (i) multiple periods and variation in treatment timing, (ii) potential violations of parallel trends, or (iii) alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.},
  keywords = {Causal Inference,Clustering,Difference-in-differences,Economics - Econometrics,Parallel trends,Sensitivity Analysis,Staggered Treatment timing,Statistics - Methodology,Treatment Effect Heterogeneity},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Roth et al_2023_What’s trending in difference-in-differences.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Roth et al_2023_What’s trending in difference-in-differences2.pdf;/Users/nseewald/Zotero/storage/7873SFQQ/S0304407623001318.html;/Users/nseewald/Zotero/storage/RYAMFHWS/2201.html}
}

@article{rothWhenParallelTrends2023,
  title = {When {{Is Parallel Trends Sensitive}} to {{Functional Form}}?},
  author = {Roth, Jonathan and Sant'Anna, Pedro H. C.},
  year = {2023},
  journal = {Econometrica},
  volume = {91},
  number = {2},
  pages = {737--747},
  issn = {1468-0262},
  doi = {10.3982/ECTA19402},
  urldate = {2024-07-12},
  abstract = {This paper assesses when the validity of difference-in-differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger ``parallel trends''-type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form.},
  langid = {english},
  keywords = {Difference-in-differences,functional form,robustness,testable implications},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Roth_Sant'Anna_2023_When_Is_Parallel_Trends_Sensitive_to_Functional_Form.pdf;/Users/nseewald/Zotero/storage/SD36ETML/ECTA19402.html}
}

@incollection{rotnitzkyInverseProbabilityWeighting2014,
  title = {Inverse {{Probability Weighting}} in {{Survival Analysis}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Rotnitzky, Andrea and Robins, James M.},
  year = {2014},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118445112.stat06031},
  urldate = {2024-07-18},
  abstract = {Survival studies usually incorporate high-dimensional covariate data, but interest may focus on low-dimensional characteristics of the survival distribution. Inverse probability weighted augmented estimation (AIPW) provides a robust approach, largely insensitive to model misspecification. The method was originally introduced to deal with coarsened (incompletely observed) data, and is discussed here with and without the assumption of random coarsening.},
  copyright = {Copyright {\copyright} 2005 John Wiley \& Sons, Ltd. All rights reserved.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {coarsening,competing risks,Cox proportional hazards model,dimensionality,marginal survival function,missing data,model misspecification,robustness},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rotnitzky_Robins_2014_Inverse Probability Weighting in Survival Analysis.pdf;/Users/nseewald/Zotero/storage/L99IVZQ8/9781118445112.html}
}

@article{roziMultilevelModelingBinary2016,
  title = {Multilevel {{Modeling}} of {{Binary Outcomes}} with {{Three-Level Complex Health Survey Data}}},
  author = {Rozi, Shafquat and Mahmud, Sadia and Lancaster, Gillian and Hadden, Wilbur and Pappas, Gregory},
  year = {2016},
  month = dec,
  journal = {Open Journal of Epidemiology},
  volume = {7},
  number = {1},
  pages = {27--43},
  publisher = {Scientific Research Publishing},
  doi = {10.4236/ojepi.2017.71004},
  urldate = {2021-12-14},
  abstract = {Complex survey designs often involve unequal selection probabilities of clus-ters or units within clusters. When estimating models for complex survey data, scaled weights are incorporated into the likelihood, producing a pseudo likeli-hood. In a 3-level weighted analysis for a binary outcome, we implemented two methods for scaling the sampling weights in the National Health Survey of Pa-kistan (NHSP). For NHSP with health care utilization as a binary outcome we found age, gender, household (HH) goods, urban/rural status, community de-velopment index, province and marital status as significant predictors of health care utilization (p-value {$<$} 0.05). The variance of the random intercepts using scaling method 1 is estimated as 0.0961 (standard error 0.0339) for PSU level, and 0.2726 (standard error 0.0995) for household level respectively. Both esti-mates are significantly different from zero (p-value {$<$} 0.05) and indicate consid-erable heterogeneity in health care utilization with respect to households and PSUs. The results of the NHSP data analysis showed that all three analyses, weighted (two scaling methods) and un-weighted, converged to almost identical results with few exceptions. This may have occurred because of the large num-ber of 3rd and 2nd level clusters and relatively small ICC. We performed a sim-ulation study to assess the effect of varying prevalence and intra-class correla-tion coefficients (ICCs) on bias of fixed effect parameters and variance components of a multilevel pseudo maximum likelihood (weighted) analysis. The simulation results showed that the performance of the scaled weighted estimators is satisfactory for both scaling methods. Incorporating simulation into the analysis of complex multilevel surveys allows the integrity of the results to be tested and is recommended as good practice.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rozi et al_2016_Multilevel Modeling of Binary Outcomes with Three-Level Complex Health Survey.pdf;/Users/nseewald/Zotero/storage/RNPRCZDJ/paperinformation.html}
}

@article{rubinCommentNeyman19231990,
  title = {Comment: {{Neyman}} (1923) and {{Causal Inference}} in {{Experiments}} and {{Observational Studies}}},
  shorttitle = {[{{On}} the {{Application}} of {{Probability Theory}} to {{Agricultural Experiments}}. {{Essay}} on {{Principles}}. {{Section}} 9.] {{Comment}}},
  author = {Rubin, Donald B.},
  year = {1990},
  month = nov,
  journal = {Statistical Science},
  volume = {5},
  number = {4},
  pages = {472--480},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177012032},
  urldate = {2023-05-23},
  abstract = {Statistical Science},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rubin_1990_Comment.pdf}
}

@article{rubinEstimatingCausalEffects1974,
  title = {Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies},
  author = {Rubin, Donald B.},
  year = {1974},
  month = oct,
  journal = {Journal of Educational Psychology},
  volume = {66},
  number = {5},
  pages = {688--701},
  issn = {0022-0663},
  doi = {10.1037/H0037350},
  urldate = {2018-11-15},
  abstract = {Presents a discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation. The objective was to specify the benefits of randomization in estimating causal effects of treatments. It is concluded that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rubin_1974_Estimating causal effects of treatments in randomized and nonrandomized studies.pdf}
}

@article{rubinInferenceMissingData2017,
  title = {Inference and Missing Data},
  author = {Rubin, Donald B},
  year = {2017},
  pages = {12},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rubin_2017_Inference and missing data.pdf}
}

@book{rubinMultipleImputationNonresponse1987,
  title = {Multiple {{Imputation}} for {{Nonresponse}} in {{Surveys}}},
  author = {Rubin, Donald B.},
  year = {1987},
  month = jun,
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9780470316696},
  urldate = {2024-03-20},
  isbn = {978-0-471-08705-2 978-0-470-31669-6},
  langid = {english}
}

@article{rubinUsingEmpiricalBayes1980,
  title = {Using {{Empirical Bayes Techniques}} in the {{Law School Validity Studies}}},
  author = {Rubin, Donald B.},
  year = {1980},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {75},
  number = {372},
  pages = {801--816},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1980.10477553},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rubin_1980_Using Empirical Bayes Techniques in the Law School Validity Studies.pdf}
}

@article{rugoImpactPalbociclibLetrozole2018,
  title = {Impact of Palbociclib plus Letrozole on Patient-Reported Health-Related Quality of Life: Results from the {{PALOMA-2}} Trial},
  shorttitle = {Impact of Palbociclib plus Letrozole on Patient-Reported Health-Related Quality of Life},
  author = {Rugo, H. S. and Di{\'e}ras, V. and Gelmon, K. A. and Finn, R. S. and Slamon, D. J. and Martin, M. and Neven, P. and Shparyk, Y. and Mori, A. and Lu, D. R. and Bhattacharyya, H. and u. a. n. g. Bartlett, C. H. and Iyer, S. and Johnston, S. and Ettl, J. and Harbeck, N.},
  year = {2018},
  month = apr,
  journal = {Annals of Oncology},
  series = {Epigenetic Modifiers as Immunomodulatory Therapies in Solid Tumours},
  volume = {29},
  number = {4},
  pages = {888--894},
  issn = {0923-7534},
  doi = {10.1093/annonc/mdy012},
  urldate = {2024-02-06},
  abstract = {Background Patient-reported outcomes are integral in benefit--risk assessments of new treatment regimens. The PALOMA-2 study provides the largest body of evidence for patient-reported health-related quality of life (QOL) for patients with metastatic breast cancer (MBC) receiving first-line endocrine-based therapy (palbociclib plus letrozole and letrozole alone). Patients and methods Treatment-na{\"i}ve postmenopausal women with estrogen receptor-positive (ER+)/human epidermal growth factor receptor 2-negative (HER2-) MBC were randomized 2~:~1 to palbociclib plus letrozole (n=444) or placebo plus letrozole (n=222). Patient-reported outcomes were assessed at baseline, day 1 of cycles 2 and 3, and day 1 of every other cycle from cycle 5 using the Functional Assessment of Cancer Therapy (FACT)-Breast and EuroQOL 5 dimensions (EQ-5D) questionnaires. Results As of 26 February 2016, the median duration of follow-up was 23months. Baseline scores were comparable between the two treatment arms. No significant between-arm differences were observed in change from baseline in FACT-Breast Total, FACT-General Total, or EQ-5D scores. Significantly greater improvement in pain scores was observed in the palbociclib plus letrozole arm (-0.256 versus -0.098; P=0.0183). In both arms, deterioration of FACT-Breast Total score was significantly delayed in patients without progression versus those with progression and patients with partial or complete response versus those without. No significant difference was observed in FACT-Breast and EQ-5D index scores in patients with and without neutropenia. Conclusions Overall, women with MBC receiving first-line endocrine therapy have a good QOL. The addition of palbociclib to letrozole maintains health-related QOL and improves pain scores in treatment-na{\"i}ve postmenopausal patients with ER+/HER2- MBC compared with letrozole alone. Significantly greater delay in deterioration of health-related QOL was observed in patients without progression versus those who progressed and in patients with an objective response versus non-responders. ClinicalTrials.gov: NCT01740427 (https://clinicaltrials.gov/ct2/show/NCT01740427)},
  keywords = {_tablet,ER+/HER2-,health-related quality of life,letrozole,metastatic breast cancer,palbociclib},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rugo et al_2018_Impact of palbociclib plus letrozole on patient-reported health-related quality.pdf;/Users/nseewald/Zotero/storage/NJNDKMH5/S0923753419454628.html}
}

@article{rumseyGeneralizedBayesianMARS2024,
  title = {Generalized {{Bayesian MARS}}: {{Tools}} for {{Stochastic Computer Model Emulation}}},
  shorttitle = {Generalized {{Bayesian MARS}}},
  author = {Rumsey, Kellin N. and Francom, Devin and Shen, Andy},
  year = {2024},
  month = jun,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {12},
  number = {2},
  pages = {646--666},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/23M1577122},
  urldate = {2025-01-17},
  abstract = {Gaussian processes are widely used in the analysis of data from a computer model. Ideally, the analysis will yield accurate predictions with correct coverage probabilities of credible intervals. In this paper, we first review several existing Bayesian implementations in the literature. We show that Bayesian approaches with squared-exponential correlation structure do not always quantify well the uncertainty in prediction. Thus, we propose new Bayesian approaches with power-exponential or Mat{\'e}rn correlation structure, which have more flexibility. Through application examples and a simulation study, we show that the proposed Bayesian methods not only have superior prediction accuracy but are closer to having the correct coverage probability.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rumsey et al. - 2024 - Generalized Bayesian MARS Tools for Stochastic Computer Model Emulation.pdf}
}

@article{ruppertApplicationSequentialMultiple2019,
  title = {Application of a Sequential Multiple Assignment Randomized Trial ({{SMART}}) Design in Older Patients with Chronic Lymphocytic Leukemia},
  author = {Ruppert, A S and Yin, J and Davidian, M and Tsiatis, A A and Byrd, J C and Woyach, J A and Mandrekar, S J},
  year = {2019},
  month = apr,
  journal = {Annals of Oncology},
  volume = {30},
  number = {4},
  pages = {542--550},
  issn = {0923-7534, 1569-8041},
  doi = {10/gf4dms},
  urldate = {2019-06-27},
  abstract = {Background: Ibrutinib therapy is safe and effective in patients with chronic lymphocytic leukemia (CLL). Currently, ibrutinib is administered continuously until disease progression. Combination regimens with ibrutinib are being developed to deepen response which could allow for ibrutinib maintenance (IM) discontinuation. Among untreated older patients with CLL, clinical investigators had the following questions: (i) does ibrutinib {\th} venetoclax {\th} obinutuzumab (IVO) with IM have superior progression-free survival (PFS) compared with ibrutinib {\th} obinutuzumab (IO) with IM, and (ii) does the treatment strategy of IVO {\th} IM for patients without minimal residual disease complete response (MRD- CR) or IVO {\th} IM discontinuation for patients with MRD- CR have superior PFS compared with IO {\th} IM. Design: Conventional designs randomize patients to IO with IM or IVO with IM to address the first objective, or randomize patients to each treatment strategy to address the second objective. A sequential multiple assignment randomized trial (SMART) design and analysis is proposed to address both objectives. Results: A SMART design strategy is appropriate when comparing adaptive interventions, which are defined by an individual's sequence of treatment decisions and guided by intermediate outcomes, such as response to therapy. A review of common applications of SMART design strategies is provided. Specific to the SMART design previously considered for Alliance study A041702, the general structure of the SMART is presented, an approach to sample size and power calculations when comparing adaptive interventions embedded in the SMART with a time-to-event end point is fully described, and analyses plans are outlined. Conclusion: SMART design strategies can be used in cancer clinical trials with adaptive interventions to identify optimal treatment strategies. Further, standard software exists to provide sample size, power calculations, and data analysis for a SMART design.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ruppert et al_2019_Application of a sequential multiple assignment randomized trial (SMART) design.pdf}
}

@article{rutkowEffectFloridaPrescription2015,
  title = {Effect of {{Florida}}'s {{Prescription Drug Monitoring Program}} and {{Pill Mill Laws}} on {{Opioid Prescribing}} and {{Use}}},
  author = {Rutkow, Lainie and Chang, Hsien-Yen and Daubresse, Matthew and Webster, Daniel W. and Stuart, Elizabeth A. and Alexander, G. Caleb},
  year = {2015},
  month = oct,
  journal = {JAMA Internal Medicine},
  volume = {175},
  number = {10},
  pages = {1642--1649},
  issn = {2168-6106},
  doi = {10/gdz4v7},
  urldate = {2021-07-01},
  abstract = {Prescription Drug Monitoring Program (PDMP) and pill mill laws are among the principal means states use to reduce prescription drug abuse and diversion, yet little high-quality evidence exists regarding their effect.To quantify the effect of Florida's PDMP and pill mill laws on overall and high-risk opioid prescribing and use.We applied comparative interrupted time-series analyses to IMS Health LifeLink LRx data to characterize the effect of PDMP and pill mill law implementation on a closed cohort of prescribers, retail pharmacies, and patients from July 2010 through September 2012 in Florida (intervention state) compared with Georgia (control state). We conducted sensitivity analyses, including varying length of observation and modifying requirements for continuous observation of individuals throughout the study period.Total opioid volume, mean morphine milligram equivalent (MME) per transaction, mean days' supply per transaction, and total number of opioid prescriptions dispensed. Analyses were conducted per prescriber and per patient, in aggregate and after stratifying by volume of baseline opioid prescribing for prescribers and use for patients.From July 2010 through September 2012, a cohort of 2.6 million patients, 431\,890 prescribers, and 2829 pharmacies was associated with approximately 480 million prescriptions in Florida and Georgia, 7.7\% of which were for opioids. Total monthly opioid volume, MME per transaction, days' supply, and prescriptions dispensed were higher in Florida than Georgia before implementation. Florida's laws were associated with statistically significant declines in opioid volume (2.5 kg/mo, P\,\&lt;\,.05; equivalent to approximately 500\,000 5-mg tablets of hydrocodone bitartrate per month) and MME per transaction (0.45 mg/mo, P\,\&lt;\,.05), without any change in days' supply. Twelve months after implementation, the policies were associated with approximately a 1.4\% decrease in opioid prescriptions, 2.5\% decrease in opioid volume, and 5.6\% decrease in MME per transaction. Reductions were limited to prescribers and patients with the highest baseline opioid prescribing and use. Sensitivity analyses, varying time windows, and enrollment criteria supported the main results.Florida's PDMP and pill mill laws were associated with modest decreases in opioid prescribing and use. Decreases were greatest among prescribers and patients with the highest baseline opioid prescribing and use.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Rutkow et al_2015_Effect of Florida’s Prescription Drug Monitoring Program and Pill Mill Laws on.pdf;/Users/nseewald/Zotero/storage/LS7V3M5K/2429105.html}
}

@article{ryanNowTrendingCoping2019,
  title = {Now Trending: {{Coping}} with Non-Parallel Trends in Difference-in-Differences Analysis},
  shorttitle = {Now Trending},
  author = {Ryan, Andrew M and Kontopantelis, Evangelos and Linden, Ariel and Burgess, James F},
  year = {2019},
  month = dec,
  journal = {Statistical Methods in Medical Research},
  volume = {28},
  number = {12},
  pages = {3697--3711},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280218814570},
  urldate = {2023-08-16},
  abstract = {Difference-in-differences (DID) analysis is used widely to estimate the causal effects of health policies and interventions. A critical assumption in DID is ``parallel trends'': that pre-intervention trends in outcomes are the same between treated and comparison groups. To date, little guidance has been available to researchers who wish to use DID when the parallel trends assumption is violated. Using a Monte Carlo simulation experiment, we tested the performance of several estimators (standard DID; DID with propensity score matching; single-group interrupted time-series analysis; and multi-group interrupted time-series analysis) when the parallel trends assumption is violated. Using nationwide data from US hospitals (n\,=\,3737) for seven data periods (four pre-interventions and three post-interventions), we used alternative estimators to evaluate the effect of a placebo intervention on common outcomes in health policy (clinical process quality and 30-day risk-standardized mortality for acute myocardial infarction, heart failure, and pneumonia). Estimator performance was assessed using mean-squared error and estimator coverage. We found that mean-squared error values were considerably lower for the DID estimator with matching than for the standard DID or interrupted time-series analysis models. The DID estimator with matching also had superior performance for estimator coverage. Our findings were robust across all outcomes evaluated.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ryan et al_2019_Now trending.pdf}
}

@article{sachsConfidenceBandsSurvival2022,
  title = {Confidence Bands in Survival Analysis},
  author = {Sachs, Michael C. and Brand, Adam and Gabriel, Erin E.},
  year = {2022},
  month = nov,
  journal = {British Journal of Cancer},
  volume = {127},
  number = {9},
  pages = {1636--1641},
  publisher = {Nature Publishing Group},
  issn = {1532-1827},
  doi = {10.1038/s41416-022-01920-5},
  urldate = {2024-03-13},
  abstract = {Providing estimates of uncertainty for statistical quantities is important for statistical inference. When the statistical quantity of interest is a survival curve, which is a function over time, the appropriate type of uncertainty estimate is a confidence band constructed to account for the correlation between points on the curve, we will call this a simultaneous confidence band. This, however, is not the type of confidence band provided in standard software, which is constructed by joining the confidence intervals at given time points.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Outcomes research,Statistics},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sachs_et_al_2022_Confidence_bands_in_survival_analysis.pdf}
}

@article{saczynskiCommonlyUsedDatacollection2013,
  title = {Commonly {{Used Data-collection Approaches}} in {{Clinical Research}}},
  author = {Saczynski, Jane S. and McManus, David D. and Goldberg, Robert J.},
  year = {2013},
  month = nov,
  journal = {The American Journal of Medicine},
  volume = {126},
  number = {11},
  pages = {946--950},
  issn = {00029343},
  doi = {10.1016/j.amjmed.2013.04.016},
  urldate = {2018-10-12},
  abstract = {We provide an overview of the different data-collection approaches that are commonly used in carrying out clinical, public health, and translational research. We discuss several of the factors that researchers need to consider in using data collected in questionnaire surveys, from proxy informants, through the review of medical records, and in the collection of biologic samples. We hope that the points raised in this overview will lead to the collection of rich and high-quality data in observational studies and randomized controlled trials.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Saczynski et al_2013_Commonly Used Data-collection Approaches in Clinical Research.pdf}
}

@article{samuelsUseAnalysisCovariance1986,
  title = {Use of Analysis of Covariance in Clinical Trials: {{A}} Clarification},
  shorttitle = {Use of Analysis of Covariance in Clinical Trials},
  author = {Samuels, Myra L.},
  year = {1986},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {7},
  number = {4},
  pages = {325--329},
  issn = {01972456},
  doi = {10/bqgsxd},
  urldate = {2019-12-09},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Samuels_1986_Use of analysis of covariance in clinical trials.pdf}
}

@article{sanchez-mecaEffectSizeIndicesDichotomized2003,
  title = {Effect-{{Size Indices}} for {{Dichotomized Outcomes}} in {{Meta-Analysis}}.},
  author = {{S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and {Chac{\'o}n-Moscoso}, Salvador},
  year = {2003},
  journal = {Psychological Methods},
  volume = {8},
  number = {4},
  pages = {448--467},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.8.4.448},
  urldate = {2018-10-12},
  abstract = {It is very common to find meta-analyses in which some of the studies compare 2 groups on continuous dependent variables and others compare groups on dichoto-mized variables. Integrating all of them in a meta-analysis requires an effect-size index in the same metric that can be applied to both types of outcomes. In this article, the performance in terms of bias and sampling variance of 7 different effect-size indices for estimating the population standardized mean difference from a 2 {\texttimes} 2 table is examined by Monte Carlo simulation, assuming normal and nonnormal distributions. The results show good performance for 2 indices, one based on the probit transformation and the other based on the logistic distribution. In the last 20 years, meta-analysis has become a very popular and useful research methodology to in-tegrate the results of a set of empirical studies about a given topic. To carry out a meta-analysis an effect-size index has to be selected to translate the results of every study into a common metric.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sánchez-Meca et al_2003_Effect-Size Indices for Dichotomized Outcomes in Meta-Analysis.pdf}
}

@article{sanfeliciIndividualizedDiagnosticPrognostic2020,
  title = {Individualized {{Diagnostic}} and {{Prognostic Models}} for {{Patients With Psychosis Risk Syndromes}}: {{A Meta-analytic View}} on the {{State}} of the {{Art}}},
  shorttitle = {Individualized {{Diagnostic}} and {{Prognostic Models}} for {{Patients With Psychosis Risk Syndromes}}},
  author = {Sanfelici, Rachele and Dwyer, Dominic B. and Antonucci, Linda A. and Koutsouleris, Nikolaos},
  year = {2020},
  month = aug,
  journal = {Biological Psychiatry},
  series = {Psychosis {{Risk Syndrome}}},
  volume = {88},
  number = {4},
  pages = {349--360},
  issn = {0006-3223},
  doi = {10.1016/j.biopsych.2020.02.009},
  urldate = {2022-08-17},
  abstract = {Background The clinical high risk (CHR) paradigm has facilitated research into the underpinnings of help-seeking individuals at risk for developing psychosis, aiming at predicting and possibly preventing transition to the overt disorder. Statistical methods such as machine learning and Cox regression have provided the methodological basis for this research by enabling the construction of diagnostic models (i.e., distinguishing CHR individuals from healthy individuals) and prognostic models (i.e., predicting a future outcome) based on different data modalities, including clinical, neurocognitive, and neurobiological data. However, their translation to clinical practice is still hindered by the high heterogeneity of both CHR populations and methodologies applied. Methods We systematically reviewed the literature on diagnostic and prognostic models built on Cox regression and machine learning. Furthermore, we conducted a meta-analysis on prediction performances investigating heterogeneity of methodological approaches and data modality. Results A total of 44 articles were included, covering 3707 individuals for prognostic studies and 1052 individuals for diagnostic studies (572 CHR patients and 480 healthy control subjects). CHR patients could be classified against healthy control subjects with 78\% sensitivity and 77\% specificity. Across prognostic models, sensitivity reached 67\% and specificity reached 78\%. Machine learning models outperformed those applying Cox regression by 10\% sensitivity. There was a publication bias for prognostic studies yet no other moderator effects. Conclusions Our results may be driven by substantial clinical and methodological heterogeneity currently affecting several aspects of the CHR field and limiting the clinical implementability of the proposed models. We discuss conceptual and methodological harmonization strategies to facilitate more reliable and generalizable models for future clinical practice.},
  langid = {english},
  keywords = {Biomarkers,Clinical psychobiology,Machine learning,Predictive psychiatry,Psychosis,Translational medicine},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sanfelici et al_2020_Individualized Diagnostic and Prognostic Models for Patients With Psychosis.pdf;/Users/nseewald/Zotero/storage/R33KBEWQ/S0006322320300986.html}
}

@article{santannaDoublyRobustDifferenceinDifferences2020,
  title = {Doubly {{Robust Difference-in-Differences Estimators}}},
  author = {Sant'Anna, Pedro H. C. and Zhao, Jun B.},
  year = {2020},
  month = may,
  journal = {arXiv:1812.01723 [econ]},
  eprint = {1812.01723},
  primaryclass = {econ},
  urldate = {2021-07-16},
  abstract = {This article proposes doubly robust estimators for the average treatment effect on the treated (ATT) in difference-in-differences (DID) research designs. In contrast to alternative DID estimators, the proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. We also derive the semiparametric efficiency bound for the ATT in DID designs when either panel or repeated cross-section data are available, and show that our proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified. Furthermore, we quantify the potential efficiency gains of having access to panel data instead of repeated cross-section data. Finally, by paying articular attention to the estimation method used to estimate the nuisance parameters, we show that one can sometimes construct doubly robust DID estimators for the ATT that are also doubly robust for inference. Simulation studies and an empirical application illustrate the desirable finite-sample performance of the proposed estimators. Open-source software for implementing the proposed policy evaluation tools is available.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sant'Anna_Zhao_2020_Doubly Robust Difference-in-Differences Estimators.pdf;/Users/nseewald/Zotero/storage/R3ZUAJ9A/1812.html}
}

@article{sattenKaplanMeierEstimator2001,
  title = {The {{Kaplan}}--{{Meier Estimator}} as an {{Inverse-Probability-of-Censoring Weighted Average}}},
  author = {Satten, Glen A and Datta, Somnath},
  year = {2001},
  month = aug,
  journal = {The American Statistician},
  volume = {55},
  number = {3},
  pages = {207--210},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/000313001317098185},
  urldate = {2024-07-18},
  abstract = {The Kaplan--Meier (product-limit) estimator of the survival function of randomly censored time-to-event data is a central quantity in survival analysis. It is usually introduced as a non-parametric maximum likelihood estimator, or else as the output of an imputation scheme for censored observations such as redistribute-to-the-right or self-consistency.Following recent work by Robins and Rotnitzky, we show that the Kaplan--Meier estimator can also be represented as a weighted average of identically distributed terms, where the weights are related to the survival function of censoring times. We give two demonstrations of this representation; the first assumes a Kaplan--Meier form for the censoring time survival function, the second estimates the survival functions of failure and censoring times simultaneously and can be developed without prior introduction to the Kaplan--Meier estimator.},
  pmid = {28845048},
  keywords = {Horvitz-Thompson estimator,Product-limit estimator,Survival analysis},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Satten_Datta_2001_The Kaplan–Meier Estimator as an Inverse-Probability-of-Censoring Weighted.pdf}
}

@article{savilleBayesianTimeMachine2022,
  title = {The {{Bayesian Time Machine}}: {{Accounting}} for Temporal Drift in Multi-Arm Platform Trials},
  shorttitle = {The {{Bayesian Time Machine}}},
  author = {Saville, Benjamin R and Berry, Donald A and Berry, Nicholas S and Viele, Kert and Berry, Scott M},
  year = {2022},
  month = oct,
  journal = {Clinical Trials},
  volume = {19},
  number = {5},
  pages = {490--501},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/17407745221112013},
  urldate = {2024-01-22},
  abstract = {Background Multi-arm platform trials investigate multiple agents simultaneously, typically with staggered entry and exit of experimental treatment arms versus a shared control arm. In such settings, there is considerable debate whether to limit analyses for a treatment arm to concurrent randomized control subjects or to allow comparisons to both concurrent and non-concurrent (pooled) control subjects. The potential bias from temporal drift over time is at the core of this debate. Methods We propose time-adjusted analyses, including a ``Bayesian Time Machine,'' to model potential temporal drift in the entire study population, such that primary analyses can incorporate all randomized control subjects from the platform trial. We conduct a simulation study to assess performance relative to utilizing concurrent or pooled controls. Results In multi-arm platform trials with staggered entry, analyses adjusting for temporal drift (either Bayesian or frequentist) have superior estimation of treatment effects and favorable testing properties compared to analyses using either concurrent or pooled controls. The Bayesian Time Machine generally provides estimates with greater precision and smaller mean square error than alternative approaches, at the risk of small bias and small Type I error inflation. Conclusions The Bayesian Time Machine provides a compromise between bias and precision by smoothing estimates across time and leveraging all available data for the estimation of treatment effects. Prior distributions controlling the behavior of dynamic smoothing across time must be pre-specified and carefully calibrated to the unique context of each trial, appropriately accounting for the population, disease, and endpoints.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Saville et al_2022_The Bayesian Time Machine.pdf}
}

@article{schaubelSequentialStratificationMethod2006,
  title = {A {{Sequential Stratification Method}} for {{Estimating}} the {{Effect}} of a {{Time-Dependent Experimental Treatment}} in {{Observational Studies}}},
  author = {Schaubel, Douglas E. and Wolfe, Robert A. and Port, Friedrich K.},
  year = {2006},
  month = sep,
  journal = {Biometrics},
  volume = {62},
  number = {3},
  pages = {910--917},
  issn = {0006-341X},
  doi = {10.1111/j.1541-0420.2006.00527.x},
  urldate = {2024-07-10},
  abstract = {Survival analysis is often used to compare experimental and conventional treatments. In observational studies, the therapy may change during follow-up and such crossovers can be summarized by time-dependent covariates. Given the ever-increasing donor organ shortage, higher-risk kidneys from expanded criterion donors (ECD) are being transplanted. Transplant candidates can choose whether to accept an ECD organ (experimental therapy), or to remain on dialysis and wait for a possible non-ECD transplant later (conventional therapy). A three-group time-dependent analysis of such data involves estimating parameters corresponding to two time-dependent indicator covariates representing ECD transplant and non-ECD transplant, each compared to remaining on dialysis on the waitlist. However, the ECD hazard ratio estimated by this time-dependent analysis fails to account for the fact that patients who forego an ECD transplant are not destined to remain on dialysis forever, but could subsequently receive a non-ECD transplant. We propose a novel method of estimating the survival benefit of ECD transplantation relative to conventional therapy (waitlist with possible subsequent non-ECD transplant). Compared to the time-dependent analysis, the proposed method more accurately characterizes the data structure and yields a more direct estimate of the relative outcome with an ECD transplant.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Schaubel et al_2006_A Sequential Stratification Method for Estimating the Effect of a.pdf;/Users/nseewald/Zotero/storage/3HKI8AZ3/7321809.html}
}

@article{schaubelSequentialStratificationMethod2006a,
  title = {A {{Sequential Stratification Method}} for {{Estimating}} the {{Effect}} of a {{Time-Dependent Experimental Treatment}} in {{Observational Studies}}},
  author = {Schaubel, Douglas E. and Wolfe, Robert A. and Port, Friedrich K.},
  year = {2006},
  month = sep,
  journal = {Biometrics},
  volume = {62},
  number = {3},
  pages = {910--917},
  issn = {0006-341X},
  doi = {10.1111/j.1541-0420.2006.00527.x},
  urldate = {2024-07-26},
  abstract = {Survival analysis is often used to compare experimental and conventional treatments. In observational studies, the therapy may change during follow-up and such crossovers can be summarized by time-dependent covariates. Given the ever-increasing donor organ shortage, higher-risk kidneys from expanded criterion donors (ECD) are being transplanted. Transplant candidates can choose whether to accept an ECD organ (experimental therapy), or to remain on dialysis and wait for a possible non-ECD transplant later (conventional therapy). A three-group time-dependent analysis of such data involves estimating parameters corresponding to two time-dependent indicator covariates representing ECD transplant and non-ECD transplant, each compared to remaining on dialysis on the waitlist. However, the ECD hazard ratio estimated by this time-dependent analysis fails to account for the fact that patients who forego an ECD transplant are not destined to remain on dialysis forever, but could subsequently receive a non-ECD transplant. We propose a novel method of estimating the survival benefit of ECD transplantation relative to conventional therapy (waitlist with possible subsequent non-ECD transplant). Compared to the time-dependent analysis, the proposed method more accurately characterizes the data structure and yields a more direct estimate of the relative outcome with an ECD transplant.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Schaubel_et_al_2006_A_Sequential_Stratification_Method_for_Estimating_the_Effect_of_a.pdf}
}

@techreport{schellEvaluatingMethodsEstimate2018,
  title = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}: {{A Simulation Study}}},
  shorttitle = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}},
  author = {Schell, Terry L. and Griffin, Beth Ann and Morral, Andrew R.},
  year = {2018},
  month = dec,
  institution = {RAND Corporation},
  urldate = {2021-10-22},
  abstract = {The authors use simulations to assess the performance of a wide range of statistical models commonly used in the gun policy literature to estimate the effects of state-level gun policies on firearm deaths and to identify the most-appropriate statistical methods for producing estimates. The results suggest substantial statistical problems with many of the methods used in this field. The authors identify the best method among those assessed.},
  langid = {english},
  keywords = {Crime and Violence Prevention,Firearms,Gun Policy,Gun Violence,Public Safety Legislation,RAND-initiated,Statistical Analysis Methodology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Schell et al_2018_Evaluating Methods to Estimate the Effect of State Laws on Firearm Deaths.pdf}
}

@article{schmitzSequentialMultipleAssignment2018,
  title = {A Sequential Multiple Assignment Randomized Trial for Cocaine Cessation and Relapse Prevention: {{Tailoring}} Treatment to the Individual},
  shorttitle = {A Sequential Multiple Assignment Randomized Trial for Cocaine Cessation and Relapse Prevention},
  author = {Schmitz, Joy M. and Stotts, Angela L. and Vujanovic, Anka A. and Weaver, Michael F. and Yoon, Jin H. and Vincent, Jessica and Green, Charles E.},
  year = {2018},
  month = feb,
  journal = {Contemporary Clinical Trials},
  volume = {65},
  pages = {109--115},
  issn = {1551-7144},
  doi = {10/gc3tqr},
  urldate = {2021-06-29},
  abstract = {Drug addiction is a chronic, devastating, but treatable disorder. A core principle of drug addiction treatment states that no single treatment is appropriate for everyone (NIDA, 2012); treatments need to adjust based on patient characteristics and response in order to be maximally effective. For cocaine use disorders (CUD), specifically, the most potent intervention currently available for initiating abstinence is behavior therapy using contingency management (CM) procedures, with early cessation being a robust predictor of future abstinence. This raises two key questions for treatment development research: First, can we significantly improve initial CM response rates with targeted adjunctive interventions? Second, for individuals who fail to achieve initial abstinence with CM, is pharmacotherapy an effective augmentation strategy? This paper describes how a sequential, multiple assignment, randomized trial (SMART) design has advantages over a fixed-intervention approach when it comes to collecting data needed to answer both questions. The first aim will examine whether Acceptance and Commitment Therapy (ACT) in combination with CM increases initial abstinence response rates (i.e., 2 consecutive weeks of cocaine-negative urine screens). The second aim will examine whether ACT+CM in combination with modafinil promotes abstinence achievement in initial non-responders. Results are expected to inform how we tailor treatment of CUD to maximize outcomes.},
  langid = {english},
  keywords = {Acceptance and Commitment Therapy (ACT),Bayesian approach,Cocaine use disorder,Contingency management (CM),Modafinil,Multiple assignment,Randomized trial (SMART),Sequential},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Schmitz et al_2018_A sequential multiple assignment randomized trial for cocaine cessation and.pdf;G\:\\My Drive\\Papers\\Schmitz_et_al_2018_A_sequential_multiple_assignment_randomized_trial_for_cocaine_cessation_and2.pdf;/Users/nseewald/Zotero/storage/2MIFLLWZ/S1551714417305426.html;/Users/nseewald/Zotero/storage/DRKNSXXU/S1551714417305426.html}
}

@article{schochetStatisticalPowerEstimating2022,
  title = {Statistical {{Power}} for {{Estimating Treatment Effects Using Difference-in-Differences}} and {{Comparative Interrupted Time Series Estimators With Variation}} in {{Treatment Timing}}},
  author = {Schochet, Peter Z.},
  year = {2022},
  month = aug,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {47},
  number = {4},
  pages = {367--405},
  publisher = {American Educational Research Association},
  issn = {1076-9986},
  doi = {10.3102/10769986211070625},
  urldate = {2022-11-16},
  abstract = {This article develops new closed-form variance expressions for power analyses for commonly used difference-in-differences (DID) and comparative interrupted time series (CITS) panel data estimators. The main contribution is to incorporate variation in treatment timing into the analysis. The power formulas also account for other key design features that arise in practice: autocorrelated errors, unequal measurement intervals, and clustering due to the unit of treatment assignment. We consider power formulas for both cross-sectional and longitudinal models and allow for covariates. An illustrative power analysis provides guidance on appropriate sample sizes. The key finding is that accounting for treatment timing increases required sample sizes. Further, DID estimators have considerably more power than standard CITS and ITS estimators. An available Shiny R dashboard performs the sample size calculations for the considered estimators.},
  langid = {english},
  keywords = {_tablet_modified},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Schochet_2022_Statistical_Power_for_Estimating_Treatment_Effects_Using.pdf}
}

@article{schulteAlearningMethodsEstimating2014,
  ids = {schulteMathbfQMathbfALearning2014},
  title = {Q- and {{A-learning Methods}} for {{Estimating Optimal Dynamic Treatment Regimes}}},
  author = {Schulte, Phillip J. and Tsiatis, Anastasios A. and Laber, Eric B. and Davidian, Marie},
  year = {2014},
  month = nov,
  journal = {Statistical science : a review journal of the Institute of Mathematical Statistics},
  volume = {29},
  number = {4},
  pages = {640--661},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  doi = {10/f6wvtr},
  urldate = {2020-04-24},
  abstract = {In clinical practice, physicians make a series of treatment decisions over the course of a patient's disease based on his/her baseline and evolving characteristics. A dynamic treatment regime is a set of sequential decision rules that operationalizes this process. Each rule corresponds to a decision point and dictates the next treatment action based on the accrued information. Using existing data, a key goal is estimating the optimal regime, that, if followed by the patient population, would yield the most favorable outcome on average. Q- and A-learning are two main approaches for this purpose. We provide a detailed account of these methods, study their performance, and illustrate them using data from a depression study.},
  mrnumber = {MR3300363},
  pmcid = {PMC4300556},
  pmid = {25620840},
  zmnumber = {1331.62437},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Schulte et al_2014_Q- and A-learning Methods for Estimating Optimal Dynamic Treatment Regimes.pdf;/Users/nseewald/Zotero/storage/5RAUEZJB/1421330551.html}
}

@article{schulzSampleSizeCalculations2005,
  title = {Sample Size Calculations in Randomised Trials: Mandatory and Mystical},
  shorttitle = {Sample Size Calculations in Randomised Trials},
  author = {Schulz, Kenneth F and Grimes, David A},
  year = {2005},
  month = apr,
  journal = {The Lancet},
  volume = {365},
  number = {9467},
  pages = {1348--1353},
  issn = {0140-6736},
  doi = {10/d49sfp},
  urldate = {2021-04-13},
  abstract = {Investigators should properly calculate sample sizes before the start of their randomised trials and adequately describe the details in their published report. In these a-priori calculations, determining the effect size to detect---eg, event rates in treatment and control groups---reflects inherently subjective clinical judgments. Furthermore, these judgments greatly affect sample size calculations. We question the branding of trials as unethical on the basis of an imprecise sample size calculation process. So-called underpowered trials might be acceptable if investigators use methodological rigor to eliminate bias, properly report to avoid misinterpretation, and always publish results to avert publication bias. Some shift of emphasis from a fixation on sample size to a focus on methodological quality would yield more trials with less bias. Unbiased trials with imprecise results trump no results at all. Clinicians and patients deserve guidance now.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Schulz_Grimes_2005_Sample size calculations in randomised trials.pdf}
}

@article{seewaldAdaptiveInterventionsDynamic2023,
  title = {Adaptive {{Interventions}} for a {{Dynamic}} and {{Responsive Public Health Approach}}},
  author = {Seewald, Nicholas J.},
  year = {2023},
  month = jan,
  journal = {American Journal of Public Health},
  volume = {113},
  number = {1},
  pages = {37--39},
  publisher = {American Public Health Association},
  issn = {0090-0036},
  doi = {10.2105/AJPH.2022.307157},
  urldate = {2022-12-15},
  copyright = {All rights reserved},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seewald_2023_Adaptive Interventions for a Dynamic and Responsive Public Health Approach.pdf}
}

@misc{seewaldBuildingEffectiveAdaptive2022,
  title = {Building {{Effective Adaptive Interventions}} in {{Mental Health Services Research}}},
  author = {Seewald, Nicholas J.},
  year = {2022},
  month = feb,
  publisher = {{Johns Hopkins Alacrity Center for Health and Longevity in Mental Illness}},
  urldate = {2022-09-02},
  abstract = {This methods workshop is part of a series put on by the NIMH-funded Johns Hopkins ALACRITY Center for Health and Longevity in Mental Illness. The focus of the Johns Hopkins ALACRITY Center is to translate evidence-based interventions to reduce premature mortality among consumers with serious mental illness (SMI) into community mental health settings in Maryland and Nationwide. This workshop series aims to highlight advanced quantitative methods and mixed methods to help answer important questions in mental health services research. Lecture 5: Building Effective Adaptive Interventions in Mental Health Services Research is presented by Dr. Nicholas Seewald, a Postdoctoral Fellow in the Department of Health Policy and Management at the Johns Hopkins University Bloomberg School of Public Health. Dr. Seewald is an ALACRITY Center trainee. Lecture 5 includes Section A: Adaptive Interventions Section B: Sequential, Multiple-Assignment Randomized Trials (SMARTs) Section C: Design and Analysis Considerations for SMARTs},
  keywords = {me},
  file = {/Users/nseewald/Zotero/storage/3TX9VCHV/playlist.html}
}

@phdthesis{seewaldDesignAnalyticConsiderations2021a,
  type = {Thesis},
  title = {Design and {{Analytic Considerations}} for {{Sequential}}, {{Multiple-Assignment Randomized Trials}} with {{Longitudinal Outcomes}}},
  author = {Seewald, Nicholas},
  year = {2021},
  doi = {10.7302/2671},
  urldate = {2024-08-13},
  abstract = {Clinicians and researchers alike are increasingly interested in how best to personalize interventions. A dynamic treatment regimen (DTR) is a sequence of pre-specified decision rules which can be used to guide the delivery of a sequence of treatments or interventions that are tailored to the changing needs of the individual. The sequential multiple-assignment randomized trial (SMART) is a research tool which allows for the construction of effective DTRs. SMARTs are multi-stage randomized trials in which some or all participants are randomized more than once, with each randomization corresponding to an open scientific question which will aid in the development of a high-quality DTR. In this dissertation, we develop a suite of tools which aid investigators in the design and analysis of SMARTs with continuous, longitudinal outcomes which are collected throughout the multiple stages of the trial.  We begin by deriving easy-to-use formulae for computing the total sample size for three common two-stage SMART designs in which the primary aim is to compare mean end-of-study outcomes for two embedded DTRs which recommend different first-stage treatments. The formulae are derived in the context of a regression model which leverages information from a longitudinal outcome collected over the entire study. We show that the sample size formula for a SMART can be written as the product of the sample size formula for a standard two-arm randomized trial, a deflation factor that accounts for the increased statistical efficiency resulting from a longitudinal analysis, and an inflation factor that accounts for the design of a SMART. The SMART design inflation factor is typically a function of the anticipated probability of response to first-stage treatment. We review modeling and estimation for DTR effect analyses using a longitudinal outcome from a SMART, as well as the estimation of standard errors. We also present estimators for the covariance matrix for a variety of common working correlation structures. Methods are motivated using the ENGAGE study, a SMART aimed at developing a DTR for increasing motivation to attend treatments among alcohol- and cocaine-dependent patients. Randomized trials are often constrained by limited financial resources; SMARTs are no different. The longitudinal deflation factor we develop allows for reduction in sample size requirements via both within-person correlation and the repeated measurements of the outcome over time. We provide guidance on how to balance sample size and the number of measurement occasions to minimize total cost of recruitment and measurement while achieving a target power. Finally, we introduce a procedure to generate data from a longitudinal SMART that will achieve an arbitrary desired covariance structure on potential outcomes, averaged over response status. This procedure, as well as user-friendly sample size tools which solve the cost optimization problems, are available in an R package called longsmart.},
  copyright = {All rights reserved},
  langid = {american},
  annotation = {Accepted: 2021-09-24T19:03:53Z},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seewald_2021_Design_and_Analytic_Considerations_for_Sequential,_Multiple-Assignment.pdf}
}

@misc{seewaldMRTSSCalculatorShiny2016,
  title = {{{MRT-SS Calculator}}: {{An R Shiny Application}} for {{Sample Size Calculation}} in {{Micro-Randomized Trials}}},
  shorttitle = {{{MRT-SS Calculator}}},
  author = {Seewald, Nicholas J. and Sun, Ji and Liao, Peng},
  year = {2016},
  month = sep,
  eprint = {1609.00695},
  urldate = {2018-10-12},
  abstract = {The micro-randomized trial (MRT) is a new experimental design which allows for the investigation of the proximal effects of a ``just-in-time'' treatment, often provided via a mobile device as part of a mobile health intervention. As with a traditional randomized controlled trial, computing the minimum required sample size to achieve a desired power is a crucial step in designing an MRT. We present MRT-SS Calculator, an online sample-size calculator for micro-randomized trials, built with R Shiny. MRT-SS Calculator requires specification of time-varying patterns for the proximal treatment effect and expected treatment availability. We illustrate the implementation of MRTSS Calculator using a mobile health trial, HeartSteps. The application can be accessed from https://pengliao.shinyapps.io/mrt-calculator.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seewald et al_2016_MRT-SS Calculator.pdf}
}

@article{seewaldPracticalConsiderationsData2019,
  title = {Practical {{Considerations}} for {{Data Collection}} and {{Management}} in {{Mobile Health Micro-randomized Trials}}},
  author = {Seewald, Nicholas J. and Smith, Shawna N. and Lee, Andy Jinseok and Klasnja, Predrag and Murphy, Susan A.},
  year = {2019},
  month = jan,
  journal = {Statistics in Biosciences},
  volume = {11},
  pages = {355--370},
  issn = {1867-1764, 1867-1772},
  doi = {10/gfsvx7},
  urldate = {2019-01-07},
  abstract = {There is a growing interest in leveraging the prevalence of mobile technology to improve health by delivering momentary, contextualized interventions to individuals' smartphones. A just-in-time adaptive intervention (JITAI) adjusts to an individual's changing state and/or context to provide the right treatment, at the right time, in the right place. Micro-randomized trials (MRTs) allow for the collection of data which aid in the construction of an optimized JITAI by sequentially randomizing participants to different treatment options at each of many decision points throughout the study. Often, these data are collected passively using a mobile phone. To assess the causal effect of treatment on a near-term outcome, care must be taken when designing the data collection system to ensure it is of appropriately high quality. Here, we make several recommendations for collecting and managing data from an MRT. We provide advice on selecting which features to collect and when, choosing between ``agents'' to implement randomization, identifying sources of missing data, and overcoming other novel challenges. The recommendations are informed by our experience with HeartSteps, an MRT designed to test the effects of an intervention aimed at increasing physical activity in sedentary adults. We also provide a checklist which can be used in designing a data collection system so that scientists can focus more on their questions of interest, and less on cleaning data.},
  copyright = {All rights reserved},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seewald et al_2019_Practical Considerations for Data Collection and Management in Mobile Health.pdf}
}

@article{seewaldSampleSizeConsiderations2020,
  title = {Sample Size Considerations for Comparing Dynamic Treatment Regimens in a Sequential Multiple-Assignment Randomized Trial with a Continuous Longitudinal Outcome},
  author = {Seewald, Nicholas J and Kidwell, Kelley M and {Nahum-Shani}, Inbal and Wu, Tianshuang and McKay, James R and Almirall, Daniel},
  year = {2020},
  month = jul,
  journal = {Statistical Methods in Medical Research},
  volume = {29},
  number = {7},
  pages = {1891--1912},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10/gf85ss},
  urldate = {2021-03-17},
  abstract = {Clinicians and researchers alike are increasingly interested in how best to personalize interventions. A dynamic treatment regimen is a sequence of prespecified decision rules which can be used to guide the delivery of a sequence of treatments or interventions that is tailored to the changing needs of the individual. The sequential multiple-assignment randomized trial is a research tool which allows for the construction of effective dynamic treatment regimens. We derive easy-to-use formulae for computing the total sample size for three common two-stage sequential multiple-assignment randomized trial designs in which the primary aim is to compare mean end-of-study outcomes for two embedded dynamic treatment regimens which recommend different first-stage treatments. The formulae are derived in the context of a regression model which leverages information from a longitudinal outcome collected over the entire study. We show that the sample size formula for a sequential multiple-assignment randomized trial can be written as the product of the sample size formula for a standard two-arm randomized trial, a deflation factor that accounts for the increased statistical efficiency resulting from a longitudinal analysis, and an inflation factor that accounts for the design of a sequential multiple-assignment randomized trial. The sequential multiple-assignment randomized trial design inflation factor is typically a function of the anticipated probability of response to first-stage treatment. We review modeling and estimation for dynamic treatment regimen effect analyses using a longitudinal outcome from a sequential multiple-assignment randomized trial, as well as the estimation of standard errors. We also present estimators for the covariance matrix for a variety of common working correlation structures. Methods are motivated using the ENGAGE study, a sequential multiple-assignment randomized trial aimed at developing a dynamic treatment regimen for increasing motivation to attend treatments among alcohol- and cocaine-dependent patients.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {dynamic treatment regimens,longitudinal data,nosource,Sample size,sequential multiple-assignment randomized trials},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seewald et al_2020_Sample size considerations for comparing dynamic treatment regimens in a.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seewald et al_2020_Sample size considerations for comparing dynamic treatment regimens in a.pdf;/Users/nseewald/Zotero/storage/MYYC36N9/1810.html}
}

@incollection{seewaldSequentialMultipleAssignment2021,
  title = {Sequential, {{Multiple Assignment}}, {{Randomized Trials}} ({{SMART}})},
  booktitle = {Principles and {{Practice}} of {{Clinical Trials}}},
  author = {Seewald, Nicholas J. and Hackworth, Olivia and Almirall, Daniel},
  editor = {Piantadosi, Steven and Meinert, Curtis L.},
  year = {2021},
  pages = {1--19},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-52677-5_280-1},
  urldate = {2021-08-16},
  copyright = {All rights reserved},
  isbn = {978-3-319-52677-5},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seewald et al_2021_Sequential, Multiple Assignment, Randomized Trials (SMART).pdf}
}

@misc{seewaldSharedControlIndividuals2023,
  title = {Shared {{Control Individuals}} in {{Health Policy Evaluations}} with {{Application}} to {{Medical Cannabis Laws}}},
  author = {Seewald, Nicholas J. and McGinty, Emma E. and Tormohlen, Kayla and Schmid, Ian and Stuart, Elizabeth A.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.18093},
  eprint = {2311.18093},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.18093},
  urldate = {2023-12-01},
  abstract = {Health policy researchers often have questions about the effects of a policy implemented at some cluster-level unit, e.g., states, counties, hospitals, etc. on individual-level outcomes collected over multiple time periods. Stacked difference-in-differences is an increasingly popular way to estimate these effects. This approach involves estimating treatment effects for each policy-implementing unit, then, if scientifically appropriate, aggregating them to an average effect estimate. However, when individual-level data are available and non-implementing units are used as comparators for multiple policy-implementing units, data from untreated individuals may be used across multiple analyses, thereby inducing correlation between effect estimates. Existing methods do not quantify or account for this sharing of controls. Here, we describe a stacked difference-in-differences study investigating the effects of state medical cannabis laws on treatment for chronic pain management that motivated this work, discuss a framework for estimating and managing this correlation due to shared control individuals, and show how accounting for it affects the substantive results.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seewald et al_2023_Shared Control Individuals in Health Policy Evaluations with Application to.pdf;/Users/nseewald/Zotero/storage/BIKDDRYD/2311.html}
}

@article{seewaldTargetTrialEmulation2024,
  title = {Target {{Trial Emulation}} for {{Evaluating Health Policy}}},
  author = {Seewald, Nicholas J. and McGinty, Emma E. and Stuart, Elizabeth A.},
  year = {2024},
  month = oct,
  journal = {Annals of Internal Medicine},
  publisher = {American College of Physicians},
  issn = {0003-4819},
  doi = {10.7326/M23-2440},
  urldate = {2024-10-08},
  abstract = {Target trial emulation is an approach to designing rigorous nonexperimental studies by ``emulating'' key features of a clinical trial. Most commonly used outside of policy contexts, this approach is also valuable for policy evaluation as policies typically are not randomly assigned. In this article, we discuss the application of the target trial emulation framework in a policy evaluation context. The policy trial emulation framework includes 7 components: the units and eligibility criteria, definitions of the exposure and comparison conditions, assignment mechanism, baseline (``time zero'') and follow-up, outcomes, causal estimand, and statistical analysis and assumptions. Policy evaluations that emulate a randomized trial across these dimensions can yield estimates of the causal effects of the policy on outcomes. Using the policy trial emulation framework to conduct and report on research design and methods supports transparent assessment of threats to causal inference in nonexperimental studies intended to assess the effect of a health policy on clinical or population health outcomes.},
  copyright = {All rights reserved},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seewald_et_al_2024_Target_Trial_Emulation_for_Evaluating_Health_Policy.pdf}
}

@article{seewaldTargetTrialEmulationForthcoming,
  title = {Target Trial Emulation for Evaluating Health Policy},
  author = {Seewald, Nicholas J and McGinty, Emma E. and Stuart, Elizabeth A},
  year = {Forthcoming},
  journal = {Annals of Internal Medicine}
}

@article{seguraAssociationUSMedical2019,
  title = {Association of {{US Medical Marijuana Laws With Nonmedical Prescription Opioid Use}} and {{Prescription Opioid Use Disorder}}},
  author = {Segura, Luis E. and Mauro, Christine M. and Levy, Natalie S. and Khauli, Nicole and Philbin, Morgan M. and Mauro, Pia M. and Martins, Silvia S.},
  year = {2019},
  month = jul,
  journal = {JAMA Network Open},
  volume = {2},
  number = {7},
  pages = {e197216},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2019.7216},
  urldate = {2022-06-17},
  abstract = {Between 1997 and 2017, the United States saw increases in nonmedical prescription opioid use and its consequences, as well as changes in marijuana policies. Ecological-level research hypothesized that medical marijuana legalization may reduce prescription opioid use by allowing medical marijuana as an alternative.To investigate the association of state-level medical marijuana law enactment with individual-level nonmedical prescription opioid use and prescription opioid use disorder among prescription opioid users and to determine whether these outcomes varied by age and racial/ethnic groups.This cross-sectional study used restricted data on 627\,000 individuals aged 12 years and older from the 2004 to 2014 National Survey on Drug Use and Health, a population-based survey representative of the civilian population of the United States. Analyses were completed from March 2018 to May 2018.Time-varying indicator of state-level medical marijuana law enactment (0\,=\,never law enactment, 1\,=\,before law enactment, and 2\,=\,after law enactment).Past-year nonmedical prescription opioid use and prescription opioid use disorder among prescription opioid users. Odds ratios of nonmedical prescription opioid use and prescription opioid use disorder comparing the period before and after law enactment were presented overall, by age and racial/ethnic group, and adjusted for individual- and state-level confounders.The study sample included 627\,000 participants (51.51\% female; 9.88\% aged 12-17 years, 13.30\% aged 18-25 years, 14.30\% aged 26-34 years, 25.02\% aged 35-49 years, and 37.50\% aged {$\geq$}50 years; the racial/ethnic distribution was 66.97\% non-Hispanic white, 11.83\% non-Hispanic black, 14.47\% Hispanic, and 6.73\% other). Screening and interview response rates were 82\% to 91\% and 71\% to 77\%, respectively. Overall, there were small changes in nonmedical prescription opioid use prevalence after medical marijuana law enactment (4.32\% to 4.86\%; adjusted odds ratio, 1.13; 95\% CI, 1.06-1.20). Prescription opioid use disorder prevalence among prescription opioid users decreased slightly after law enactment, but the change was not statistically significant (15.41\% to 14.76\%; adjusted odds ratio, 0.95; 95\% CI, 0.81-1.11). Outcomes were similar when stratified by age and race/ethnicity.This study found little evidence of an association between medical marijuana law enactment and nonmedical prescription opioid use or prescription opioid use disorder among prescription opioid users. Further research should disentangle the potential mechanisms through which medical marijuana laws may reduce opioid-related harm.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Segura et al_2019_Association of US Medical Marijuana Laws With Nonmedical Prescription Opioid.pdf;/Users/nseewald/Zotero/storage/U9GJHS7P/2738028.html}
}

@article{seidenfeldFisherFiducialArgument1992,
  title = {R. {{A}}. {{Fisher}}'s {{Fiducial Argument}} and {{Bayes}}' {{Theorem}}},
  author = {Seidenfeld, Teddy},
  year = {1992},
  month = aug,
  journal = {Statistical Science},
  volume = {7},
  number = {3},
  pages = {358--368},
  issn = {0883-4237},
  doi = {10.1214/ss/1177011232},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seidenfeld_1992_R.pdf}
}

@article{seidlSpatialObfuscationMethods2015,
  title = {Spatial Obfuscation Methods for Privacy Protection of Household-Level Data},
  author = {Seidl, Dara E. and Paulus, Gernot and Jankowski, Piotr and Regenfelder, Melanie},
  year = {2015},
  month = sep,
  journal = {Applied Geography},
  volume = {63},
  pages = {253--263},
  issn = {01436228},
  doi = {10.1016/j.apgeog.2015.07.001},
  urldate = {2018-10-12},
  abstract = {The topic of geoprivacy is increasingly relevant as larger quantities of personal location data are collected and shared. The results of scientific inquiries are often spatially suppressed to protect confidentiality, limiting possible benefits of public distribution. Obfuscation techniques for point data hold the potential to enable the public release of more accurate location data without compromising personal identities. This paper examines the application of four spatial obfuscation methods for household survey data. Household privacy is evaluated by a nearest neighbor analysis, and spatial distribution is measured by a cross-k function and cluster analysis. A new obfuscation technique, Voronoi masking, is demonstrated to be distinctively equipped to balance between protecting both household privacy and spatial distribution. {\copyright} 2015 Elsevier Ltd. All rights reserved.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Seidl et al_2015_Spatial obfuscation methods for privacy protection of household-level data.pdf}
}

@article{selfPowerSampleSize1988,
  title = {Power/{{Sample Size Calculations}} for {{Generalized Linear Models}}},
  author = {Self, Steven G. and Mauritsen, Robert H.},
  year = {1988},
  journal = {Biometrics},
  volume = {44},
  number = {1},
  eprint = {2531897},
  eprinttype = {jstor},
  pages = {79--86},
  issn = {0006-341X},
  doi = {10/dtz9nc},
  urldate = {2019-05-23},
  abstract = {[An approach for estimating power/sample size is described within the framework of generalized linear models. This approach is based on an asymptotic approximation to the power of the score test under contiguous alternatives and is applicable to tests of composite null hypotheses. An implementation is described for the special case of logistic regression models. Simulation studies are presented which indicate that the asymptotic approximation to the finite-sample situation is good over a range of parameter configuration.]},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Self_Mauritsen_1988_Power-Sample Size Calculations for Generalized Linear Models.pdf}
}

@incollection{sennBaselineAdjustmentLongitudinal2005,
  title = {Baseline {{Adjustment}} in {{Longitudinal Studies}}},
  booktitle = {Encyclopedia of {{Biostatistics}}},
  author = {Senn, Stephen},
  year = {2005},
  publisher = {American Cancer Society},
  doi = {10.1002/0470011815.b2a12007},
  urldate = {2020-01-14},
  abstract = {A baseline variable may be a demographic characteristic such as sex, a pretreatment observation on the variable to be used for measuring outcome, or a pretreatment observation on a variable correlated with the outcome. Adjustment by the analysis of covariance is preferable to the analysis of changes. Topics discussed include testing for baseline balance, adjustment for baselines taken after start of treatment, cut-off designs, cohort studies, and uncontrolled studies.},
  isbn = {978-0-470-01181-2},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Senn_2005_Baseline Adjustment in Longitudinal Studies.pdf;/Users/nseewald/Zotero/storage/3JPDJ696/0470011815.html}
}

@article{sennBaselineComparisonsRandomized1991,
  title = {Baseline Comparisons in Randomized Clinical Trials},
  author = {Senn, Stephen},
  year = {1991},
  journal = {Statistics in Medicine},
  volume = {10},
  number = {7},
  pages = {1157--1159},
  issn = {1097-0258},
  doi = {10/cnd827},
  urldate = {2020-01-14},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Senn_1991_Baseline comparisons in randomized clinical trials.pdf;/Users/nseewald/Zotero/storage/FBEPUVI6/sim.html}
}

@article{sennChangeBaselineAnalysis2006,
  title = {Change from Baseline and Analysis of Covariance Revisited},
  author = {Senn, Stephen},
  year = {2006},
  journal = {Statistics in Medicine},
  volume = {25},
  number = {24},
  pages = {4334--4344},
  issn = {1097-0258},
  doi = {10.1002/sim.2682},
  urldate = {2023-10-23},
  abstract = {The case for preferring analysis of covariance (ANCOVA) to the simple analysis of change scores (SACS) has often been made. Nevertheless, claims continue to be made that analysis of covariance is biased if the groups are not equal at baseline. If the required equality were in expectation only, this would permit the use of ANCOVA in randomized clinical trials but not in observational studies. The discussion is related to Lord's paradox. In this note, it is shown, however that it is not a necessary condition for groups to be equal at baseline, not even in expectation, for ANCOVA to provide unbiased estimates of treatment effects. It is also shown that although many situations can be envisaged where ANCOVA is biased it is very difficult to imagine circumstances under which SACS would then be unbiased and a causal interpretation could be made. Copyright {\copyright} 2006 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {analysis of covariance,baselines,change score,Lord's paradox,repeated measures},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Senn_2006_Change from baseline and analysis of covariance revisited.pdf;/Users/nseewald/Zotero/storage/ZNGAVSVQ/sim.html}
}

@article{sennSevenMythsRandomisation2013,
  title = {Seven Myths of Randomisation in Clinical Trials},
  author = {Senn, Stephen},
  year = {2013},
  journal = {Statistics in Medicine},
  volume = {32},
  number = {9},
  pages = {1439--1450},
  issn = {1097-0258},
  doi = {10.1002/sim.5713},
  urldate = {2022-01-17},
  abstract = {I consider seven misunderstandings that may be encountered about the nature, purpose and properties of randomisation in clinical trials. Some concern the practical realities of clinical research on patients. Others are to do with the value and purpose of balance. Still others are to do with a confusion about the role of conditioning in valid statistical inference. I consider a simple game of chance involving two dice to illustrate some points about inference and then consider the seven misunderstandings in turn. I conclude that although one should not make a fetish of randomisation, when proposing alternatives to randomisation in clinical trials, one should be very careful to be precise about the exact nature of the alternative being considered if one is to avoid the danger of underestimating the advantages that randomisation can offer. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {blinding,conditioning,covariates,randomisation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Senn_2013_Seven myths of randomisation in clinical trials.pdf;/Users/nseewald/Zotero/storage/Y7SCMT8M/sim.html}
}

@book{sennStatisticalIssuesDrug2021,
  title = {Statistical {{Issues}} in {{Drug Development}}},
  author = {Senn, Stephen},
  year = {2021},
  month = jun,
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9781119238614},
  urldate = {2022-04-11},
  isbn = {978-1-119-23857-7 978-1-119-23861-4},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Senn_2021_Statistical Issues in Drug Development.pdf}
}

@article{sennStatisticalPitfallsPersonalized2018,
  title = {Statistical Pitfalls of Personalized Medicine},
  author = {Senn, Stephen},
  year = {2018},
  month = nov,
  journal = {Nature},
  volume = {563},
  number = {7733},
  pages = {619--621},
  publisher = {Nature Publishing Group},
  doi = {10/gfkhtj},
  urldate = {2020-04-30},
  abstract = {Misleading terminology and arbitrary divisions stymie drug trials and can give false hope about the potential of tailoring drugs to individuals, warns Stephen Senn.},
  copyright = {2020 Nature},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Senn_2018_Statistical pitfalls of personalized medicine.pdf;/Users/nseewald/Zotero/storage/E9K5F3L4/d41586-018-07535-2.html}
}

@article{sertkayaKeyCostDrivers2016,
  title = {Key Cost Drivers of Pharmaceutical Clinical Trials in the {{United States}}},
  author = {Sertkaya, Aylin and Wong, Hui-Hsing and Jessup, Amber and Beleche, Trinidad},
  year = {2016},
  month = apr,
  journal = {Clinical Trials},
  volume = {13},
  number = {2},
  pages = {117--126},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774515625964},
  urldate = {2021-04-09},
  abstract = {Background: The increasing cost of clinical research has significant implications for public health, as it affects drug companies' willingness to undertake clinical trials, which in turn limits patient access to novel treatments. Thus, gaining a better understanding of the key cost drivers of clinical research in the United States is important. Purpose: The study which is based on a report prepared by Eastern Research Group, Inc., for the US Department of Health and Human Services, examined different factors, such as therapeutic area, patient recruitment, administrative staff, and clinical procedure expenditures, and their contribution to pharmaceutical clinical trial costs in the United States by clinical trial phase. Methods: The study used aggregate data from three proprietary databases on clinical trial costs provided by Medidata Solutions. We evaluated per-study costs across therapeutic areas by aggregating detailed (per patient and per site) cost information. We also compared average expenditures on cost drivers with the use of weighted mean and standard deviation statistics. Results: Therapeutic area was an important determinant of clinical trial costs by phase. The average cost of a Phase 1 study conducted at a US site ranged from US\$1.4 million (pain and anesthesia) to US\$6.6 million (immunomodulation), including estimated site overhead and monitoring costs of the sponsoring organization. A Phase 2 study cost from US\$7.0 million (cardiovascular) to US\$19.6 million (hematology), whereas a Phase 3 study cost ranged from US\$11.5 million (dermatology) to US\$52.9 (pain and anesthesia) on average. Across all study phases and excluding estimated site overhead costs and costs for sponsors to monitor the study, the top three cost drivers of clinical trial expenditures were clinical procedure costs (15\%--22\% of total), administrative staff costs (11\%--29\% of total), and site monitoring costs (9\%--14\% of total). Limitations: The data were from 2004 through 2012 and were not adjusted for inflation. Additionally, the databases used represented a convenience, that is, non-probability, sample and did not allow for statistically valid estimates of cost drivers. Finally, the data were from trials funded by the global pharmaceutical and biotechnology industry only. Hence, our study findings are limited to that segment. Conclusion: Therapeutic area being studied as well as number and types of clinical procedures involved were the key drivers of direct costs in Phase 1 through Phase 3 studies. Research shows that strategies exist for reducing the price tag of some of these major direct cost components. Therefore, to increase clinical trial efficiency and reduce costs, gaining a better understanding of the key direct cost drivers is an important step.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sertkaya et al_2016_Key cost drivers of pharmaceutical clinical trials in the United States.pdf}
}

@book{shadishExperimentalQuasiexperimentalDesigns2001,
  title = {Experimental and Quasi-Experimental Designs for Generalized Causal Inference},
  author = {Shadish, William R. and Cook, Thomas D. and Campbell, Donald T.},
  year = {2001},
  publisher = {Houghton Mifflin},
  address = {Boston},
  isbn = {978-0-395-61556-0},
  lccn = {BD591 .S48 2001},
  keywords = {Causation,Experiments}
}

@article{shahImpactMedicalMarijuana2019,
  title = {Impact of {{Medical Marijuana Legalization}} on {{Opioid Use}}, {{Chronic Opioid Use}}, and {{High-risk Opioid Use}}},
  author = {Shah, Anuj and Hayes, Corey J. and Lakkad, Mrinmayee and Martin, Bradley C.},
  year = {2019},
  month = aug,
  journal = {Journal of General Internal Medicine},
  volume = {34},
  number = {8},
  pages = {1419--1426},
  issn = {1525-1497},
  doi = {10.1007/s11606-018-4782-2},
  urldate = {2022-06-17},
  abstract = {To determine the association of medical marijuana legalization with prescription opioid utilization.},
  langid = {english},
  keywords = {chronic opioid use,marijuana,medical marijuana legalization,opioids,pain},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Shah et al_2019_Impact of Medical Marijuana Legalization on Opioid Use, Chronic Opioid Use, and.pdf}
}

@misc{shanahanTalkingLargeLanguage2022,
  title = {Talking {{About Large Language Models}}},
  author = {Shanahan, Murray},
  year = {2022},
  month = dec,
  number = {arXiv:2212.03551},
  eprint = {2212.03551},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.03551},
  urldate = {2022-12-20},
  abstract = {Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as "knows", "believes", and "thinks", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Shanahan_2022_Talking About Large Language Models.pdf;/Users/nseewald/Zotero/storage/RZMK9MJN/2212.html}
}

@misc{shapiroGovernorShapiro2024252024,
  title = {Governor {{Shapiro}}'s 2024-25 {{Budget Address}} as {{Prepared}}},
  author = {Shapiro, Josh},
  year = {2024},
  month = feb,
  address = {Harrisburg, PA},
  urldate = {2024-05-01},
  abstract = {Harrisburg, PA -- Today, Governor Josh Shapiro delivered his 2024-25 Budget Address to the General Assembly. The following is the address as prepared: Thank you. Lieutenant Governor Davis, Madam Speaker McClinton, Madam President Pro Tem Ward{\dots} Leader Pittman, Leader Costa, Leader Bradford, Leader Cutler{\dots} Thank you for convening this special session and giving me the [{\dots}]},
  langid = {american},
  file = {/Users/nseewald/Zotero/storage/HIX9JH9W/governor-shapiros-2024-25-budget-address-as-prepared.html}
}

@article{shermanComparisonBootstrapMethods1997,
  title = {A Comparison between Bootstrap Methods and Generalized Estimating Equations for Correlated Outcomes in Generalized Linear Models},
  author = {Sherman, Michael and le Cessie, Saskia},
  year = {1997},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {26},
  number = {3},
  pages = {901--925},
  publisher = {Taylor \& Francis},
  issn = {0361-0918},
  doi = {10.1080/03610919708813417},
  urldate = {2023-12-17},
  abstract = {We discuss and evaluate bootstrap algorithms for obtaining confidence intervals for parameters in Generalized Linear Models when the data are correlated. The methods are based on a stratified bootstrap and are suited to correlation occurring within ``blocks'' of data (e.g., individuals within a family, teeth within a mouth, etc.). Application of the intervals to data from a Dutch follow-up study on preterm infants shows the corroborative usefulness of the intervals, while the intervals are seen to be a powerful diagnostic in studying annual measles data. In a simulation study, we compare the coverage rates of the proposed intervals with existing methods (e.g., via Generalized Estimating Equations). In most cases, the bootstrap intervals are seen to perform better than current methods, and are produced in an automatic fashion, so that the user need not know (or have to guess) the dependence structure within a block.},
  keywords = {correlated data,logistic regression,resampling}
}

@article{sherwoodBestFITTrialSMART2016,
  title = {The {{BestFIT}} Trial: {{A SMART}} Approach to Developing Individualized Weight Loss Treatments},
  shorttitle = {The {{BestFIT}} Trial},
  author = {Sherwood, Nancy E. and Butryn, Meghan L. and Forman, Evan M. and Almirall, Daniel and Seburg, Elisabeth M. and Lauren Crain, A. and {Kunin-Batson}, Alicia S. and Hayes, Marcia G. and Levy, Rona L. and Jeffery, Robert W.},
  year = {2016},
  month = mar,
  journal = {Contemporary Clinical Trials},
  volume = {47},
  pages = {209--216},
  issn = {15517144},
  doi = {10.1016/j.cct.2016.01.011},
  urldate = {2018-10-12},
  abstract = {Behavioral weight loss programs help people achieve clinically meaningful weight losses (8--10\% of starting body weight). Despite data showing that only half of participants achieve this goal, a ``one size fits all'' approach is normative. This weight loss intervention science gap calls for adaptive interventions that provide the ``right treatment at the right time for the right person.'' Sequential Multiple Assignment Randomized Trials (SMART), use experimental design principles to answer questions for building adaptive interventions including whether, how, or when to alter treatment intensity, type, or delivery. This paper describes the rationale and design of the BestFIT study, a SMART designed to evaluate the optimal timing for intervening with sub-optimal responders to weight loss treatment and relative efficacy of two treatments that address self-regulation challenges which impede weight loss: 1) augmenting treatment with portion-controlled meals (PCM) which decrease the need for self-regulation; and 2) switching to acceptance-based behavior treatment (ABT) which boosts capacity for selfregulation. The primary aim is to evaluate the benefit of changing treatment with PCM versus ABT. The secondary aim is to evaluate the best time to intervene with sub-optimal responders. BestFIT results will lead to the empirically-supported construction of an adaptive intervention that will optimize weight loss outcomes and associated health benefits.},
  langid = {english},
  keywords = {trial},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sherwood et al_2016_The BestFIT trial.pdf}
}

@article{shiehPowerSampleSize2017,
  title = {Power and {{Sample Size Calculations}} for {{Contrast Analysis}} in {{ANCOVA}}},
  author = {Shieh, Gwowen},
  year = {2017},
  month = jan,
  journal = {Multivariate Behavioral Research},
  volume = {52},
  number = {1},
  pages = {1--11},
  issn = {0027-3171, 1532-7906},
  doi = {10.1080/00273171.2016.1219841},
  urldate = {2019-08-22},
  abstract = {Analysis of covariance (ANCOVA) is commonly used in behavioral and educational research to reduce the error variance and improve the power of analysis of variance by adjusting the covariate effects. For planning and evaluating randomized ANCOVA designs, a simple sample-size formula has been proposed to account for the variance deflation factor in the comparison of two treatment groups. The objective of this article is to highlight an overlooked and potential problem of the exiting approximation and to provide an alternative and exact solution of power and sample size assessments for testing treatment contrasts. Numerical investigations are conducted to reveal the relative performance of the two procedures as a reliable technique to accommodate the covariate features that make ANCOVA design particularly distinctive. The described approach has important advantages over the current method in general applicability, methodological justification, and overall accuracy. To enhance the practical usefulness, computer algorithms are presented to implement the recommended power calculations and sample-size determinations.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Shieh_2017_Power and Sample Size Calculations for Contrast Analysis in ANCOVA.pdf}
}

@article{shihSampleSizePower1997a,
  ids = {shihSampleSizePower1997},
  title = {Sample {{Size}} and {{Power Calculations}} for {{Periodontal}} and {{Other Studies}} with {{Clustered Samples Using}} the {{Method}} of {{Generalized Estimating Equations}}},
  author = {Shih, Weichung Joseph},
  year = {1997},
  journal = {Biometrical Journal},
  volume = {39},
  number = {8},
  pages = {899--908},
  issn = {03233847, 15214036},
  doi = {10.1002/bimj.4710390803},
  urldate = {2018-10-12},
  abstract = {In this paper we provide sample size or power estimation formulas for clinical trials that involve clustered samples and intend to use the method of Generalized Estimating Equations (GEE)for data analyses. The general formulas and two special cases, one for continuous and another for binary variables are given. The motivating example of periodontal clinical trial is given for illustration. These formulas should help researchers in designing studies with appropriate sample size and power.},
  langid = {english},
  keywords = {- within-subject and between-subject,anova,gee,intra-class correlation,link function,nested model,No DOI found,peri-,quasi-likelihood,variances},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Shih_1997_Sample Size and Power Calculations for Periodontal and Other Studies with.pdf}
}

@article{shinozakiUnderstandingMarginalStructural2020,
  title = {Understanding {{Marginal Structural Models}} for {{Time-Varying Exposures}}: {{Pitfalls}} and {{Tips}}},
  shorttitle = {Understanding {{Marginal Structural Models}} for {{Time-Varying Exposures}}},
  author = {Shinozaki, Tomohiro and Suzuki, Etsuji},
  year = {2020},
  month = sep,
  journal = {Journal of Epidemiology},
  volume = {30},
  number = {9},
  pages = {377--389},
  issn = {0917-5040},
  doi = {10/gp2rvn},
  urldate = {2022-04-30},
  abstract = {Epidemiologists are increasingly encountering complex longitudinal data, in which exposures and their confounders vary during follow-up. When a prior exposure affects the confounders of the subsequent exposures, estimating the effects of the time-varying exposures requires special statistical techniques, possibly with structural (ie, counterfactual) models for targeted effects, even if all confounders are accurately measured. Among the methods used to estimate such effects, which can be cast as a marginal structural model in a straightforward way, one popular approach is inverse probability weighting. Despite the seemingly intuitive theory and easy-to-implement software, misunderstandings (or ``pitfalls'') remain. For example, one may mistakenly equate marginal structural models with inverse probability weighting, failing to distinguish a marginal structural model encoding the causal parameters of interest from a nuisance model for exposure probability, and thereby failing to separate the problems of variable selection and model specification for these distinct models. Assuming the causal parameters of interest are identified given the study design and measurements, we provide a step-by-step illustration of generalized computation of standardization (called the g-formula) and inverse probability weighting, as well as the specification of marginal structural models, particularly for time-varying exposures. We use a novel hypothetical example, which allows us access to typically hidden potential outcomes. This illustration provides steppingstones (or ``tips'') to understand more concretely the estimation of the effects of complex time-varying exposures.},
  pmcid = {PMC7429147},
  pmid = {32684529},
  keywords = {causal inference,g-formula,inverse probability weighting,marginal structural model,time-varying exposure},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Shinozaki_Suzuki_2020_Understanding Marginal Structural Models for Time-Varying Exposures.pdf;/Users/nseewald/Zotero/storage/DVCJK55A/_article.html}
}

@article{shmueliCommentBreimanTwo2021,
  title = {Comment on {{Breiman}}'s "{{Two Cultures}}" (2002): {{From Two Cultures}} to {{Multicultural}}},
  shorttitle = {Comment on {{Breiman}}'s "{{Two Cultures}}" (2002)},
  author = {Shmueli, Galit},
  year = {2021},
  journal = {Observational Studies},
  volume = {7},
  number = {1},
  pages = {197--201},
  publisher = {University of Pennsylvania Press},
  issn = {2767-3324},
  doi = {10.1353/obs.2021.0010},
  urldate = {2022-09-07},
  abstract = {Since Breiman's "Two Cultures" paper's appearance in 2002, the term prediction has gained incredible significance in research, practice, society, and humanity. "Two Cultures" led to many useful advancements and surprising discoveries. Experiencing first hand the different cultures in the statistics and machine learning communities that Brieman expressed so early and clearly, I've then encountered even more differences. I describe additional modeling distinctions and further modeling "cultures". Recognizing these cultures, understanding their reasoning, and comparing and contrasting them, opens our eyes to new ways of viewing the world and creates opportunities for innovation and collaboration.},
  keywords = {causal explanation,description,prediction},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Shmueli_2021_Comment on Breiman's Two Cultures (2002).pdf}
}

@article{shobenMoreDataLess2017,
  title = {More Data, Less Information? {{Potential}} for Nonmonotonic Information Growth Using {{GEE}}},
  shorttitle = {More Data, Less Information?},
  author = {Shoben, Abigail B. and Rudser, Kyle D. and Emerson, Scott S.},
  year = {2017},
  month = jan,
  journal = {Journal of Biopharmaceutical Statistics},
  volume = {27},
  number = {1},
  pages = {135--147},
  issn = {1054-3406},
  doi = {10/ggfrvx},
  urldate = {2019-12-21},
  abstract = {Statistical intuition suggests that increasing the total number of observations available for analysis should increase the precision with which parameters can be estimated. Such monotonic growth of statistical information is of particular importance when data are analyzed sequentially, such as in confirmatory clinical trials. However, monotonic information growth is not always guaranteed, even when using a valid, but inefficient estimator. In this article, we demonstrate the theoretical possibility of nonmonotonic information growth when using generalized estimating equations (GEE) to estimate a slope and provide intuition for why this possibility exists. We use theoretical and simulation-based results to characterize situations that may result in nonmonotonic information growth. Nonmonotonic information growth is most likely to occur when (1) accrual is fast relative to follow-up on each individual, (2) correlation among measurements from the same individual is high, and (3) measurements are becoming more variable further from randomization. In situations that may lead to nonmonotonic information growth, study designers should plan interim analyses to avoid situations most likely to result in nonmonotonic information growth.},
  pmid = {27049897},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Shoben et al_2017_More data, less information.pdf}
}

@article{shoverAssociationMedicalCannabis2019,
  title = {Association between Medical Cannabis Laws and Opioid Overdose Mortality Has Reversed over Time},
  author = {Shover, Chelsea L. and Davis, Corey S. and Gordon, Sanford C. and Humphreys, Keith},
  year = {2019},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {26},
  pages = {12624--12626},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1903434116},
  urldate = {2022-06-17},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Shover et al_2019_Association between medical cannabis laws and opioid overdose mortality has.pdf}
}

@misc{SignORCID,
  title = {Sign in - {{ORCID}}},
  urldate = {2024-08-16},
  howpublished = {https://orcid.org/signin?client\_id=0000-0003-2551-3059\&response\_type=code\&scope=\%2Fread-limited\%20\%2Factivities\%2Fupdate\&redirect\_uri=https:\%2F\%2Fwww.editorialmanager.com\%2Fpmedicine\%2FDotNetPopUps\%2FOrcidPopUp.aspx\&show\_login=true},
  file = {/Users/nseewald/Zotero/storage/RQV4HGMV/signin.html}
}

@article{simoneauNonregularInferenceDynamic2018,
  title = {Non-Regular Inference for Dynamic Weighted Ordinary Least Squares: Understanding the Impact of Solid Food Intake in Infancy on Childhood Weight},
  shorttitle = {Non-Regular Inference for Dynamic Weighted Ordinary Least Squares},
  author = {Simoneau, Gabrielle and Moodie, Erica E. M. and Platt, Robert W. and Chakraborty, Bibhas},
  year = {2018},
  month = apr,
  journal = {Biostatistics},
  volume = {19},
  number = {2},
  pages = {233--246},
  publisher = {Oxford Academic},
  issn = {1465-4644},
  doi = {10/ggtjkq},
  urldate = {2020-04-30},
  abstract = {SUMMARY.  A dynamic treatment regime (DTR) is a set of decision rules to be applied across multiple stages of treatments. The decisions are tailored to individu},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Simoneau et al_2018_Non-regular inference for dynamic weighted ordinary least squares.pdf;/Users/nseewald/Zotero/storage/LJDZ3TLQ/weblogin.umich.edu.html}
}

@article{simonOptimalTwostageDesigns1989,
  title = {Optimal Two-Stage Designs for Phase {{II}} Clinical Trials},
  author = {Simon, Richard},
  year = {1989},
  month = mar,
  journal = {Controlled Clinical Trials},
  volume = {10},
  number = {1},
  pages = {1--10},
  issn = {01972456},
  doi = {10.1016/0197-2456(89)90015-9},
  urldate = {2023-10-11},
  abstract = {The primary objective of a phase II clinical trial of a new drug or regimen is to determine whether it has sufficient biological activity against the disease under study to warrant more extensive development. Such trials are often conducted in a multiinstitution setting where designs of more than two stages are difficult to manage. This paper presents two-stage designs that are optimal in the sense that the expected sample size is minimized if the regimen has low activity subject to constraints upon the size of the type 1 and type 2 errors. Two-stage designs which minimize the maximum sample size are also determined. Optimum and "minimax" designs for a range of design parameters are tabulated. These designs can also be used for pilot studies of new regimens where toxicity is the endpoint of interest.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Simon_1989_Optimal two-stage designs for phase II clinical trials.pdf}
}

@article{simonPersonalizedMedicineDepression2010,
  title = {Personalized {{Medicine}} for {{Depression}}: {{Can We Match Patients With Treatments}}?},
  author = {Simon, Gregory E and Perlis, Roy H},
  year = {2010},
  journal = {Am J Psychiatry},
  pages = {11},
  doi = {10.1176/appi.ajp.2010.09111680},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Simon_Perlis_2010_Personalized Medicine for Depression.pdf}
}

@article{simsReinforcementLearningModelbased,
  title = {Reinforcement {{Learning}}: {{Model-based}}},
  author = {Sims, Chris R},
  pages = {4},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sims_Reinforcement Learning.pdf}
}

@article{singhIntrinsicallyMotivatedReinforcement2010,
  title = {Intrinsically {{Motivated Reinforcement Learning}}: {{An Evolutionary Perspective}}},
  shorttitle = {Intrinsically {{Motivated Reinforcement Learning}}},
  author = {Singh, Satinder and Lewis, Richard L. and Barto, Andrew G. and Sorg, Jonathan},
  year = {2010},
  month = jun,
  journal = {IEEE Transactions on Autonomous Mental Development},
  volume = {2},
  number = {2},
  pages = {70--82},
  issn = {1943-0604, 1943-0612},
  doi = {10.1109/TAMD.2010.2051031},
  urldate = {2018-10-12},
  abstract = {There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.},
  langid = {english},
  keywords = {artificial system,intrinsic motivation,learning (artificial intelligence),optimal primary reward signal,Reinforcement learning,reward functions},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Singh et al_2010_Intrinsically Motivated Reinforcement Learning.pdf}
}

@article{singhOpioidEpidemicUnited2019,
  title = {Opioid {{Epidemic}} in the {{United States}}: {{Empirical Trends}}, and {{A Literature Review}} of {{Social Determinants}} and {{Epidemiological}}, {{Pain Management}}, and {{Treatment Patterns}}},
  shorttitle = {Opioid {{Epidemic}} in the {{United States}}},
  author = {Singh, Gopal K. and Kim, Isaac E. and Girmay, Mehrete and Perry, Chrisp and Daus, Gem P. and Vedamuthu, Ivy P. and De Los Reyes, Andrew A. and Ramey, Christine T. and Martin, Elijah K. and Allender, Michelle},
  year = {2019},
  journal = {International Journal of Maternal and Child Health and AIDS},
  volume = {8},
  number = {2},
  pages = {89--100},
  issn = {2161-8674},
  doi = {10.21106/ijma.284},
  urldate = {2024-05-03},
  abstract = {Objectives: Dramatic increases in opioid and drug overdose mortality have occurred in the United States (US) over the past two decades. To address this national public health crisis and identify gaps in the literature, we analyzed recent empirical trends in US drug overdose mortality by key social determinants and conducted a selective review of the recent literature on the magnitude of the opioid crisis facing different racial/ethnic, socioeconomic, and rural-urban segments of the US population. Methods: We used the 1999-2017 mortality data from the US National Vital Statistics System to analyze trends in drug overdose mortality by race/ethnicity, age, and geographic area. Log-linear regression was used to model mortality trends. Using various key words and their combinations, we searched PubMed and Google Scholar for select peer-reviewed journal articles and government reports published on the opioid epidemic between 2010 and 2018. Results: Our original analysis and review indicate marked increases in drug overdose mortality overall and by race/ethnicity and geographic regions, with adolescents and young adults experiencing steep increases in mortality between 1999 and 2017. Our selective search yielded 405 articles, of which 39 publications were selected for detailed review. Suicide mortality from drug overdose among teens aged 12-19 increased consistently between 2009 and 2017, particularly among teen girls. The rise of efficient global supply chains has increased opioid prescription use and undoubtedly contributed to the opioid epidemic. Many other important contributing factors to the epidemic include lack of education and economic opportunities, poor working conditions, and low social capital in disadvantaged communities. Conclusions and Global Health Implications: Our analysis and review indicate substantial disparities in drug overdoses and related mortality, pain management, and treatment outcomes according to social determinants. Increases in drug overdoses and resultant mortality are not only unique to the US, but have also been observed in other industrialized countries. Healthcare systems, community leaders, and policymakers addressing the opioid epidemic should focus on upstream structural factors including education, economic opportunity, social cohesion, racial/ethnic disadvantage, geographic isolation, and life satisfaction.},
  pmcid = {PMC6804319},
  pmid = {31723479},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Singh_et_al_2019_Opioid_Epidemic_in_the_United_States.pdf}
}

@article{slovakSituChildLedIntervention2021,
  title = {An {{In Situ}}, {{Child-Led Intervention}} to {{Promote Emotion Regulation Competence}} in {{Middle Childhood}}: {{Protocol}} for an {{Exploratory Randomized Controlled Trial}}},
  shorttitle = {An {{In Situ}}, {{Child-Led Intervention}} to {{Promote Emotion Regulation Competence}} in {{Middle Childhood}}},
  author = {Slovak, Petr and Ford, Brett Q. and Widen, Sherri and Roquet, Claudia Daud{\'e}n and Theofanopoulou, Nikki and Gross, James J. and Hankin, Benjamin and Klasnja, Predrag},
  year = {2021},
  month = nov,
  journal = {JMIR Research Protocols},
  volume = {10},
  number = {11},
  pages = {e28914},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/28914},
  urldate = {2021-11-10},
  abstract = {Background: Emotion regulation is a key transdiagnostic risk factor for a range of psychopathologies, making it a prime target for both prevention and treatment interventions in childhood. Existing interventions predominantly rely on workshops or in-person therapy-based approaches, limiting the ability to promote emotion regulation competence for children in everyday settings and at scale. Purrble is a newly developed, inexpensive, socially assistive robot---in the form of an interactive plush toy---that uses haptic feedback to support in-the-moment emotion regulation. It is accessible to children as needed in their daily lives, without the need for a priori training. Although qualitative data from previous studies show high engagement in situ and anecdotal evidence of the robot being incorporated into children's emotion regulation routines, there is no quantitative evidence of the intervention's impact on child outcomes. Objective: The aim of this study is to examine the efficacy of a new intervention model for child-led emotion regulation---Purrble---that can be deployed across prevention and treatment contexts. Methods: Overall, 134 children aged 8 to 10 years will be selected from an enriched nonclinical North American population; for inclusion, the cutoff for the parents' rating of child dysregulation will be {$\geq$}10 points in the total difficulties score on the Strengths and Difficulties Questionnaire. This cutoff was selected to obtain a measurable, but not necessarily clinical, level of the child's emotion regulatory difficulties. The selected families will be randomly assigned with .5 probability to receive either a Purrble or an active control (noninteractive plush toy). The primary outcome will be a daily ecological momentary assessment measure of child emotion regulation capability (as reported by parents) over a period of 4 weeks. Exploratory analyses will investigate the intervention impact on secondary outcomes of child emotion regulation, collected weekly over the same 4-week period, with follow-ups at 1 month and 6 months postdeployment. Quantitative data will be analyzed on an intent-to-treat basis. A proportion of families (approximately 30\% of the sample) will be interviewed after deployment as part of the process analysis. Results: The study is funded by the UKRI Future Leaders Fellowship (MR/T041897/1) and an in-kind contribution from the Committee for Children. This study received ethical approval from the Pearl institutional review board (\#18-CFC-101). Participant recruitment started in February 2021, with the 1-month deployment in April-May 2021. The results of this analysis will be published in 2022. Conclusions: This study will be the first quantitative evaluation of the efficacy of an innovative, proof-of-concept intervention model for an in situ, child-led emotion regulation intervention. Insights into the trajectory of daily changes, complemented with weekly questionnaire batteries and postdeployment interviews, will result in an in-depth understanding of whether and how the hypothesized intervention logic model works, leading to further intervention optimization. Trial Registration: ClinicalTrials.gov NCT04810455; http://clinicaltrials.gov/ct2/show/NCT04810455},
  copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in JMIR Research Protocols...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.researchprotocols.org/, as well as this copyright and license information must be included.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Slovak et al_2021_An In Situ, Child-Led Intervention to Promote Emotion Regulation Competence in.pdf;/Users/nseewald/Zotero/storage/FDQUFSKP/e28914.html;/Users/nseewald/Zotero/storage/LE3PM7YQ/an-insitu-childled-intervention-to-promote-emotion-regulation-competence-in-middle-childhood-pr.html}
}

@article{smithBehavioralHealthService2022,
  title = {Behavioral Health Service Use Post-Jail Release and Reduced Risk of Return to Jail},
  author = {Smith, Joseph L. and Khatri, Utsha G. and Olubiyi, Oluwatoyin and Hadley, Trevor and Lim, Suet and Mandell, David and {Kang-Yi}, Christina},
  year = {2022},
  journal = {Journal of Community Psychology},
  volume = {50},
  number = {7},
  pages = {3044--3053},
  issn = {1520-6629},
  doi = {10.1002/jcop.22813},
  urldate = {2025-03-27},
  abstract = {This study examined whether behavioral health service use post-jail release was associated with reduced risk of jail reincarceration. The study sample included 20,615 individuals who had behavioral health diagnoses and were released from the Philadelphia County jail. Using administrative records of the county jail and state-, county-, and Medicaid-funded behavioral health service use from 2010 to 2018, we conducted Cox proportional hazard analyses to estimate the association between behavioral health service use post-jail release and the risk of return to jail within 3 years. Nearly 50\% of the sample returned to jail within 3 years. Individuals who used behavioral health services were 26\%--38\% less likely to return to jail within 3 years than were individuals who did not. The study results suggest that connecting individuals with behavioral health services upon release from jail can reduce the risk of repeated jail incarceration.},
  copyright = {{\copyright} 2022 Wiley Periodicals LLC},
  langid = {english},
  keywords = {behavioral health service use,co-occurring substance use and psychiatric disorder,jail reincarceration,psychiatric disorder,substance use disorder},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Smith et al. - 2022 - Behavioral health service use post-jail release and reduced risk of return to jail.pdf;/Users/nseewald/Zotero/storage/5ECQYIHR/jcop.html}
}

@incollection{smithDesignConsiderationsPreparation2023,
  title = {Design Considerations for Preparation, Optimization, and Evaluation of Digital Therapeutics},
  booktitle = {Digital {{Therapeutics}} for {{Mental Health}} and {{Addiction}}},
  author = {Smith, Shawna N. and Seewald, Nicholas J. and Klasnja, Predrag},
  editor = {Jacobson, Nicholas and Marsch, Lisa and Kowatsch, Tobias},
  year = {2023},
  month = jan,
  pages = {135--150},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-323-90045-4.00015-0},
  urldate = {2022-10-07},
  abstract = {This chapter offers a conceptual framework that ties together two domains of design decisions for digital therapeutics---those related to intervention design (i.e., which components or treatments to include) and those related to study design (i.e., how to test whether components or treatments work as intended). This framework is intended to help researchers in identifying and testing key digital therapeutics design considerations across the intervention development lifespan---that is, from formative work through longer-term implementation. The framework encourages intervention developers to make design decisions by considering three motivating questions: first, what health outcomes are we trying to impact; second, what are we trying to learn; and third, cross-cutting both prior questions to inform potential evidence, how much is at stake in terms of health impacts if we get this decision wrong? We consider how these questions can help to inform design decisions across three phases of the intervention lifespan---preparation and formative work, optimization of intervention components, and evaluation of effectiveness and implementation---to maximize the likelihood of positive health impact and accumulation of evidence about the intervention's functioning.},
  copyright = {All rights reserved},
  isbn = {978-0-323-90045-4},
  langid = {english},
  keywords = {Design,Evaluation,Intervention development,Optimization,Research methods},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Smith et al_2023_Design considerations for preparation, optimization, and evaluation of digital.pdf;/Users/nseewald/Zotero/storage/8MZYXXID/B9780323900454000150.html}
}

@incollection{smithDesignLessonsMicroRandomized2017,
  title = {Design {{Lessons}} from a {{Micro-Randomized Pilot Study}} in {{Mobile Health}}},
  booktitle = {Mobile {{Health}}},
  author = {Smith, Shawna N. and Lee, Andy Jinseok and Hall, Kelly and Seewald, Nicholas J. and Boruvka, Audrey and Murphy, Susan A. and Klasnja, Predrag},
  editor = {Rehg, James M. and Murphy, Susan A. and Kumar, Santosh},
  year = {2017},
  pages = {59--82},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-51394-2_4},
  urldate = {2018-10-12},
  abstract = {Micro-randomized trials (MRTs) offer promise for informing the development of effective mobile just-in-time adaptive interventions (JITAIs) intended to support individuals' health behavior change, but both their novelty and the novelty of JITAIs introduces new problems in implementation. An understanding of the practical challenges unique to rolling out MRTs and JITAIs is a prerequisite to valid empirical tests of such interventions. In this chapter, we relay lessons learned from the first MRT pilot study of HeartSteps, a JITAI intended to encourage sedentary adults to increase their physical activity by sending contextually-relevant, actionable activity suggestions and by supporting activity planning for the following day. This chapter outlines the lessons our study team learned from the HeartSteps pilot across four domains: (1) study recruitment and retention; (2) technical challenges in architecting a just-in-time adaptive intervention; (3) considerations of treatment delivery unique to JITAIs and MRTs; and (4) participant usage of and reflections on the HeartSteps study.},
  copyright = {All rights reserved},
  isbn = {978-3-319-51393-5 978-3-319-51394-2},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Smith et al_2017_Design Lessons from a Micro-Randomized Pilot Study in Mobile Health.pdf}
}

@article{smithInterpretingResultsIntentiontoTreat2021,
  title = {Interpreting the {{Results}} of {{Intention-to-Treat}}, {{Per-Protocol}}, and {{As-Treated Analyses}} of {{Clinical Trials}}},
  author = {Smith, Valerie A. and Coffman, Cynthia J. and Hudgens, Michael G.},
  year = {2021},
  month = aug,
  journal = {JAMA},
  volume = {326},
  number = {5},
  pages = {433--434},
  issn = {0098-7484},
  doi = {10.1001/jama.2021.2825},
  urldate = {2024-07-11},
  abstract = {Nonadherence in a randomized clinical trial (RCT) occurs when study participants do not follow the randomly assigned treatment protocol. Reasons for nonadherence may include the study participant not taking trial medications, crossing over to the other intervention being studied, or accessing treatment outside of the trial. Nonadherence also may occur when the clinician is unable to complete the assigned therapy (eg, a surgical procedure) as intended.The CABANA clinical trial published in JAMA by Packer et al was difficult to interpret because of nonadherence with the treatment protocol that resulted from substantial crossover between groups. In this trial, patients with atrial fibrillation were randomized to either undergo catheter ablation or receive conventional medical therapy. Of the 1108 participants randomized to ablation, 102 (9\%) did not receive the procedure. Of the 1096 patients randomized to drug therapy, 301 (27\%) underwent ablation during the follow-up period, resulting in nonadherence to assigned treatment in both groups of the study. Interpretation of the effect of catheter ablation on atrial fibrillation differed based on alternate ways of analyzing the trials results. Intention-to-treat (ITT), per-protocol (PP), and as-treated (AT) approaches to analysis differ in how the included patient population and treatment assignments are defined, with important implications for interpretation of treatment effects in clinical trials.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Smith et al_2021_Interpreting the Results of Intention-to-Treat, Per-Protocol, and As-Treated.pdf;/Users/nseewald/Zotero/storage/F2NMDRAH/2782658.html}
}

@article{smithMarginalizedTwopartModel2014,
  title = {A Marginalized Two-Part Model for Semicontinuous Data},
  author = {Smith, Valerie A. and Preisser, John S. and Neelon, Brian and Maciejewski, Matthew L.},
  year = {2014},
  journal = {Statistics in Medicine},
  volume = {33},
  number = {28},
  pages = {4891--4903},
  issn = {10970258},
  doi = {10.1002/sim.6263},
  abstract = {In health services research, it is common to encounter semicontinuous data characterized by a point mass at zero followed by a right-skewed continuous distribution with positive support. Examples include health expenditures, in which the zeros represent a subpopulation of patients who do not use health services, while the continuous distribution describes the level of expenditures among health services users. Semicontinuous data are typically analyzed using two-part mixture models that separately model the probability of health services use and the distribution of positive expenditures among users. However, because the second part conditions on a non-zero response, conventional two-part models do not provide a marginal interpretation of covariate effects on the overall population of health service users and non-users, even though this is often of greatest interest to investigators. Here, we propose a marginalized two-part model that yields more interpretable effect estimates in two-part models by parameterizing the model in terms of the marginal mean. This model maintains many of the important features of conventional two-part models, such as capturing zero-inflation and skewness, but allows investigators to examine covariate effects on the overall marginal mean, a target of primary interest in many applications. Using a simulation study, we examine properties of the maximum likelihood estimates from this model. We illustrate the approach by evaluating the effect of a behavioral weight loss intervention on health-care expenditures in the Veterans Affairs health-care system.},
  pmid = {25043491},
  keywords = {Health-care expenditures,Log-skew-normal distribution,Marginalized models,Semicontinuous data,Two-part model,Weight loss intervention},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Smith et al_2014_A marginalized two-part model for semicontinuous data.pdf}
}

@article{spiekermanCheckingMarginalCox1996,
  title = {Checking the {{Marginal Cox}} Model for {{Correlated Failure Time Data}}},
  author = {Spiekerman, C. F. and Lin, D. Y.},
  year = {1996},
  journal = {Biometrika},
  volume = {83},
  number = {1},
  eprint = {2337438},
  eprinttype = {jstor},
  pages = {143--156},
  doi = {10.1093/biomet/83.1.143},
  abstract = {Correlated failure time data arise frequently in scientific investigations because there exists natural or artificial clustering of study subjects such that failure times within the same cluster are correlated. It is convenient and useful to perform regression analysis by formulating the marginal distributions of the correlated failure times with the Cox proportional hazards model. In this paper, we develop a class of graphical and numerical techniques for checking the adequacy of the marginal Cox model. The proposed methods are derived from cumulative sums of martingale-based residuals over the failure time and/or covariates. The distributions of these stochastic processes under the assumed model can be approximated through simulating certain zero-mean Gaussian processes. Each observed residual pattern can then be compared objectively with a number of realisations from the approximating process. An illustration with the Diabetic Retinopathy Study is provided.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Spiekerman_Lin_1996_Checking the Marginal Cox model for Correlated Failure Time Data.pdf}
}

@article{spitznerSignificantConversationsSignificant2021,
  title = {Significant Conversations, Significant Others: Intimate Dialogues about Teaching Statistics},
  shorttitle = {Significant Conversations, Significant Others},
  author = {Spitzner, Dan J. and Meixner, Cara},
  year = {2021},
  month = jul,
  journal = {International Journal for Academic Development},
  volume = {26},
  number = {3},
  pages = {292--306},
  publisher = {Routledge},
  issn = {1360-144X},
  doi = {10.1080/1360144X.2021.1954931},
  urldate = {2024-06-05},
  abstract = {This article features a collaborative autoethnography of its two authors: we are instructors, also life partners, whose teaching of statistics has been transformed through significant, intimate conversations. We implemented the collaborative autoethnography within an integrated theoretical framework, designed to underpin the function of significant conversations within the academy through insights into the psychosocial conditions within which they occur, especially as they relate to intimacy and vulnerability. Themes of trust, respect, mutuality, and investment anchored these conversations, tethered to both positive and negative motivators. Recommendations for academic developers include creating space for learning communities; upholding the role of significant conversations in the work of equity and inclusion; and bolstering collaborative autoethnography as an actionable, reflective methodology.},
  keywords = {Academic development,collaborative autoethnography,intimate dialogue,significant conversations}
}

@article{spritzlerTwoSampleTestsAreaUndertheCurve2008,
  title = {Two-{{Sample Tests}} of {{Area-Under-the-Curve}} in the {{Presence}} of {{Missing Data}}},
  author = {Spritzler, John and DeGruttola, Victor G and Pei, Lixia},
  year = {2008},
  month = jan,
  journal = {The International Journal of Biostatistics},
  volume = {4},
  number = {1},
  issn = {1557-4679},
  doi = {10.2202/1557-4679.1068},
  urldate = {2018-10-12},
  abstract = {The commonly used two-sample tests of equal area-under-the-curve (AUC), where AUC is based on the linear trapezoidal rule, may have poor properties when observations are missing, even if they are missing completely at random (MCAR). We propose two tests: one that has good properties when data are MCAR and another that has good properties when the data are missing at random (MAR), provided that the pattern of missingness is monotonic. In addition, we discuss other non-parametric tests of hypotheses that are similar, but not identical, to the hypothesis of equal AUCs, but that often have better statistical properties than do AUC tests and may be more scientifically appropriate for many settings.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Spritzler et al_2008_Two-Sample Tests of Area-Under-the-Curve in the Presence of Missing Data.pdf}
}

@article{spruijt-metzInnovationsUseInteractive2015,
  title = {Innovations in the {{Use}} of {{Interactive Technology}} to {{Support Weight Management}}},
  author = {{Spruijt-Metz}, D. and Wen, C. K. F. and O'Reilly, G. and Li, M. and Lee, S and Emken, B. A. and Mitra, U. and Annavaram, M. and Ragusa, G. and Narayanan, S.},
  year = {2015},
  month = dec,
  journal = {Current Obesity Reports},
  volume = {4},
  number = {4},
  pages = {510--519},
  issn = {2162-4968},
  doi = {10.1007/s13679-015-0183-6},
  urldate = {2018-10-12},
  abstract = {New and emerging mobile technologies are providing unprecedented possibilities for understanding and intervening on obesity-related behaviors in real time. However, the mobile health (mHealth) field has yet to catch up with the fast-paced development of technology. Current mHealth efforts in weight management still tend to focus mainly on short message systems (SMS) interventions, rather than taking advantage of real-time sensing to develop just-in-time adaptive interventions (JITAIs). This paper will give an overview of the current technology landscape for sensing and intervening on three behaviors that are central to weight management: diet, physical activity, and sleep. Then five studies that really dig into the possibilities that these new technologies afford will be showcased. We conclude with a discussion of hurdles that mHealth obesity research has yet to overcome and a future-facing discussion.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Spruijt-Metz et al_2015_Innovations in the Use of Interactive Technology to Support Weight Management.pdf}
}

@misc{staffPennsylvaniaShouldLegalize2024,
  title = {Pennsylvania Should Legalize Marijuana, {{Gov}}. {{Shapiro}} Says},
  author = {Staff, Billy Penn},
  year = {2024},
  month = feb,
  journal = {Billy Penn at WHYY},
  urldate = {2024-05-01},
  abstract = {The state is losing out in marijuana sales to other states, Shapiro said, and it's time for PA to start competing.},
  howpublished = {http://billypenn.com/2024/02/06/pennsylvania-legalize-marijuana-josh-shapiro/},
  langid = {american},
  file = {/Users/nseewald/Zotero/storage/Z6RKIZ4G/pennsylvania-legalize-marijuana-josh-shapiro.html}
}

@article{stanekChoosingPretestPosttestAnalysis1988,
  title = {Choosing a {{Pretest-Posttest Analysis}}},
  author = {Stanek, Edward J.},
  year = {1988},
  month = aug,
  journal = {The American Statistician},
  volume = {42},
  number = {3},
  pages = {178--183},
  issn = {0003-1305, 1537-2731},
  doi = {10/gghp8d},
  urldate = {2020-01-14},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Stanek_1988_Choosing a Pretest-Posttest Analysis.pdf}
}

@book{StatisticalAnalysisFailure2002,
  title = {The {{Statistical Analysis}} of {{Failure Time Data}}},
  year = {2002},
  edition = {1},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118032985},
  urldate = {2024-01-11},
  file = {/Users/nseewald/Zotero/storage/YF5S5J3Q/9781118032985.html}
}

@incollection{StatisticalInference2008,
  title = {Statistical Inference},
  booktitle = {A {{Dictionary}} of {{Statistics}}},
  year = {2008},
  month = jan,
  publisher = {Oxford University Press},
  urldate = {2022-08-28},
  abstract = {"statistical inference" published on  by Oxford University Press.},
  isbn = {978-0-19-954145-4},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/VNMJXYMB/acref-9780199541454-e-1557.html}
}

@misc{StatisticalMethodsMedical,
  title = {Statistical {{Methods}} in {{Medical Research}} - {{Volume}} 27, {{Number}} 2, {{Feb}} 01, 2018},
  urldate = {2024-07-18},
  howpublished = {https://journals.sagepub.com/toc/smma/27/2},
  file = {/Users/nseewald/Zotero/storage/ZHSVXCUV/2.html}
}

@misc{StatisticalPowerEstimating,
  title = {Statistical {{Power}} for {{Estimating Treatment Effects Using Difference-in-Differences}} and {{Comparative Interrupted Time Series Estimators With Variation}} in {{Treatment Timing}}},
  doi = {10.3102/10769986211070625},
  urldate = {2023-11-07},
  howpublished = {https://journals.sagepub.com/doi/epub/10.3102/10769986211070625},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/2ZBTQ7GM/10769986211070625.html}
}

@article{steinEditorialWhatMakes2023,
  title = {Editorial: {{What Makes}} for a {{Great Applications}} and {{Case Studies Paper}}?},
  shorttitle = {Editorial},
  author = {Stein, Michael L.},
  year = {2023},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {118},
  number = {541},
  pages = {1--2},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2023.2173458},
  urldate = {2023-06-23},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Stein_2023_Editorial.pdf}
}

@incollection{stephensGestimationDynamicTreatment2015,
  title = {G-Estimation for Dynamic Treatment Regimes in the Longitudinal Setting},
  shorttitle = {Chapter 7},
  booktitle = {Adaptive {{Treatment Strategies}} in {{Practice}}},
  author = {Stephens, D. A.},
  year = {2015},
  month = dec,
  series = {{{ASA-SIAM Series}} on {{Statistics}} and {{Applied Mathematics}}},
  pages = {89--117},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611974188.ch7},
  urldate = {2020-05-06},
  abstract = {In this chapter, the method of G-estimation is extended beyond the single interval setting of Chapter 6, and applications to the design of optimal dynamic treatment regimes are discussed. We consider cases of binary treatments, treatments lying in a finite set, or where treatment can take arbitrary continuous values. Crucial in the construction is the idea that there is a baseline ``zero'' level of treatment against which responses to other treatments are compared. For continuous treatments, the numerical value zero provides this baseline. For discrete-valued treatments, we choose a baseline from amongst the list of treatment levels. G-estimation for dynamic regimes proceeds by proposing expected (counterfactual) response models relative to the response at baseline treatment.},
  isbn = {978-1-61197-417-1},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Stephens_2015_G-estimation for dynamic treatment regimes in the longitudinal setting.pdf;/Users/nseewald/Zotero/storage/G727JDCP/1.9781611974188.html}
}

@inbook{steyerbergStudyDesignPrediction2019,
  title = {Study {{Design}} for {{Prediction Modeling}}},
  booktitle = {Clinical {{Prediction Models}}},
  author = {Steyerberg, Ewout W.},
  year = {2019},
  pages = {37--58},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-16399-0_3},
  urldate = {2022-08-27},
  collaborator = {Steyerberg, Ewout W.},
  isbn = {978-3-030-16398-3 978-3-030-16399-0},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Steyerberg_2019_Study Design for Prediction Modeling.pdf}
}

@article{stoneEffectsTexasState2024,
  title = {Effects of {{Texas State Agency Integration}} on {{Mental Health Service Use Among Individuals}} with {{Co-occurring Cognitive Disabilities}} and {{Mental Health Conditions}}},
  author = {Stone, Elizabeth M. and Jopson, Andrew D. and Seewald, Nicholas J. and Stuart, Elizabeth A. and Wise, Elizabeth and McCourt, Alexander D. and German, Danielle and McGinty, Emma E.},
  year = {2024},
  month = aug,
  journal = {Community Mental Health Journal},
  issn = {1573-2789},
  doi = {10.1007/s10597-024-01332-0},
  urldate = {2024-08-01},
  abstract = {This study uses Texas's 2017 integration of the state disability and mental health agencies as a case study, combining interviews with Texas agency and advocacy organization leaders to examine perceptions of agency integration and augmented synthetic control analyses of 2014--2020 Medical Expenditure Panel Survey to examine impacts on mental health service use among individuals with co-occurring cognitive disabilities (including intellectual and developmental disabilities) and mental health conditions. Interviewees described the intensive process of agency integration and identified primarily positive (e.g., decreased administrative burden) impacts of integration. Quantitative analyses indicated no effects of integration on receipt of mental health-related services among people with co-occurring conditions. While leaders identified some potentially beneficial impacts of state agency integration, the limited impact of integration beyond the agency suggests that interventions at multiple levels of the service system, including those targeting providers, are needed to better meet the mental health service needs for this population.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {Administrative agencies,Cognitive disability,Developmental disability,Intellectual disability,Mental health,State government},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Stone_et_al_2024_Effects_of_Texas_State_Agency_Integration_on_Mental_Health_Service_Use_Among.pdf}
}

@article{stoneEffectsTexasState2025,
  title = {Effects of {{Texas State Agency Integration}} on {{Mental Health Service Use Among Individuals}} with {{Co-occurring Cognitive Disabilities}} and {{Mental Health Conditions}}},
  author = {Stone, Elizabeth M. and Jopson, Andrew D. and Seewald, Nicholas J. and Stuart, Elizabeth A. and Wise, Elizabeth and McCourt, Alexander D. and German, Danielle and McGinty, Emma E.},
  year = {2025},
  month = jan,
  journal = {Community Mental Health Journal},
  volume = {61},
  number = {1},
  pages = {111--121},
  issn = {1573-2789},
  doi = {10.1007/s10597-024-01332-0},
  urldate = {2025-03-17},
  abstract = {This study uses Texas's 2017 integration of the state disability and mental health agencies as a case study, combining interviews with Texas agency and advocacy organization leaders to examine perceptions of agency integration and augmented synthetic control analyses of 2014--2020 Medical Expenditure Panel Survey to examine impacts on mental health service use among individuals with co-occurring cognitive disabilities (including intellectual and developmental disabilities) and mental health conditions. Interviewees described the intensive process of agency integration and identified primarily positive (e.g., decreased administrative burden) impacts of integration. Quantitative analyses indicated no effects of integration on receipt of mental health-related services among people with co-occurring conditions. While leaders identified some potentially beneficial impacts of state agency integration, the limited impact of integration beyond the agency suggests that interventions at multiple levels of the service system, including those targeting providers, are needed to better meet the mental health service needs for this population.},
  langid = {english},
  keywords = {Administrative agencies,Cognitive disability,Developmental disability,Intellectual disability,Mental health,State government},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Stone et al. - 2025 - Effects of Texas State Agency Integration on Mental Health Service Use Among Individuals with Co-occ.pdf}
}

@article{stoneImplementationEnforcementState2020,
  title = {Implementation and Enforcement of State Opioid Prescribing Laws},
  author = {Stone, Elizabeth M. and Rutkow, Lainie and Bicket, Mark C. and Barry, Colleen L. and Alexander, G. Caleb and McGinty, Emma E.},
  year = {2020},
  month = aug,
  journal = {Drug and Alcohol Dependence},
  volume = {213},
  pages = {108107},
  issn = {0376-8716},
  doi = {10.1016/j.drugalcdep.2020.108107},
  urldate = {2024-10-22},
  abstract = {Background In response to the role overprescribing has played in the U.S. opioid crisis, in the past decade states have enacted four main types of laws to curb opioid prescribing: mandatory prescription drug monitoring program (PDMP) enrollment laws requiring clinicians to register with a PDMP; mandatory PDMP query laws requiring clinicians to check a PDMP prior to prescribing opioids; pill mill laws regulating pain management clinics; and opioid prescribing cap laws limiting the dose/duration of opioid prescriptions. While 47 states now have one or more of these laws in place, little is known about implementation and enforcement strategies, facilitators, and barriers. Methods From November 2017 to February 2019, we interviewed 114 professionals involved in state opioid prescribing law implementation and enforcement in 20 states and identified common themes. Results Implementation efforts focused on awareness campaigns and targeted training of key front-line implementers. Enforcement strategies included active, complaint-based, and automated strategies. Collaboration across agencies and stakeholders, particularly health agencies and law enforcement, was identified as an important facilitator of implementation and enforcement. Two key interrelated barriers were identified: the complexity of state opioid prescribing laws in terms of which providers, patients, and prescriptions they applied to, and IT infrastructure. Conclusion Despite differing approaches, our findings suggest similar barriers to implementation and enforcement across state opioid prescribing laws. Strategies are needed to ease implementation and enforcement of laws that apply only to specific sub-sets of providers, patients, or prescriptions and address issues of access and data utilization of the PDMP.},
  keywords = {Enforcement,Implementation,Opioid,Policy},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Stone et al. - 2020 - Implementation and enforcement of state opioid prescribing laws.pdf;/Users/nseewald/Zotero/storage/3GQWQPHG/S0376871620302726.html}
}

@article{stuartEstimatingInterventionEffects2008,
  title = {Estimating {{Intervention Effects}} of {{Prevention Programs}}: {{Accounting}} for {{Noncompliance}}},
  shorttitle = {Estimating {{Intervention Effects}} of {{Prevention Programs}}},
  author = {Stuart, Elizabeth A. and Perry, Deborah F. and Le, Huynh-Nhu and Ialongo, Nicholas S.},
  year = {2008},
  month = dec,
  journal = {Prevention Science},
  volume = {9},
  number = {4},
  pages = {288--298},
  issn = {1573-6695},
  doi = {10.1007/s11121-008-0104-y},
  urldate = {2022-04-29},
  abstract = {Individuals not fully complying with their assigned treatments is a common problem encountered in randomized evaluations of behavioral interventions. Treatment group members rarely attend all sessions or do all ``required'' activities; control group members sometimes find ways to participate in aspects of the intervention. As a result, there is often interest in estimating both the effect of being assigned to participate in the intervention, as well as the impact of actually participating and doing all of the required activities. Methods known broadly as ``complier average causal effects'' (CACE) or ``instrumental variables'' (IV) methods have been developed to estimate this latter effect, but they are more commonly applied in medical and treatment research. Since the use of these statistical techniques in prevention trials has been less widespread, many prevention scientists may not be familiar with the underlying assumptions and limitations of CACE and IV approaches. This paper provides an introduction to these methods, described in the context of randomized controlled trials of two preventive interventions: one for perinatal depression among at-risk women and the other for aggressive disruptive behavior in children. Through these case studies, the underlying assumptions and limitations of these methods are highlighted.},
  langid = {english},
  keywords = {Complier average causal effect,Dosage effects,Instrumental variables,Randomized controlled trials},
  file = {C:\Users\nseew\Google Drive\Papers\Stuart_et_al_2008_Estimating_Intervention_Effects_of_Prevention_Programs.pdf}
}

@article{stuartMatchingMethodsCausal2010,
  title = {Matching {{Methods}} for {{Causal Inference}}: {{A Review}} and a {{Look Forward}}},
  shorttitle = {Matching {{Methods}} for {{Causal Inference}}},
  author = {Stuart, Elizabeth A.},
  year = {2010},
  month = feb,
  journal = {Statistical Science},
  volume = {25},
  number = {1},
  pages = {1--21},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/09-STS313},
  urldate = {2022-04-29},
  abstract = {When estimating causal effects using observational data, it is desirable to replicate a randomized experiment as closely as possible by obtaining treated and control groups with similar covariate distributions. This goal can often be achieved by choosing well-matched samples of the original treated and control groups, thereby reducing bias due to the covariates. Since the 1970s, work on matching methods has examined how to best choose treated and control subjects for comparison. Matching methods are gaining popularity in fields such as economics, epidemiology, medicine and political science. However, until now the literature and related advice has been scattered across disciplines. Researchers who are interested in using matching methods---or developing methods related to matching---do not have a single place to turn to learn about past and current research. This paper provides a structure for thinking about matching methods and guidance on their use, coalescing the existing research (both old and new) and providing a summary of where the literature on matching methods is now and where it should be headed.},
  keywords = {observational study,propensity scores,subclassification,weighting},
  file = {C\:\\Users\\nseew\\Google Drive\\Papers\\Stuart_2010_Matching_Methods_for_Causal_Inference.pdf;/Users/nseewald/Zotero/storage/RE8MVN3Y/09-STS313.html}
}

@article{stuartUsingPropensityScores2014,
  title = {Using Propensity Scores in Difference-in-Differences Models to Estimate the Effects of a Policy Change},
  author = {Stuart, Elizabeth A. and Huskamp, Haiden A. and Duckworth, Kenneth and Simmons, Jeffrey and Song, Zirui and Chernew, Michael E. and Barry, Colleen L.},
  year = {2014},
  month = dec,
  journal = {Health Services and Outcomes Research Methodology},
  volume = {14},
  number = {4},
  pages = {166--182},
  publisher = {Springer US},
  issn = {1572-9400},
  doi = {10/gfzdfv},
  urldate = {2021-07-01},
  abstract = {Difference-in-difference (DD) methods are a common strategy for evaluating the effects of policies or programs that are instituted at a particular point in time, such as the implementation of a new law. The DD method compares changes over time in a group unaffected by the policy intervention to the changes over time in a group affected by the policy intervention, and attributes the ``difference-in-differences'' to the effect of the policy. DD methods provide unbiased effect estimates if the trend over time would have been the same between the intervention and comparison groups in the absence of the intervention. However, a concern with DD models is that the program and intervention groups may differ in ways that would affect their trends over time, or their compositions may change over time. Propensity score methods are commonly used to handle this type of confounding in other non-experimental studies, but the particular considerations when using them in the context of a DD model have not been well investigated. In this paper, we describe the use of propensity scores in conjunction with DD models, in particular investigating a propensity score weighting strategy that weights the four groups (defined by time and intervention status) to be balanced on a set of characteristics. We discuss the conceptual issues associated with this approach, including the need for caution when selecting variables to include in the propensity score model, particularly given the multiple time point nature of the analysis. We illustrate the ideas and method with an application estimating the effects of a new payment and delivery system innovation (an accountable care organization model called the ``Alternative Quality Contract'' (AQC) implemented by Blue Cross Blue Shield of Massachusetts) on health plan enrollee out-of-pocket mental health service expenditures. We find no evidence that the AQC affected out-of-pocket mental health service expenditures of enrollees.},
  copyright = {2014 Springer Science+Business Media New York},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Stuart et al_2014_Using propensity scores in difference-in-differences models to estimate the.pdf;/Users/nseewald/Zotero/storage/AZNZSXHK/s10742-014-0123-z.html}
}

@article{studerusCanNeuropsychologicalTesting2018,
  title = {Can Neuropsychological Testing Facilitate Differential Diagnosis between At-Risk Mental State ({{ARMS}}) for Psychosis and Adult Attention-Deficit/Hyperactivity Disorder ({{ADHD}})?},
  author = {Studerus, Erich and Corbisiero, Salvatore and Mazzariello, Nadine and Ittig, Sarah and Leanza, Letizia and Egloff, Laura and Beck, Katharina and Heitz, Ulrike and Andreou, Christina and Stieglitz, Rolf-Dieter and {Riecher-R{\"o}ssler}, Anita},
  year = {2018},
  month = aug,
  journal = {European Psychiatry},
  volume = {52},
  pages = {38--44},
  issn = {0924-9338},
  doi = {10.1016/j.eurpsy.2018.02.006},
  urldate = {2022-08-17},
  abstract = {Background Patients with an at-risk mental state (ARMS) for psychosis and patients with attention-deficit/hyperactivity disorder (ADHD) have many overlapping signs and symptoms and hence can be difficult to differentiate clinically. The aim of this study was to investigate whether the differential diagnosis between ARMS and adult ADHD could be improved by neuropsychological testing. Methods 168 ARMS patients, 123 adult ADHD patients and 109 healthy controls (HC) were recruited via specialized clinics of the University of Basel Psychiatric Hospital. Sustained attention and impulsivity were tested with the Continuous Performance Test, verbal learning and memory with the California Verbal Learning Test, and problem solving abilities with the Tower of Hanoi Task. Group differences in neuropsychological performance were analyzed using generalized linear models. Furthermore, to investigate whether adult ADHD and ARMS can be correctly classified based on the pattern of cognitive deficits, machine learning (i.e. random forests) was applied. Results Compared to HC, both patient groups showed deficits in attention and impulsivity and verbal learning and memory. However, in adult ADHD patients the deficits were comparatively larger. Accordingly, a machine learning model predicted group membership based on the individual neurocognitive performance profile with good accuracy (AUC\,=\,0.82). Conclusions Our results are in line with current meta-analyses reporting that impairments in the domains of attention and verbal learning are of medium effect size in adult ADHD and of small effect size in ARMS patients and suggest that measures of these domains can be exploited to improve the differential diagnosis between adult ADHD and ARMS patients.},
  langid = {english},
  keywords = {ADHD,ARMS,Differential diagnosis,Neurocognition,Psychosis},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Studerus et al_2018_Can neuropsychological testing facilitate differential diagnosis between.pdf}
}

@article{sutradharInversionCorrelationMatrix2003,
  title = {The Inversion of Correlation Matrix for {{MA}}(1) Process},
  author = {Sutradhar, B. C. and Kumar, P.},
  year = {2003},
  month = apr,
  journal = {Applied Mathematics Letters},
  volume = {16},
  number = {3},
  pages = {317--321},
  issn = {0893-9659},
  doi = {10/dxqs68},
  urldate = {2019-05-25},
  abstract = {The exact expression for the inverse of the correlation matrix for the moving average order one, MA(1) process, is obtained. Its application in the context of longitudinal data analysis is discussed.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sutradhar_Kumar_2003_The inversion of correlation matrix for MA(1) process.pdf;/Users/nseewald/Zotero/storage/EWNH6JJ5/S0893965903800514.html}
}

@inproceedings{suttonPlanningIncrementalDynamic1991,
  title = {Planning by {{Incremental Dynamic Programming}}},
  booktitle = {Proceedings of the {{Ninth Conference}} on {{Machine Learning}}},
  author = {Sutton, Richard S.},
  year = {1991},
  pages = {353--357},
  doi = {10.1016/B978-1-55860-200-7.50073-8},
  abstract = {This paper presents the basic results and ideas of dynamic programming as they relate most directly to the concerns of planning in AI. These form the theoretical basis for the incremental planning methods used in the integrated architecture Dyna. These incremental planning methods are based on continually updating an evaluation function and the situation-action mapping of a reactive system. Actions are generated by the reactive system and thus involve minimal delay, while the incremental planning process guarantees that the actions and evaluation function will eventually be optimal -- no matter how extensive a search is required. These methods are well suited to stochastic tasks and to tasks in which a complete and accurate model is not available. For tasks too large to implement the situation-action mapping as a table, supervised-learning methods must be used, and their capabilities remain a significant limitation of the approach.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sutton_1991_Planning by Incremental Dynamic Programming.pdf}
}

@book{suttonReinforcementLearningIntroduction1998,
  title = {Reinforcement {{Learning}} : {{An Introduction}}},
  author = {Sutton, Richard S and Barto, Andrew G},
  year = {1998},
  doi = {10.1109/TNN.1998.712192},
  abstract = {Below are links to a variety of software related to examples and exercises in the book, organized by chapters (some files appear in multiple places). See particularly the Mountain Car code. Most of the rest of the code is written in Common Lisp and requires utility routines available here. For the graphics, you will need the the packages for G and in some cases my graphing tool. Even if you can not run this code, it still may clarify some of the details of the experiments. However, there is no guarantee that the examples in the book were run using exactly the software given. This code also has not been extensively tested or documented and is being made available "as is". If you have corrections, extensions, additions or improvements of any kind, please send them to me at rich@richsutton.com for inclusion here. \ding{108}},
  isbn = {0-262-19398-1},
  pmid = {18255791},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Sutton_Barto_1998_Reinforcement Learning.pdf}
}

@article{szmulewiczEmulatingTargetTrial2023,
  title = {Emulating a {{Target Trial}} of {{Dynamic Treatment Strategies}} for {{Major Depressive Disorder Using Data From}} the {{STAR}}{$\ast$}{{D Randomized Trial}}},
  author = {Szmulewicz, Alejandro G. and Wanis, Kerollos N. and Perlis, Roy H. and {Hern{\'a}ndez-D{\'i}az}, Sonia and {\"O}ng{\"u}r, Dost and Hern{\'a}n, Miguel A.},
  year = {2023},
  month = jun,
  journal = {Biological Psychiatry},
  series = {Serotonin {{Signaling}} in {{Depression}}},
  volume = {93},
  number = {12},
  pages = {1127--1136},
  issn = {0006-3223},
  doi = {10.1016/j.biopsych.2022.09.028},
  urldate = {2025-01-15},
  abstract = {Background Clinical guidelines recommend adding a second drug for patients with major depressive disorder who have a partial response and switching antidepressants for those who show no response or intolerance. This guidelines-based strategy was compared with other strategies for the management of unresponsive depression. Methods A total of 1436 individuals experiencing treatment failure with citalopram and still requiring antidepressant therapy were identified in the STAR{$\ast$}D (Sequenced Treatment Alternatives to Relieve Depression) trial. A (hypothetical) target trial was then designed and emulated. The following strategies for decision making were compared: sequential monotherapy, sequential dual non--selective serotonin reuptake inhibitor therapy (SD), and a guidelines-based strategy. The primary outcome was symptomatic remission defined as a Hamilton Depression Rating Scale score {$\leq$}7 or 2 consecutive scores {$\leq$}5 on the 16-item Quick Inventory of Depressive Symptomatology--Clinician Rated. Secondary outcomes were serious events (hospitalizations, suicide, and mortality). Inverse probability weighting was used to control for possible confounding. Results A total of 971 patients were eligible for our emulation. Patients initiating SD had the lowest levels of depression at baseline. The estimated 9-month probability of remission was 43.5\% for the sequential monotherapy group, 47.6\% for the SD group, and 53.2\% for the guidelines-based strategy group. Compared with the sequential monotherapy group, the difference in 9-month probability of remission was~-4.2\% (95\% CI,~-15.6 to 4.6) for the SD group and~-9.7\% (-19.3 to 1.9) for the guidelines-based strategy group. The 9-month relative risks of remission were 1.09 (0.90 to 1.38) and 1.22 (0.96 to 1.46), respectively. Results were consistent across sensitivity analyses. The 9-month relative risks of serious events were 0.77 (0.38 to 1.40) and 0.62 (0.33 to 1.00), respectively. Conclusions Using the guidelines-based strategy was associated with an increased probability of remission and a lower risk of serious adverse events. The potential implications are substantial given the large number of patients experiencing treatment failure to antidepressants.},
  keywords = {Antidepressant-resistant depression,Antidepressants,Epidemiology,Major depressive disorder,Methodology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Szmulewicz et al. - 2023 - Emulating a Target Trial of Dynamic Treatment Strategies for Major Depressive Disorder Using Data Fr.pdf;/Users/nseewald/Zotero/storage/WYR9EXF4/S0006322322016365.html}
}

@article{tadepalliModelbasedAverageReward1998,
  title = {Model-Based Average Reward Reinforcement Learning},
  author = {Tadepalli, Prasad and Ok, DoKyeong},
  year = {1998},
  month = apr,
  journal = {Artificial Intelligence},
  volume = {100},
  number = {1-2},
  pages = {177--224},
  issn = {00043702},
  doi = {10.1016/S0004-3702(98)00002-2},
  urldate = {2018-10-12},
  abstract = {Reinforcement Learning (RL) is the study of programs that improve their performance by receiving rewards and punishments from the environment. Most RL methods optimize the discounted total reward received by an agent, while, in many domains, the natural criterion is to optimize the average reward per time step. In this paper, we introduce a model-based Averagereward Reinforcement Learning method called H-learning and show that it converges more quickly and robustly than its discounted counterpart in the domain of scheduling a simulated Automatic Guided Vehicle (AGV). We also introduce a version of H-learning that automatically explores the unexplored parts of the state space, while always choosing greedy actions with respect to the current value function. We show that this ``Auto-exploratory H-Learning'' performs better than the previously studied exploration strategies. To scale H-learning to larger state spaces, we extend it to learn action models and reward functions in the form of dynamic Bayesian networks, and approximate its value function using local linear regression. We show that both of these extensions are effective in significantly reducing the space requirement of H-learning and making it converge faster in some AGV scheduling tasks. @ 1998 Published by Elsevier Science B.V.},
  langid = {english},
  keywords = {AGV scheduling,Average reward,Bayesian networks,Exploration,Linear regression,Machine learning,Model-based,Reinforcement learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tadepalli_Ok_1998_Model-based average reward reinforcement learning.pdf}
}

@article{taiMathematicalModelDetermination1994,
  title = {A {{Mathematical Model}} for the {{Determination}} of {{Total Area Under Glucose Tolerance}} and {{Other Metabolic Curves}}},
  author = {Tai, Mary M},
  year = {1994},
  month = feb,
  journal = {Diabetes Care},
  volume = {17},
  number = {2},
  pages = {152--154},
  issn = {0149-5992, 1935-5548},
  doi = {10.2337/diacare.17.2.152},
  urldate = {2024-06-05},
  abstract = {OBJECTIVE--- To develop a mathematical model for the determination of total areas under curves from various metabolic studies. RESEARCH DESIGN AND METHODS--- In Tai's Model, the total area under a curve is computed by dividing the area under the curve between two designated values on the X-axis (abscissas) into small segments (rectangles and triangles) whose areas can be accurately calculated from their respective geometrical formulas. The total sum of these individual areas thus represents the total area under the curve. Validity of the model is established by comparing total areas obtained from this model to these same areas obtained from graphic method Gess than {\textpm}0.4\%). Other formulas widely applied by researchers under- or overestimated total area under a metabolic curve by a great margin. RESULTS --- Tai's model proves to be able to 1) determine total area under a curve with precision; 2) calculate area with varied shapes that may or may not intercept on one or both X/Y axes; 3) estimate total area under a curve plotted against varied time intervals (abscissas), whereas other formulas only allow the same time interval; and 4) compare total areas of metabolic curves produced by different studies. CONCLUSIONS --- The Tai model allows flexibility in experimental conditions, which means, in the case of the glucose-response curve, samples can be taken with differing time intervals and total area under the curve can still be determined with precision.},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/XPMQXPKW/Tai - 1994 - A Mathematical Model for the Determination of Tota.pdf}
}

@article{tamuraSmallSequentialMultiple2016,
  title = {A Small n Sequential Multiple Assignment Randomized Trial Design for Use in Rare Disease Research},
  author = {Tamura, Roy N. and Krischer, Jeffrey P. and Pagnoux, Christian and Micheletti, Robert and Grayson, Peter C. and Chen, Yeh-Fong and Merkel, Peter A.},
  year = {2016},
  month = jan,
  journal = {Contemporary Clinical Trials},
  volume = {46},
  pages = {48--51},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2015.11.010},
  urldate = {2024-08-02},
  abstract = {Background Clinical trials in rare diseases are difficult to conduct due to the limited number of patients available with each disorder. We developed a Phase 2 trial which is a small n sequential multiple assignment randomized trial (snSMART) design to test several treatments for a rare disease for which no standard therapy exists. Purpose This paper illustrates the design, sample size estimation and operating characteristics of an snSMART. Methods We investigate the performance of a class of weighted Z statistics via computer simulations. Results We demonstrate the increase in power over traditional single stage designs, and indicate how the power changes as a function of the weight given to each stage. Conclusion The snSMART design is promising in a rare disease setting where several alternative treatments are under consideration and small sample sizes are necessary.},
  keywords = {Binary data,Re-randomization,Weighted Z statistic},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tamura_et_al_2016_A_small_n_sequential_multiple_assignment_randomized_trial_design_for_use_in.pdf;/Users/nseewald/Zotero/storage/FTSZAKW4/S155171441530121X.html}
}

@article{tangDTRPackageEstimation2015,
  title = {{{DTR}}: {{An R Package}} for {{Estimation}} and {{Comparison}} of {{Survival Outcomes}} of {{Dynamic Treatment Regimes}}},
  shorttitle = {{{{\textbf{DTR}}}}},
  author = {Tang, Xinyu and Melguizo, Maria},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {65},
  number = {7},
  issn = {1548-7660},
  doi = {10.18637/jss.v065.i07},
  urldate = {2024-08-16},
  langid = {english},
  file = {C\:\\Users\\Nick\\Box\\Zotero\\Tang_Melguizo_2015_DTR.pdf;/Users/nseewald/Zotero/storage/G3UJP8Z3/v065i07.html}
}

@inproceedings{tangReinforcementLearningNonstationary2014a,
  ids = {tangReinforcementLearningNonstationary2014},
  title = {Reinforcement {{Learning}} in Non-Stationary Environments: {{An}} Intrinsically Motivated Stress Based Memory Retrieval Performance ({{SBMRP}}) Model},
  shorttitle = {Reinforcement {{Learning}} in Non-Stationary Environments},
  booktitle = {2014 {{IEEE International Conference}} on {{Fuzzy Systems}} ({{FUZZ-IEEE}})},
  author = {Tang, Tiong Yew and Egerton, Simon and Kubota, Naoyuki},
  year = {2014},
  month = jul,
  pages = {1728--1735},
  publisher = {IEEE},
  address = {Beijing, China},
  doi = {10.1109/FUZZ-IEEE.2014.6891757},
  urldate = {2018-10-12},
  abstract = {Biological systems are said to learn from both intrinsic and extrinsic motivations. Extrinsic motivations, largely based on environmental conditions, have been well explored by Reinforcement Learning (RL) methods. Less explored, and more interesting in our opinion, are the possible intrinsic motivations that may drive a learning agent. In this paper we explore such a possibility. We develop a novel intrinsic motivation model which is based on the well known Yerkes and Dodson stress curve theory and the biological principles associated with stress. We use a stress feedback loop to affect the agent's memory capacity for retrieval. The stress and memory signals are fed into a fuzzy logic system which decides upon the best action for the agent to perform against the current best action policy. Our simulated results show that our model significantly improves upon agent learning performance and stability when objectively compared against existing state-ofthe-art RL approaches in non-stationary environments and can effectively deal with significantly larger problem domains.},
  isbn = {978-1-4799-2072-3 978-1-4799-2073-0},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tang et al_2014_Reinforcement Learning in non-stationary environments.pdf}
}

@article{tanTimevaryingEffectModel2012,
  title = {A Time-Varying Effect Model for Intensive Longitudinal Data.},
  author = {Tan, Xianming and Shiyko, Mariya P. and Li, Runze and Li, Yuelin and Dierker, Lisa},
  year = {2012},
  journal = {Psychological Methods},
  volume = {17},
  number = {1},
  pages = {61--77},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0025814},
  urldate = {2018-10-12},
  abstract = {Understanding temporal change in human behavior and psychological processes is a central issue in the behavioral sciences. With technological advances, intensive longitudinal data (ILD) are increasingly generated by studies of human behavior that repeatedly administer assessments over time. ILD offer unique opportunities to describe temporal behavioral changes in detail and identify related environmental and psychosocial antecedents and consequences. Traditional analytical approaches impose strong parametric assumptions about the nature of change in the relationship between time-varying covariates and outcomes of interest. This article introduces time-varying effect models (TVEMs) that explicitly model changes in the association between ILD covariates and ILD outcomes over time in a flexible manner. In this article, we describe unique research questions that the TVEM addresses, outline the model-estimation procedure, share a SAS macro for implementing the model, demonstrate model utility with a simulated example, and illustrate model applications in ILD collected as part of a smoking-cessation study to explore the relationship between smoking urges and self-efficacy during the course of the pre- and postcessation period.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tan et al_2012_A time-varying effect model for intensive longitudinal data.pdf}
}

@article{tatumTalkingRaceLearning1992,
  title = {Talking about Race, Learning about Racism: {{The}} Application of Racial Identity Development Theory in the Classroom},
  author = {Tatum, Beverly Daniel},
  year = {1992},
  journal = {Harvard Educational Review},
  volume = {62},
  number = {1},
  keywords = {nosource}
}

@book{tatumWhyAreAll2017,
  title = {"{{Why}} Are All the Black Kids Sitting Together in the Cafeteria?": And Other Conversations about Race},
  shorttitle = {"{{Why}} Are All the Black Kids Sitting Together in the Cafeteria?},
  author = {Tatum, Beverly Daniel},
  year = {2017},
  edition = {Third trade paperback edition},
  publisher = {Basic Books},
  address = {New York},
  isbn = {978-0-465-06068-9 978-1-5416-1658-5},
  lccn = {E185.625 .T38 2017},
  keywords = {nosource}
}

@misc{taylorHowWorkStatistician2012,
  title = {How to {{Work}} with a {{Statistician}}},
  author = {Taylor, Sandra},
  year = {2012},
  month = apr,
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Taylor_2012_How to Work with a Statistician.pdf}
}

@article{taylorSurvivalEstimationTesting2002,
  title = {Survival Estimation and Testing via Multiple Imputation},
  author = {Taylor, Jeremy M. G. and Murray, Susan and Hsu, Chiu-Hsieh},
  year = {2002},
  month = jul,
  journal = {Statistics \& Probability Letters},
  volume = {58},
  number = {3},
  pages = {221--232},
  issn = {0167-7152},
  doi = {10.1016/S0167-7152(02)00030-5},
  urldate = {2024-02-22},
  abstract = {Multiple imputation is a technique for handling data sets with missing values. The method fills in each missing value several times, creating many augmented data sets. Each augmented data set is analyzed separately and the results combined to give a final result consisting of an estimate and a measure of uncertainty. In this paper we consider nonparametric multiple-imputation methods to handle missing event times for censored observations in the context of nonparametric survival estimation and testing. Two nonparametric imputation schemes are considered. In risk set imputation the censored time is replaced by a random draw of the observed times amongst those at risk after the censoring time. In Kaplan--Meier (KM) imputation the imputed time is a draw from the estimated distribution of event times amongst those at risk after the censoring time. We show that with a large number of imputes the estimates from both methods reproduce the KM estimator. In a simulation study we show that the inclusion of a bootstrap stage in the multiple imputation algorithm gives coverage rates of confidence intervals that are comparable to that from Greenwood's formula. Connections to the redistribute to the right algorithm are discussed.},
  keywords = {Kaplan-Meier estimate,Multiple imputation,Redistribute to the right},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Taylor et al_2002_Survival estimation and testing via multiple imputation.pdf;/Users/nseewald/Zotero/storage/ESQKDI25/S0167715202000305.html}
}

@article{tchetgenCausalInferencePresence2012,
  title = {On Causal Inference in the Presence of Interference},
  author = {Tchetgen, Eric J Tchetgen and VanderWeele, Tyler J},
  year = {2012},
  month = feb,
  journal = {Statistical Methods in Medical Research},
  volume = {21},
  number = {1},
  pages = {55--75},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280210386779},
  urldate = {2024-05-02},
  abstract = {Interference is said to be present when the exposure or treatment received by one individual may affect the outcomes of other individuals. Such interference can arise in settings in which the outcomes of the various individuals come about through social interactions. When interference is present, causal inference is rendered considerably more complex, and the literature on causal inference in the presence of interference has just recently begun to develop. In this article we summarise some of the concepts and results from the existing literature and extend that literature in considering new results for finite sample inference, new inverse probability weighting estimators in the presence of interference and new causal estimands of interest.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tchetgen_VanderWeele_2012_On causal inference in the presence of interference.pdf}
}

@article{tchetgentchetgenUniversalDifferenceinDifferencesCausal2024,
  title = {Universal {{Difference-in-Differences}} for {{Causal Inference}} in {{Epidemiology}}},
  author = {Tchetgen Tchetgen, Eric J. and Park, Chan and Richardson, David B.},
  year = {2024},
  month = jan,
  journal = {Epidemiology},
  volume = {35},
  number = {1},
  pages = {16},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000001676},
  urldate = {2024-07-12},
  abstract = {Difference-in-differences is undoubtedly one of the most widely used methods for evaluating the causal effect of an intervention in observational (i.e., nonrandomized) settings. The approach is typically used when pre- and postexposure outcome measurements are available, and one can reasonably assume that the association of the unobserved confounder with the outcome has the same absolute magnitude in the two exposure arms and is constant over time; a so-called parallel trends assumption. The parallel trends assumption may not be credible in many practical settings, for example, if the outcome is binary, a count, or polytomous, as well as when an uncontrolled confounder exhibits nonadditive effects on the distribution of the outcome, even if such effects are constant over time. We introduce an alternative approach that replaces the parallel trends assumption with an odds ratio equi-confounding assumption under which an association between treatment and the potential outcome under no treatment is identified with a well-specified generalized linear model relating the pre-exposure outcome and the exposure. Because the proposed method identifies any causal effect that is conceivably identified in the absence of confounding bias, including nonlinear effects such as quantile treatment effects, the approach is aptly called universal difference-in-differences. We describe and illustrate both fully parametric and more robust semiparametric universal difference-in-differences estimators in a real-world application concerning the causal effects of a Zika virus outbreak on birth rate in Brazil.           A supplementary digital video is available at: https://links.lww.com/EDE/C90},
  langid = {american},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tchetgen_Tchetgen_et_al_2024_Universal_Difference-in-Differences_for_Causal_Inference_in_Epidemiology.pdf;/Users/nseewald/Zotero/storage/WV5KERRG/universal_difference_in_differences_for_causal.3.html}
}

@article{teerenstraSimpleSampleSize2012,
  title = {A Simple Sample Size Formula for Analysis of Covariance in Cluster Randomized Trials},
  author = {Teerenstra, Steven and Eldridge, Sandra and Graff, Maud and {de Hoop}, Esther and Borm, George F.},
  year = {2012},
  journal = {Statistics in Medicine},
  volume = {31},
  number = {20},
  pages = {2169--2178},
  issn = {1097-0258},
  doi = {10.1002/sim.5352},
  urldate = {2024-10-21},
  abstract = {For cluster randomized trials with a continuous outcome, the sample size is often calculated as if an analysis of the outcomes at the end of the treatment period (follow-up scores) would be performed. However, often a baseline measurement of the outcome is available or feasible to obtain. An analysis of covariance (ANCOVA) using both the baseline and follow-up score of the outcome will then have more power. We calculate the efficiency of an ANCOVA analysis using the baseline scores compared with an analysis on follow-up scores only. The sample size for such an ANCOVA analysis is a factor r2 smaller, where r is the correlation of the cluster means between baseline and follow-up. This correlation can be expressed in clinically interpretable parameters: the correlation between baseline and follow-up of subjects (subject autocorrelation) and that of clusters (cluster autocorrelation). Because of this, subject matter knowledge can be used to provide (range of) plausible values for these correlations, when estimates from previous studies are lacking. Depending on how large the subject and cluster autocorrelations are, analysis of covariance can substantially reduce the number of clusters needed. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {ANCOVA,cluster randomization,power,pretest-posttest,sample size},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Teerenstra et al. - 2012 - A simple sample size formula for analysis of covariance in cluster randomized trials.pdf;/Users/nseewald/Zotero/storage/C6PGEFSP/sim.html}
}

@article{tetzlaffGuidelinesRandomizedClinical2012,
  title = {Guidelines for Randomized Clinical Trial Protocol Content: A Systematic Review},
  shorttitle = {Guidelines for Randomized Clinical Trial Protocol Content},
  author = {Tetzlaff, Jennifer M. and Chan, An-Wen and Kitchen, Jessica and Sampson, Margaret and Tricco, Andrea C. and Moher, David},
  year = {2012},
  month = sep,
  journal = {Systematic Reviews},
  volume = {1},
  number = {1},
  pages = {43},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-1-43},
  urldate = {2022-05-05},
  abstract = {All randomized clinical trials (RCTs) require a protocol; however, numerous studies have highlighted protocol deficiencies. Reporting guidelines may improve the content of research reports and, if developed using robust methods, may increase the utility of reports to stakeholders. The objective of this study was to systematically identify and review RCT protocol guidelines, to assess their characteristics and methods of development, and to compare recommendations.},
  keywords = {Clinical trials,Protocols,Randomized controlled trials,Reporting guideline,SPIRIT initiative,Systematic review},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tetzlaff et al_2012_Guidelines for randomized clinical trial protocol content.pdf;/Users/nseewald/Zotero/storage/YBAFLMUS/2046-4053-1-43.html}
}

@article{thallCovariateadjustedAdaptiveRandomization2005,
  title = {Covariate-Adjusted Adaptive Randomization in a Sarcoma Trial with Multi-Stage Treatments},
  author = {Thall, Peter F. and Wathen, J. K.},
  year = {2005},
  journal = {Statistics in Medicine},
  volume = {24},
  number = {13},
  pages = {1947--1964},
  issn = {1097-0258},
  doi = {10/d5ztnt},
  urldate = {2020-12-13},
  abstract = {We present a Bayesian design for a multi-centre, randomized clinical trial of two chemotherapy regimens for advanced or metastatic unresectable soft tissue sarcoma. After randomization, each patient receives up to four stages of chemotherapy, with the patient's disease evaluated after each stage and categorized on a trinary scale of severity. Therapy is continued to the next stage if the patient's disease is stable, and is discontinued if either tumour response or treatment failure is observed. We assume a probability model that accounts for baseline covariates and the multi-stage treatment and disease evaluation structure. The design uses covariate-adjusted adaptive randomization based on a score that combines the patient's probabilities of overall treatment success or failure. The adaptive randomization procedure generalizes the method proposed by Thompson (1933) for two binomial distributions with beta priors. A simulation study of the design in the context of the sarcoma trial is presented. Copyright {\copyright} 2005 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2005 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Thall_Wathen_2005_Covariate-adjusted adaptive randomization in a sarcoma trial with multi-stage.pdf;/Users/nseewald/Zotero/storage/EDIR39T5/sim.html}
}

@article{thallEvaluatingMultipleTreatment2000,
  title = {Evaluating Multiple Treatment Courses in Clinical Trials},
  author = {Thall, Peter F. and Millikan, Randall E. and Sung, Hsi-Guang},
  year = {2000},
  journal = {Statistics in Medicine},
  volume = {19},
  number = {8},
  pages = {1011--1028},
  issn = {1097-0258},
  doi = {10/bmv5jc},
  urldate = {2020-12-13},
  abstract = {In oncology, a patient's treatment often involves multiple courses of chemotherapy. The most common medical practice in choosing treatments for successive courses is to repeat a treatment that is successful in a given course and otherwise switch to a different treatment. Patient outcome thus consists of a sequence of dependent response variables and corresponding treatments. Despite the widespread use of such adaptive `play-the-winner-and-drop-the-loser' algorithms in medical settings involving multiple treatment courses, most statistical methods for treatment evaluation characterize early patient outcome as a single response to a single treatment, resulting in a substantial loss of information. In this paper, we provide a statistical framework for multi-course clinical trials involving some variant of the play-the-winner-and-drop-the-loser strategy. The aim is to design and conduct the trial to more closely reflect actual clinical practice, and thus increase the amount of information per patient. The proposed design is similar to a multi-stage cross-over trial, with the essential difference that here all treatments after the first course are assigned adaptively. We illustrate the method by application to a randomized phase II trial for androgen independent prostate cancer. We consider the goals of selecting one best treatment, or selecting a best ordered pair of treatments with the second given if the first fails to achieve a patient success. A simulation study is reported, and extensions to trials involving toxicity or regimen-related death are discussed. Copyright {\copyright} 2000 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2000 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Thall et al_2000_Evaluating multiple treatment courses in clinical trials.pdf;/Users/nseewald/Zotero/storage/X24HZYNB/(SICI)1097-0258(20000430)1981011AID-SIM4143.0.html}
}

@article{thallSelectingTherapeuticStrategies2002,
  title = {Selecting {{Therapeutic Strategies Based}} on {{Efficacy}} and {{Death}} in {{Multicourse Clinical Trials}}},
  author = {Thall, Peter F. and Sung, Hsi-Guang and Estey, Elihu H.},
  year = {2002},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {97},
  number = {457},
  pages = {29--39},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10/dx3fkb},
  urldate = {2020-12-13},
  abstract = {Therapy of rapidly fatal diseases often requires multiple courses of treatment. In each course, the treatment may achieve the desired clinical goal (``response''), the patient may survive without response (``failure''), or the patient may die. When treatment fails in a given course, it is common medical practice to switch to a different treatment for the next course. Most statistical approaches to such settings simply ignore the multicourse structure. They characterize patient outcome as a single binary variable, combine death and failure, and identify only one treatment for each patient. Such approaches waste important information. We provide a statistical framework, including a family of generalized logistic regression models and an approximate Bayesian method, that incorporates historical data while accommodating multiple treatment courses, a trinary outcome in each course, and patient prognostic covariates. The framework serves as a basis for data analysis, treatment evaluation, and clinical trial design. In contrast with the usual approach of evaluating individual treatments, our methodology evaluates outcome-adaptive, multicourse treatment strategies that specify, within prognostic subgroups, which treatment to give in each course. We describe a general approach for constructing clinical trial designs that may be tailored to different multicourse settings. For each prognostic subgroup, based on a real-valued function of the covariate-adjusted probabilities of response and death, the design drops inferior treatment strategies during the trial and selects the best strategy at the end. The methodology is illustrated in the context of designing a randomized two-course, three-treatment acute leukemia trial with two prognostic covariates. To validate the model and develop a prior, we first fit the model to a historical dataset. We describe a simulation study of the design under several clinical scenarios. The simulations show that the method can reliably identify treatment--subgroup interactions based on moderate sample sizes.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Thall et al_2002_Selecting Therapeutic Strategies Based on Efficacy and Death in Multicourse.pdf;/Users/nseewald/Zotero/storage/IF9BSTDC/016214502753479202.html}
}

@incollection{thallSMARTDesignConduct2015,
  title = {{{SMART}} Design, Conduct, and Analysis in Oncology},
  shorttitle = {Adaptive {{Treatment Strategies}} in {{Practice}}},
  booktitle = {Adaptive {{Treatment Strategies}} in {{Practice}}: {{Planning Trials}} and {{Analyzing Data}} for {{Personalized Medicine}}},
  author = {Thall, Peter F.},
  editor = {Kosorok, Michael R. and Moodie, Erica E. M.},
  year = {2015},
  month = dec,
  pages = {41--54},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, PA},
  isbn = {978-1-61197-417-1},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Thall_2015_SMART design, conduct, and analysis in oncology.pdf}
}

@article{thompsonSimpleFormulasStandard2011,
  title = {Simple Formulas for Standard Errors That Cluster by Both Firm and Time},
  author = {Thompson, Samuel B.},
  year = {2011},
  month = jan,
  journal = {Journal of Financial Economics},
  volume = {99},
  number = {1},
  pages = {1--10},
  issn = {0304-405X},
  doi = {10.1016/j.jfineco.2010.08.016},
  urldate = {2022-04-15},
  abstract = {When estimating finance panel regressions, it is common practice to adjust standard errors for correlation either across firms or across time. These procedures are valid only if the residuals are correlated either across time or across firms, but not across both. This paper shows that it is very easy to calculate standard errors that are robust to simultaneous correlation along two dimensions, such as firms and time. The covariance estimator is equal to the estimator that clusters by firm, plus the estimator that clusters by time, minus the usual heteroskedasticity-robust ordinary least squares (OLS) covariance matrix. Any statistical package with a clustering command can be used to easily calculate these standard errors.},
  langid = {english},
  keywords = {Cluster standard errors,Finance panel data,Panel data},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Thompson_2011_Simple formulas for standard errors that cluster by both firm and time.pdf;/Users/nseewald/Zotero/storage/FLPNPMXI/S0304405X10001923.html}
}

@article{tianCoxModelTimeVarying2005,
  title = {On the {{Cox Model With Time-Varying Regression Coefficients}}},
  author = {Tian, Lu and Zucker, David and Wei, L. J},
  year = {2005},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {469},
  pages = {172--183},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/016214504000000845},
  urldate = {2024-01-11},
  abstract = {In the analysis of censored failure time observations, the standard Cox proportional hazards model assumes that the regression coefficients are time invariant. Often, these parameters vary over time, and the temporal covariate effects on the failure time are of great interest. In this article, following previous work of Cai and Sun, we propose a simple estimation procedure for the Cox model with time-varying coefficients based on a kernel-weighted partial likelihood approach. We construct pointwise and simultaneous confidence intervals for the regression parameters over a properly chosen time interval via a simple resampling technique. We derive a prediction method for future patients' survival with any specific set of covariates. Building on the estimates for the time-varying coefficients, we also consider the mixed case and present an estimation procedure for time-independent parameters in the model. Furthermore, we show how to use an integrated function of the estimate for a specific regression coefficient to examine the adequacy of proportional hazards assumption for the corresponding covariate graphically and numerically. All of the proposals are illustrated extensively with a well-known study from the Mayo Clinic.},
  keywords = {Confidence band,Kernel estimation,Martingale,Model checking and selection,Partial likelihood,Prediction,Survival analysis},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tian et al_2005_On the Cox Model With Time-Varying Regression Coefficients.pdf}
}

@article{tibshiraniRegressionShrinkageSelection1996,
  title = {Regression {{Shrinkage}} and {{Selection}} via the {{Lasso}}},
  author = {Tibshirani, Robert},
  year = {1996},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {58},
  number = {1},
  eprint = {2346178},
  eprinttype = {jstor},
  pages = {267--288},
  abstract = {We propose a new method for estimation in linear models. The 'lasso' minim residual sum of squares subject to the sum of the absolute value of the coefficients than a constant. Because of the nature of this constraint it tends to produce coefficients that are exactly 0 and hence gives interpretable models. Our simulatio suggest that the lasso enjoys some of the favourable properties of both subset sele ridge regression. It produces interpretable models like subset selection and exh stability of ridge regression. There is also an interesting relationship with recent adaptive function estimation by Donoho and Johnstone. The lasso idea is quite ge can be applied in a variety of statistical models: extensions to generalized regressio and tree-based models are briefly described.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tibshirani_1996_Regression Shrinkage and Selection via the Lasso.pdf}
}

@article{tompsettTargetTrialEmulation2023,
  title = {Target {{Trial Emulation}} and {{Bias Through Missing Eligibility Data}}: {{An Application}} to a {{Study}} of {{Palivizumab}} for the {{Prevention}} of {{Hospitalization Due}} to {{Infant Respiratory Illness}}},
  shorttitle = {Target {{Trial Emulation}} and {{Bias Through Missing Eligibility Data}}},
  author = {Tompsett, Daniel and Zylbersztejn, Ania and Hardelid, Pia and De Stavola, Bianca},
  year = {2023},
  month = apr,
  journal = {American Journal of Epidemiology},
  volume = {192},
  number = {4},
  pages = {600--611},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwac202},
  urldate = {2024-02-14},
  abstract = {Abstract             Target trial emulation (TTE) applies the principles of randomized controlled trials to the causal analysis of observational data sets. One challenge that is rarely considered in TTE is the sources of bias that may arise if the variables involved in the definition of eligibility for the trial are missing. We highlight patterns of bias that might arise when estimating the causal effect of a point exposure when restricting the target trial to individuals with complete eligibility data. Simulations consider realistic scenarios where the variables affecting eligibility modify the causal effect of the exposure and are missing at random or missing not at random. We discuss means to address these patterns of bias, namely: 1) controlling for the collider bias induced by the missing data on eligibility, and 2) imputing the missing values of the eligibility variables prior to selection into the target trial. Results are compared with the results when TTE is performed ignoring the impact of missing eligibility. A study of palivizumab, a monoclonal antibody recommended for the prevention of respiratory hospital admissions due to respiratory syncytial virus in high-risk infants, is used for illustration.},
  langid = {english}
}

@article{tompsettTargetTrialEmulation2023a,
  title = {Target {{Trial Emulation}} and {{Bias Through Missing Eligibility Data}}: {{An Application}} to a {{Study}} of {{Palivizumab}} for the {{Prevention}} of {{Hospitalization Due}} to {{Infant Respiratory Illness}}},
  shorttitle = {Target {{Trial Emulation}} and {{Bias Through Missing Eligibility Data}}},
  author = {Tompsett, Daniel and Zylbersztejn, Ania and Hardelid, Pia and De Stavola, Bianca},
  year = {2023},
  month = apr,
  journal = {American Journal of Epidemiology},
  volume = {192},
  number = {4},
  pages = {600--611},
  issn = {0002-9262},
  doi = {10.1093/aje/kwac202},
  urldate = {2024-02-14},
  abstract = {Target trial emulation (TTE) applies the principles of randomized controlled trials to the causal analysis of observational data sets. One challenge that is rarely considered in TTE is the sources of bias that may arise if the variables involved in the definition of eligibility for the trial are missing. We highlight patterns of bias that might arise when estimating the causal effect of a point exposure when restricting the target trial to individuals with complete eligibility data. Simulations consider realistic scenarios where the variables affecting eligibility modify the causal effect of the exposure and are missing at random or missing not at random. We discuss means to address these patterns of bias, namely: 1) controlling for the collider bias induced by the missing data on eligibility, and 2) imputing the missing values of the eligibility variables prior to selection into the target trial. Results are compared with the results when TTE is performed ignoring the impact of missing eligibility. A study of palivizumab, a monoclonal antibody recommended for the prevention of respiratory hospital admissions due to respiratory syncytial virus in high-risk infants, is used for illustration.},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tompsett_et_al_2023_Target_Trial_Emulation_and_Bias_Through_Missing_Eligibility_Data.pdf;/Users/nseewald/Zotero/storage/VRM5NC9G/6887820.html}
}

@article{tormohlenStateEvidenceAssociation2021,
  title = {The {{State}} of the {{Evidence}} on the {{Association Between State Cannabis Laws}} and {{Opioid-Related Outcomes}}: A {{Review}}},
  shorttitle = {The {{State}} of the {{Evidence}} on the {{Association Between State Cannabis Laws}} and {{Opioid-Related Outcomes}}},
  author = {Tormohlen, Kayla N. and Bicket, Mark C. and White, Sarah and Barry, Colleen L. and Stuart, Elizabeth A. and Rutkow, Lainie and McGinty, Emma E.},
  year = {2021},
  month = dec,
  journal = {Current Addiction Reports},
  volume = {8},
  number = {4},
  pages = {538--545},
  issn = {2196-2952},
  doi = {10.1007/s40429-021-00397-1},
  urldate = {2022-01-31},
  abstract = {Purpose of Review This review summarizes studies examining impacts of medical and recreational cannabis laws on opioid prescribing, opioid use, opioid use disorder, opioid-related service utilization, and opioid-involved mortality. We also discuss research challenges and recommendations for future work. Recent Findings Twenty-one US-based studies published between 2014 and 2021 that assessed state cannabis laws' asso- ciation with opioid-related outcomes were reviewed. Study results were largely inconclusive. We identifed six challenges of existing work: (1) inability to directly measure cannabis/opioid substitution; (2) use of general population samples and lack of individual-level longitudinal studies; (3) challenges disentangling efects of cannabis laws from other state laws; (4) methodological challenges with staggered policy implementation; (5) limited consideration of cannabis law provisions; (6) lack of triangulation across data sources. Summary While existing research suggests the potential for cannabis laws to reduce high-risk opioid prescribing and other opioid-related adverse outcomes, studies should be interpreted in light of limitations.},
  langid = {english},
  keywords = {notion},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tormohlen et al_2021_The State of the Evidence on the Association Between State Cannabis Laws and.pdf}
}

@article{touloumisSimulatingCorrelatedBinary2016,
  title = {Simulating {{Correlated Binary}} and {{Multinomial Responses}} under {{Marginal Model Specification}}: {{The SimCorMultRes Package}}},
  shorttitle = {Simulating {{Correlated Binary}} and {{Multinomial Responses}} under {{Marginal Model Specification}}},
  author = {Touloumis, Anestis},
  year = {2016},
  journal = {The R Journal},
  volume = {8},
  number = {2},
  pages = {79},
  issn = {2073-4859},
  doi = {10.32614/RJ-2016-034},
  urldate = {2021-10-20},
  abstract = {We developed the R package SimCorMultRes to facilitate simulation of correlated categorical (binary and multinomial) responses under a desired marginal model specification. The simulated correlated categorical responses are obtained by applying threshold approaches to correlated continuous responses of underlying regression models and the dependence structure is parametrized in terms of the correlation matrix of the latent continuous responses. This article provides an elaborate introduction to the SimCorMultRes package demonstrating its design and usage via three examples. The package can be obtained via CRAN.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Touloumis_2016_Simulating Correlated Binary and Multinomial Responses under Marginal Model.pdf}
}

@article{tripathiMatrixExtensionCauchySchwarz1999,
  title = {A Matrix Extension of the {{Cauchy-Schwarz}} Inequality},
  author = {Tripathi, Gautam},
  year = {1999},
  month = apr,
  journal = {Economics Letters},
  volume = {63},
  number = {1},
  pages = {1--3},
  issn = {0165-1765},
  doi = {10/dw23t4},
  urldate = {2020-05-07},
  abstract = {A simple argument is used to obtain a very useful generalization of the well known Cauchy-Schwarz inequality.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tripathi_1999_A matrix extension of the Cauchy-Schwarz inequality.pdf;/Users/nseewald/Zotero/storage/PF9CKCL2/S0165176599000142.html}
}

@book{tsiatisDynamicTreatmentRegimes2019,
  title = {Dynamic {{Treatment Regimes}}: {{Statistical Methods}} for {{Precision Medicine}}},
  shorttitle = {Dynamic {{Treatment Regimes}}},
  author = {Tsiatis, Anastasios A. and Davidian, Marie and Holloway, Shannon T. and Laber, Eric B.},
  year = {2019},
  series = {Monographs on {{Statistics}} and {{Applied Probability}}},
  number = {164},
  publisher = {CRC Press LLC},
  address = {Milton, United Kingdom},
  urldate = {2020-04-29},
  isbn = {978-1-4987-6978-5},
  file = {/Users/nseewald/Zotero/storage/ZH29ECIW/detail.html}
}

@article{tsiatisSemiparametricEstimatorProportional2001,
  title = {A Semiparametric Estimator for the Proportional Hazards Model with Longitudinal Covariates Measured with Error},
  author = {Tsiatis, Anastasios A. and Davidian, Marie},
  year = {2001},
  month = jun,
  journal = {Biometrika},
  volume = {88},
  number = {2},
  pages = {447--458},
  issn = {0006-3444},
  doi = {10.1093/biomet/88.2.447},
  urldate = {2024-01-11},
  abstract = {A common objective in longitudinal studies is to characterise the relationship between a failure time process and time-independent and time-dependent covariates. Time-dependent covariates are generally available as longitudinal data collected periodically during the course of the study. We assume that these data follow a linear mixed effects model with normal measurement error and that the hazard of failure depends both on the underlying random effects describing the covariate process and other time-independent covariates through a proportional hazards relationship. A routine assumption is that the random effects are normally distributed; however, this need not hold in practice. Within this framework, we develop a simple method for estimating the proportional hazards model parameters that requires no assumption on the distribution of the random effects. Large-sample properties are discussed, and finite-sample performance is assessed and compared to competing methods via simulation.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tsiatis_Davidian_2001_A_semiparametric_estimator_for_the_proportional_hazards_model_with_longitudinal.pdf;/Users/nseewald/Zotero/storage/5BZKQW8W/264942.html}
}

@book{tsiatisSemiparametricTheoryMissing2006,
  title = {Semiparametric Theory and Missing Data},
  author = {Tsiatis, Anastasios A.},
  year = {2006},
  series = {Springer Series in Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-32448-7},
  langid = {english},
  lccn = {QA276.8 .T75 2006},
  keywords = {Missing observations (Statistics),Parameter estimation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tsiatis_2006_Semiparametric theory and missing data.pdf}
}

@article{tukeyFutureDataAnalysis1962,
  title = {The {{Future}} of {{Data Analysis}}},
  author = {Tukey, John W.},
  year = {1962},
  journal = {The Annals of Mathematical Statistics},
  volume = {33},
  number = {1},
  eprint = {2237638},
  eprinttype = {jstor},
  pages = {1--67},
  issn = {0003-4851},
  doi = {10/d48nqg},
  urldate = {2018-12-05},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tukey_1962_The Future of Data Analysis.pdf}
}

@article{tukeyPhilosophyMultipleComparisons1991,
  title = {The {{Philosophy}} of {{Multiple Comparisons}}},
  author = {Tukey, John W.},
  year = {1991},
  journal = {Statistical Science},
  volume = {6},
  number = {1},
  pages = {100--116},
  doi = {10.1214/ss/1177011945},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tukey_1991_The Philosophy of Multiple Comparisons.pdf}
}

@article{tuPowerAnalysesLongitudinal2004,
  title = {Power Analyses for Longitudinal Trials and Other Clustered Designs},
  author = {Tu, X. M. and Kowalski, J. and Zhang, J. and Lynch, K. G. and {Crits-Christoph}, P.},
  year = {2004},
  month = sep,
  journal = {Statistics in Medicine},
  volume = {23},
  number = {18},
  pages = {2799--2815},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.1869},
  urldate = {2018-10-12},
  abstract = {Existing methods for power and sample size estimation for longitudinal and other clustered study designs have limited applications. In this paper, we review and extend existing approaches to improve these limitations. In particular, we focus on power analysis for the two most popular approaches for clustered data analysis, the generalized estimating equations and the linear mixed-e ects models. By basing the derivation of the power function on the asymptotic distribution of the model estimates, the proposed approach provides estimates of power that are consistent with the methods of inference for data analysis. The proposed methodology is illustrated with numerous examples that are motivated by real study designs. Copyright ? 2004 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Tu et al_2004_Power analyses for longitudinal trials and other clustered designs.pdf}
}

@article{turchettaBayesianSampleSize2021,
  title = {Bayesian {{Sample Size Calculations}} for {{SMART Studies}}},
  author = {Turchetta, Armando and Moodie, Erica E. M. and Stephens, David A. and Lambert, Sylvie D.},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.01041 [stat]},
  eprint = {2108.01041},
  primaryclass = {stat},
  urldate = {2021-08-03},
  abstract = {In the management of most chronic conditions characterized by the lack of universally effective treatments, adaptive treatment strategies (ATSs) have been growing in popularity as they offer a more individualized approach, and sequential multiple assignment randomized trials (SMARTs) have gained attention as the most suitable clinical trial design to formalize the study of these strategies. While the number of SMARTs has increased in recent years, their design has remained limited to the frequentist setting, which may not fully or appropriately account for uncertainty in design parameters and hence not yield appropriate sample size recommendations. Specifically, standard frequentist formulae rely on several assumptions that can be easily misspecified. The Bayesian framework offers a straightforward path to alleviate some of these concerns. In this paper, we provide calculations in a Bayesian setting to allow more realistic and robust estimates that account for uncertainty in inputs through the `two priors' approach. Additionally, compared to the standard formulae, this methodology allows us to rely on fewer assumptions, integrate pre-trial knowledge, and switch the focus from the standardized effect size to the minimal detectable difference. The proposed methodology is evaluated in a thorough simulation study and is implemented to estimate the sample size for a full-scale SMART of an Internet-Based Adaptive Stress Management intervention based on a pilot SMART conducted on cardiovascular disease patients from two Canadian provinces.},
  archiveprefix = {arXiv},
  keywords = {No DOI found,Statistics - Methodology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Turchetta et al_2021_Bayesian Sample Size Calculations for SMART Studies.pdf;/Users/nseewald/Zotero/storage/DLYGH4SI/2108.html}
}

@article{twiskAnalysisCovarianceMost2005,
  title = {Is Analysis of Covariance the Most Appropriate Way to Analyse Changes in Randomized Controlled Trials?},
  author = {Twisk, Jos and Proper, Karin},
  year = {2005},
  month = feb,
  journal = {Journal of Clinical Epidemiology},
  volume = {58},
  number = {2},
  pages = {211--212},
  issn = {0895-4356},
  doi = {10/brs8cn},
  urldate = {2019-12-10},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Twisk_Proper_2005_Is analysis of covariance the most appropriate way to analyse changes in.pdf;/Users/nseewald/Zotero/storage/6BKH6M3Z/S0895435604002379.html}
}

@article{twiskDifferentWaysEstimate2018,
  title = {Different Ways to Estimate Treatment Effects in Randomised Controlled Trials},
  author = {Twisk, J and Bosman, L and Hoekstra, T and Rijnhart, J and Welten, M and Heymans, M},
  year = {2018},
  month = jun,
  journal = {Contemporary Clinical Trials Communications},
  volume = {10},
  pages = {80--85},
  issn = {2451-8654},
  doi = {10/gdfspr},
  urldate = {2019-11-22},
  abstract = {Background Regarding the analysis of RCT data there is a debate going on whether an adjustment for the baseline value of the outcome variable should be made. When an adjustment is made, there is a lot of misunderstanding regarding the way this should be done. Therefore, the aims of this educational paper are: 1) to explain different methods used to estimate treatment effects in RCTs, 2) to illustrate the different methods with a real life example and 3) to give an advise on how to analyse RCT data. Methods Longitudinal analysis of covariance, repeated measures analysis in which also the baseline value is used as outcome and the analysis of changes were theoretically explained and applied to an example dataset investigating a systolic blood pressure lowering treatment. Results It was shown that differences at baseline should be taken into account and that regular repeated measures analysis and regular analysis of changes did not adjust for the baseline differences between the groups and therefore lead to biased estimates of the treatment effect. In the real life example, due to the differences at baseline between the treatment and control group, the different methods lead to different estimates of the treatment effect. Conclusion Regarding the analysis of RCT data, it is advised to use longitudinal analysis of covariance or a repeated measures analysis without the treatment variable, but with the interaction between treatment and time in the model.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Twisk et al_2018_Different ways to estimate treatment effects in randomised controlled trials.pdf;/Users/nseewald/Zotero/storage/F4FTWTDH/J et al. - 2018 - Different ways to estimate treatment effects in ra.html}
}

@misc{USDEAWill,
  title = {{{US DEA}} Will Reclassify Marijuana, Ease Restrictions, {{AP}} Sources Say {\textbar} {{AP News}}},
  urldate = {2024-05-01},
  howpublished = {https://apnews.com/article/marijuana-biden-dea-criminal-justice-pot-f833a8dae6ceb31a8658a5d65832a3b8},
  file = {/Users/nseewald/Zotero/storage/SYVJMG86/marijuana-biden-dea-criminal-justice-pot-f833a8dae6ceb31a8658a5d65832a3b8.html}
}

@misc{UseOnesidedTests,
  title = {The Use of One-Sided Tests in Drug Trials: An Fda Advisory Committee Member's Perspective},
  shorttitle = {The Use of One-Sided Tests in Drug Trials},
  doi = {10.1080/10543409108835012},
  urldate = {2024-06-17},
  howpublished = {https://www-tandfonline-com.proxy.library.upenn.edu/doi/epdf/10.1080/10543409108835012?needAccess=true},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/M2C5R5F9/10543409108835012.html}
}

@article{vanbreukelenANCOVACHANGEBaseline2013,
  title = {{{ANCOVA Versus CHANGE From Baseline}} in {{Nonrandomized Studies}}: {{The Difference}}},
  shorttitle = {{{ANCOVA Versus CHANGE From Baseline}} in {{Nonrandomized Studies}}},
  author = {{van Breukelen}, Gerard J. P.},
  year = {2013},
  month = nov,
  journal = {Multivariate Behavioral Research},
  volume = {48},
  number = {6},
  pages = {895--922},
  issn = {0027-3171, 1532-7906},
  doi = {10/ggdb7n},
  urldate = {2019-11-21},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/van Breukelen_2013_ANCOVA Versus CHANGE From Baseline in Nonrandomized Studies.pdf}
}

@article{vanbuurenMultipleImputationMissing1999,
  title = {Multiple Imputation of Missing Blood Pressure Covariates in Survival Analysis},
  author = {{van Buuren}, S. and Boshuizen, H. C. and Knook, D. L.},
  year = {1999},
  journal = {Statistics in Medicine},
  volume = {18},
  number = {6},
  pages = {681--694},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19990330)18:6<681::AID-SIM71>3.0.CO;2-R},
  urldate = {2024-01-09},
  abstract = {This paper studies a non-response problem in survival analysis where the occurrence of missing data in the risk factor is related to mortality. In a study to determine the influence of blood pressure on survival in the very old (85+ years), blood pressure measurements are missing in about 12{$\cdot$}5 per cent of the sample. The available data suggest that the process that created the missing data depends jointly on survival and the unknown blood pressure, thereby distorting the relation of interest. Multiple imputation is used to impute missing blood pressure and then analyse the data under a variety of non-response models. One special modelling problem is treated in detail; the construction of a predictive model for drawing imputations if the number of variables is large. Risk estimates for these data appear robust to even large departures from the simplest non-response model, and are similar to those derived under deletion of the incomplete records. Copyright {\copyright} 1999 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 1999 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/van Buuren et al_1999_Multiple imputation of missing blood pressure covariates in survival analysis.pdf;/Users/nseewald/Zotero/storage/K5VQ5GFQ/(SICI)1097-0258(19990330)186681AID-SIM713.0.html}
}

@book{vandervaartAsymptoticStatistics1998,
  title = {Asymptotic Statistics},
  author = {{van der Vaart}, A. W.},
  year = {1998},
  series = {Cambridge Series in Statistical and Probabilistic Mathematics},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK ; New York, NY, USA},
  isbn = {978-0-521-49603-2},
  lccn = {QA276 .V22 1998},
  keywords = {nosource},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/van der Vaart_1998_Asymptotic statistics.pdf}
}

@article{vanderweeleCausalInferenceLongitudinal2016,
  title = {Causal Inference and Longitudinal Data: A Case Study of Religion and Mental Health},
  shorttitle = {Causal Inference and Longitudinal Data},
  author = {VanderWeele, Tyler J. and Jackson, John W. and Li, Shanshan},
  year = {2016},
  month = nov,
  journal = {Social Psychiatry and Psychiatric Epidemiology},
  volume = {51},
  number = {11},
  pages = {1457--1466},
  issn = {1433-9285},
  doi = {10/f9dps9},
  urldate = {2021-08-26},
  abstract = {We provide an introduction to causal inference with longitudinal data and discuss the complexities of analysis and interpretation when exposures can vary over time.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/VanderWeele et al_2016_Causal inference and longitudinal data.pdf}
}

@article{vanderweeleConceptualIssuesConcerning2009,
  title = {Conceptual Issues Concerning Mediation, Interventions and Composition},
  author = {Vanderweele, Tyler J. and Vansteelandt, Stijn},
  year = {2009},
  journal = {Statistics and Its Interface},
  volume = {2},
  number = {4},
  pages = {457--468},
  issn = {19387989, 19387997},
  doi = {10.4310/SII.2009.v2.n4.a7},
  urldate = {2023-05-02},
  abstract = {Concepts concerning mediation in the causal inference literature are reviewed. Notions of direct and indirect effects from a counterfactual approach to mediation are compared with those arising from the standard regression approach to mediation of Baron and Kenny (1986), commonly utilized in the social science literature. It is shown that concepts of direct and indirect effect from causal inference generalize those described by Baron and Kenny and that under appropriate identification assumptions these more general direct and indirect effects from causal inference can be estimated using regression even when there are interactions between the primary exposure of interest and the mediator. A number of conceptual issues are discussed concerning the interpretation of identification conditions for mediation, the notion of counterfactuals based on hypothetical interventions and the so called consistency and composition assumptions.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vanderweele_Vansteelandt_2009_Conceptual issues concerning mediation, interventions and composition.pdf}
}

@article{vanderweeleDefinitionConfounder2013,
  title = {On the {{Definition}} of a {{Confounder}}},
  author = {VanderWeele, Tyler J. and Shpitser, Ilya},
  year = {2013},
  journal = {The Annals of Statistics},
  volume = {41},
  number = {1},
  eprint = {41806604},
  eprinttype = {jstor},
  pages = {196--220},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  urldate = {2021-12-15},
  abstract = {The causal inference literature has provided a clear formal definition of confounding expressed in terms of counterfactual independence. The literature has not, however, come to any consensus on a formal definition of a confounder, as it has given priority to the concept of confounding over that of a confounder. We consider a number of candidate definitions arising from various more informal statements made in the literature. We consider the properties satisfied by each candidate definition, principally focusing on (i) whether under the candidate definition control for all "confounders" suffices to control for "confounding" and (ii) whether each confounder in some context helps eliminate or reduce confounding bias. Several of the candidate definitions do not have these two properties. Only one candidate definition of those considered satisfies both properties. We propose that a "confounder" be defined as a pre-exposure covariate C for which there exists a set of other covariates X such that effect of the exposure on the outcome is unconfounded conditional on (X, C) but such that for no proper subset of (X, C) is the effect of the exposure on the outcome unconfounded given the subset. We also provide a conditional analogue of the above definition; and we propose a variable that helps reduce bias but not eliminate bias be referred to as a "surrogate confounder." These definitions are closely related to those given by Robins and Morgenstern [Comput. Math. Appl. 14 (1987) 869-916]. The implications that hold among the various candidate definitions are discussed.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/VanderWeele_Shpitser_2013_On the Definition of a Confounder.pdf}
}

@article{vanderweeleMarginalStructuralModel2011,
  title = {A {{Marginal Structural Model Analysis}} for {{Loneliness}}: {{Implications}} for {{Intervention Trials}} and {{Clinical Practice}}},
  shorttitle = {A {{Marginal Structural Model Analysis}} for {{Loneliness}}},
  author = {VanderWeele, Tyler J. and Hawkley, Louise C. and Thisted, Ronald A. and Cacioppo, John T.},
  year = {2011},
  month = apr,
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {79},
  number = {2},
  pages = {225--235},
  publisher = {{Journal of Consulting and Clinical Psychology}},
  issn = {0022-006X},
  doi = {10/c9m52q},
  urldate = {2021-08-17},
  abstract = {Objective: Clinical scientists, policymakers, and individuals must make decisions concerning effective interventions that address health-related issues. We use longitudinal data on loneliness and depressive symptoms and a new class of causal models to illustrate how empirical evidence can be used to inform intervention trial design and clinical practice. Method: Data were obtained from a population-based study of non-Hispanic Caucasians, African Americans, and Latino Americans (N = 229) born between 1935 and 1952. Loneliness and depressive symptoms were measured with the UCLA Loneliness Scale-Revised and Center for Epidemiologic Studies Depression Scale, respectively. Marginal structural causal models were employed to evaluate the extent to which depressive symptoms depend not only on loneliness measured at a single point in time (as in prior studies of the effect of loneliness) but also on an individual's entire loneliness history. Results: Our results indicate that if interventions to reduce loneliness by 1 standard deviation were made 1 and 2 years prior to assessing depressive symptoms, both would have an effect; together, they would result in an average reduction in depressive symptoms of 0.33 standard deviations, 95\% CI [0.21, 0.44], p less than 0.0001. Conclusions: The magnitude and persistence of these effects suggest that greater effort should be devoted to developing practical interventions on alleviating loneliness and that doing so could be useful in the treatment and prevention of depressive symptoms. In light of the persistence of the effects of loneliness, our results also suggest that, in the evaluation of interventions on loneliness, it may be important to allow for a considerable follow-up period in assessing outcomes. (Contains 2 tables, 1 figure, and 1 footnote.)},
  keywords = {African Americans,Causal Models,Center for Epidemiologic Studies Depression Scale,Depression (Psychology),Hispanic Americans,Intervention,Longitudinal Studies,Measures (Individuals),Outcomes of Treatment,Persistence,Prevention,Psychological Patterns,Symptoms (Individual Disorders),UCLA Loneliness Scale,Whites},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/VanderWeele et al_2011_A Marginal Structural Model Analysis for Loneliness.pdf}
}

@article{vandiepenPredictionAetiologyCommon2017,
  title = {Prediction versus Aetiology: Common Pitfalls and How to Avoid Them},
  shorttitle = {Prediction versus Aetiology},
  author = {{van Diepen}, Merel and Ramspek, Chava L. and Jager, Kitty J. and Zoccali, Carmine and Dekker, Friedo W.},
  year = {2017},
  month = apr,
  journal = {Nephrology Dialysis Transplantation},
  volume = {32},
  number = {suppl\_2},
  pages = {ii1-ii5},
  issn = {0931-0509},
  doi = {10.1093/ndt/gfw459},
  urldate = {2022-09-07},
  abstract = {Prediction research is a distinct field of epidemiologic research, which should be clearly separated from aetiological research. Both prediction and aetiology make use of multivariable modelling, but the underlying research aim and interpretation of results are very different. Aetiology aims at uncovering the causal effect of a specific risk factor on an outcome, adjusting for confounding factors that are selected based on pre-existing knowledge of causal relations. In contrast, prediction aims at accurately predicting the risk of an outcome using multiple predictors collectively, where the final prediction model is usually based on statistically significant, but not necessarily causal, associations in the data at hand.In both scientific and clinical practice, however, the two are often confused, resulting in poor-quality publications with limited interpretability and applicability. A major problem is the frequently encountered aetiological interpretation of prediction results, where individual variables in a prediction model are attributed causal meaning. This article stresses the differences in use and interpretation of aetiological and prediction studies, and gives examples of common pitfalls.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/van Diepen et al_2017_Prediction versus aetiology.pdf;/Users/nseewald/Zotero/storage/PDVZ9YQC/3056968.html}
}

@article{vanhouwelingenDynamicPredictingLandmarking2008,
  title = {Dynamic Predicting by Landmarking as an Alternative for Multi-State Modeling: An Application to Acute Lymphoid Leukemia Data},
  shorttitle = {Dynamic Predicting by Landmarking as an Alternative for Multi-State Modeling},
  author = {{van Houwelingen}, Hans C. and Putter, Hein},
  year = {2008},
  month = dec,
  journal = {Lifetime Data Analysis},
  volume = {14},
  number = {4},
  pages = {447--463},
  issn = {1572-9249},
  doi = {10.1007/s10985-008-9099-8},
  urldate = {2024-02-05},
  abstract = {This paper considers the problem of obtaining a dynamic prediction for 5-year failure free survival after bone marrow transplantation in ALL patients using data from the EBMT, the European Group for Blood and Marrow Transplantation. The paper compares the new landmark methodology as developed by the first author and the established multi-state modeling as described in a recent Tutorial in Biostatistics in Statistics in Medicine by the second author and colleagues. As expected the two approaches give similar results. The landmark methodology does not need complex modeling and leads to easy prediction rules. On the other hand, it does not give the insight in the biological processes as obtained for the multi-state model.},
  langid = {english},
  keywords = {ALL,Dynamic prediction,Landmark model,Multi-state model},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/van Houwelingen_Putter_2008_Dynamic predicting by landmarking as an alternative for multi-state modeling.pdf}
}

@article{vanhouwelingenDynamicPredictionLandmarking2007,
  title = {Dynamic {{Prediction}} by {{Landmarking}} in {{Event History Analysis}}},
  author = {Van Houwelingen, Hans C.},
  year = {2007},
  journal = {Scandinavian Journal of Statistics},
  volume = {34},
  number = {1},
  pages = {70--85},
  issn = {1467-9469},
  doi = {10.1111/j.1467-9469.2006.00529.x},
  urldate = {2024-02-07},
  abstract = {Abstract. This article advocates the landmarking approach that dynamically adjusts predictive models for survival data during the follow up. This updating is achieved by directly fitting models for the individuals still at risk at the landmark point. Using this approach, simple proportional hazards models are able to catch the development over time for models with time-varying effects of covariates or data with time-dependent covariates (biomarkers). To smooth the effect of the landmarking, sequences of models are considered with parametric effects of the landmark time point and fitted by maximizing appropriate pseudo log-likelihoods that extend the partial log-likelihood to cover the landmarking approach.},
  langid = {english},
  keywords = {landmark analysis,landmarking,pseudo-partial likelihood,survival analysis,time-dependent covariates,time-varying effects},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Van Houwelingen_2007_Dynamic Prediction by Landmarking in Event History Analysis.pdf;/Users/nseewald/Zotero/storage/HUV8F9WX/j.1467-9469.2006.00529.html}
}

@article{vansteelandtCausalInferenceGeneralized2003,
  ids = {vansteelandtCausalInferenceGeneralized2003a,vansteelandtCausalInferenceGeneralized2003c},
  title = {Causal Inference with Generalized Structural Mean Models},
  author = {Vansteelandt, S. and Goetghebeur, E.},
  year = {2003},
  month = nov,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {65},
  number = {4},
  pages = {817--835},
  issn = {1369-7412, 1467-9868},
  doi = {10/d3dmsh},
  urldate = {2020-05-10},
  abstract = {We estimate cause--effect relationships in empirical research where exposures are not completely controlled, as in observational studies or with patient non-compliance and selfselected treatment switches in randomized clinical trials. Additive and multiplicative structural mean models have proved useful for this but suffer from the classical limitations of linear and log-linear models when accommodating binary data. We propose the generalized structural mean model to overcome these limitations. This is a semiparametric two-stage model which extends the structural mean model to handle non-linear average exposure effects. The first-stage structural model describes the causal effect of received exposure by contrasting the means of observed and potential exposure-free outcomes in exposed subsets of the population. For identification of the structural parameters, a second stage `nuisance' model is introduced. This takes the form of a classical association model for expected outcomes given observed exposure. Under the model, we derive estimating equations which yield consistent, asymptotically normal and efficient estimators of the structural effects. We examine their robustness to model misspecification and construct robust estimators in the absence of any exposure effect. The double-logistic structural mean model is developed in more detail to estimate the effect of observed exposure on the success of treatment in a randomized controlled blood pressure reduction trial with self-selected non-compliance.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vansteelandt_Goetghebeur_2003_Causal inference with generalized structural mean models.pdf;/Users/nseewald/Zotero/storage/7QVSJLEG/j.1369-7412.2003.00417.html}
}

@article{vansteelandtConfoundingPredictionEfficiency2007,
  title = {On {{Confounding}}, {{Prediction}} and {{Efficiency}} in the {{Analysis}} of {{Longitudinal}} and {{Cross-sectional Clustered Data}}},
  author = {Vansteelandt, Stijn},
  year = {2007},
  month = sep,
  journal = {Scandinavian Journal of Statistics},
  volume = {34},
  number = {3},
  pages = {478--498},
  issn = {0303-6898, 1467-9469},
  doi = {10.1111/j.1467-9469.2006.00555.x},
  urldate = {2018-10-12},
  abstract = {In the analysis of clustered and/or longitudinal data, it is usually desirable to ignore covariate information for other cluster members as well as future covariate information when predicting outcome for a given subject at a given time. This can be accomplished through conditional mean models which merely condition on the considered subject's covariate history at each time. Pepe \& Anderson (Commun. Stat. Simul. Comput. 23, 1994, 939) have shown that ordinary generalized estimating equations may yield biased estimates for the parameters in such models, but that valid inferences can be guaranteed by using a diagonal working covariance matrix in the equations. In this paper, we provide insight into the nature of this problem by uncovering substantive data-generating mechanisms under which such biases will result. We then propose a class of asymptotically unbiased estimators for the parameters indexing the suggested conditional mean models. In addition, we provide a representation for the efficient estimator in our class, which attains the semi-parametric efficiency bound under the model, along with an efficient algorithm for calculating it. This algorithm is easy to apply and may realize major efficiency improvements as demonstrated through simulation studies. The results suggest ways to improve the efficiency of inverse-probability-of-treatment estimators which adjust for time-varying confounding, and are used to estimate the effect of discontinuing highly active anti-retroviral therapy (HAART) on viral load in HIV-infected patients.},
  langid = {english},
  keywords = {causal diagrams,causal inference,interference between subjects,inverse weighting,marginal structural models,semi-parametric efficiency,SUTVA},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vansteelandt_2007_On Confounding, Prediction and Efficiency in the Analysis of Longitudinal and.pdf;/Users/nseewald/Zotero/storage/MMPNQ7Q3/j.1467-9469.2006.00555.html}
}

@article{vansteelandtInterventionalEffectsMediation2017,
  title = {Interventional {{Effects}} for {{Mediation Analysis}} with {{Multiple Mediators}}},
  author = {Vansteelandt, Stijn and Daniel, Rhian M.},
  year = {2017},
  month = mar,
  journal = {Epidemiology},
  volume = {28},
  number = {2},
  pages = {258--265},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000000596},
  urldate = {2022-10-13},
  abstract = {The mediation formula for the identification of natural (in)direct effects has facilitated mediation analyses that better respect the nature of the data, with greater consideration of the need for confounding control. The default assumptions on which it relies are strong, however. In particular, they are known to be violated when confounders of the mediator--outcome association are affected by the exposure. This complicates extensions of counterfactual-based mediation analysis to settings that involve repeatedly measured mediators, or multiple correlated mediators. VanderWeele, Vansteelandt, and Robins introduced so-called interventional (in)direct effects. These can be identified under much weaker conditions than natural (in)direct effects, but have the drawback of not adding up to the total effect. In this article, we adapt their proposal to achieve an exact decomposition of the total effect, and extend it to the multiple mediator setting. Interestingly, the proposed effects capture the path-specific effects of an exposure on an outcome that are mediated by distinct mediators, even when---as often---the structural dependence between the multiple mediators is unknown, for instance, when the direction of the causal effects between the mediators is unknown, or there may be unmeasured common causes of the mediators.},
  langid = {american},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vansteelandt_Daniel_2017_Interventional Effects for Mediation Analysis with Multiple Mediators.pdf;/Users/nseewald/Zotero/storage/3VUDGD3A/Interventional_Effects_for_Mediation_Analysis_with.15.html}
}

@article{vansteelandtStructuralNestedModels2014,
  title = {Structural {{Nested Models}} and {{G-estimation}}: {{The Partially Realized Promise}}},
  shorttitle = {Structural {{Nested Models}} and {{G-estimation}}},
  author = {Vansteelandt, Stijn and Joffe, Marshall},
  year = {2014},
  month = nov,
  journal = {Statistical Science},
  volume = {29},
  number = {4},
  pages = {707--731},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10/f6wv3q},
  urldate = {2020-05-29},
  abstract = {Structural nested models (SNMs) and the associated method of G-estimation were first proposed by James Robins over two decades ago as approaches to modeling and estimating the joint effects of a sequence of treatments or exposures. The models and estimation methods have since been extended to dealing with a broader series of problems, and have considerable advantages over the other methods developed for estimating such joint effects. Despite these advantages, the application of these methods in applied research has been relatively infrequent; we view this as unfortunate. To remedy this, we provide an overview of the models and estimation methods as developed, primarily by Robins, over the years. We provide insight into their advantages over other methods, and consider some possible reasons for failure of the methods to be more broadly adopted, as well as possible remedies. Finally, we consider several extensions of the standard models and estimation methods.},
  langid = {english},
  mrnumber = {MR3300367},
  zmnumber = {1331.62208},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vansteelandt_Joffe_2014_Structural Nested Models and G-estimation.pdf;/Users/nseewald/Zotero/storage/6ME6IC5T/1421330555.html}
}

@article{velascoUseRaceEGFR2021,
  title = {The {{Use}} of {{Race}} in {{eGFR}}: {{Why Racial Justice Requires Accuracy}}},
  shorttitle = {The {{Use}} of {{Race}} in {{eGFR}}},
  author = {Velasco, Joel D. and Snodgrass, Brad},
  year = {2021},
  month = jul,
  journal = {The American Journal of Medicine},
  volume = {134},
  number = {7},
  pages = {827--828},
  publisher = {Elsevier},
  issn = {0002-9343, 1555-7162},
  doi = {10.1016/j.amjmed.2021.02.013},
  urldate = {2022-10-05},
  langid = {english},
  pmid = {33773970},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Velasco_Snodgrass_2021_The Use of Race in eGFR.pdf;/Users/nseewald/Zotero/storage/6YZ8KV9I/fulltext.html}
}

@article{vensGeneralizedEstimatingEquations2012,
  title = {Generalized Estimating Equations and Regression Diagnostics for Longitudinal Controlled Clinical Trials: {{A}} Case Study},
  shorttitle = {Generalized Estimating Equations and Regression Diagnostics for Longitudinal Controlled Clinical Trials},
  author = {Vens, Maren and Ziegler, Andreas},
  year = {2012},
  month = may,
  journal = {Computational Statistics \& Data Analysis},
  series = {Second {{Issue}} for {{COMPUTATIONAL STATISTICS FOR CLINICAL RESEARCH}}},
  volume = {56},
  number = {5},
  pages = {1232--1242},
  issn = {0167-9473},
  doi = {10/fjvxqs},
  urldate = {2019-12-21},
  abstract = {Generalized estimating equations (GEE) were proposed for the analysis of correlated data. They are popular because regression parameters can be consistently estimated even if only the mean structure is correctly specified. GEE have been extended in several ways, including regression diagnostics for outlier detection. However, GEE have rarely been used for analyzing controlled clinical trials. The SB-LOT trial, a double-blind placebo-controlled randomized multicenter trial in which the oedema-protective effect of a vasoactive drug was investigated in patients suffering from chronic insufficiency was re-analyzed using the GEE approach. It is demonstrated that the autoregressive working correlation structure is the most plausible working correlation structure in this study. The effect of the vasoactive drug is a difference in lower leg volume of 2.64 ml per week (p=0.0288, 95\% confidence interval 0.27--4.99 ml per week), making a difference of 30 ml at the end of the study. Deletion diagnostics are used for identification of outliers and influential probands. After exclusion of the most influential patients from the analysis, the overall conclusion of the study is not altered. At the same time, the goodness of fit as assessed by half-normal plots increases substantially. In summary, the use of GEE in a longitudinal clinical trial is an alternative to the standard analysis which usually involves only the last follow-up. Both the GEE and the regression diagnostic techniques should accompany the GEE analysis to serve as sensitivity analysis.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vens_Ziegler_2012_Generalized estimating equations and regression diagnostics for longitudinal.pdf;/Users/nseewald/Zotero/storage/LRHKLLZB/S0167947311001393.html}
}

@article{verbekeAnalysisMultivariateLongitudinal2014,
  title = {The Analysis of Multivariate Longitudinal Data: {{A}} Review},
  shorttitle = {The Analysis of Multivariate Longitudinal Data},
  author = {Verbeke, Geert and Fieuws, Steffen and Molenberghs, Geert and Davidian, Marie},
  year = {2014},
  month = feb,
  journal = {Statistical Methods in Medical Research},
  volume = {23},
  number = {1},
  pages = {42--59},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10/f5rn7h},
  urldate = {2021-09-07},
  abstract = {Longitudinal experiments often involve multiple outcomes measured repeatedly within a set of study participants. While many questions can be answered by modeling the various outcomes separately, some questions can only be answered in a joint analysis of all of them. In this article, we will present a review of the many approaches proposed in the statistical literature. Four main model families will be presented, discussed and compared. Focus will be on presenting advantages and disadvantages of the different models rather than on the mathematical or computational details.},
  langid = {english},
  keywords = {conditional models,latent variables,marginal models,Mixed models,random effects,shared parameters},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Verbeke et al_2014_The analysis of multivariate longitudinal data.pdf}
}

@article{verbitsky-savitzCausalInferenceInterference2012,
  title = {Causal {{Inference Under Interference}} in {{Spatial Settings}}: {{A Case Study Evaluating Community Policing Program}} in {{Chicago}}},
  shorttitle = {Causal {{Inference Under Interference}} in {{Spatial Settings}}},
  author = {{Verbitsky-Savitz}, Natalya and Raudenbush, Stephen W.},
  year = {2012},
  month = aug,
  journal = {Epidemiologic Methods},
  volume = {1},
  number = {1},
  pages = {107--130},
  publisher = {De Gruyter},
  issn = {2161-962X},
  doi = {10.1515/2161-962X.1020},
  urldate = {2024-05-08},
  abstract = {For decades, social scientists have been trying to answer causal questions about the effectiveness of certain programs or policies. The conventional methodology for answering such causal questions relies on the ``no interference between different units'' assumption; that is, a unit's outcome depends solely on the treatment that the unit is assigned to (or exposed to) and does not depend on the treatment assignment (or exposure) of other units in the population. However, this assumption is likely to be violated in spatial settings because of various spillover, diffusion, and displacement effects. In this paper, we present a case study evaluating the causal effects of Chicago's community policing program (a community-wide intervention) on neighborhoods' crime rates. We use semiannual crime data from Chicago to evaluate (1) whether community policing is more effective at decreasing rates of reported personal crime when implemented everywhere versus nowhere in a city; (2) whether community policing is effective if implemented in a single local area, holding constant policing in adjacent areas; (3) whether implementing community policing in surrounding areas affects crime in a focal area; and (4) whether community policing's impact on crime in surrounding areas depends on whether community policing is also implemented in the focal area. To answer these questions, we relax the no-interference assumption. Our approach allows the potential outcomes in any local area to depend on a function of the treatment assignments in all other units. We define causal effects and evaluate assumptions that make the framework tractable within the framework of a generalized linear model with spatially auto-correlated random effects.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {causal inference,community policing,hierarchical models (HLM),neighborhood effects,no-interference assumption,spatial statistics},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Verbitsky-Savitz_Raudenbush_2012_Causal Inference Under Interference in Spatial Settings.pdf}
}

@article{vickersHowManyRepeated2003,
  ids = {vickersHowManyRepeated2003a},
  title = {How Many Repeated Measures in Repeated Measures Designs? {{Statistical}} Issues for Comparative Trials},
  shorttitle = {How Many Repeated Measures in Repeated Measures Designs?},
  author = {Vickers, Andrew J.},
  year = {2003},
  month = oct,
  journal = {BMC Medical Research Methodology},
  volume = {3},
  number = {1},
  pages = {22},
  issn = {1471-2288},
  doi = {10/fhscjs},
  urldate = {2019-11-14},
  abstract = {In many randomized and non-randomized comparative trials, researchers measure a continuous endpoint repeatedly in order to decrease intra-patient variability and thus increase statistical power. There has been little guidance in the literature as to selecting the optimal number of repeated measures.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vickers_2003_How many repeated measures in repeated measures designs.pdf;/Users/nseewald/Zotero/storage/FN38X7R9/1471-2288-3-22.html}
}

@article{vickersNotTreatBill,
  title = {Do Not Treat {{Bill Gates}} for Prostate Cancer! {{Algorithmic}} Bias and Causality in Medical Prediction},
  author = {Vickers, Andrew},
  journal = {BJU International},
  volume = {n/a},
  number = {n/a},
  issn = {1464-410X},
  doi = {10.1111/bju.15951},
  urldate = {2023-02-02},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vickers_Do not treat Bill Gates for prostate cancer.pdf;/Users/nseewald/Zotero/storage/SWYMGWKD/bju.html}
}

@article{vigoritoIntrinsicallyMotivatedHierarchical2010,
  title = {Intrinsically {{Motivated Hierarchical Skill Learning}} in {{Structured Environments}}},
  author = {Vigorito, Christopher M and Barto, Andrew G},
  year = {2010},
  month = jun,
  journal = {IEEE Transactions on Autonomous Mental Development},
  volume = {2},
  number = {2},
  pages = {132--143},
  issn = {1943-0604, 1943-0612},
  doi = {10.1109/TAMD.2010.2050205},
  urldate = {2018-10-12},
  abstract = {We present a framework for intrinsically motivated developmental learning of abstract skill hierarchies by reinforcement learning agents in structured environments. Long-term learning of skill hierarchies can drastically improve an agent's efficiency in solving ensembles of related tasks in a complex domain. In structured domains composed of many features, understanding the causal relationships between actions and their effects on different features of the environment can greatly facilitate skill learning. Using Bayesian network structure (learning techniques and structured dynamic programming algorithms), we show that reinforcement learning agents can learn incrementally and autonomously both the causal structure of their environment and a hierarchy of skills that exploit this structure. Furthermore, we present a novel active learning scheme that employs intrinsic motivation to maximize the efficiency with which this structure is learned. As new structure is acquired using an agent's current set of skills, more complex skills are learned, which in turn allow the agent to discover more structure, and so on. This bootstrapping property makes our approach a developmental learning process that results in steadily increasing domain knowledge and behavioral complexity as an agent continues to explore its environment. Index Terms---Active learning, intrinsic motivation, options, planning, reinforcement learning, structure learning.},
  langid = {english},
  keywords = {Active learning,intrinsic motivation,options,planning,Reinforcement learning,structure learning},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vigorito_Barto_2010_Intrinsically Motivated Hierarchical Skill Learning in Structured Environments.pdf}
}

@incollection{vockSequentialMultipleAssignment2018,
  title = {Sequential {{Multiple Assignment Randomized Trial}} ({{SMART}})},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Vock, David M. and Almirall, Daniel},
  editor = {Balakrishnan, N and Colton, T and Everitt, W and Piegorsch, F and Teugels, J L},
  year = {2018},
  doi = {10.1002/9781118445112.stat08073},
  urldate = {2020-12-13},
  abstract = {A sequential multiple assignment randomized trial (SMART) is an experimental design that scientists can use to develop high-quality dynamic treatment regimens (DTRs), a prespecified treatment plan in which the type(s) or dosage/intensity of an intervention, the delivery of the intervention, or the monitoring schedule is repeatedly adjusted in response to new information collected about the individual. Specifically, a SMART design is a multistage trial design in which each stage corresponds to a critical decision where there is a scientific question about how best to intervene at that stage. Each participant progresses through the stages and can be randomly assigned to one of several intervention options at each stage. SMART designs allow one to answer a variety of questions concerning the development of a DTR. We highlight some of the common research questions and hypotheses and briefly discuss the appropriate analytical approaches to test these hypotheses. We conclude by contrasting a SMART design with other experimental designs that could be used for developing or for evaluating different DTRs.},
  copyright = {Copyright {\copyright} 2018 John Wiley \& Sons, Ltd. All rights reserved.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vock_Almirall_2018_Sequential Multiple Assignment Randomized Trial (SMART).pdf;/Users/nseewald/Zotero/storage/EC92DFYD/9781118445112.html}
}

@article{voelkerStatesMoveSubstitute2018,
  title = {States {{Move}} to {{Substitute Opioids With Medical Marijuana}} to {{Quell Epidemic}}},
  author = {Voelker, Rebecca},
  year = {2018},
  month = dec,
  journal = {JAMA},
  volume = {320},
  number = {23},
  pages = {2408--2410},
  issn = {0098-7484},
  doi = {10.1001/jama.2018.17329},
  urldate = {2023-05-21},
  abstract = {As state governments grapple with ways to curb the opioid epidemic in their own backyard, New York and Illinois took a relatively new approach last summer by modifying existing medical marijuana laws to allow certain patients to substitute their opioids with medicinal cannabis.In each state, patients with an opioid prescription or a condition for which an opioid is indicated can instead buy cannabis at a registered dispensary with a physician's written certification.},
  file = {/Users/nseewald/Zotero/storage/E2X228YD/2717192.html}
}

@article{vuIntroductoryStatisticsLife,
  title = {Introductory {{Statistics}} for the {{Life}} and {{Biomedical Sciences}}},
  author = {Vu, Julie and Harrington, David},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vu_Harrington_Introductory_Statistics_for_the_Life_and_Biomedical_Sciences.pdf}
}

@article{vyasHiddenPlainSight2020,
  title = {Hidden in {{Plain Sight}} --- {{Reconsidering}} the {{Use}} of {{Race Correction}} in {{Clinical Algorithms}}},
  author = {Vyas, Darshali A. and Eisenstein, Leo G. and Jones, David S.},
  year = {2020},
  month = aug,
  journal = {New England Journal of Medicine},
  volume = {383},
  number = {9},
  pages = {874--882},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMms2004740},
  urldate = {2022-10-05},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Vyas et al_2020_Hidden in Plain Sight — Reconsidering the Use of Race Correction in Clinical.pdf}
}

@article{wagerEstimationInferenceHeterogeneous2018,
  title = {Estimation and {{Inference}} of {{Heterogeneous Treatment Effects}} Using {{Random Forests}}},
  author = {Wager, Stefan and Athey, Susan},
  year = {2018},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {523},
  pages = {1228--1242},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10/gfdc56},
  urldate = {2021-07-01},
  abstract = {Many scientific and engineering challenges---ranging from personalized medicine to customized marketing recommendations---require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
  keywords = {Adaptive nearest neighbors matching,Asymptotic normality,Potential outcomes,Unconfoundedness},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wager_Athey_2018_Estimation and Inference of Heterogeneous Treatment Effects using Random Forests.pdf;/Users/nseewald/Zotero/storage/F7XEJ7HH/01621459.2017.html}
}

@article{wahedOptimalEstimatorSurvival2004,
  title = {Optimal {{Estimator}} for the {{Survival Distribution}} and {{Related Quantities}} for {{Treatment Policies}} in {{Two-Stage Randomization Designs}} in {{Clinical Trials}}},
  author = {Wahed, Abdus S. and Tsiatis, Anastasios A.},
  year = {2004},
  journal = {Biometrics},
  volume = {60},
  number = {1},
  pages = {124--133},
  issn = {1541-0420},
  doi = {10/dc4kfb},
  urldate = {2020-12-13},
  abstract = {Summary. Two-stage designs, where patients are initially randomized to an induction therapy and then depending upon their response and consent, are randomized to a maintenance therapy, are common in cancer and other clinical trials. The goal is to compare different combinations of primary and maintenance therapies to find the combination that is most beneficial. In practice, the analysis is usually conducted in two separate stages which does not directly address the major objective of finding the best combination. Recently Lunceford, Davidian, and Tsiatis (2002, Biometrics58, 48--57) introduced ad hoc estimators for the survival distribution and mean restricted survival time under different treatment policies. These estimators are consistent but not efficient, and do not include information from auxiliary covariates. In this article we derive estimators that are easy to compute and are more efficient than previous estimators. We also show how to improve efficiency further by taking into account additional information from auxiliary variables. Large sample properties of these estimators are derived and comparisons with other estimators are made using simulation. We apply our estimators to a leukemia clinical trial data set that motivated this study.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wahed_Tsiatis_2004_Optimal Estimator for the Survival Distribution and Related Quantities for.pdf;/Users/nseewald/Zotero/storage/8EE98V7M/j.0006-341X.2004.00160.html}
}

@article{wahedSemiparametricEfficientEstimation2006,
  title = {Semiparametric Efficient Estimation of Survival Distributions in Two-Stage Randomisation Designs in Clinical Trials with Censored Data},
  author = {Wahed, Abdus S. and Tsiatis, Anastasios A.},
  year = {2006},
  month = mar,
  journal = {Biometrika},
  volume = {93},
  number = {1},
  pages = {163--177},
  issn = {0006-3444},
  doi = {10/cgchp6},
  urldate = {2020-12-28},
  abstract = {Two-stage randomisation designs are useful in the evaluation of combination therapies where patients are initially randomised to an induction therapy and then, depending upon their response and consent, are randomised to a maintenance therapy. In this paper we derive the best regular asymptotically linear estimator for the survival distribution and related quantities of treatment regimes. We propose an estimator which is easily computable and is more efficient than existing estimators. Large-sample properties of the proposed estimator are derived and comparisons with other estimators are made using simulation.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wahed_Tsiatis_2006_Semiparametric efficient estimation of survival distributions in two-stage.pdf;/Users/nseewald/Zotero/storage/VXPLAYM2/235616.html}
}

@article{wainerHowDisplayData1984,
  title = {How to {{Display Data Badly}}},
  author = {Wainer, Howard},
  year = {1984},
  month = may,
  journal = {The American Statistician},
  volume = {38},
  number = {2},
  pages = {137--147},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10/gdthq6},
  urldate = {2020-11-25},
  abstract = {Methods for displaying data badly have been developing for many years, and a wide variety of interesting and inventive schemes have emerged. Presented here is a synthesis yielding the 12 most powerful techniques that seem to underlie many of the realizations found in practice. These 12 (the dirty dozen) are identified and illustrated.},
  file = {/Users/nseewald/Zotero/storage/KM5CIPFX/00031305.1984.html}
}

@article{wakefieldStatisticalFrameworkEcological2001,
  title = {A {{Statistical Framework}} for {{Ecological}} and {{Aggregate Studies}}},
  author = {Wakefield, Jonathan and Salway, Ruth},
  year = {2001},
  month = jan,
  journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
  volume = {164},
  number = {1},
  pages = {119--137},
  issn = {0964-1998, 1467-985X},
  doi = {10.1111/1467-985X.00191},
  urldate = {2024-02-02},
  abstract = {Inference from studies that make use of data at the level of the area, rather than at the level of the individual, is more dif{\textregistered}cult for a variety of reasons. Some of these dif{\textregistered}culties arise because frequently exposures (including confounders) vary within areas. In the most basic form of ecological study the outcome measure is regressed against a simple area level summary of exposure. In the aggregate data approach a survey of exposures and confounders is taken within each area. An alternative approach is to assume a parametric form for the within-area exposure distribution. We provide a framework within which ecological and aggregate data studies may be viewed, and we review some approaches to inference in such studies, clarifying the assumptions on which they are based. General strategies for analysis are provided including an estimator based on Monte Carlo integration that allows inference in the case of a general risk{\textpm}exposure model. We also consider the implications of the introduction of random effects, and the existence of confounding and errors in variables.},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/9WKX832A/Wakefield and Salway - 2001 - A Statistical Framework for Ecological and Aggrega.pdf}
}

@article{waldmanGuideCancerImmunotherapy2020,
  title = {A Guide to Cancer Immunotherapy: From {{T}} Cell Basic Science to Clinical Practice},
  shorttitle = {A Guide to Cancer Immunotherapy},
  author = {Waldman, Alex D. and Fritz, Jill M. and Lenardo, Michael J.},
  year = {2020},
  month = nov,
  journal = {Nature Reviews Immunology},
  volume = {20},
  number = {11},
  pages = {651--668},
  publisher = {Nature Publishing Group},
  issn = {1474-1741},
  doi = {10.1038/s41577-020-0306-5},
  urldate = {2024-01-22},
  abstract = {The T lymphocyte, especially its capacity for antigen-directed cytotoxicity, has become a central focus for engaging the immune system in the fight against cancer. Basic science discoveries elucidating the molecular and cellular biology of the T cell have led to new strategies in this fight, including checkpoint blockade, adoptive cellular therapy and cancer vaccinology. This area of immunological research has been highly active for the past 50 years and is now enjoying unprecedented bench-to-bedside clinical success. Here, we provide a comprehensive historical and biological perspective regarding the advent and clinical implementation of cancer immunotherapeutics, with an emphasis on the fundamental importance of T lymphocyte regulation. We highlight clinical trials that demonstrate therapeutic efficacy and toxicities associated with each class of drug. Finally, we summarize emerging therapies and emphasize the yet to be elucidated questions and future promise within the field of cancer immunotherapy.},
  copyright = {2020 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply},
  langid = {english},
  keywords = {_tablet_modified,Cancer immunotherapy,Drug discovery,Immunology},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Waldman et al_2020_A guide to cancer immunotherapy.pdf}
}

@article{walIpwPackageInverse2011,
  title = {Ipw: {{An R Package}} for {{Inverse Probability Weighting}}},
  shorttitle = {Ipw},
  author = {van der Wal, Willem M. and Geskus, Ronald B.},
  year = {2011},
  month = sep,
  journal = {Journal of Statistical Software},
  volume = {43},
  pages = {1--23},
  issn = {1548-7660},
  doi = {10.18637/jss.v043.i13},
  urldate = {2024-09-16},
  abstract = {We describe the R package ipw for estimating inverse probability weights. We show how to use the package to fit marginal structural models through inverse probability weighting, to estimate causal effects. Our package can be used with data from a point treatment situation as well as with a time-varying exposure and time-varying confounders. It can be used with binomial, categorical, ordinal and continuous exposure variables.},
  copyright = {Copyright (c) 2010 Willem M. van der Wal, Ronald B. Geskus},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/PLQI7RRL/Wal and Geskus - 2011 - ipw  An R Package for Inverse Proba.pdf}
}

@article{wallaceDoublyrobustDynamicTreatment2015,
  title = {Doubly-Robust Dynamic Treatment Regimen Estimation via Weighted Least Squares},
  author = {Wallace, Michael P. and Moodie, Erica E. M.},
  year = {2015},
  journal = {Biometrics},
  volume = {71},
  number = {3},
  pages = {636--644},
  issn = {1541-0420},
  doi = {10/f7rwkg},
  urldate = {2020-04-30},
  abstract = {Personalized medicine is a rapidly expanding area of health research wherein patient level information is used to inform their treatment. Dynamic treatment regimens (DTRs) are a means of formalizing the sequence of treatment decisions that characterize personalized management plans. Identifying the DTR which optimizes expected patient outcome is of obvious interest and numerous methods have been proposed for this purpose. We present a new approach which builds on two established methods: Q-learning and G-estimation, offering the doubly robust property of the latter but with ease of implementation much more akin to the former. We outline the underlying theory, provide simulation studies that demonstrate the double-robustness and efficiency properties of our approach, and illustrate its use on data from the Promotion of Breastfeeding Intervention Trial.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wallace_Moodie_2015_Doubly-robust dynamic treatment regimen estimation via weighted least squares.pdf;/Users/nseewald/Zotero/storage/7KTXXA6Q/biom.html}
}

@misc{wallaceModelSelectionEstimation2019,
  title = {Model Selection for {{G}}-estimation of {{Dynamic Treatment Regimes}}},
  author = {Wallace, Michael P. and Moodie, Erica E. M. and Stephens, David A.},
  year = {2019},
  month = jun,
  journal = {Biometrics},
  doi = {10.1111/biom.13104},
  urldate = {2019-07-09},
  howpublished = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13104},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wallace et al_2019_Model selection for G‐estimation of Dynamic Treatment Regimes.pdf;/Users/nseewald/Zotero/storage/BA3BGB88/biom.html}
}

@article{wallacePersonalizingMedicineReview2014,
  title = {Personalizing Medicine: {{A}} Review of Adaptive Treatment Strategies},
  author = {Wallace, Michael P. and Moodie, Erica E. M.},
  year = {2014},
  journal = {Pharmacoepidemiology and Drug Safety},
  volume = {23},
  number = {6},
  pages = {580--585},
  issn = {1099-1557 (Electronic){\textbackslash}r1053-8569 (Linking)},
  doi = {10.1002/pds.3606},
  abstract = {Much of current pharmacological practice focuses on identifying the single 'best' treatment (or course of treatments) for a particular disease. Recently, however, focus has begun to shift towards a more patient-centric rather than disease-centric approach, where personal characteristics are used to identify the optimal treatment for an individual. Adaptive treatment strategies (also known as dynamic treatment regimes) are part of a rapidly expanding area of research whereby such personalized treatments can be identified. These methods can lead to improved results over standard 'one size fits all' approaches, as well as provide a route to formalizing a common practice of using ad hoc approaches when deciding or updating management plans. Here, we provide an introduction to adaptive treatment strategies, explaining their background, their purpose, and how they can be employed in practice. Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  keywords = {Adaptive treatment strategies,dynamic treatment regimes,Personalized medicine,Pharmacoepidemiology,Q-learning,Sequential randomization},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wallace_Moodie_2014_Personalizing medicine.pdf}
}

@book{wallsModelsIntensiveLongitudinal2006,
  title = {Models for Intensive Longitudinal Data},
  editor = {Walls, Theodore A. and Schafer, J. L.},
  year = {2006},
  publisher = {Oxford University Press},
  address = {Oxford; New York},
  isbn = {978-0-19-517344-4},
  lccn = {HA29 .M673 2006},
  keywords = {nosource}
}

@article{walterEstimationDiagnosticTest2021,
  title = {Estimation of Diagnostic Test Accuracy: {{A}} ``{{Rule}} of {{Three}}'' for Data with Repeated Observations but without a Gold Standard},
  shorttitle = {Estimation of Diagnostic Test Accuracy},
  author = {Walter, Stephen D.},
  year = {2021},
  journal = {Statistics in Medicine},
  volume = {40},
  number = {22},
  pages = {4815--4829},
  issn = {1097-0258},
  doi = {10.1002/sim.9097},
  urldate = {2025-02-12},
  abstract = {This article considers how to estimate the accuracy of a diagnostic test when there are repeated observations, but without the availability of a gold standard or reference test. We identify conditions under which the structure of the observed data is rich enough to provide sufficient degrees of freedom, such that a suitable latent class model can be fitted with identifiable accuracy parameters. We show that a Rule of Three applies, specifying that accuracy can be evaluated as long as there are at least three observations per individual with the given test. This rule also applies if the three observations arise from combinations of different test methods, or from a sequential design in which individuals are tested for a maximum number of times with the same test but stopping if a positive (or negative) result occurs. The rule pertains to tests having an arbitrary number of response categories. Accuracy is evaluated by parameters reflecting rates of misclassification among the response categories, and the model also provides estimates of the underlying distribution of the true disease state. These ideas are illustrated by data from two medical studies. Issues discussed include the advantages and disadvantages of analyzing the response variable as binary or multinomial, as well as the feasibility of testing goodness of fit when the model incorporates a large number of parameters. Comparisons are possible between models that do or do not assume equal accuracy rates for the observations, and between models where certain misclassification parameters are or are not assumed to be zero.},
  copyright = {{\copyright} 2021 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {diagnostic tests,gold standard,latent class model,sensitivity,specificity},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Walter - 2021 - Estimation of diagnostic test accuracy A “Rule of Three” for data with repeated observations but wi.pdf;/Users/nseewald/Zotero/storage/TH7NANCK/sim.html}
}

@article{waltersAnalysingDataCluster2011,
  title = {Analysing Data from a Cluster Randomized Trial ({{cRCT}}) in Primary Care: A Case Study},
  shorttitle = {Analysing Data from a Cluster Randomized Trial ({{cRCT}}) in Primary Care},
  author = {Walters, Stephen J. and Jane Morrell, C. and Slade, Pauline},
  year = {2011},
  month = oct,
  journal = {Journal of Applied Statistics},
  volume = {38},
  number = {10},
  pages = {2253--2269},
  publisher = {Taylor \& Francis},
  issn = {0266-4763},
  doi = {10.1080/02664763.2010.545375},
  urldate = {2021-10-15},
  abstract = {Health technology assessment often requires the evaluation of interventions which are implemented at the level of the health service organization unit (e.g. GP practice) for clusters of individuals. In a cluster randomized controlled trial (cRCT), clusters of patients are randomized; not each patient individually. The majority of statistical analyses, in individually RCT, assume that the outcomes on different patients are independent. In cRCTs there is doubt about the validity of this assumption as the outcomes of patients, in the same cluster, may be correlated. Hence, the analysis of data from cRCTs presents a number of difficulties. The aim of this paper is to describe the statistical methods of adjusting for clustering, in the context of cRCTs. There are essentially four approaches to analysing cRCTs: 1. Cluster-level analysis using aggregate summary data. 2. Regression analysis with robust standard errors. 3. Random-effects/cluster-specific approach. 4. Marginal/population-averaged approach. This paper will compare and contrast the four approaches, using example data, with binary and continuous outcomes, from a cRCT designed to evaluate the effectiveness of training Health Visitors in psychological approaches to identify post-natal depressive symptoms and support post-natal women compared with usual care. The PoNDER Trial randomized 101 clusters (GP practices) and collected data on 2659 new mothers with an 18-month follow-up.},
  keywords = {cluster randomized trial,GEEs,GLM,marginal model,random-effects model},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Walters et al_2011_Analysing data from a cluster randomized trial (cRCT) in primary care.pdf;/Users/nseewald/Zotero/storage/367BUI6Z/02664763.2010.html}
}

@article{waltonAdaptiveInterventionsAlcohol2023,
  title = {Adaptive Interventions for Alcohol Misuse and Violent Behaviors among Adolescents and Emerging Adults in the Emergency Department: {{A}} Sequential Multiple Assignment Randomized Controlled Trial Protocol},
  shorttitle = {Adaptive Interventions for Alcohol Misuse and Violent Behaviors among Adolescents and Emerging Adults in the Emergency Department},
  author = {Walton, Maureen A. and Carter, Patrick M. and Seewald, Laura and Ngo, Quyen and Battisti, Katherine A. and Pearson, Claire and Blow, Frederic C. and Cunningham, Rebecca M. and Bourque, Carrie and Kidwell, Kelley M.},
  year = {2023},
  month = jul,
  journal = {Contemporary Clinical Trials},
  volume = {130},
  pages = {107218},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2023.107218},
  urldate = {2024-07-20},
  abstract = {Alcohol use and violent behaviors among youth are associated with morbidity and mortality. An emergency department (ED) visit provides an opportunity to initiate prevention efforts. Despite promising findings from our single session SafERteens brief intervention (BI), impact is limited by modest effect sizes, with data lacking on optimal boosters to enhance effects. This paper describes the protocol for a sequential, multiple assignment, randomized trial (SMART). Adolescents and emerging adults (ages 14--20) in the ED screening positive for alcohol use and violent behaviors (physical aggression) were randomly assigned to: 1) SafERteens BI + Text Messaging (TM), or 2) SafERteens BI + remote Health Coach (HC). Participants completed weekly surveys over 8~weeks after the ED visit to tailor intervention content and measure mechanisms of change. At one-month, intervention response/non-response is determined (e.g., binge drinking or violent behaviors). Responders are re-randomized to continued intervention condition (e.g., maintenance) or minimized condition (e.g., stepped down). Non-responders are re-randomized to continued condition (e.g., maintenance), or intensified condition (e.g., stepped up). Outcomes were measured at 4 and 8~months, including primary outcomes of alcohol consumption and violence, with secondary outcomes of alcohol consequences and violence consequences. Although the original goal was to enroll 700 participants, COVID-19 impacts on research diminished recruitment in this trial (enrolled n~=~400). Nonetheless, the proposed SMART is highly innovative by blending real-time assessment methodologies with adaptive intervention delivery among teens with comorbid alcohol misuse and violent behaviors. Findings will inform the content and timing booster interventions to alter risk behavior trajectories. Trial Registration: ClinicalTrials.gov NCT03344666. University of Michigan \# HUM00109156.},
  keywords = {_tablet,Aggression,Alcohol misuse,Intervention,Violent behaviors,Youth},
  file = {C\:\\Users\\Nick\\Box\\Zotero\\Walton_et_al_2023_Adaptive_interventions_for_alcohol_misuse_and_violent_behaviors_among.pdf;/Users/nseewald/Zotero/storage/94XIG8FR/S1551714423001416.html}
}

@article{waltonAdaptiveInterventionsAlcohol2023a,
  title = {Adaptive Interventions for Alcohol Misuse and Violent Behaviors among Adolescents and Emerging Adults in the Emergency Department: {{A}} Sequential Multiple Assignment Randomized Controlled Trial Protocol},
  shorttitle = {Adaptive Interventions for Alcohol Misuse and Violent Behaviors among Adolescents and Emerging Adults in the Emergency Department},
  author = {Walton, Maureen A. and Carter, Patrick M. and Seewald, Laura and Ngo, Quyen and Battisti, Katherine A. and Pearson, Claire and Blow, Frederic C. and Cunningham, Rebecca M. and Bourque, Carrie and Kidwell, Kelley M.},
  year = {2023},
  month = jul,
  journal = {Contemporary Clinical Trials},
  volume = {130},
  pages = {107218},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2023.107218},
  urldate = {2024-07-25},
  abstract = {Alcohol use and violent behaviors among youth are associated with morbidity and mortality. An emergency department (ED) visit provides an opportunity to initiate prevention efforts. Despite promising findings from our single session SafERteens brief intervention (BI), impact is limited by modest effect sizes, with data lacking on optimal boosters to enhance effects. This paper describes the protocol for a sequential, multiple assignment, randomized trial (SMART). Adolescents and emerging adults (ages 14--20) in the ED screening positive for alcohol use and violent behaviors (physical aggression) were randomly assigned to: 1) SafERteens BI + Text Messaging (TM), or 2) SafERteens BI + remote Health Coach (HC). Participants completed weekly surveys over 8~weeks after the ED visit to tailor intervention content and measure mechanisms of change. At one-month, intervention response/non-response is determined (e.g., binge drinking or violent behaviors). Responders are re-randomized to continued intervention condition (e.g., maintenance) or minimized condition (e.g., stepped down). Non-responders are re-randomized to continued condition (e.g., maintenance), or intensified condition (e.g., stepped up). Outcomes were measured at 4 and 8~months, including primary outcomes of alcohol consumption and violence, with secondary outcomes of alcohol consequences and violence consequences. Although the original goal was to enroll 700 participants, COVID-19 impacts on research diminished recruitment in this trial (enrolled n~=~400). Nonetheless, the proposed SMART is highly innovative by blending real-time assessment methodologies with adaptive intervention delivery among teens with comorbid alcohol misuse and violent behaviors. Findings will inform the content and timing booster interventions to alter risk behavior trajectories. Trial Registration: ClinicalTrials.gov NCT03344666. University of Michigan \# HUM00109156.},
  keywords = {Aggression,Alcohol misuse,Intervention,Violent behaviors,Youth},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Walton et al_2023_Adaptive interventions for alcohol misuse and violent behaviors among.pdf;/Users/nseewald/Zotero/storage/P8MUHNIU/S1551714423001416.html}
}

@article{wanEffectivenessMolnupiravirNirmatrelvir2023,
  title = {Effectiveness of {{Molnupiravir}} and {{Nirmatrelvir}}--{{Ritonavir}} in {{Hospitalized Patients With COVID-19}}},
  author = {Wan, Eric Yuk Fai and Yan, Vincent Ka Chun and Mok, Anna Hoi Ying and Wang, Boyuan and Xu, Wanchun and Cheng, Franco Wing Tak and Lai, Francisco Tsz Tsun and Chui, Celine Sze Ling and Li, Xue and Wong, Carlos King Ho and Li, Philip Hei and Cowling, Benjamin John and Hung, Ivan Fan Ngai and Lau, Chak Sing and Wong, Ian Chi Kei and Chan, Esther Wai Yin},
  year = {2023},
  month = apr,
  journal = {Annals of Internal Medicine},
  volume = {176},
  number = {4},
  pages = {505--514},
  publisher = {American College of Physicians},
  issn = {0003-4819},
  doi = {10.7326/M22-3057},
  urldate = {2023-07-25},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wan et al_2023_Effectiveness of Molnupiravir and Nirmatrelvir–Ritonavir in Hospitalized.pdf}
}

@article{wangAdaptiveRandomizationTwostage2021,
  title = {Adaptive Randomization in a Two-Stage Sequential Multiple Assignment Randomized Trial},
  author = {Wang, Junyao and Wu, Liwen and Wahed, Abdus S.},
  year = {2021},
  month = may,
  journal = {Biostatistics},
  pages = {kxab020},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxab020},
  urldate = {2021-10-15},
  abstract = {Summary             Sequential multiple assignment randomized trials (SMARTs) are systematic and efficient media for comparing dynamic treatment regimes (DTRs), where each patient is involved in multiple stages of treatment with the randomization at each stage depending on the patient's previous treatment history and interim outcomes. Generally, patients enrolled in SMARTs are randomized equally to ethically acceptable treatment options regardless of how effective those treatments were during the previous stages, which results in some undesirable consequences in practice, such as low recruitment, less retention, and lower treatment adherence. In this article, we propose a response-adaptive SMART (RA-SMART) design where the allocation probabilities are imbalanced in favor of more promising treatments based on the accumulated information on treatment efficacy from previous patients and stages. The operating characteristics of the RA-SMART design relative to SMART design, including the consistency and efficiency of estimated response rate under each DTR, the power of identifying the optimal DTR, and the number of patients treated with the optimal and the worst DTRs, are assessed through extensive simulation studies. Some practical suggestions are discussed in the conclusion.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang et al_2021_Adaptive randomization in a two-stage sequential multiple assignment randomized.pdf}
}

@article{wangAssessingMethodAgreement2020a,
  title = {Assessing Method Agreement for Paired Repeated Binary Measurements Administered by Multiple Raters},
  author = {Wang, Wei and Lin, Nan and Oberhaus, Jordan D. and Avidan, Michael S.},
  year = {2020},
  journal = {Statistics in Medicine},
  volume = {39},
  number = {3},
  pages = {279--293},
  issn = {1097-0258},
  doi = {10.1002/sim.8398},
  urldate = {2024-04-10},
  abstract = {Method comparison studies are essential for development in medical and clinical fields. These studies often compare a cheaper, faster, or less invasive measuring method with a widely used one to see if they have sufficient agreement for interchangeable use. Moreover, unlike simply reading measurements from devices, eg, reading body temperature from a thermometer, the response measurement in many clinical and medical assessments is impacted not only by the measuring device but also by the rater. For example, widespread inconsistencies are commonly observed among raters in psychological or cognitive assessment studies due to different characteristics such as rater training and experience, especially in large-scale assessment studies when many raters are employed. This paper proposes a model-based approach to assess agreement of two measuring methods for paired repeated binary measurements under the scenario where the agreement between two measuring methods and the agreement among raters are required to be studied simultaneously. Based upon the generalized linear mixed models (GLMMs), the decision on the adequacy of interchangeable use is made by testing the equality of fixed effects of methods. Approaches for assessing method agreement, such as the Bland-Altman diagram and Cohen's kappa, are also developed for repeated binary measurements based upon the latent variables in GLMMs. We assess our novel model-based approach by simulation studies and a real clinical application, in which patients are evaluated repeatedly for delirium with two validated screening methods. Both the simulation studies and the real data analyses demonstrate that our proposed approach can effectively assess method agreement.},
  copyright = {{\copyright} 2019 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Bland-Altman diagram,generalized linear mixed model,interrater reliability,method agreement,paired repeated binary measurement},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang_et_al_2020_Assessing_method_agreement_for_paired_repeated_binary_measurements_administered2.pdf;/Users/nseewald/Zotero/storage/IVDXLSJV/sim.html}
}

@article{wangAsymptoticPropertiesMestimators1999,
  title = {Asymptotic {{Properties}} of {{M-estimators Based}} on {{Estimating Equations}} and {{Censored Data}}},
  author = {Wang, Jane-Ling},
  year = {1999},
  month = jun,
  journal = {Scandinavian Journal of Statistics},
  volume = {26},
  number = {2},
  pages = {297--318},
  issn = {0303-6898, 1467-9469},
  doi = {10/bbw3r5},
  urldate = {2018-11-30},
  abstract = {Properties of Huber's M-estimators based on estimating equations have been studied extensively and are well understood for complete (i.i.d.) data. Although the concepts of M-estimators and influence curves have been extended for some time by Reid (1981) to incomplete data that are subject to right censoring, results on the general behavior of Mestimators based on incomplete data remain scattered and restrictive. This paper establishes a general large sample theory for M-estimators based on censored data. We show how to extend any asymptotic result available for M-estimators based on complete data to the case of censored data. The extensions are usually straightforward and include the multiparameter situation. Both the lifetime and censoring distributions may be discontinuous. We illustrate several extensions which provide simple and tractable sufficient conditions for an Mestimator to be strongly consistent and asymptotically normal. The influence curves and asymptotic variance of the M-estimators are also derived. The applicability of the new sufficient conditions is demonstrated through several examples, including location and scale M-estimators.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang_1999_Asymptotic Properties of M-estimators Based on Estimating Equations and.pdf}
}

@article{wangEvaluationViableDynamic2012,
  title = {Evaluation of Viable Dynamic Treatment Regimes in a Sequentially Randomized Trial of Advanced Prostate Cancer},
  author = {Wang, Lu and Rotnitzky, Andrea and Lin, Xihong and Millikan, Randall E and Thall, Peter F},
  year = {2012},
  journal = {Journal of the American Statistical Association},
  volume = {107},
  number = {498},
  pages = {493--508},
  issn = {0162-1459},
  doi = {10.1080/01621459.2011.641416},
  abstract = {We present new statistical analyses of data arising from a clinical trial designed to compare two-stage dynamic treatment regimes (DTRs) for advanced prostate cancer. The trial protocol mandated that patients were to be initially randomized among four chemotherapies, and that those who responded poorly were to be rerandomized to one of the remaining candidate therapies. The primary aim was to compare the DTRs' overall success rates, with success defined by the occurrence of successful responses in each of two consecutive courses of the patient's therapy. Of the one hundred and fifty study participants, forty seven did not complete their therapy per the algorithm. However, thirty five of them did so for reasons that precluded further chemotherapy; i.e. toxicity and/or progressive disease. Consequently, rather than comparing the overall success rates of the DTRs in the unrealistic event that these patients had remained on their assigned chemotherapies, we conducted an analysis that compared viable switch rules defined by the per-protocol rules but with the additional provision that patients who developed toxicity or progressive disease switch to a non-prespecified therapeutic or palliative strategy. This modification involved consideration of bivariate per-course outcomes encoding both efficacy and toxicity. We used numerical scores elicited from the trial's Principal Investigator to quantify the clinical desirability of each bivariate per-course outcome, and defined one endpoint as their average over all courses of treatment. Two other simpler sets of scores as well as log survival time also were used as endpoints. Estimation of each DTR-specific mean score was conducted using inverse probability weighted methods that assumed that missingness in the twelve remaining drop-outs was informative but explainable in that it only depended on past recorded data. We conducted additional worst-best case analyses to evaluate sensitivity of our findings to extreme departures from the explainable drop-out assumption.},
  pmid = {22956855},
  keywords = {Causal inference,efficiency,informative dropout,inverse probability weighting,marginal structural models,optimal,regime,simultaneous confidence intervals},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang et al_2012_Evaluation of viable dynamic treatment regimes in a sequentially randomized.pdf}
}

@article{wangFlexibleSampleSize2021,
  title = {A Flexible Sample Size Solution for Longitudinal and Crossover Cluster Randomized Trials with Continuous Outcomes},
  author = {Wang, Jijia and Cao, Jing and Zhang, Song and Ahn, Chul},
  year = {2021},
  month = oct,
  journal = {Contemporary Clinical Trials},
  volume = {109},
  pages = {106543},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2021.106543},
  urldate = {2021-10-15},
  abstract = {Longitudinal cluster randomized trial (LCRT) and crossover cluster randomized trial (CCRT) are two variants of cluster randomized trials. In LCRTs, clusters of subjects are randomly assigned to different treatment groups and each subject has repeated measurements over the study period. In CCRTs, clusters of subjects are randomly assigned to different sequences. Within each sequence, clusters receive all treatments in a particular order. Both LCRTs and CCRTs lead to complicated correlation structures that involve longitudinal and intracluster correlations. Generalized linear mixed model (GLMM) and generalized estimating equation (GEE) approaches have been frequently employed in data analysis and sample size estimation. In this study we propose closed-form sample size and power formulas for LCRTs and CCRTs based on the GEE approach. These formulas are flexible to incorporate unbalanced randomization, different missing patterns, arbitrary correlation structures, and randomly varying cluster sizes, providing a practical yet robust sample size solution. Simulation studies show that the proposed methods achieve good performance with empirical powers and type I errors close to their nominal values.},
  langid = {english},
  keywords = {Cluster randomized trials,Crossover,GEE,Longitudinal,Missing data,notion,Sample size calculation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang et al_2021_A flexible sample size solution for longitudinal and crossover cluster.pdf}
}

@article{wangGeneralizedEstimatingEquations2014,
  title = {Generalized {{Estimating Equations}} in {{Longitudinal Data Analysis}}: {{A Review}} and {{Recent Developments}}},
  shorttitle = {Generalized {{Estimating Equations}} in {{Longitudinal Data Analysis}}},
  author = {Wang, Ming},
  year = {2014},
  journal = {Advances in Statistics},
  doi = {10.1155/2014/303728},
  urldate = {2019-12-21},
  abstract = {Generalized Estimating Equation (GEE) is a marginal model popularly applied for longitudinal/clustered data analysis in clinical trials or biomedical studies. We provide a systematic review on GEE including basic concepts as well as several recent developments due to practical challenges in real applications. The topics including the selection of \&\#x201C;working\&\#x201D; correlation structure, sample size and power calculation, and the issue of informative cluster size are covered because these aspects play important roles in GEE utilization and its statistical inference. A brief summary and discussion of potential research interests regarding GEE are provided in the end.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang_2014_Generalized Estimating Equations in Longitudinal Data Analysis.pdf;/Users/nseewald/Zotero/storage/64H8WSVX/303728.html}
}

@article{wangIdemPackageInferences,
  title = {Idem: {{An R Package}} for {{Inferences}} in {{Clinical Trials}} with {{Death}} and {{Missingness}}},
  author = {Wang, Chenguang and Colantuoni, Elizabeth and Leroux, Andrew and Scharfstein, Daniel O.},
  journal = {Journal of Statistical Software},
  doi = {10/ggxvq9},
  abstract = {In randomized controlled trials of seriously ill patients, death is common and often defined as the primary endpoint. Increasingly, non-mortality outcomes such as functional outcomes are co-primary or secondary endpoints. Functional outcomes are not defined for patients who die, referred to as "truncation due to death", and among survivors, functional outcomes are often unobserved due to missed clinic visits or loss to follow-up. It is well known that if the functional outcomes "truncated due to death" or missing are handled inappropriately, treatment effect estimation can be biased. In this paper, we describe the package idem that implements a procedure for comparing treatments that is based on a composite endpoint of mortality and the functional outcome among survivors. Among survivors, the procedure incorporates a missing data imputation procedure with a sensitivity analysis strategy. A web-based graphical user interface is provided in the idem package to facilitate users conducting the proposed analysis in an interactive and user-friendly manner. We demonstrate idem using data from a recent trial of sedation interruption among mechanically ventilated patients.},
  keywords = {nosource,Researcher App}
}

@article{wangImpactMisclassificationCovariate,
  title = {The Impact of Misclassification on Covariate-adaptive Randomized Clinical Trials},
  author = {Wang, Tong and Ma, Wei},
  journal = {Biometrics},
  doi = {10/ggxvq6},
  abstract = {{$<$}p{$>$}Covariate-adaptive randomization is widely used in clinical trials to balance treatment allocation over covariates. Over the past decade, significant progress has been made on the theoretical properties of covariate-adaptive design and associated inference. However, most results are established under the assumption that the covariates are correctly measured. In practice, measurement error is inevitable, resulting in misclassification for discrete covariates. When covariate misclassification is present in a clinical trial conducted using covariate-adaptive randomization, the impact is twofold: it impairs the intended covariate balance, and raises concerns over the validity of test procedures. In this paper, we consider the impact of misclassification on covariate-adaptive randomized trials from the perspectives of both design and inference. We derive the asymptotic normality, and thereby the convergence rate, of the imbalance of the true covariates for a general family of covariate-adaptive randomization methods, and show that a superior covariate balance can still be attained compared to complete randomization. We also show that the two sample \emph{t }-test is conservative, with a reduced Type I error, but that this can be corrected using a bootstrap method. Moreover, if the misclassified covariates are adjusted in the model used for analysis, the test maintains its nominal Type I error, with an increased power. Our results support the use of covariate-adaptive randomization in clinical trials, even when the covariates are subject to~misclassification.{$<$}/p{$>$}},
  keywords = {nosource,Researcher App}
}

@incollection{wangMachineLearningSuicide2022,
  title = {Machine {{Learning}} for {{Suicide Prediction}} and {{Prevention}}: {{Advances}}, {{Challenges}}, and {{Future Directions}}},
  shorttitle = {Machine {{Learning}} for {{Suicide Prediction}} and {{Prevention}}},
  booktitle = {Youth {{Suicide Prevention}} and {{Intervention}}},
  author = {Wang, Shirley B. and Dempsey, Walter and Nock, Matthew K.},
  editor = {Ackerman, John P. and Horowitz, Lisa M.},
  year = {2022},
  pages = {21--28},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-06127-1_3},
  urldate = {2022-08-31},
  abstract = {Abstract             This chapter describes the role of machine learning in youth suicide prevention. Following a brief history of suicide prediction, research is reviewed demonstrating that machine learning can enhance suicide prediction beyond traditional clinical and statistical approaches. Strategies for internal and external model evaluation, methods for integrating model results into clinical decision-making processes, and ethical issues raised by building and implementing suicide prediction models are discussed. Finally, future directions for this work are highlighted, including the need for collaborative science and the importance of both data- and theory-driven computational methods.},
  isbn = {978-3-031-06126-4 978-3-031-06127-1},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang et al_2022_Machine Learning for Suicide Prediction and Prevention.pdf}
}

@article{wangSimulatingClusteredDependent,
  title = {Simulating {{Clustered}} and {{Dependent Binary Variables}}},
  author = {Wang, Aobo and Sabo, Roy T},
  pages = {5},
  abstract = {Dependent binary data can be simply simulated using the multivariate normal- and multinomial sampling-based approaches. We extend these methods to simulate dependent binary data with clustered random effect structures. Several distributions are considered for constructing random effects among cluster-specific parameters and effect sizes, including the normal, uniform and beta distributions. We present results from simulation studies to show proof of concept for these two methods in creating data sets of repeated-measure binary outcomes with clustered random effect structures in various scenarios. The simulation studies show that multivariate normal- and multinomial sampling approaches can be successfully adapted to simulate dependent binary data with desired random effect structures.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang_Sabo_Simulating Clustered and Dependent Binary Variables.pdf}
}

@article{wangUnbiasedEstimatingEquations2004,
  title = {Unbiased {{Estimating Equations From Working Correlation Models}} for {{Irregularly Timed Repeated Measures}}},
  author = {Wang, You-Gan and Carey, Vincent J},
  year = {2004},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {99},
  number = {467},
  pages = {845--853},
  issn = {0162-1459, 1537-274X},
  doi = {10/c6jdnj},
  urldate = {2019-08-15},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang_Carey_2004_Unbiased Estimating Equations From Working Correlation Models for Irregularly.pdf}
}

@article{wangWorkingCorrelationStructure2003,
  title = {Working Correlation Structure Misspecification, Estimation and Covariate Design: {{Implications}} for Generalised Estimating Equations Performance},
  shorttitle = {Working Correlation Structure Misspecification, Estimation and Covariate Design},
  author = {Wang, You-Gan and Carey, Vincent},
  year = {2003},
  journal = {Biometrika},
  volume = {90},
  number = {1},
  eprint = {30042017},
  eprinttype = {jstor},
  pages = {29--41},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/90.1.29},
  abstract = {The method of generalised estimating equations for regression modellin outcomes allows for specification of a working matrix that is intended the true correlation matrix of the observations. We investigate the asym efficiency of the generalised estimating equation for the mean parameters w lation parameters are estimated by various methods. The asymptotic rel depends on three features of the analysis, namely (i) the discrepancy betwee correlation structure and the unobservable true correlation structure, (ii which the correlation parameters are estimated and (iii) the 'design', by whic both the structures of the predictor matrices within clusters and distrib sizes. Analytical and numerical studies of realistic data-analysis scenarios sho of working covariance model has a substantial impact on regression estim Protection against avoidable loss of efficiency associated with covariance is obtained when a 'Gaussian estimation' pseudolikelihood procedure AR(1) structure.},
  langid = {english},
  keywords = {Design matrix,efficiency,Estimating function,Longitudinal data,Pseudolikelihood,Repeated measures},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wang_Carey_2003_Working correlation structure misspecification, estimation and covariate design.pdf}
}

@misc{warrenIPhone14Pro2022,
  title = {{{iPhone}} 14 {{Pro}} and {{Pro Max}} Announced with Animated Notches and Always-on Displays},
  author = {Warren, Tom},
  year = {2022},
  month = sep,
  journal = {The Verge},
  urldate = {2022-09-07},
  abstract = {The iPhone 14 Pro has plenty of hardware upgrades.},
  howpublished = {https://www.theverge.com/2022/9/7/23338810/iphone-14-pro-screen-cameras-notch-specs-price-release-date-apple-event},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/LSYZ8SCR/iphone-14-pro-screen-cameras-notch-specs-price-release-date-apple-event.html}
}

@misc{wassersteinASA175ASA2014,
  title = {{{ASA}} at 175 -- {{Some ASA}} Demographics},
  author = {Wasserstein, Ronald},
  year = {2014},
  month = apr,
  journal = {ASA Community},
  keywords = {nosource}
}

@phdthesis{watkinsLearningDelayedRewards1989,
  title = {Learning from {{Delayed Rewards}}},
  author = {Watkins, Christopher J. C. H.},
  year = {1989},
  address = {Cambridge, UK},
  langid = {english},
  school = {King's College},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Watkins_1989_Learning from Delayed Rewards.pdf}
}

@article{watkinsQlearning1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10/dvm8f4},
  urldate = {2019-01-04},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Watkins_Dayan_1992_Q-learning.pdf}
}

@article{wedderburnQuasilikelihoodFunctionsGeneralized1974,
  title = {Quasi-Likelihood Functions, Generalized Linear Models, and the {{Gauss}}---{{Newton}} Method},
  author = {Wedderburn, R. W. M.},
  year = {1974},
  month = dec,
  journal = {Biometrika},
  volume = {61},
  number = {3},
  pages = {439--447},
  issn = {0006-3444},
  doi = {10/frhvzp},
  urldate = {2019-08-13},
  abstract = {Abstract.  To define a likelihood we have to specify the form of distribution of the observations, but to define a quasi-likelihood function we need only specif},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wedderburn_1974_Quasi-likelihood functions, generalized linear models, and the Gauss—Newton.pdf;/Users/nseewald/Zotero/storage/KRTLR9FF/249095.html}
}

@article{weiAnalysisDataImbalance2001,
  ids = {weiAnalysisDataImbalance2016},
  title = {Analysis of {{Data}} with {{Imbalance}} in the {{Baseline Outcome Variable}} for {{Randomized Clinical Trials}}},
  author = {Wei, Lynn and Zhang, Ji},
  year = {2001},
  month = oct,
  journal = {Drug Information Journal},
  volume = {35},
  number = {4},
  pages = {1201--1214},
  issn = {0092-8615},
  doi = {10/fzkcfj},
  urldate = {2019-12-10},
  abstract = {The ANCOVA approach provides unconditionallyunbiased estimates and is an unconditionally more powerful test. Conditioning on the imbalance in the baseline outcome variable, the ANCOVA maintains its unbiasedness and conditional a-level and power. The ANOVA estimator is unconditionally unbiased and it has a designated size unconditionally though less powerful than the ANCOVA test. Conditioning on the observed baseline imbalance, however, the ANOVA approach provides a biased estimate of treatment effect and a test with inflated conditional type I error rate (3) or reduced conditional power under certain conditions.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wei_Zhang_2001_Analysis of Data with Imbalance in the Baseline Outcome Variable for Randomized.pdf;/Users/nseewald/Zotero/storage/8U445ZGJ/009286150103500417.html}
}

@article{weiBayesianAnalysisSmall2018,
  title = {A {{Bayesian}} Analysis of Small n Sequential Multiple Assignment Randomized Trials ({{snSMARTs}})},
  author = {Wei, Boxian and Braun, Thomas M. and Tamura, Roy N. and Kidwell, Kelley M.},
  year = {2018},
  journal = {Statistics in Medicine},
  volume = {37},
  number = {26},
  pages = {3723--3732},
  issn = {1097-0258},
  doi = {10/gf3jrq},
  urldate = {2019-06-04},
  abstract = {Designing clinical trials to study treatments for rare diseases is challenging because of the limited number of available patients. A suggested design is known as the small n sequential multiple assignment randomized trial (snSMART), in which patients are first randomized to one of multiple treatments (stage 1). Patients who respond to their initial treatment continue the same treatment for another stage, while those who fail to respond are rerandomized to one of the remaining treatments (stage 2). The data from both stages are used to compare the efficacy between treatments. Analysis approaches for snSMARTs are limited, and we propose a Bayesian approach that allows for borrowing of information across both stages. Through simulation, we compare the bias, root-mean-square error, width, and coverage rate of 95\% confidence/credible interval of estimators from of our approach to estimators produced from (i) standard approaches that only use the data from stage 1, and (ii) a log-Poisson model using data from both stages whose parameters are estimated via generalized estimating equations. We demonstrate the root-mean-square error and width of 95\% confidence/credible intervals of our estimators are smaller than the other approaches in realistic settings, so that the collection and use of stage 2 data in snSMARTs provide improved inference for treatments of rare diseases.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wei et al_2018_A Bayesian analysis of small n sequential multiple assignment randomized trials.pdf;/Users/nseewald/Zotero/storage/32C3XYHY/sim.html}
}

@article{weiCombiningDependentTests1985,
  title = {Combining Dependent Tests with Incomplete Repeated Measurements},
  author = {Wei, L. J. and Johnson, Wayne E.},
  year = {1985},
  month = aug,
  journal = {Biometrika},
  volume = {72},
  number = {2},
  pages = {359--364},
  issn = {0006-3444},
  doi = {10.1093/biomet/72.2.359},
  urldate = {2023-10-27},
  abstract = {In comparing the effectiveness of two treatments, repeated measurements of the same characteristic are often taken under two or more distinct conditions for each experimental subject. A locally best linear combination of individual test statistics obtained under different conditions is proposed to test the equality of the two treatments. The test procedure allows different patterns of missing observations for the two groups to be compared. Special cases such as combining Wilcoxon tests, t tests and 2{\texttimes}2 tables are discussed in detail with real examples.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wei_Johnson_1985_Combining dependent tests with incomplete repeated measurements.pdf;/Users/nseewald/Zotero/storage/YEV7WSLE/229723.html}
}

@article{weiRegressionAnalysisMultivariate1989,
  title = {Regression {{Analysis}} of {{Multivariate Incomplete Failure Time Data}} by {{Modeling Marginal Distributions}}},
  author = {Wei, L. J. and Lin, D. Y. and Weissfeld, L.},
  year = {1989},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {84},
  number = {408},
  eprint = {2290084},
  eprinttype = {jstor},
  pages = {1065},
  issn = {01621459},
  doi = {10.2307/2290084},
  urldate = {2018-10-12},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wei et al_1989_Regression Analysis of Multivariate Incomplete Failure Time Data by Modeling2.pdf}
}

@article{weiRegressionAnalysisMultivariate1989a,
  title = {Regression {{Analysis}} of {{Multivariate Incomplete Failure Time Data}} by {{Modeling Marginal Distributions}}},
  author = {Wei, L. J. and Lin, D. Y. and Weissfeld, L.},
  year = {1989},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {84},
  number = {408},
  pages = {1065--1073},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1989.10478873},
  urldate = {2023-10-27},
  abstract = {Many survival studies record the times to two or more distinct failures on each subject. The failures may be events of different natures or may be repetitions of the same kind of event. In this article, we consider the regression analysis of such multivariate failure time observations. Each marginal distribution of the failure times is formulated by a Cox proportional hazards model. No specific structure of dependence among the distinct failure times on each subject is imposed. The regression parameters in the Cox models are estimated by maximizing the failure-specific partial likelihoods. The resulting estimators are shown to be asymptotically jointly normal with a covariance matrix that can be consistently estimated. Simultaneous inferential procedures are then proposed. Extensive Monte Carlo studies indicate that the normal approximation is adequate for practical use. The new methods allow time-dependent covariates, missing observations, and arbitrary patterns of censorship. They are illustrated with two real-life examples. For recurrent failure time data, various regression methods have been proposed in the literature. These methods, however, generally assume stringent structures of dependence among the recurrences of each subject. Moreover, as shown in the present article, they are rather sensitive to model misspecification.},
  keywords = {Cox model,Martingale,Partial likelihood,Proportional hazards,Simultaneous inference},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wei et al_1989_Regression Analysis of Multivariate Incomplete Failure Time Data by Modeling.pdf}
}

@article{welchNoteCriticismsMade1956,
  title = {Note on {{Some Criticisms Made}} by {{Sir Ronald Fisher}}},
  author = {Welch, B. L.},
  year = {1956},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {18},
  number = {2},
  eprint = {2983718},
  eprinttype = {jstor},
  pages = {297--302},
  abstract = {IN a recent note Sir Ronald Fisher (1956) has commented upon some tables which were calculated under my direction and first published in Biometrika about seven years ago (Aspin, 1949). Fisher is concerned with what he supposes to be errors in the values given. The object of the present reply is to explain once more what the tables aim to do, and to maintain that there is no justification in Fisher's criticism.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Welch_1956_Note on Some Criticisms Made by Sir Ronald Fisher.pdf}
}

@article{wellesMinoritiesOutliersCase2014,
  title = {On Minorities and Outliers: {{The}} Case for Making {{Big Data}} Small},
  shorttitle = {On Minorities and Outliers},
  author = {Welles, Brooke Foucault},
  year = {2014},
  month = apr,
  journal = {Big Data \& Society},
  volume = {1},
  number = {1},
  pages = {205395171454061},
  issn = {2053-9517, 2053-9517},
  doi = {10/gcdzc2},
  urldate = {2021-07-02},
  abstract = {In this essay, I make the case for choosing to examine small subsets of Big Data datasets---making big data small. Big Data allows us to produce summaries of human behavior at a scale never before possible. But in the push to produce these summaries, we risk losing sight of a secondary but equally important advantage of Big Data---the plentiful representation of minorities. Women, minorities and statistical outliers have historically been omitted from the scientific record, with problematic consequences. Big Data affords the opportunity to remedy those omissions. However, to do so, Big Data researchers must choose to examine very small subsets of otherwise large datasets. I encourage researchers to embrace an ethical, empirical and epistemological stance on Big Data that includes minorities and outliers as reference categories, rather than the exceptions to statistical norms.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Welles_2014_On minorities and outliers.pdf}
}

@article{wenAssociationMedicalAdultUse2018,
  title = {Association of {{Medical}} and {{Adult-Use Marijuana Laws With Opioid Prescribing}} for {{Medicaid Enrollees}}},
  author = {Wen, Hefei and Hockenberry, Jason M.},
  year = {2018},
  month = may,
  journal = {JAMA Internal Medicine},
  volume = {178},
  number = {5},
  pages = {673--679},
  issn = {2168-6106},
  doi = {10.1001/jamainternmed.2018.1007},
  urldate = {2022-06-17},
  abstract = {Overprescribing of opioids is considered a major driving force behind the opioid epidemic in the United States. Marijuana is one of the potential nonopioid alternatives that can relieve pain at a relatively lower risk of addiction and virtually no risk of overdose. Marijuana liberalization, including medical and adult-use marijuana laws, has made marijuana available to more Americans.To examine the association of state implementation of medical and adult-use marijuana laws with opioid prescribing rates and spending among Medicaid enrollees.This cross-sectional study used a quasi-experimental difference-in-differences design comparing opioid prescribing trends between states that started to implement medical and adult-use marijuana laws between 2011 and 2016 and the remaining states. This population-based study across the United States included all Medicaid fee-for-service and managed care enrollees, a high-risk population for chronic pain, opioid use disorder, and opioid overdose.State implementation of medical and adult-use marijuana laws from 2011 to 2016.Opioid prescribing rate, measured as the number of opioid prescriptions covered by Medicaid on a quarterly, per-1000-Medicaid-enrollee basis.State implementation of medical marijuana laws was associated with a 5.88\% lower rate of opioid prescribing (95\% CI, -11.55\%\,to approximately -0.21\%). Moreover, the implementation of adult-use marijuana laws, which all occurred in states with existing medical marijuana laws, was associated with a 6.38\% lower rate of opioid prescribing (95\% CI, -12.20\%\,to approximately -0.56\%).The potential of marijuana liberalization to reduce the use and consequences of prescription opioids among Medicaid enrollees deserves consideration during the policy discussions about marijuana reform and the opioid epidemic.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wen_Hockenberry_2018_Association of Medical and Adult-Use Marijuana Laws With Opioid Prescribing for.pdf;/Users/nseewald/Zotero/storage/RPSA4JEQ/2677000.html}
}

@article{wenImpactMedicalRecreational2021,
  title = {The Impact of Medical and Recreational Marijuana Laws on Opioid Prescribing in Employer-Sponsored Health Insurance},
  author = {Wen, Jiebing and Wen, Hefei and Butler, J. S. and Talbert, Jeffery C.},
  year = {2021},
  month = may,
  journal = {Health Economics},
  volume = {30},
  number = {5},
  pages = {989--1000},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1099-1050},
  doi = {10.1002/hec.4237},
  urldate = {2024-05-01},
  abstract = {Health Economics is an international health policy journal publishing articles on all aspects of global health economics and health care systems.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wen_et_al_2021_The_impact_of_medical_and_recreational_marijuana_laws_on_opioid_prescribing_in.pdf}
}

@article{wernerEffectPayForPerformanceHospitals2011,
  title = {The {{Effect Of Pay-For-Performance In Hospitals}}: {{Lessons For Quality Improvement}}},
  shorttitle = {The {{Effect Of Pay-For-Performance In Hospitals}}},
  author = {Werner, Rachel M. and Kolstad, Jonathan T. and Stuart, Elizabeth A. and Polsky, Daniel},
  year = {2011},
  month = apr,
  journal = {Health Affairs},
  volume = {30},
  number = {4},
  pages = {690--698},
  publisher = {Health Affairs},
  issn = {0278-2715},
  doi = {10.1377/hlthaff.2010.1277},
  urldate = {2023-09-11},
  abstract = {The payment approach known as ``pay-for-performance'' has been widely adopted with the aim of improving the quality of health care. Nonetheless, little is known about how to use the approach most effectively to improve care. We examined the effects in 260 hospitals of a pay-for-performance demonstration project carried out by the Centers for Medicare and Medicaid Services in partnership with Premier Inc., a nationwide hospital system. We compared these results to those of a control group of 780~hospitals not in the demonstration project. The performance of the hospitals in the project initially improved more than the performance of the control group: More than half of the pay-for-performance hospitals achieved high performance scores, compared to fewer than a third of the control hospitals. However, after five years, the two groups' scores were virtually identical. Improvements were largest among hospitals that were eligible for larger bonuses, were well financed, or operated in less competitive markets. These findings suggest that tailoring pay-for-performance programs to hospitals' specific situations could have the greatest effect on health care quality.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Werner et al_2011_The Effect Of Pay-For-Performance In Hospitals.pdf}
}

@article{westAlternativesRandomizedControlled2008,
  title = {Alternatives to the {{Randomized Controlled Trial}}},
  author = {West, Stephen G. and Duan, Naihua and Pequegnat, Willo and Gaist, Paul and Des Jarlais, Don C. and Holtgrave, David and Szapocznik, Jos{\'e} and Fishbein, Martin and Rapkin, Bruce and Clatts, Michael and Mullen, Patricia Dolan},
  year = {2008},
  month = aug,
  journal = {American Journal of Public Health},
  volume = {98},
  number = {8},
  pages = {1359--1366},
  publisher = {American Public Health Association},
  issn = {0090-0036},
  doi = {10.2105/AJPH.2007.124446},
  urldate = {2022-04-05},
  abstract = {Public health researchers are addressing new research questions (e.g., effects of environmental tobacco smoke, Hurricane Katrina) for which the randomized controlled trial (RCT) may not be a feasible option. Drawing on the potential outcomes framework (Rubin Causal Model) and Campbellian perspectives, we consider alternative research designs that permit relatively strong causal inferences. In randomized encouragement designs, participants are randomly invited to participate in one of the treatment conditions, but are allowed to decide whether to receive treatment. In quantitative assignment designs, treatment is assigned on the basis of a quantitative measure (e.g., need, merit, risk). In observational studies, treatment assignment is unknown and presumed to be nonrandom. Major threats to the validity of each design and statistical strategies for mitigating those threats are presented.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/West et al_2008_Alternatives to the Randomized Controlled Trial.pdf}
}

@misc{WhenParallelTrends,
  title = {When {{Is Parallel Trends Sensitive}} to {{Functional Form}}? - {{The Econometric Society}}},
  shorttitle = {When {{Is Parallel Trends Sensitive}} to {{Functional Form}}?},
  urldate = {2024-07-12},
  howpublished = {http://www.econometricsociety.org/publications/econometrica/2023/03/01/When-Is-Parallel-Trends-Sensitive-to-Functional-Form},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/9RLLA5IX/When-Is-Parallel-Trends-Sensitive-to-Functional-Form.html}
}

@misc{WhenShouldYou,
  title = {When {{Should You Adjust Standard Errors}} for {{Clustering}}?* {\textbar} {{The Quarterly Journal}} of {{Economics}} {\textbar} {{Oxford Academic}}},
  urldate = {2022-10-24},
  howpublished = {https://academic-oup-com.proxy1.library.jhu.edu/qje/advance-article/doi/10.1093/qje/qjac038/6750017?login=false},
  file = {/Users/nseewald/Zotero/storage/CUSQMWVN/6750017.html}
}

@article{whitakerCoincidenceAnalysisNew2020,
  title = {Coincidence Analysis: A New Method for Causal Inference in Implementation Science},
  shorttitle = {Coincidence Analysis},
  author = {Whitaker, Rebecca Garr and Sperber, Nina and Baumgartner, Michael and Thiem, Alrik and Cragun, Deborah and Damschroder, Laura and Miech, Edward J. and Slade, Alecia and Birken, Sarah},
  year = {2020},
  month = dec,
  journal = {Implementation Science},
  volume = {15},
  number = {1},
  pages = {108},
  issn = {1748-5908},
  doi = {10.1186/s13012-020-01070-3},
  urldate = {2022-04-26},
  abstract = {Implementation of multifaceted interventions typically involves many diverse elements working together in interrelated ways, including intervention components, implementation strategies, and features of local context. Given this real-world complexity, implementation researchers may be interested in a new mathematical, cross-case method called Coincidence Analysis (CNA) that has been designed explicitly to support causal inference, answer research questions about combinations of conditions that are minimally necessary or sufficient for an outcome, and identify the possible presence of multiple causal paths to an outcome. CNA can be applied as a standalone method or in conjunction with other approaches and can reveal new empirical findings related to implementation that might otherwise have gone undetected.},
  langid = {english},
  keywords = {Causal inference,Coincidence analysis,Comparative analysis,Configurational comparative methods},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Whitaker et al_2020_Coincidence analysis.pdf}
}

@article{whiteImputingMissingCovariate2009,
  title = {Imputing Missing Covariate Values for the {{Cox}} Model},
  author = {White, Ian R. and Royston, Patrick},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {15},
  pages = {1982--1998},
  issn = {1097-0258},
  doi = {10.1002/sim.3618},
  urldate = {2024-02-28},
  abstract = {Multiple imputation is commonly used to impute missing data, and is typically more efficient than complete cases analysis in regression analysis when covariates have missing values. Imputation may be performed using a regression model for the incomplete covariates on other covariates and, importantly, on the outcome. With a survival outcome, it is a common practice to use the event indicator D and the log of the observed event or censoring time T in the imputation model, but the rationale is not clear. We assume that the survival outcome follows a proportional hazards model given covariates X and Z. We show that a suitable model for imputing binary or Normal X is a logistic or linear regression on the event indicator D, the cumulative baseline hazard H0(T), and the other covariates Z. This result is exact in the case of a single binary covariate; in other cases, it is approximately valid for small covariate effects and/or small cumulative incidence. If we do not know H0(T), we approximate it by the Nelson--Aalen estimator of H(T) or estimate it by Cox regression. We compare the methods using simulation studies. We find that using logT biases covariate-outcome associations towards the null, while the new methods have lower bias. Overall, we recommend including the event indicator and the Nelson--Aalen estimator of H(T) in the imputation model. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {missing covariates,missing data,multiple imputation,proportional hazards model},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/White_Royston_2009_Imputing missing covariate values for the Cox model.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/White_Royston_2009_Imputing missing covariate values for the Cox model2.pdf;/Users/nseewald/Zotero/storage/I8KVZLTU/sim.html}
}

@article{wijnConfoundingAdjustmentMethods2022,
  title = {Confounding Adjustment Methods in Longitudinal Observational Data with a Time-Varying Treatment: A Mapping Review},
  shorttitle = {Confounding Adjustment Methods in Longitudinal Observational Data with a Time-Varying Treatment},
  author = {Wijn, Stan R. W. and Rovers, Maroeska M. and Hannink, Gerjon},
  year = {2022},
  month = mar,
  journal = {BMJ Open},
  volume = {12},
  number = {3},
  pages = {e058977},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2021-058977},
  urldate = {2023-12-11},
  abstract = {Objectives To adjust for confounding in observational data, researchers use propensity score matching (PSM), but more advanced methods might be required when dealing with longitudinal data and time-varying treatments as PSM might not include possible changes that occurred over time. This study aims to explore which confounding adjustment methods have been used in longitudinal observational data to estimate a treatment effect and identify potential inappropriate use of PSM. Design Mapping review. Data sources We searched PubMed, from inception up to January 2021, for studies in which a treatment was evaluated using longitudinal observational data. Eligibility criteria Methodological, non-medical and cost-effectiveness papers were excluded, as were non-English studies and studies that did not study a treatment effect. Data extraction and synthesis Studies were categorised based on time of treatment: at baseline (interventions performed at start of follow-up) or time-varying (interventions received asynchronously during follow-up) and sorted based on publication year, time of treatment and confounding adjustment method. Cumulative time series plots were used to investigate the use of different methods over time. No risk-of-bias assessment was performed as it was not applicable. Results In total, 764 studies were included that met the eligibility criteria. PSM (165/201, 82\%) and inverse probability weighting (IPW; 154/502, 31\%) were most common for studies with a treatment at baseline (n=201) and time-varying treatment (n=502), respectively. Of the 502 studies with a time-varying treatment, 123 (25\%) used PSM with baseline covariates, which might be inappropriate. In the past 5 years, the proportion of studies with a time-varying treatment that used PSM over IPW increased. Conclusions PSM is the most frequently used method to correct for confounding in longitudinal observational data. In studies with a time-varying treatment, PSM was potentially inappropriately used in 25\% of studies. Confounding adjustment methods designed to deal with a time-varying treatment and time-varying confounding are available, but were only used in 45\% of the studies with a time-varying treatment.},
  chapter = {Research methods},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2022. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See:~http://creativecommons.org/licenses/by-nc/4.0/.},
  langid = {english},
  pmid = {35304403},
  keywords = {epidemiology,orthopaedic & trauma surgery,statistics & research methods},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wijn_et_al_2022_Confounding_adjustment_methods_in_longitudinal_observational_data_with_a.pdf}
}

@article{willemsCorrectingDependentCensoring2018,
  title = {Correcting for Dependent Censoring in Routine Outcome Monitoring Data by Applying the Inverse Probability Censoring Weighted Estimator},
  author = {Willems, {\relax SJW} and Schat, A and {van Noorden}, {\relax MS} and Fiocco, M},
  year = {2018},
  month = feb,
  journal = {Statistical Methods in Medical Research},
  volume = {27},
  number = {2},
  pages = {323--335},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280216628900},
  urldate = {2024-07-18},
  abstract = {Censored data make survival analysis more complicated because exact event times are not observed. Statistical methodology developed to account for censored observations assumes that patients' withdrawal from a study is independent of the event of interest. However, in practice, some covariates might be associated to both lifetime and censoring mechanism, inducing dependent censoring. In this case, standard survival techniques, like Kaplan--Meier estimator, give biased results. The inverse probability censoring weighted estimator was developed to correct for bias due to dependent censoring. In this article, we explore the use of inverse probability censoring weighting methodology and describe why it is effective in removing the bias. Since implementing this method is highly time consuming and requires programming and mathematical skills, we propose a user friendly algorithm in R. Applications to a toy example and to a medical data set illustrate how the algorithm works. A simulation study was carried out to investigate the performance of the inverse probability censoring weighted estimators in situations where dependent censoring is present in the data. In the simulation process, different sample sizes, strengths of the censoring model, and percentages of censored individuals were chosen. Results show that in each scenario inverse probability censoring weighting reduces the bias induced in the traditional Kaplan--Meier approach where dependent censoring is ignored.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Willems et al_2018_Correcting for dependent censoring in routine outcome monitoring data by.pdf}
}

@article{wingDesigningDifferenceDifference2018,
  title = {Designing {{Difference}} in {{Difference Studies}}: {{Best Practices}} for {{Public Health Policy Research}}},
  shorttitle = {Designing {{Difference}} in {{Difference Studies}}},
  author = {Wing, Coady and Simon, Kosali and {Bello-Gomez}, Ricardo A.},
  year = {2018},
  month = apr,
  journal = {Annual Review of Public Health},
  volume = {39},
  number = {1},
  pages = {453--469},
  publisher = {Annual Reviews},
  issn = {0163-7525},
  doi = {10/gd2xxj},
  urldate = {2021-07-13},
  abstract = {The difference in difference (DID) design is a quasi-experimental research design that researchers often use to study causal relationships in public health settings where randomized controlled trials (RCTs) are infeasible or unethical. However, causal inference poses many challenges in DID designs. In this article, we review key features of DID designs with an emphasis on public health policy research. Contemporary researchers should take an active approach to the design of DID studies, seeking to construct comparison groups, sensitivity analyses, and robustness checks that help validate the method's assumptions. We explain the key assumptions of the design and discuss analytic tactics, supplementary analysis, and approaches to statistical inference that are often important in applied research. The DID design is not a perfect substitute for randomized experiments, but it often represents a feasible way to learn about casual relationships. We conclude by noting that combining elements from multiple quasi-experimental techniques may be important in the next wave of innovations to the DID approach.},
  keywords = {causal inference,difference in difference,policy analysis,quasi-experiments,research design},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wing et al_2018_Designing Difference in Difference Studies.pdf}
}

@article{wingDesigningDifferenceinDifferenceStudies2024,
  title = {Designing {{Difference-in-Difference Studies}} with {{Staggered Treatment Adoption}}: {{Key Concepts}} and {{Practical Guidelines}}},
  shorttitle = {Designing {{Difference-in-Difference Studies}} with {{Staggered Treatment Adoption}}},
  author = {Wing, Coady and Yozwiak, Madeline and Hollingsworth, Alex and Freedman, Seth and Simon, Kosali},
  year = {2024},
  month = may,
  journal = {Annual Review of Public Health},
  volume = {45},
  number = {Volume 45, 2024},
  pages = {485--505},
  publisher = {Annual Reviews},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev-publhealth-061022-050825},
  urldate = {2024-07-10},
  abstract = {Difference-in-difference (DID) estimators are a valuable method for identifying causal effects in the public health researcher\&apos;s toolkit. A growing methods literature points out potential problems with DID estimators when treatment is staggered in adoption and varies with time. Despite this, no practical guide exists for addressing these new critiques in public health research. We illustrate these new DID concepts with step-by-step examples, code, and a checklist. We draw insights by comparing the simple 2 {\texttimes} 2 DID design (single treatment group, single control group, two time periods) with more complex cases: additional treated groups, additional time periods of treatment, and treatment effects possibly varying over time. We outline newly uncovered threats to causal interpretation of DID estimates and the solutions the literature has proposed, relying on a decomposition that shows how the more complex DIDs are an average of simpler 2 {\texttimes} 2 DID subexperiments.},
  langid = {english},
  file = {/Users/nseewald/Zotero/storage/UWSWATQ2/annurev-publhealth-061022-050825.html}
}

@article{winkensOptimalNumberRepeated2006,
  title = {Optimal Number of Repeated Measures and Group Sizes in Clinical Trials with Linearly Divergent Treatment Effects},
  author = {Winkens, Bjorn and Schouten, Hubert J.A. and {van Breukelen}, Gerard J.P. and Berger, Martijn P.F.},
  year = {2006},
  month = feb,
  journal = {Contemporary Clinical Trials},
  volume = {27},
  number = {1},
  pages = {57--69},
  issn = {15517144},
  doi = {10.1016/J.CCT.2005.09.005},
  urldate = {2018-11-17},
  abstract = {The effect of number of repeated measures on the variance of the generalized least squares (GLS) treatment effect estimator is considered assuming a linearly divergent treatment effect, equidistant time-points and either a fixed number of subjects or a fixed study budget. The optimal combination of group sizes and number of repeated measures is calculated by minimizing this variance subject to a linear cost function. For a fixed number of subjects, the variance of the GLS treatment effect estimator can be decreased by adding intermediate measures per subject. This decrease is relatively large if a) the covariance structure is compound symmetric or b) the structure approaches compound symmetry and the correlation between two repeated measures does not exceed 0.80, or c) the correlation between two repeated measures does not exceed 0.60 if the time-lag goes to zero. In case the sample sizes and number of repeated measures are limited by budget constraints and the covariance structure includes a first-order auto-regression part, two repeated measures per subject yield highly efficient treatment effect estimators. Otherwise, it is more efficient to have more than two repeated measures. If the covariance structure is unknown, the optimal design based on a first-order auto-regressive structure with measurement error is preferable in terms of robustness against misspecification of the covariance structure. The numerical results are illustrated by three examples.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero}
}

@article{winkensRandomizedClinicalTrials2007,
  title = {Randomized Clinical Trials with a Pre- and a Post-Treatment Measurement: {{Repeated}} Measures versus {{ANCOVA}} Models},
  shorttitle = {Randomized Clinical Trials with a Pre- and a Post-Treatment Measurement},
  author = {Winkens, Bjorn and {van Breukelen}, Gerard J. P. and Schouten, Hubert J. A. and Berger, Martijn P. F.},
  year = {2007},
  month = nov,
  journal = {Contemporary Clinical Trials},
  volume = {28},
  number = {6},
  pages = {713--719},
  issn = {1551-7144},
  doi = {10/dmg4hr},
  urldate = {2020-01-17},
  abstract = {Repeated measures (RM) and ANCOVA models are compared with respect to treatment effect estimation in randomized clinical trials with a pre- and a post-treatment measure. The covariance matrices of repeated measures are assumed to be I) homogeneous or II) heterogeneous across groups. In situation I, ANCOVA is preferred to RM, because the estimated variance of the treatment effect estimator is unbiased for ANCOVA and biased downwards for RM. In situation II, RM with Kenward and Roger's adjustment is preferred to ANCOVA, because the ANCOVA variance estimator does not correct for unknown pre-treatment expectation. The results are illustrated with an example.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Winkens et al_2007_Randomized clinical trials with a pre- and a post-treatment measurement.pdf;/Users/nseewald/Zotero/storage/YNVTIRF9/S1551714407000481.html}
}

@article{witkiewitzDevelopmentEvaluationMobile2014,
  title = {Development and Evaluation of a Mobile Intervention for Heavy Drinking and Smoking among College Students.},
  author = {Witkiewitz, Katie and Desai, Sruti A. and Bowen, Sarah and Leigh, Barbara C. and Kirouac, Megan and Larimer, Mary E.},
  year = {2014},
  journal = {Psychology of Addictive Behaviors},
  volume = {28},
  number = {3},
  pages = {639--650},
  issn = {1939-1501, 0893-164X},
  doi = {10.1037/a0034747},
  urldate = {2018-10-12},
  abstract = {Nearly all college student smokers also drink alcohol, and smoking and heavy episodic drinking (HED) commonly co-occur. However, few studies have examined the factors that concurrently influence smoking and HED among college students and, to date, no interventions have been developed that target both HED and smoking in this population. The objective of the current study was to develop and evaluate a mobile feedback intervention that targets HED and smoking. Participants (N ϭ 94) were non-treatmentseeking college students (Mage ϭ 20.5 years, SD ϭ 1.7) who engaged in at least a single HED episode in the past 2 weeks and reported concurrent smoking and drinking at least once a week. Participants were randomized to receive either the mobile intervention for 14 days, complete mobile assessments (without intervention) for 14 days, or complete minimal assessments (without intervention or mobile assessments). At a 1-month follow-up, compared with the minimal assessment condition, we observed significant reductions in the number of cigarettes per smoking day in both the mobile intervention (d ϭ 0.55) and mobile assessment (d ϭ 0.45) conditions. Among those randomized to the mobile intervention, receiving more modules of the intervention was significantly associated with a lower likelihood of any drinking during the 14-day assessment period and significant reductions in smoking at 1-month follow-up. The mobile intervention did not result in significant reductions in HED or concurrent smoking and drinking. Future research should continue to examine ways of using technology and the real-time environment to improve interventions for HED and smoking.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Witkiewitz et al_2014_Development and evaluation of a mobile intervention for heavy drinking and.pdf}
}

@article{wittemanGestimationCausalEffects1998,
  title = {G-Estimation of {{Causal Effects}}: {{Isolated Systolic Hypertension}} and {{Cardiovascular Death}} in the {{Framingham Heart Study}}},
  shorttitle = {G-Estimation of {{Causal Effects}}},
  author = {Witteman, J. C. M. and D'Agostino, R. B. and Stijnen, T. and Kannel, W. B. and Cobb, J. C. and {de Ridder}, M. A. J. and Hofman, A. and Robins, J. M.},
  year = {1998},
  month = aug,
  journal = {American Journal of Epidemiology},
  volume = {148},
  number = {4},
  pages = {390--401},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/oxfordjournals.aje.a009658},
  urldate = {2018-10-12},
  langid = {english},
  keywords = {Adult,Cardiovascular Diseases/*etiology/mortality,Epidemiologic Methods,Female,Humans,Hypertension/*complications,Male,Massachusetts/epidemiology,Middle Aged,Models,Risk Factors,Statistical},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Witteman et al_1998_G-estimation of Causal Effects.pdf}
}

@article{wooldridgeClusterSampleMethodsApplied2003,
  title = {Cluster-{{Sample Methods}} in {{Applied Econometrics}}},
  author = {Wooldridge, Jeffrey M.},
  year = {2003},
  journal = {The American Economic Review},
  volume = {93},
  number = {2},
  eprint = {3132213},
  eprinttype = {jstor},
  pages = {133--138},
  publisher = {American Economic Association},
  issn = {0002-8282},
  urldate = {2025-01-16},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wooldridge - 2003 - Cluster-Sample Methods in Applied Econometrics.pdf}
}

@article{wooldridgeSimpleApproachesNonlinear2023,
  title = {Simple Approaches to Nonlinear Difference-in-Differences with Panel~Data},
  author = {Wooldridge, Jeffrey M},
  year = {2023},
  month = sep,
  journal = {The Econometrics Journal},
  volume = {26},
  number = {3},
  pages = {C31-C66},
  issn = {1368-4221},
  doi = {10.1093/ectj/utad016},
  urldate = {2024-03-02},
  abstract = {I derive simple, flexible strategies for difference-in-differences settings where the nature of the response variable may warrant a nonlinear model. I allow for general staggered interventions, with and without covariates. Under an index version of parallel trends, I show that average treatment effects on the treated (ATTs) are identified for each cohort and calendar time period in which a cohort was subjected to the intervention. The pooled quasi-maximum likelihood estimators in the linear exponential family extend pooled ordinary least squares estimation of linear models. By using the conditional mean associated with the canonical link function, imputation and pooling across the entire sample produce identical estimates. Generally, pooled estimation results in very simple computation of the ATTs and their standard errors. The leading cases are a logit functional form for binary and fractional outcomes---combined with the Bernoulli quasi-log likelihood (QLL)---and an exponential mean combined with the Poisson QLL.},
  keywords = {_tablet},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wooldridge_2023_Simple approaches to nonlinear difference-in-differences with panel data.pdf;/Users/nseewald/Zotero/storage/NDSD8L6G/7250479.html}
}

@techreport{wooldridgeTwoWayFixedEffects2021,
  title = {Two-{{Way Fixed Effects}}, the {{Two-Way Mundlak Regression}}, and {{Difference-in-Differences Estimators}}},
  author = {Wooldridge, Jeffrey},
  year = {2021},
  month = aug,
  abstract = {I establish the equivalence between the two-way fixed effects (TWFE) estimator and an estimator obtained from a pooled ordinary least squares regression that includes unit-specific time averages and time-period specific cross-sectional averages, which I call the two-way Mundlak (TWM) regression. This equivalence furthers our understanding of the anatomy of TWFE, and has several applications. The equivalence between TWFE and TWM implies that various estimators used for intervention analysis-with a common entry time into treatment or staggered entry, with or without covariates-can be computed using TWFE or pooled OLS regressions that control for time-constant treatment intensities, covariates, and interactions between them. The approach allows considerable heterogeneity in treatment effects across treatment intensity, calendar time, and covariates. The equivalence implies that standard strategies for heterogeneous trends are available to relax the common trends assumption. Further, the two-way Mundlak regression is easily adapted to nonlinear models such as exponential models and logit and probit models.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wooldridge_2021_Two-Way Fixed Effects, the Two-Way Mundlak Regression, and.pdf}
}

@article{wuAnalysisLongitudinalSurvival2011,
  title = {Analysis of {{Longitudinal}} and {{Survival Data}}: {{Joint Modeling}}, {{Inference Methods}}, and {{Issues}}},
  shorttitle = {Analysis of {{Longitudinal}} and {{Survival Data}}},
  author = {Wu, Lang and Liu, Wei and Yi, Grace Y. and Huang, Yangxin},
  year = {2011},
  month = dec,
  journal = {Journal of Probability and Statistics},
  volume = {2012},
  pages = {e640153},
  publisher = {Hindawi},
  issn = {1687-952X},
  doi = {10.1155/2012/640153},
  urldate = {2024-01-11},
  abstract = {In the past two decades, joint models of longitudinal and survival data have received much attention in the literature. These models are often desirable in the following situations: (i) survival models with measurement errors or missing data in time-dependent covariates, (ii) longitudinal models with informative dropouts, and (iii) a survival process and a longitudinal process are associated via latent variables. In these cases, separate inferences based on the longitudinal model and the survival model may lead to biased or inefficient results. In this paper, we provide a brief overview of joint models for longitudinal and survival data and commonly used methods, including the likelihood method and two-stage methods.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wu et al_2011_Analysis of Longitudinal and Survival Data.pdf}
}

@article{wuInterimMonitoringSequential2023,
  title = {Interim Monitoring in Sequential Multiple Assignment Randomized Trials},
  author = {Wu, Liwen and Wang, Junyao and Wahed, Abdus S.},
  year = {2023},
  journal = {Biometrics},
  volume = {79},
  number = {1},
  pages = {368--380},
  issn = {1541-0420},
  doi = {10.1111/biom.13562},
  urldate = {2025-03-11},
  abstract = {A sequential multiple assignment randomized trial (SMART) facilitates the comparison of multiple adaptive treatment strategies (ATSs) simultaneously. Previous studies have established a framework to test the homogeneity of multiple ATSs by a global Wald test through inverse probability weighting. SMARTs are generally lengthier than classical clinical trials due to the sequential nature of treatment randomization in multiple stages. Thus, it would be beneficial to add interim analyses allowing for an early stop if overwhelming efficacy is observed. We introduce group sequential methods to SMARTs to facilitate interim monitoring based on the multivariate chi-square distribution. Simulation studies demonstrate that the proposed interim monitoring in SMART (IM-SMART) maintains the desired type I error and power with reduced expected sample size compared to the classical SMART. Finally, we illustrate our method by reanalyzing a SMART assessing the effects of cognitive behavioral and physical therapies in patients with knee osteoarthritis and comorbid subsyndromal depressive symptoms.},
  copyright = {{\copyright} 2021 The International Biometric Society.},
  langid = {english},
  keywords = {adaptive treatment strategy,dynamic treatment regime,group sequential analysis,interim monitoring,sequential multiple assignment randomized trial},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wu et al. - 2023 - Interim monitoring in sequential multiple assignment randomized trials.pdf;/Users/nseewald/Zotero/storage/VS2RJLBM/biom.html}
}

@article{wuJackknifeBootstrapOther1986,
  title = {Jackknife, {{Bootstrap}} and {{Other Resampling Methods}} in {{Regression Analysis}}},
  author = {Wu, C. F. J.},
  year = {1986},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {14},
  number = {4},
  pages = {1261--1295},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176350142},
  urldate = {2022-06-16},
  abstract = {Motivated by a representation for the least squares estimator, we propose a class of weighted jackknife variance estimators for the least squares estimator by deleting any fixed number of observations at a time. They are unbiased for homoscedastic errors and a special case, the delete-one jackknife, is almost unbiased for heteroscedastic errors. The method is extended to cover nonlinear parameters, regression \$M\$-estimators, nonlinear regression and generalized linear models. Interval estimators can be constructed from the jackknife histogram. Three bootstrap methods are considered. Two are shown to give biased variance estimators and one does not have the bias-robustness property enjoyed by the weighted delete-one jackknife. A general method for resampling residuals is proposed. It gives variance estimators that are bias-robust. Several bias-reducing estimators are proposed. Some simulation results are reported.},
  keywords = {$M$-regression,62G05,62J02,62J05,balanced residuals,bias reduction,bias-robustness,bootstrap,Fieller's linterval,generalized linear models,jackknife percentile,Linear regression,Nonlinear regression,representation of the least squares estimator,variable jackknife,Weighted jackknife},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wu_1986_Jackknife, Bootstrap and Other Resampling Methods in Regression Analysis.pdf;/Users/nseewald/Zotero/storage/3PMRPEIU/1176350142.html}
}

@article{wyattReflexologyMeditativePractices2021,
  title = {Reflexology and Meditative Practices for Symptom Management among People with Cancer: {{Results}} from a Sequential Multiple Assignment Randomized Trial},
  shorttitle = {Reflexology and Meditative Practices for Symptom Management among People with Cancer},
  author = {Wyatt, Gwen and Lehto, Rebecca and {Guha-Niyogi}, Pratim and Brewer, Sarah and Victorson, David and Pace, Thaddeus and Badger, Terry and Sikorskii, Alla},
  year = {2021},
  journal = {Research in Nursing \& Health},
  volume = {44},
  number = {5},
  pages = {796--810},
  issn = {1098-240X},
  doi = {10.1002/nur.22169},
  urldate = {2022-10-11},
  abstract = {Optimal sequencing of complementary therapies can help improve symptom management through nonpharmacological approaches. A 12-week sequential multiple assignment randomized trial comparing home-based reflexology and meditative practices on severity of fatigue and other symptoms was conducted among patients with cancer and their informal caregivers. Dyads were initially randomized to reflexology (N = 150), meditative practices (N = 150), or control (N = 47). If patient's fatigue did not improve (nonresponse) after 4 weeks of reflexology or meditative practices, the dyad was rerandomized to either add the other therapy or continue with the original therapy for weeks 5--8. Four decision rules (DRs) were compared: (1) Initiating reflexology, and if nonresponse on fatigue after 4 weeks, continue with reflexology for another 4 weeks, thus providing a higher dose; (2) Initiating reflexology, and if nonresponse on fatigue after 4 weeks, add meditative practices for the next 4 weeks; (3) Initiating meditative practices, and if nonresponse on fatigue after 4 weeks, continue meditative practices for another 4 weeks, thus providing a higher dose; and (4) Initiating meditative practices, and if nonresponse on fatigue after 4 weeks, add reflexology for the next 4 weeks. Symptoms were evaluated weekly using the M.D. Anderson Symptom Inventory (MDASI). Clinically, nurses can recommend either therapy since no differences were found among the 4 DRs, with the exception of lower severity for summed MDASI symptoms at week 8 for the use of reflexology only (DR-1) versus DR-2 (sequencing reflexology to meditative practices). Adding the other therapy for nonresponders after 4 weeks may not be warranted.},
  langid = {english},
  keywords = {cancer,caregivers,meditative practices,reflexology,SMART design},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Wyatt et al_2021_Reflexology and meditative practices for symptom management among people with.pdf;/Users/nseewald/Zotero/storage/7HZXCXWX/nur.html}
}

@article{xueIncorporatingCorrelationMultivariate2010,
  title = {Incorporating {{Correlation}} for {{Multivariate Failure Time Data When Cluster Size Is Large}}},
  author = {Xue, L. and Wang, L. and Qu, A.},
  year = {2010},
  journal = {Biometrics},
  volume = {66},
  number = {2},
  eprint = {40663232},
  eprinttype = {jstor},
  pages = {393--404},
  doi = {10.1111/j.1541-0420.2009.01307.x},
  abstract = {We propose a new estimation method for multivariate failure time data using the quadratic inference function (QIF) approach. The proposed method efficiently incorporates within-cluster correlations. Therefore, it is more efficient than those that ignore within-cluster correlation. Furthermore, the proposed method is easy to implement. Unlike the weighted estimating equations in Cai and Prentice (1995, Biometrika 82, 151-164), it is not necessary to explicitly estimate the correlation parameters. This simplification is particularly useful in analyzing data with large cluster size where it is difficult to estimate intracluster correlation. Under certain regularity conditions, we show the consistency and asymptotic normality of the proposed QIF estimators. A chi-squared test is also developed for hypothesis testing. We conduct extensive Monte Carlo simulation studies to assess the finite sample performance of the proposed methods. We also illustrate the proposed methods by analyzing primary biliary cirrhosis (PBC) data. Key WORDS: Chi-squared test; Correlated failure times; Cox's model; Generalized estimating equation; Marginal hazard rate; Quadratic inference function.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Xue et al_2010_Incorporating Correlation for Multivariate Failure Time Data When Cluster Size.pdf}
}

@article{xuSMARTpSMARTDesign2019,
  title = {{{SMARTp}}: {{A SMART}} Design for Non-Surgical Treatments of Chronic Periodontitis with Spatially-Referenced and Non-Randomly Missing Skewed Outcomes},
  shorttitle = {{{SMARTp}}},
  author = {Xu, Jing and Bandyopadhyay, Dipankar and Mirzaei, Sedigheh and Michalowicz, Bryan and Chakraaborty, Bibhas},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.09386 [stat]},
  eprint = {1902.09386},
  primaryclass = {stat},
  urldate = {2019-07-09},
  abstract = {This paper proposes dynamic treatment regimes (DRTs) for choosing individualized effective treatment strategies of chronic periodontitis. The proposed DTRs are studied via SMARTp -- a two-stage sequential multiple assignment randomized trial (SMART) design. For this design, we propose a statistical analysis plan and a novel cluster-level sample size calculation method that factors in typical features of periodontal responses, such as non-Gaussianity, spatial clustering, and non-random missingness. Here, each patient/subject is viewed as a cluster, and a tooth within a subject's mouth is viewed as an individual unit inside the cluster, with the tooth-level covariance structure described by a conditionally autoregressive process. To accommodate possible skewness and tail behavior, the tooth-level clinical attachment level (CAL) response is assumed to be skew-t, with the non-randomly missing structure captured via a shared parameter model corresponding to the missingness indicator. The proposed method considers mean comparison for the regimes with or without sharing an initial treatment, where the expected values and corresponding variances or covariance for the sample means of a pair of DTRs are derived by the inverse probability weighting and method of moments. Simulation studies are conducted to investigate the finite-sample performance of the proposed sample size formula under a variety of outcome-generating scenarios. A major contribution of this work is the implementation of the sample size formula via a R package available in GitHub.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Design,No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Xu et al_2019_SMARTp.pdf}
}

@article{xuUsingMarginalStructural,
  title = {Using Marginal Structural Models to Analyze the Impact of Subsequent Therapy on the Treatment Effect in Survival Data: {{Simulations}} and Clinical Trial Examples},
  shorttitle = {Using Marginal Structural Models to Analyze the Impact of Subsequent Therapy on the Treatment Effect in Survival Data},
  author = {Xu, Qing and Przepiorka, Donna},
  journal = {Pharmaceutical Statistics},
  volume = {n/a},
  number = {n/a},
  issn = {1539-1612},
  doi = {10/gmhkp9},
  urldate = {2021-08-17},
  abstract = {We explore the impact of time-varying subsequent therapy on the statistical power and treatment effects in survival analysis. The marginal structural model (MSM) with stabilized inverse probability treatment weights (sIPTW) was used to account for the effects due to the subsequent therapy. Simulations were performed to compare the MSM-sIPTW method with the conventional method without accounting for the time-varying covariate such as subsequent therapy that is dependent on the initial response of the treatment effect. The results of the simulations indicated that the statistical power, thereby the Type I error, of the trials to detect the frontline treatment effect could be inflated if no appropriate adjustment was made for the impact due to the add-on effects of the subsequent therapy. Correspondingly, the hazard ratio between the treatment groups may be overestimated by the conventional analysis methods. In contrast, MSM-sIPTW can maintain the Type I error rate and gave unbiased estimates of the hazard ratio for the treatment. Two real examples were used to discuss the potential clinical implications. The study demonstrated the importance of accounting for time-varying subsequent therapy for obtaining unbiased interpretation of data.},
  langid = {english},
  keywords = {inverse probability treatment weights,marginal structural models,subsequent therapy,survival analysis,time-varying covariate},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Xu_Przepiorka_Using marginal structural models to analyze the impact of subsequent therapy on.pdf}
}

@article{yanEstimatingEquationsAssociation2004,
  title = {Estimating Equations for Association Structures},
  author = {Yan, Jun and Fine, Jason},
  year = {2004},
  journal = {Statistics in Medicine},
  volume = {23},
  number = {6},
  pages = {859--874},
  issn = {02776715},
  doi = {10.1002/sim.1650},
  abstract = {This paper investigates generalized estimating equations for association parameters, which are frequently of interest in family studies, with emphasis on covariance estimation. Separate link functions are used to connect the mean, the scale, and the correlation to linear predictors involving possibly different sets of covariates, and separate estimating equations are proposed for the three sets of parameters. Simulations show that the robust 'sandwich' variance estimator and the jackknife variance estimator for the correlation parameters are generally close to the empirical variance for the sample size of 50 clusters. The results contradict Ziegler et al. and Kastner and Ziegler, where the 'sandwich' estimator obtained from the software MAREG was shown to be unsuitable for practical usage. The problem appears to arise because the MAREG variance estimator does not account for variability in estimation of the scale parameters, but may be valid with fixed scale. We also find that the formula for the approximate jackknife variance estimator in Ziegler et al. is deficient, resulting in systematic deviations from the fully iterated jackknife variance estimator. A general jackknife formula is provided and performs well in numerical studies. Data from a study on the genetics of alcoholism is used to illustrate the importance of reliable variance estimation in biomedical applications.},
  pmid = {15027075},
  keywords = {Estimating equations,Jackknife,Marginal regression,Sandwich estimator},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Yan_Fine_2004_Estimating equations for association structures.pdf}
}

@article{yanSampleSizeCalculation,
  title = {Sample Size Calculation Based on Precision for Pilot Sequential Multiple Assignment Randomized Trial ({{SMART}})},
  author = {Yan, Xiaoxi and Ghosh, Palash and Chakraborty, Bibhas},
  journal = {Biometrical Journal},
  volume = {n/a},
  number = {n/a},
  issn = {1521-4036},
  doi = {10/gg2gg7},
  urldate = {2020-06-17},
  abstract = {The sequential multiple assignment randomized trial (SMART) is a design used to develop dynamic treatment regimes (DTRs). Given that DTRs are generally less well researched, pilot SMART studies are often necessary. One challenge in pilot SMART is to determine the sample size such that it is small yet meaningfully informative for future full-fledged SMART. Here, we develop a precision-based approach, where the calculated sample size confines the marginal mean outcome of a DTR within a prespecified margin of error. The sample size calculations will be presented for two-stage SMARTs, and for various common outcome types.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Yan et al_Sample size calculation based on precision for pilot sequential multiple.pdf;/Users/nseewald/Zotero/storage/QX8YKRR5/bimj.html}
}

@article{yapDevelopmentTherapeuticCombinations2013,
  title = {Development of {{Therapeutic Combinations Targeting Major Cancer Signaling Pathways}}},
  author = {Yap, Timothy A. and Omlin, Aurelius and {de Bono}, Johann S.},
  year = {2013},
  month = apr,
  journal = {Journal of Clinical Oncology},
  volume = {31},
  number = {12},
  pages = {1592--1605},
  publisher = {Wolters Kluwer},
  issn = {0732-183X},
  doi = {10.1200/JCO.2011.37.6418},
  urldate = {2022-10-12},
  abstract = {Signaling networks play key homeostatic processes in living organisms but are commonly hijacked in oncogenesis. Prominent examples include genetically altered receptor tyrosine kinases and dysregulated intracellular signaling molecules. The discovery and development of targeted therapies against such oncogenic proteins has imparted clinical benefit. Nevertheless, concerns remain about the limited single-agent efficacy and narrow therapeutic indices of many of these antitumor agents. Moreover, it is apparent that oncogenic proteins comprise complex signaling networks that interact through crosstalk and feedback loops, which modify therapeutic vulnerability. These complexities mandate the study of drug combinations, which will also become necessary to reverse tumor drug resistance. Here, we outline the challenges associated with rational drug codevelopment strategies, with a focus on the importance of analytically validated biomarkers for patient selection and pharmacokinetic-pharmacodynamic (PK-PD) studies. Overall, the most informative clinical studies of novel combinations will have the following characteristics: robust scientific hypotheses leading to their selection; supportive preclinical data from contextually appropriate preclinical model systems; sufficient preclinical PK data to inform on the risk of drug-drug interactions; and detailed PD studies to determine the biologically active dose range for each agent. Toward this end, several novel clinical trial designs may be envisioned to accelerate successful drug combination development while minimizing the risk of late drug combination attrition. Although considerable challenges remain, these efforts may enable important steps to be taken toward more durable therapeutic control of many cancers.}
}

@article{yatesTestSignificanceContingency1984,
  title = {Test of {{Significance}} for 2 {\texttimes} 2 {{Contingency Tables}}},
  author = {Yates, F.},
  year = {1984},
  journal = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {147},
  number = {3},
  eprint = {10.2307/2981577},
  eprinttype = {jstor},
  pages = {426},
  issn = {00359238},
  doi = {10.2307/2981577},
  urldate = {2018-10-12},
  abstract = {Fisher's exact test, and the approximation to it by the continuity-corrected X2 test, have repeatedly been attacked over the past 40 years, recently with the support of extensive computer exercises. The present paper argues, on commonsense grounds, supported by simple examples, that these attacks are misconceived, and are mainly due to uncritical acceptance of the Neyman-Pearson approach to tests of significance, the use of nominal levels, and refusal to accept the arguments for conditioning on the margins.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Yates_1984_Test of Significance for 2 × 2 Contingency Tables.pdf}
}

@article{yellandSampleSizeCalculations2017,
  title = {Sample Size Calculations for Randomised Trials Including Both Independent and Paired Data},
  author = {Yelland, Lisa N. and Sullivan, Thomas R. and Price, David J. and Lee, Katherine J.},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {8},
  pages = {1227--1239},
  issn = {1097-0258},
  doi = {10/ggfrww},
  urldate = {2019-12-21},
  abstract = {Randomised trials including a mixture of independent and paired data arise in many areas of health research, yet methods for determining the sample size for such trials are lacking. We derive design effects algebraically assuming clustering because of paired data will be taken into account in the analysis using generalised estimating equations with either an independence or exchangeable working correlation structure. Continuous and binary outcomes are considered, along with three different methods of randomisation: cluster randomisation, individual randomisation and randomisation to opposite treatment groups. The design effect is shown to depend on the intracluster correlation coefficient, proportion of observations belonging to a pair, working correlation structure, type of outcome and method of randomisation. The derived design effects are validated through simulation and example calculations are presented to illustrate their use in sample size planning. These design effects will enable appropriate sample size calculations to be performed for future randomised trials including both independent and paired data. Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Yelland et al_2017_Sample size calculations for randomised trials including both independent and.pdf;/Users/nseewald/Zotero/storage/H5MDV6FL/sim.html}
}

@article{yongPrevalenceChronicPain2022,
  title = {Prevalence of Chronic Pain among Adults in the {{United States}}},
  author = {Yong, R. Jason and Mullins, Peter M. and Bhattacharyya, Neil},
  year = {2022},
  month = feb,
  journal = {PAIN},
  volume = {163},
  number = {2},
  pages = {e328},
  issn = {0304-3959},
  doi = {10.1097/j.pain.0000000000002291},
  urldate = {2023-06-09},
  abstract = {Chronic pain is associated with reduced quality of life, increased medical expenditures, and significant economic costs. Chronic pain is among the most common chronic conditions in the United States, although estimates vary widely regarding its precise prevalence. Understanding the scope of the problem using the most contemporaneous data is therefore an important goal. This study sought to determine the prevalence of chronic pain and its impacts among adults in the United States using the National Health Interview Survey, a household-based annual survey of self-reported health status of U.S. adults that can be used to generate national-level estimates. Using a chronic pain module introduced in the 2019 edition of National Health Interview Survey, we found that 50.2 million adults (20.5\%) reported pain on most days or every day. The most common pain locations were back pain and hip, knee, or foot pain. The most commonly used management strategies for chronic pain were physical therapy and massage. Respondents with chronic pain reported limitations in daily functioning, including social activities and activities of daily living. Respondents with chronic pain reported significantly more workdays missed compared with those without chronic pain (10.3 vs 2.8, P {$<$} 0.001). Overall, these findings indicate that more than 1 in 5 adults in America experiences chronic pain; additional attention to managing the burden of this disease is warranted.},
  langid = {american},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Yong et al_2022_Prevalence of chronic pain among adults in the United States.pdf;/Users/nseewald/Zotero/storage/B8A4LHMX/Prevalence_of_chronic_pain_among_adults_in_the.31.html}
}

@article{yuInverseProbabilityWeighted2023,
  title = {An Inverse Probability Weighted Regression Method That Accounts for Right-Censoring for Causal Inference with Multiple Treatments and a Binary Outcome},
  author = {Yu, Youfei and Zhang, Min and Mukherjee, Bhramar},
  year = {2023},
  journal = {Statistics in Medicine},
  volume = {42},
  number = {20},
  pages = {3699--3715},
  issn = {1097-0258},
  doi = {10.1002/sim.9826},
  urldate = {2024-08-30},
  abstract = {Comparative effectiveness research often involves evaluating the differences in the risks of an event of interest between two or more treatments using observational data. Often, the post-treatment outcome of interest is whether the event happens within a pre-specified time window, which leads to a binary outcome. One source of bias for estimating the causal treatment effect is the presence of confounders, which are usually controlled using propensity score-based methods. An additional source of bias is right-censoring, which occurs when the information on the outcome of interest is not completely available due to dropout, study termination, or treatment switch before the event of interest. We propose an inverse probability weighted regression-based estimator that can simultaneously handle both confounding and right-censoring, calling the method CIPWR, with the letter C highlighting the censoring component. CIPWR estimates the average treatment effects by averaging the predicted outcomes obtained from a logistic regression model that is fitted using a weighted score function. The CIPWR estimator has a double robustness property such that estimation consistency can be achieved when either the model for the outcome or the models for both treatment and censoring are correctly specified. We establish the asymptotic properties of the CIPWR estimator for conducting inference, and compare its finite sample performance with that of several alternatives through simulation studies. The methods under comparison are applied to a cohort of prostate cancer patients from an insurance claims database for comparing the adverse effects of four candidate drugs for advanced stage prostate cancer.},
  langid = {english},
  keywords = {causal inference,claims data,double robustness,observational studies,right censoring},
  file = {/Users/nseewald/Zotero/storage/C6MA5EJE/Yu et al. - 2023 - An inverse probability weighted regression method .pdf;/Users/nseewald/Zotero/storage/7XCPRITM/sim.html}
}

@article{yurochkinLearningFairPredictors2019,
  title = {Learning Fair Predictors with {{Sensitive Subspace Robustness}}},
  author = {Yurochkin, Mikhail and Bower, Amanda and Sun, Yuekai},
  year = {2019},
  month = jun,
  journal = {arXiv:1907.00020 [cs, stat]},
  eprint = {1907.00020},
  primaryclass = {cs, stat},
  urldate = {2019-07-21},
  abstract = {We consider an approach to training machine learning systems that are fair in the sense that their performance is invariant under certain perturbations to the features. For example, the performance of a resume screening system should be invariant under changes to the name of the applicant or switching the gender pronouns. We connect this intuitive notion of algorithmic fairness to individual fairness and study how to certify ML algorithms as algorithmically fair. We also demonstrate the effectiveness of our approach on three machine learning tasks that are susceptible to gender and racial biases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Yurochkin et al_2019_Learning fair predictors with Sensitive Subspace Robustness.pdf;/Users/nseewald/Zotero/storage/MHJZLP4X/1907.html}
}

@article{zaykinPvalueBasedAnalysis2010,
  title = {P-Value Based Analysis for Shared Controls Design in Genome-Wide Association Studies},
  author = {Zaykin, Dmitri V. and Kozbur, Damian O.},
  year = {2010},
  journal = {Genetic Epidemiology},
  volume = {34},
  number = {7},
  pages = {725--738},
  issn = {1098-2272},
  doi = {10.1002/gepi.20536},
  urldate = {2023-10-27},
  abstract = {An appealing genome-wide association study design compares one large control group against several disease samples. A pioneering study by the Wellcome Trust Case Control Consortium that employed such a design has identified multiple susceptibility regions, many of which have been independently replicated. While reusing a control sample provides effective utilization of data, it also creates correlation between association statistics across diseases. An observation of a large association statistic for one of the diseases may greatly increase chances of observing a spuriously large association for a different disease. Accounting for the correlation is also particularly important when screening for SNPs that might be involved in a set of diseases with overlapping etiology. We describe methods that correct association statistics for dependency due to shared controls, and we describe ways to obtain a measure of overall evidence and to combine association signals across multiple diseases. The methods we describe require no access to individual subject data, instead, they efficiently utilize information contained in P-values for association reported for individual diseases. P-value based combined tests for association are flexible and essentially as powerful as the approach based on aggregating the individual subject data. Genet. Epidemiol. 34:725--738, 2010.{\copyright} 2010 Wiley-Liss, Inc.},
  langid = {english},
  keywords = {combining correlated P-values,GWAS,meta-analysis,multiple testing,shared controls},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zaykin_Kozbur_2010_P-value based analysis for shared controls design in genome-wide association.pdf;/Users/nseewald/Zotero/storage/LK3EVTBI/gepi.html}
}

@article{zegerComment1999,
  title = {Comment},
  author = {Zeger, Scott L. and Liang, Kung-Yee},
  year = {1999},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {447},
  pages = {706--707},
  issn = {0162-1459},
  doi = {10/ggfrwr},
  urldate = {2019-12-21},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zeger_Liang_1999_Comment.pdf}
}

@article{zeileisObjectorientedComputationSandwich2006,
  title = {Object-Oriented {{Computation}} of {{Sandwich Estimators}}},
  author = {Zeileis, Achim},
  year = {2006},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {16},
  pages = {1--16},
  issn = {1548-7660},
  doi = {10.18637/jss.v016.i09},
  urldate = {2021-10-14},
  abstract = {Sandwich covariance matrix estimators are a popular tool in applied regression modeling for performing inference that is robust to certain types of model misspecification. Suitable implementations are available in the R system for statistical computing for certain model fitting functions only (in particular lm()), but not for other standard regression functions, such as glm(), nls(), or survreg().  Therefore, conceptual tools and their translation to computational tools in the package sandwich are discussed, enabling the computation of sandwich estimators in general parametric models. Object orientation can be achieved by providing a few extractor functions' most importantly for the empirical estimating functions' from which various types of sandwich estimators can be computed.},
  copyright = {Copyright (c) 2006 Achim Zeileis},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zeileis_2006_Object-oriented Computation of Sandwich Estimators.pdf}
}

@article{zeileisVariousVersatileVariances2020,
  title = {Various {{Versatile Variances}}: {{An Object-Oriented Implementation}} of {{Clustered Covariances}} in {{R}}},
  shorttitle = {Various {{Versatile Variances}}},
  author = {Zeileis, Achim and K{\"o}ll, Susanne and Graham, Nathaniel},
  year = {2020},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {95},
  pages = {1--36},
  issn = {1548-7660},
  doi = {10.18637/jss.v095.i01},
  urldate = {2022-01-25},
  abstract = {Clustered covariances or clustered standard errors are very widely used to account for correlated or clustered data, especially in economics, political sciences, and other social sciences. They are employed to adjust the inference following estimation of a standard least-squares regression or generalized linear model estimated by maximum likelihood. Although many publications just refer to "the" clustered standard errors, there is a surprisingly wide variety of clustered covariances, particularly due to different flavors of bias corrections. Furthermore, while the linear regression model is certainly the most important application case, the same strategies can be employed in more general models (e.g., for zero-inflated, censored, or limited responses). In R, functions for covariances in clustered or panel models have been somewhat scattered or available only for certain modeling functions, notably the (generalized) linear regression model. In contrast, an object-oriented approach to "robust" covariance matrix estimation  -  applicable beyond lm() and glm()  -  is available in the sandwich package but has been limited to the case of cross-section or time series data. Starting with sandwich 2.4.0, this shortcoming has been corrected: Based on methods for two generic functions (estfun() and bread()), clustered and panel covariances are provided in vcovCL(), vcovPL(), and vcovPC(). Moreover, clustered bootstrap covariances are provided in vcovBS(), using model update() on bootstrap samples. These are directly applicable to models from packages including MASS, pscl, countreg, and betareg, among many others. Some empirical illustrations are provided as well as an assessment of the methods' performance in a simulation study.},
  copyright = {Copyright (c) 2020 Achim Zeileis, Susanne K{\"o}ll, Nathaniel Graham},
  langid = {english},
  keywords = {R},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zeileis et al_2020_Various Versatile Variances.pdf}
}

@article{zeldowConfoundingRegressionAdjustment2021,
  title = {Confounding and Regression Adjustment in Difference-in-Differences Studies},
  author = {Zeldow, Bret and Hatfield, Laura A.},
  year = {2021},
  journal = {Health Services Research},
  volume = {56},
  number = {5},
  pages = {932--941},
  issn = {1475-6773},
  doi = {10.1111/1475-6773.13666},
  urldate = {2021-12-10},
  abstract = {Objective To define confounding bias in difference-in-difference studies and compare regression- and matching-based estimators designed to correct bias due to observed confounders. Data sources We simulated data from linear models that incorporated different confounding relationships: time-invariant covariates with a time-varying effect on the outcome, time-varying covariates with a constant effect on the outcome, and time-varying covariates with a time-varying effect on the outcome. We considered a simple setting that is common in the applied literature: treatment is introduced at a single time point and there is no unobserved treatment effect heterogeneity. Study design We compared the bias and root mean squared error of treatment effect estimates from six model specifications, including simple linear regression models and matching techniques. Data collection Simulation code is provided for replication. Principal findings Confounders in difference-in-differences are covariates that change differently over time in the treated and comparison group or have a time-varying effect on the outcome. When such a confounding variable is measured, appropriately adjusting for this confounder (ie, including the confounder in a regression model that is consistent with the causal model) can provide unbiased estimates with optimal SE. However, when a time-varying confounder is affected by treatment, recovering an unbiased causal effect using difference-in-differences is difficult. Conclusions Confounding in difference-in-differences is more complicated than in cross-sectional settings, from which techniques and intuition to address observed confounding cannot be imported wholesale. Instead, analysts should begin by postulating a causal model that relates covariates, both time-varying and those with time-varying effects on the outcome, to treatment. This causal model will then guide the specification of an appropriate analytical model (eg, using regression or matching) that can produce unbiased treatment effect estimates. We emphasize the importance of thoughtful incorporation of covariates to address confounding bias in difference-in-difference studies.},
  langid = {english},
  keywords = {difference-in-differences,matching,parallel trends,regression adjustment,time-varying confounding},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zeldow_Hatfield_2021_Confounding and regression adjustment in difference-in-differences studies.pdf;/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zeldow_Hatfield_2021_Confounding and regression adjustment in difference-in-differences studies2.pdf;/Users/nseewald/Zotero/storage/WCSXI5PT/1475-6773.html}
}

@misc{zeldowDifferenceinDifferences2019,
  title = {Difference-in-{{Differences}}},
  author = {Zeldow, Bret and Hatfield, Laura A.},
  year = {2019},
  urldate = {2023-08-16},
  howpublished = {https://diff.healthpolicydatascience.org/},
  file = {/Users/nseewald/Zotero/storage/EVYWU69R/diff.healthpolicydatascience.org.html}
}

@article{zellnerEfficientMethodEstimating1962,
  title = {An {{Efficient Method}} of {{Estimating Seemingly Unrelated Regressions}} and {{Tests}} for {{Aggregation Bias}}},
  author = {Zellner, Arnold},
  year = {1962},
  journal = {Journal of the American Statistical Association},
  volume = {57},
  number = {298},
  eprint = {2281644},
  eprinttype = {jstor},
  pages = {348--368},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2281644},
  urldate = {2022-01-25},
  abstract = {In this paper a method of estimating the parameters of a set of regression equations is reported which involves application of Aitken's generalized least-squares [1] to the whole system of equations. Under conditions generally encountered in practice, it is found that the regression coefficient estimators so obtained are at least asymptotically more efficient than those obtained by an equation-by-equation application of least squares. This gain in efficiency can be quite large if "independent" variables in different equations are not highly correlated and if disturbance terms in different equations are highly correlated. Further, tests of the hypothesis that all regression equation coefficient vectors are equal, based on "micro" and "macro" data, are described. If this hypothesis is accepted, there will be no aggregation bias. Finally, the estimation procedure and the "micro-test" for aggregation bias are applied in the analysis of annual investment data, 1935-1954, for two firms.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zellner_1962_An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for.pdf}
}

@article{zhangAddingSubjectsAdding2011,
  title = {Adding {{Subjects}} or {{Adding Measurements}} in {{Repeated Measurement Studies Under Financial Constraints}}},
  author = {Zhang, Song and Ahn, Chul},
  year = {2011},
  month = feb,
  journal = {Statistics in Biopharmaceutical Research},
  volume = {3},
  number = {1},
  pages = {54--64},
  issn = {null},
  doi = {10/bftphq},
  urldate = {2018-12-05},
  abstract = {Budget constraint is a challenge faced by investigators in planning almost every clinical trial. For a repeated measurement study, investigators need to decide whether to increase the number of participating subjects or to increase the number of repeated measurements per subject, with the ultimate goal of maximizing power for a given financial constraint. This financially constrained design problem is further complicated when taking into account things such as missing data and various correlation structures among the repeated measurements. We propose an approach that combines a GEE estimator of slope coefficients with the cost constraint. In the case where we have no missing data and the compound symmetric correlation structure, the optimal design is derived analytically. In the case where we have missing data or other correlation structures, the optimal design is identified through numerical search. We present an extensive simulation study to explore the impacts of cost ratio, missing pattern, dropout rate, and correlation structure. We also present an application example.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhang_Ahn_2011_Adding Subjects or Adding Measurements in Repeated Measurement Studies Under.pdf;/Users/nseewald/Zotero/storage/EHAS37T2/sbr.2010.html}
}

@article{zhangGEEApproachDetermine2014a,
  ids = {zhangGEEApproachDetermine2014},
  title = {A {{GEE}} Approach to Determine Sample Size for Pre- and Post-Intervention Experiments with Dropout},
  author = {Zhang, Song and Cao, Jing and Ahn, Chul},
  year = {2014},
  month = jan,
  journal = {Computational Statistics \& Data Analysis},
  volume = {69},
  pages = {114--121},
  issn = {0167-9473},
  doi = {10/ggh65b},
  urldate = {2020-01-20},
  abstract = {Pre- and post-intervention experiments are widely used in medical and social behavioral studies, where each subject is supposed to contribute a pair of observations. In this paper we investigate sample size requirement for a scenario frequently encountered by practitioners: all enrolled subjects participate in the pre-intervention phase of study, but some of them will drop out due to various reasons, thus resulting in missing values in the post-intervention measurements. Traditional sample size calculation based on McNemar's test could not accommodate missing data. Through the GEE approach, we derive a closed-form sample size formula that properly accounts for the impact of partial observations. We demonstrate that when there are no missing data, the proposed sample size estimate under the GEE approach is very close to that under McNemar's test. When there are missing data, the proposed method can lead to substantial saving in sample size. Simulation studies and an example are presented.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhang et al_2014_A GEE approach to determine sample size for pre- and post-intervention.pdf;/Users/nseewald/Zotero/storage/NRK77XSU/S016794731300282X.html}
}

@article{zhangHowManyMeasurements2011,
  title = {How Many Measurements for Time-Averaged Differences in Repeated Measurement Studies?},
  author = {Zhang, Song and Ahn, Chul},
  year = {2011},
  month = may,
  journal = {Contemporary Clinical Trials},
  volume = {32},
  number = {3},
  pages = {412--417},
  issn = {1551-7144},
  doi = {10/dwv3qn},
  urldate = {2019-11-14},
  abstract = {In many studies, investigators have perceived the number of repeated measurements as a fixed design characteristic. However, the number of repeated measurements is a design choice that can be informed by statistical considerations. In this paper, we investigate how the number of repeated measurements affects the required sample size in longitudinal studies with scheduled assessment times and a fixed total duration. It is shown that the required sample size always decreases as the number of measurements per subject increases under the compound symmetry (CS) correlation. The magnitude of sample size reduction, however, quickly shrinks to less than 5\% when the number of measurements per subject increases beyond 4. We then reveal a counterintuitive property of the AR(1) correlation structure, under which making additional measurements from each subject might increase the sample size requirement. This observation suggests that practitioners should be cautious about assuming the AR(1) model in repeated measurements studies, whether in experimental design or in data analysis. Finally, we show that by introducing measurement error into the AR(1) model, the counterintuitive behavior disappears. That is, additional measurements per subject result in reduced sample sizes.},
  langid = {english},
  keywords = {AR(1),Compound symmetry,Measurement error,Sample size,Time-averaged response},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhang_Ahn_2011_How many measurements for time-averaged differences in repeated measurement.pdf}
}

@article{zhangRobustMethodEstimating2012,
  title = {A {{Robust Method}} for {{Estimating Optimal Treatment Regimes}}},
  author = {Zhang, Baqun and Tsiatis, Anastasios A. and Laber, Eric B. and Davidian, Marie},
  year = {2012},
  journal = {Biometrics},
  volume = {68},
  number = {4},
  pages = {1010--1018},
  issn = {1541-0420},
  doi = {10/f4gxnk},
  urldate = {2020-12-13},
  abstract = {A treatment regime is a rule that assigns a treatment, among a set of possible treatments, to a patient as a function of his/her observed characteristics, hence ``personalizing'' treatment to the patient. The goal is to identify the optimal treatment regime that, if followed by the entire population of patients, would lead to the best outcome on average. Given data from a clinical trial or observational study, for a single treatment decision, the optimal regime can be found by assuming a regression model for the expected outcome conditional on treatment and covariates, where, for a given set of covariates, the optimal treatment is the one that yields the most favorable expected outcome. However, treatment assignment via such a regime is suspect if the regression model is incorrectly specified. Recognizing that, even if misspecified, such a regression model defines a class of regimes, we instead consider finding the optimal regime within such a class by finding the regime that optimizes an estimator of overall population mean outcome. To take into account possible confounding in an observational study and to increase precision, we use a doubly robust augmented inverse probability weighted estimator for this purpose. Simulations and application to data from a breast cancer clinical trial demonstrate the performance of the method.},
  copyright = {{\copyright} 2012, The International Biometric Society},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhang et al_2012_A Robust Method for Estimating Optimal Treatment Regimes.pdf;/Users/nseewald/Zotero/storage/6DMPI2U7/j.1541-0420.2012.01763.html}
}

@article{zhangTimevaryingCovariatesCoefficients2018,
  title = {Time-Varying Covariates and Coefficients in {{Cox}} Regression Models},
  author = {Zhang, Zhongheng and Reinikainen, Jaakko and Adeleke, Kazeem Adedayo and Pieterse, Marcel E. and {Groothuis-Oudshoorn}, Catharina G. M.},
  year = {2018},
  month = apr,
  journal = {Annals of Translational Medicine},
  volume = {6},
  number = {7},
  pages = {121--121},
  publisher = {AME Publishing Company},
  issn = {2305-5847, 2305-5839},
  doi = {10.21037/atm.2018.02.12},
  urldate = {2024-01-10},
  abstract = {Time-varying covariates and coefficients in Cox regression models},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhang_et_al_2018_Time-varying_covariates_and_coefficients_in_Cox_regression_models.pdf}
}

@article{zhangUsingDecisionLists2015,
  title = {Using {{Decision Lists}} to {{Construct Interpretable}} and {{Parsimonious Treatment Regimes}}},
  shorttitle = {Using Decision Lists to Construct Interpretable and Parsimonious Treatment Regimes},
  author = {Zhang, Yichi and Laber, Eric B. and Tsiatis, Anastasios and Davidian, Marie},
  year = {2015},
  month = dec,
  journal = {Biometrics},
  volume = {71},
  number = {4},
  pages = {895--904},
  issn = {0006341X},
  doi = {10.1111/biom.12354},
  urldate = {2018-10-12},
  abstract = {A treatment regime formalizes personalized medicine as a function from individual patient characteristics to a recommended treatment. A high-quality treatment regime can improve patient outcomes while reducing cost, resource consumption, and treatment burden. Thus, there is tremendous interest in estimating treatment regimes from observational and randomized studies. However, the development of treatment regimes for application in clinical practice requires the longterm, joint effort of statisticians and clinical scientists. In this collaborative process, the statistician must integrate clinical science into the statistical models underlying a treatment regime and the clinician must scrutinize the estimated treatment regime for scientific validity. To facilitate meaningful information exchange, it is important that estimated treatment regimes be interpretable in a subject-matter context. We propose a simple, yet flexible class of treatment regimes whose members are representable as a short list of if--then statements. Regimes in this class are immediately interpretable and are therefore an appealing choice for broad application in practice. We derive a robust estimator of the optimal regime within this class and demonstrate its finite sample performance using simulation experiments. The proposed method is illustrated with data from two clinical trials.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhang et al_2015_Using Decision Lists to Construct Interpretable and Parsimonious Treatment.pdf}
}

@article{zhangVarianceEstimationConstruction2006,
  title = {Variance {{Estimation}} and {{Construction}} of {{Confidence Intervals}} for {{GEE Estimator}}},
  author = {Zhang, Shenghai and Thompson, Mary E.},
  year = {2006},
  month = may,
  journal = {Journal of Modern Applied Statistical Methods},
  volume = {5},
  number = {1},
  pages = {217--229},
  issn = {1538-9472},
  doi = {10.22237/jmasm/1146457020},
  urldate = {2019-08-22},
  abstract = {The sandwich estimator, also known as the robust covariance matrix estimator, has achieved increasing use in the statistical literature as well as with the growing popularity of generalized estimating equations (GEE). A modified sandwich variance estimator is proposed, and its consistency and efficiency are studied. It is compared with other variance estimators, such as a model based estimator, the sandwich estimator and a corrected sandwich estimator. Confidence intervals for regression parameters based on these estimators are discussed. Simulation studies using clustered data to compare the performance of variance estimators are reported.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhang_Thompson_2006_Variance Estimation and Construction of Confidence Intervals for GEE Estimator.pdf}
}

@article{zhaoAdaptiveInterventionsOptimise2022,
  title = {Adaptive Interventions to Optimise the Mobile Phone-Based Smoking Cessation Support: Study Protocol for a Sequential, Multiple Assignment, Randomised Trial ({{SMART}})},
  shorttitle = {Adaptive Interventions to Optimise the Mobile Phone-Based Smoking Cessation Support},
  author = {Zhao, Sheng Zhi and Weng, Xue and Luk, Tzu Tsun and Wu, Yongda and Cheung, Derek Yee Tak and Li, William Ho Cheung and Tong, Henry and Lai, Vienna and Lam, Tai Hing and Wang, Man Ping},
  year = {2022},
  month = aug,
  journal = {Trials},
  volume = {23},
  number = {1},
  pages = {681},
  issn = {1745-6215},
  doi = {10.1186/s13063-022-06502-7},
  urldate = {2022-08-29},
  abstract = {Mobile health (mHealth) is promising in developing personalised smoking cessation interventions. By using an adaptive trial design, we aim to evaluate the effectiveness of personalised mHealth intervention in increasing smoking cessation.},
  keywords = {Adaptive trial,Instant messaging,mHealth,Smoking cessation,Stepped care},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhao et al_2022_Adaptive interventions to optimise the mobile phone-based smoking cessation.pdf;/Users/nseewald/Zotero/storage/JSSG8CQS/s13063-022-06502-7.html}
}

@article{zhaoEstimationOptimalDynamic2014,
  title = {Estimation of Optimal Dynamic Treatment Regimes},
  author = {Zhao, Ying-Qi and Laber, Eric B},
  year = {2014},
  month = aug,
  journal = {Clinical Trials: Journal of the Society for Clinical Trials},
  volume = {11},
  number = {4},
  pages = {400--407},
  issn = {1740-7745, 1740-7753},
  doi = {10/f6cjrn},
  urldate = {2019-02-26},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhao_Laber_2014_Estimation of optimal dynamic treatment regimes.pdf}
}

@article{zhaoNewStatisticalLearning2015,
  title = {New {{Statistical Learning Methods}} for {{Estimating Optimal Dynamic Treatment Regimes}}.},
  author = {Zhao, Ying-Qi and Zeng, Donglin and Laber, Eric B and Kosorok, Michael R},
  year = {2015},
  journal = {Journal of the American Statistical Association},
  volume = {110},
  number = {510},
  pages = {583--598},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.937488},
  abstract = {Dynamic treatment regimes (DTRs) are sequential decision rules for individual patients that can adapt over time to an evolving illness. The goal is to accommodate heterogeneity among patients and find the DTR which will produce the best long term outcome if implemented. We introduce two new statistical learning methods for estimating the optimal DTR, termed backward outcome weighted learning (BOWL), and simultaneous outcome weighted learning (SOWL). These approaches convert individualized treatment selection into an either sequential or simultaneous classification problem, and can thus be applied by modifying existing machine learning techniques. The proposed methods are based on directly maximizing over all DTRs a nonparametric estimator of the expected long-term outcome; this is fundamentally different than regression-based methods, for example Q-learning, which indirectly attempt such maximization and rely heavily on the correctness of postulated regression models. We prove that the resulting rules are consistent, and provide finite sample bounds for the errors using the estimated rules. Simulation results suggest the proposed methods produce superior DTRs compared with Q-learning especially in small samples. We illustrate the methods using data from a clinical trial for smoking cessation.},
  pmid = {26236062},
  keywords = {Classification,Personalized medicine,Q-learning,Reinforcement learning,Risk bound,Support vector machine},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhao et al_2015_New Statistical Learning Methods for Estimating Optimal Dynamic Treatment.pdf}
}

@article{zhaoPromisingSubgroupIdentification2024,
  title = {A Promising Subgroup Identification Method Based on a Genetic Algorithm for Censored Survival Data},
  author = {Zhao, Liang and Zhang, Wenjie and Wu, Ying and Cao, Lei and Wang, Liuying and Li, Kang},
  year = {2024},
  month = jan,
  journal = {Journal of Biopharmaceutical Statistics},
  publisher = {Taylor \& Francis},
  issn = {1054-3406},
  urldate = {2024-06-18},
  abstract = {Modern precision medicine requires drug development to account for patients' heterogeneity, as only a subgroup of the patient population is likely to benefit from the targeted therapy. In this pape...},
  copyright = {{\copyright} 2023 Taylor \& Francis Group, LLC},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhao et al_2024_A promising subgroup identification method based on a genetic algorithm for.pdf;/Users/nseewald/Zotero/storage/YN7IQHNA/10543406.2023.html}
}

@article{zhaoRecentDevelopmentStatistical2013,
  title = {Recent Development on Statistical Methods for Personalized Medicine Discovery},
  author = {Zhao, Yingqi and Zeng, Donglin},
  year = {2013},
  month = mar,
  journal = {Frontiers of Medicine},
  volume = {7},
  number = {1},
  pages = {102--110},
  issn = {2095-0217, 2095-0225},
  doi = {10.1007/s11684-013-0245-7},
  urldate = {2018-10-12},
  abstract = {It is well documented that patients can show significant heterogeneous responses to treatments so the best treatment strategies may require adaptation over individuals and time. Recently, a number of new statistical methods have been developed to tackle the important problem of estimating personalized treatment rules using single-stage or multiple-stage clinical data. In this paper, we provide an overview of these methods and list a number of challenges.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhao_Zeng_2013_Recent development on statistical methods for personalized medicine discovery.pdf}
}

@article{zhaoVersatilityClonecensorweightApproach2021,
  title = {Versatility of the Clone-Censor-Weight Approach: Response to ``Trial Emulation in the Presence of Immortal-Time Bias''},
  shorttitle = {Versatility of the Clone-Censor-Weight Approach},
  author = {Zhao, Sizheng Steven and Lyu, Houchen and Yoshida, Kazuki},
  year = {2021},
  month = apr,
  journal = {International Journal of Epidemiology},
  volume = {50},
  number = {2},
  pages = {694--695},
  issn = {0300-5771},
  doi = {10.1093/ije/dyaa223},
  urldate = {2024-06-25},
  abstract = {We welcome the tutorial by Maringe and colleagues on the clone-censor-weight approach to target trial emulation for observational comparative effectiveness research (CER).1 The authors used a motivating example (effect of surgery within 6\,months of diagnosis on lung cancer survival) to demonstrate its application for immortal time bias. We would like to expand on the versatility of the clone-censor-weight approach by bringing other applications to the readers' attention (see Table~1 for a non-exhaustive overview). In all cases, cloning overcomes assignment ambiguity at follow-up time zero that can occur both in dynamic treatment strategies (that change according to evolving patient characteristics) or static treatment strategies (that do not). We propose three main categories (and combinations thereof) relating to: grace periods, static time-related strategies and dynamic strategies.},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhao_et_al_2021_Versatility_of_the_clone-censor-weight_approach.pdf;/Users/nseewald/Zotero/storage/AUCWTC7Z/6012818.html}
}

@article{zhengPartlyConditionalSurvival2005,
  title = {Partly {{Conditional Survival Models}} for {{Longitudinal Data}}},
  author = {Zheng, Yingye and Heagerty, Patrick J.},
  year = {2005},
  month = jun,
  journal = {Biometrics},
  volume = {61},
  number = {2},
  pages = {379--391},
  issn = {0006-341X},
  doi = {10.1111/j.1541-0420.2005.00323.x},
  urldate = {2024-02-05},
  abstract = {It is common in longitudinal studies to collect information on the time until a key clinical event, such as death, and to measure markers of patient health at multiple follow-up times. One approach to the joint analysis of survival and repeated measures data adopts a time-varying covariate regression model for the event time hazard. Using this standard approach, the instantaneous risk of death at time t is specified as a possibly semiparametric function of covariate information that has accrued through time t. In this manuscript, we decouple the time scale for modeling the hazard from the time scale for accrual of available longitudinal covariate information. Specifically, we propose a class of models that condition on the covariate information through time s and then specifies the conditional hazard for times t, where t \&gt; s. Our approach parallels the ``partly conditional'' models proposed by Pepe and Couper (1997, Journal of the American Statistical Association ~92, 991--998) for pure repeated measures applications. Estimation is based on the use of estimating equations applied to clusters of data formed through the creation of derived survival times that measure the time from measurement of covariates to the end of follow-up. Patient follow-up may be terminated either by the occurrence of the event or by censoring. The proposed methods allow a flexible characterization of the association between a longitudinal covariate process and a survival time, and facilitate the direct prediction of survival probabilities in the time-varying covariate setting.},
  keywords = {_tablet},
  file = {/Users/nseewald/Zotero/storage/WZ2KVUVJ/Zheng_Heagerty_2005_Partly Conditional Survival Models for Longitudinal Data.pdf;/Users/nseewald/Zotero/storage/3MIDHCUV/7306495.html}
}

@article{zhouAdaptiveInterventionsOptimizing2020,
  title = {Adaptive Interventions for Optimizing Malaria Control: An Implementation Study Protocol for a Block-Cluster Randomized, Sequential Multiple Assignment Trial},
  shorttitle = {Adaptive Interventions for Optimizing Malaria Control},
  author = {Zhou, Guofa and Lee, Ming-chieh and Atieli, Harrysone E. and Githure, John I. and Githeko, Andrew K. and Kazura, James W. and Yan, Guiyun},
  year = {2020},
  month = jul,
  journal = {Trials},
  volume = {21},
  number = {1},
  pages = {665},
  issn = {1745-6215},
  doi = {10.1186/s13063-020-04573-y},
  urldate = {2022-08-29},
  abstract = {In the past two decades, the massive scale-up of long-lasting insecticidal nets (LLINs) and indoor residual spraying (IRS) has led to significant reductions in malaria mortality and morbidity. Nonetheless, the malaria burden remains high, and a dozen countries in Africa show a trend of increasing malaria incidence over the past several years. This underscores the need to improve the effectiveness of interventions by optimizing first-line intervention tools and integrating newly approved products into control programs. Because transmission settings and vector ecologies vary from place to place, malaria interventions should be adapted and readapted over time in response to evolving malaria risks. An adaptive approach based on local malaria epidemiology and vector ecology may lead to significant reductions in malaria incidence and transmission risk.},
  keywords = {Active case surveillance,Adaptive intervention,Block-cluster randomized,Clinical malaria incidence rate,Cost-effectiveness,Indoor residual spraying,Larval source management,Long-lasting insecticidal net (LLIN),Piperonyl butoxide-treated LLIN,Q-learning,Sequential multiple assignment randomized trial},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhou et al_2020_Adaptive interventions for optimizing malaria control.pdf;/Users/nseewald/Zotero/storage/FYE42JZ5/s13063-020-04573-y.html}
}

@article{zhouPropensityScoreWeighting2020,
  title = {Propensity Score Weighting under Limited Overlap and Model Misspecification},
  author = {Zhou, Yunji and Matsouaka, Roland A and Thomas, Laine},
  year = {2020},
  month = dec,
  journal = {Statistical Methods in Medical Research},
  volume = {29},
  number = {12},
  pages = {3721--3756},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280220940334},
  urldate = {2024-09-16},
  abstract = {Propensity score weighting methods are often used in non-randomized studies to adjust for confounding and assess treatment effects. The most popular among them, the inverse probability weighting, assigns weights that are proportional to the inverse of the conditional probability of a specific treatment assignment, given observed covariates. A key requirement for inverse probability weighting estimation is the positivity assumption, i.e. the propensity score must be bounded away from 0 and 1. In practice, violations of the positivity assumption often manifest by the presence of limited overlap in the propensity score distributions between treatment groups. When these practical violations occur, a small number of highly influential inverse probability weights may lead to unstable inverse probability weighting estimators, with biased estimates and large variances. To mitigate these issues, a number of alternative methods have been proposed, including inverse probability weighting trimming, overlap weights, matching weights, and entropy weights. Because overlap weights, matching weights, and entropy weights target the population for whom there is equipoise (and with adequate overlap) and their estimands depend on the true propensity score, a common criticism is that these estimators may be more sensitive to misspecifications of the propensity score model. In this paper, we conduct extensive simulation studies to compare the performances of inverse probability weighting and inverse probability weighting trimming against those of overlap weights, matching weights, and entropy weights under limited overlap and misspecified propensity score models. Across the wide range of scenarios we considered, overlap weights, matching weights, and entropy weights consistently outperform inverse probability weighting in terms of bias, root mean squared error, and coverage probability.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zhou_et_al_2020_Propensity_score_weighting_under_limited_overlap_and_model_misspecification.pdf}
}

@article{zieglerFamilialAssociationsLipid2000,
  title = {Familial Associations of Lipid Profiles: A Generalized Estimating Equations Approach},
  shorttitle = {Familial Associations of Lipid Profiles},
  author = {Ziegler, Andreas and Kastner, Christian and Brunner, Daniel and Blettner, Maria},
  year = {2000},
  month = dec,
  journal = {Statistics in Medicine},
  volume = {19},
  number = {24},
  pages = {3345--3357},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/1097-0258(20001230)19:24<3345::AID-SIM829>3.0.CO;2-5},
  urldate = {2018-10-12},
  abstract = {Elevated plasma levels of apolipoproteins A1 (apoA1) and B (apoB) are important protective factors and risk factors, respectively, for atherosclerosis and coronary heart disease. It is well known that both apoA1 and apoB reveal strong familial aggregation. Our goal was to investigate whether exogenous variables in uence these associations. We used marginal regression models for the mean and association structure (generalized estimating equations 2; GEE2) to analyse data from 1435 family members within 469 families of di erent sizes included in the Donolo-Tel Aviv Three-Generation O spring Study. The usual robust variance matrix was approximated by extensions of jack-knife estimators of variance to GEE2 models. Estimation of standard errors in models with quite complex correlation structures was possible using this approach. All analyses were easily carried out using a menu-driven stand-alone software tool for marginal regression modelling. We demonstrate that a variety of hypotheses can be tested using Wald statistics by modelling regression matrices for the association structure. We show that correlation for apoB between parent-o spring pairs increased with decreasing age di erence and that pairs with individuals of the same gender had more similar apoA1 levels than individuals of di erent gender. Associations between di erent relative pairs did not all agree with those expected from di erences in kinship coe cients. The analysis using GEE2 models revealed structures that would not have been detected by other models and should therefore be used in addition to traditional approaches of analysing family data. GEE2 should be considered a standard method for the investigation of familial aggregation. Copyright ? 2000 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ziegler et al_2000_Familial associations of lipid profiles.pdf}
}

@article{zieglerGeneralisedEstimatingEquations1998,
  title = {The {{Generalised Estimating Equations}}: {{An Annotated Bibliography}}},
  shorttitle = {The {{Generalised Estimating Equations}}},
  author = {Ziegler, Andreas and Kastner, Christian and Blettner, Maria},
  year = {1998},
  month = jun,
  journal = {Biometrical Journal},
  volume = {40},
  number = {2},
  pages = {115--139},
  issn = {0323-3847, 1521-4036},
  doi = {10.1002/(SICI)1521-4036(199806)40:2<115::AID-BIMJ115>3.0.CO;2-6},
  urldate = {2018-10-12},
  abstract = {The Generalised Estimating Equations (GEE) proposed by Liang and Zeger (1986) and Zeger and Liang (1986) have found considerable attention in the last ten years and several extensions have been proposed. In this annotated bibliography we describe the development of the GEE and its extensions during the last decade. Additionally, we discuss advantages and disadvantages of the different parametrisations that have been proposed in the literature. Furthermore, we review regression diagnostic techniques and approaches for dealing with missing data. We give an insight to the different fields of application in biometry. We also describe the software available for the GEE.},
  langid = {english},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Ziegler et al_1998_The Generalised Estimating Equations.pdf}
}

@article{zienowiczHerniaPreventionAesthetic1995,
  title = {Hernia Prevention and Aesthetic Contouring of the Abdomen Following {{TRAM}} Flap Breast Reconstruction by the Use of Polypropylene Mesh},
  author = {Zienowicz, R. J. and May, J. W.},
  year = {1995},
  month = nov,
  journal = {Plastic and Reconstructive Surgery},
  volume = {96},
  number = {6},
  pages = {1346--1350},
  issn = {0032-1052},
  doi = {10.1097/00006534-199511000-00017},
  abstract = {The value of synthetic mesh use in the treatment of recurrent abdominal hernias is well recognized and has led to its advocacy by some authors as an adjunct in primary hernia repair. Mesh use in the donor-site closure associated with TRAM flap reconstruction is typically restricted to situations where undue tension or questionable tissue integrity may be predisposing factors to herniation. Although more liberal use of mesh has been advocated for these circumstances, fear of mesh complications may continue to restrict its use. We present a series of 65 consecutive patients who had routine mesh application to fascial closures following TRAM flap breast reconstruction. The use of mesh provides an added margin of strength to fascial reconstruction and was found to have additional benefit as a technical adjunct to the aesthetic aspects of the abdominoplasty. Mean patient follow-up was 56.4 months. The resulting rates of hernia (1.5 percent) and mesh-related infection (1.5 percent) demonstrate its considerable safety. We recommend consideration of polypropylene mesh use for improved strength and aesthetic quality of the donor-site closure following TRAM flap breast reconstruction.},
  langid = {english},
  pmid = {7480232},
  keywords = {Abdomen,Abdominal Muscles,Adult,Female,Hernia Ventral,Humans,Mammaplasty,Middle Aged,Postoperative Complications,Surgical Flaps,Surgical Mesh}
}

@article{zinkFairRegressionHealth2020,
  title = {Fair Regression for Health Care Spending},
  author = {Zink, Anna and Rose, Sherri},
  year = {2020},
  month = jan,
  journal = {Biometrics},
  issn = {0006-341X},
  doi = {10/ggztkx},
  urldate = {2020-06-09},
  annotation = {Saved from BrowZine: http://thirdiron.com/download},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zink_Rose_2020_Fair regression for health care spending.pdf}
}

@misc{zotero-2484,
  type = {Misc}
}

@inproceedings{zotero-3023,
  type = {Inproceedings}
}

@article{zubizarretaMatchingBalancePairing2014,
  title = {Matching for Balance, Pairing for Heterogeneity in an Observational Study of the Effectiveness of for-Profit and Not-for-Profit High Schools in {{Chile}}},
  author = {Zubizarreta, Jos{\'e} R. and Paredes, Ricardo D. and Rosenbaum, Paul R.},
  year = {2014},
  month = mar,
  journal = {The Annals of Applied Statistics},
  volume = {8},
  number = {1},
  pages = {204--231},
  publisher = {Institute of Mathematical Statistics},
  issn = {1932-6157, 1941-7330},
  doi = {10/gg2n8t},
  urldate = {2021-07-20},
  abstract = {Conventionally, the construction of a pair-matched sample selects treated and control units and pairs them in a single step with a view to balancing observed covariates \${\textbackslash}mathbf\{x\}\$ and reducing the heterogeneity or dispersion of treated-minus-control response differences, \$Y\$. In contrast, the method of cardinality matching developed here first selects the maximum number of units subject to covariate balance constraints and, with a balanced sample for \${\textbackslash}mathbf\{x\}\$ in hand, then separately pairs the units to minimize heterogeneity in \$Y\$. Reduced heterogeneity of pair differences in responses \$Y\$ is known to reduce sensitivity to unmeasured biases, so one might hope that cardinality matching would succeed at both tasks, balancing \${\textbackslash}mathbf\{x\}\$, stabilizing \$Y\$. We use cardinality matching in an observational study of the effectiveness of for-profit and not-for-profit private high schools in Chile---a controversial subject in Chile---focusing on students who were in government run primary schools in 2004 but then switched to private high schools. By pairing to minimize heterogeneity in a cardinality match that has balanced covariates, a meaningful reduction in sensitivity to unmeasured biases is obtained.},
  keywords = {design sensitivity,integer programming,testing twice},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zubizarreta et al_2014_Matching for balance, pairing for heterogeneity in an observational study of.pdf;/Users/nseewald/Zotero/storage/LT58LG2J/13-AOAS713.html}
}

@article{zuoImplementationTargetTrial2023,
  title = {The Implementation of Target Trial Emulation for Causal Inference: A~Scoping Review},
  shorttitle = {The Implementation of Target Trial Emulation for Causal Inference},
  author = {Zuo, Hanxiao and Yu, Lin and Campbell, Sandra M. and Yamamoto, Shelby S. and Yuan, Yan},
  year = {2023},
  month = oct,
  journal = {Journal of Clinical Epidemiology},
  volume = {162},
  pages = {29--37},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2023.08.003},
  urldate = {2024-01-17},
  abstract = {Objectives We aim to investigate the implementation of Target Trial Emulation (TTE) for causal inference, involving research topics, frequently used strategies, and issues indicating the need for future improvements. Study Design and Setting We performed a scoping review by following the Joanna Briggs Institute (JBI) guidance and Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR) checklist. A health research--focused librarian searched multiple medical databases, and two independent reviewers completed screening and extraction within covidence review management software. Results Our search resulted in 1,240 papers, of which 96 papers were eligible for data extraction. Results show a significant increase in the use of TTE in 2018 and 2021. The study topics varied and focused primarily on cancer, cardiovascular and cerebrovascular diseases, and infectious diseases. However, not all papers specified well all three critical components for generating robust causal evidence: time-zero, random assignment simulation, and comparison strategy. Some common issues were observed from retrieved papers, and key limitations include residual confounding, limited generalizability, and a lack of reporting guidance that need to be improved. Conclusion Uneven adherence to the TTE framework exists, and future improvements are needed to progress applications using causal inference with observational data.},
  keywords = {Causal inference,Methodology,Observational data,Residual confounding,Scoping review,Target trial emulation},
  file = {/Users/nseewald/Library/CloudStorage/GoogleDrive-seewaldn@pennmedicine.upenn.edu/My Drive/Zotero/Zuo_et_al_2023_The_implementation_of_target_trial_emulation_for_causal_inference.pdf;/Users/nseewald/Zotero/storage/626IJ39Z/S0895435623002032.html}
}
